<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>LLM-agent - 2025-03-12</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>LLM-agent - 2025-03-12</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08683v1' target='_blank'>CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous
  Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Changxing Liu, Genjia Liu, Zijun Wang, Jinchang Yang, Siheng Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 17:58:42</h6>
<p class='card-text'>Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise
for improving safety by addressing the perception and prediction uncertainties
inherent in single-agent systems. However, traditional cooperative methods are
constrained by rigid collaboration protocols and limited generalization to
unseen interactive scenarios. While LLM-based approaches offer generalized
reasoning capabilities, their challenges in spatial planning and unstable
inference latency hinder their direct application in cooperative driving. To
address these limitations, we propose CoLMDriver, the first full-pipeline
LLM-based cooperative driving system, enabling effective language-based
negotiation and real-time driving control. CoLMDriver features a parallel
driving pipeline with two key components: (i) an LLM-based negotiation module
under an actor-critic paradigm, which continuously refines cooperation policies
through feedback from previous decisions of all vehicles; and (ii) an
intention-guided waypoint generator, which translates negotiation outcomes into
executable waypoints. Additionally, we introduce InterDrive, a CARLA-based
simulation benchmark comprising 10 challenging interactive driving scenarios
for evaluating V2V cooperation. Experimental results demonstrate that
CoLMDriver significantly outperforms existing approaches, achieving an 11%
higher success rate across diverse highly interactive V2V driving scenarios.
Code will be released on https://github.com/cxliu0314/CoLMDriver.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08604v1' target='_blank'>EMMOE: A Comprehensive Benchmark for Embodied Mobile Manipulation in
  Open Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dongping Li, Tielong Cai, Tianci Tang, Wenhao Chai, Katherine Rose Driggs-Campbell, Gaoang Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 16:42:36</h6>
<p class='card-text'>Developing autonomous home robots controlled by natural language has long
been a pursuit of human. While advancements in large language models (LLMs) and
embodied intelligence make this goal closer, several challenges persist: the
lack of a unified benchmark for more complex robot tasks, limited evaluation
methods and metrics, data incompatibility between LLMs and mobile manipulation
trajectories. To address these issues, we introduce Embodied Mobile
Manipulation in Open Environments (EMMOE), which requires agents to interpret
user instructions and execute long-horizon everyday tasks in continuous space.
EMMOE seamlessly integrates high-level and low-level embodied tasks into a
unified framework, along with three new metrics for more diverse assessment.
Additionally, we collect EMMOE-100, which features in various task attributes,
detailed process annotations, re-plans after failures, and two sub-datasets for
LLM training. Furthermore, we design HomieBot, a sophisticated agent system
consists of LLM with Direct Preference Optimization (DPO), light weighted
navigation and manipulation models, and multiple error detection mechanisms.
Finally, we demonstrate HomieBot's performance and the evaluation of different
models and policies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08525v1' target='_blank'>GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based
  VLM Agent Training</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tong Wei, Yijun Yang, Junliang Xing, Yuanchun Shi, Zongqing Lu, Deheng Ye</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 15:17:02</h6>
<p class='card-text'>Reinforcement learning with verifiable outcome rewards (RLVR) has effectively
scaled up chain-of-thought (CoT) reasoning in large language models (LLMs).
Yet, its efficacy in training vision-language model (VLM) agents for
goal-directed action reasoning in visual environments is less established. This
work investigates this problem through extensive experiments on complex card
games, such as 24 points, and embodied tasks from ALFWorld. We find that when
rewards are based solely on action outcomes, RL fails to incentivize CoT
reasoning in VLMs, instead leading to a phenomenon we termed thought collapse,
characterized by a rapid loss of diversity in the agent's thoughts,
state-irrelevant and incomplete reasoning, and subsequent invalid actions,
resulting in negative rewards. To counteract thought collapse, we highlight the
necessity of process guidance and propose an automated corrector that evaluates
and refines the agent's reasoning at each RL step. This simple and scalable GTR
(Guided Thought Reinforcement) framework trains reasoning and action
simultaneously without the need for dense, per-step human labeling. Our
experiments demonstrate that GTR significantly enhances the performance and
generalization of the LLaVA-7b model across various visual environments,
achieving 3-5 times higher task success rates compared to SoTA models with
notably smaller model sizes.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08506v1' target='_blank'>ReviewAgents: Bridging the Gap Between Human and AI-Generated Paper
  Reviews</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xian Gao, Jiacheng Ruan, Jingsheng Gao, Ting Liu, Yuzhuo Fu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 14:56:58</h6>
<p class='card-text'>Academic paper review is a critical yet time-consuming task within the
research community. With the increasing volume of academic publications,
automating the review process has become a significant challenge. The primary
issue lies in generating comprehensive, accurate, and reasoning-consistent
review comments that align with human reviewers' judgments. In this paper, we
address this challenge by proposing ReviewAgents, a framework that leverages
large language models (LLMs) to generate academic paper reviews. We first
introduce a novel dataset, Review-CoT, consisting of 142k review comments,
designed for training LLM agents. This dataset emulates the structured
reasoning process of human reviewers-summarizing the paper, referencing
relevant works, identifying strengths and weaknesses, and generating a review
conclusion. Building upon this, we train LLM reviewer agents capable of
structured reasoning using a relevant-paper-aware training method. Furthermore,
we construct ReviewAgents, a multi-role, multi-LLM agent review framework, to
enhance the review comment generation process. Additionally, we propose
ReviewBench, a benchmark for evaluating the review comments generated by LLMs.
Our experimental results on ReviewBench demonstrate that while existing LLMs
exhibit a certain degree of potential for automating the review process, there
remains a gap when compared to human-generated reviews. Moreover, our
ReviewAgents framework further narrows this gap, outperforming advanced LLMs in
generating review comments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08308v1' target='_blank'>Seeing and Reasoning with Confidence: Supercharging Multimodal LLMs with
  an Uncertainty-Aware Agentic Framework</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhuo Zhi, Chen Feng, Adam Daneshmend, Mine Orlu, Andreas Demosthenous, Lu Yin, Da Li, Ziquan Liu, Miguel R. D. Rodrigues</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 11:18:53</h6>
<p class='card-text'>Multimodal large language models (MLLMs) show promise in tasks like visual
question answering (VQA) but still face challenges in multimodal reasoning.
Recent works adapt agentic frameworks or chain-of-thought (CoT) reasoning to
improve performance. However, CoT-based multimodal reasoning often demands
costly data annotation and fine-tuning, while agentic approaches relying on
external tools risk introducing unreliable output from these tools. In this
paper, we propose Seeing and Reasoning with Confidence (SRICE), a training-free
multimodal reasoning framework that integrates external vision models with
uncertainty quantification (UQ) into an MLLM to address these challenges.
Specifically, SRICE guides the inference process by allowing MLLM to
autonomously select regions of interest through multi-stage interactions with
the help of external tools. We propose to use a conformal prediction-based
approach to calibrate the output of external tools and select the optimal tool
by estimating the uncertainty of an MLLM's output. Our experiment shows that
the average improvement of SRICE over the base MLLM is 4.6% on five datasets
and the performance on some datasets even outperforms fine-tuning-based
methods, revealing the significance of ensuring reliable tool use in an MLLM
agent.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08302v1' target='_blank'>General-Purpose Aerial Intelligent Agents Empowered by Large Language
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ji Zhao, Xiao Lin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 11:13:58</h6>
<p class='card-text'>The emergence of large language models (LLMs) opens new frontiers for
unmanned aerial vehicle (UAVs), yet existing systems remain confined to
predefined tasks due to hardware-software co-design challenges. This paper
presents the first aerial intelligent agent capable of open-world task
execution through tight integration of LLM-based reasoning and robotic
autonomy. Our hardware-software co-designed system addresses two fundamental
limitations: (1) Onboard LLM operation via an edge-optimized computing
platform, achieving 5-6 tokens/sec inference for 14B-parameter models at 220W
peak power; (2) A bidirectional cognitive architecture that synergizes slow
deliberative planning (LLM task planning) with fast reactive control (state
estimation, mapping, obstacle avoidance, and motion planning). Validated
through preliminary results using our prototype, the system demonstrates
reliable task planning and scene understanding in communication-constrained
environments, such as sugarcane monitoring, power grid inspection, mine tunnel
exploration, and biological observation applications. This work establishes a
novel framework for embodied aerial artificial intelligence, bridging the gap
between task planning and robotic autonomy in open environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08199v1' target='_blank'>A Cascading Cooperative Multi-agent Framework for On-ramp Merging
  Control Integrating Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Miao Zhang, Zhenlong Fang, Tianyi Wang, Qian Zhang, Shuai Lu, Junfeng Jiao, Tianyu Shi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 09:08:04</h6>
<p class='card-text'>Traditional Reinforcement Learning (RL) suffers from replicating human-like
behaviors, generalizing effectively in multi-agent scenarios, and overcoming
inherent interpretability issues.These tasks are compounded when deep
environment understanding, agent coordination and dynamic optimization are
required. While Large Language Model (LLM) enhanced methods have shown promise
in generalization and interoperability, they often neglect necessary
multi-agent coordination. Therefore, we introduce the Cascading Cooperative
Multi-agent (CCMA) framework, integrating RL for individual interactions, a
fine-tuned LLM for regional cooperation, a reward function for global
optimization, and the Retrieval-augmented Generation mechanism to dynamically
optimize decision-making across complex driving scenarios. Our experiments
demonstrate that the CCMA outperforms existing RL methods, demonstrating
significant improvements in both micro and macro-level performance in complex
driving environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08193v1' target='_blank'>Guess What I am Thinking: A Benchmark for Inner Thought Reasoning of
  Role-Playing Language Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rui Xu, MingYu Wang, XinTao Wang, Dakuan Lu, Xiaoyu Tan, Wei Chu, Yinghui Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 08:57:07</h6>
<p class='card-text'>Recent advances in LLM-based role-playing language agents (RPLAs) have
attracted broad attention in various applications. While chain-of-thought
reasoning has shown importance in many tasks for LLMs, the internal thinking
processes of RPLAs remain unexplored. Understanding characters' inner thoughts
is crucial for developing advanced RPLAs. In this paper, we introduce
ROLETHINK, a novel benchmark constructed from literature for evaluating
character thought generation. We propose the task of inner thought reasoning,
which includes two sets: the gold set that compares generated thoughts with
original character monologues, and the silver set that uses expert synthesized
character analyses as references. To address this challenge, we propose MIRROR,
a chain-of-thought approach that generates character thoughts by retrieving
memories, predicting character reactions, and synthesizing motivations. Through
extensive experiments, we demonstrate the importance of inner thought reasoning
for RPLAs, and MIRROR consistently outperforms existing methods. Resources are
available at https://github.com/airaer1998/RPA_Thought.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08175v1' target='_blank'>Privacy-Enhancing Paradigms within Federated Multi-Agent Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zitong Shi, Guancheng Wan, Wenke Huang, Guibin Zhang, Jiawei Shao, Mang Ye, Carl Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 08:38:45</h6>
<p class='card-text'>LLM-based Multi-Agent Systems (MAS) have proven highly effective in solving
complex problems by integrating multiple agents, each performing different
roles. However, in sensitive domains, they face emerging privacy protection
challenges. In this paper, we introduce the concept of Federated MAS,
highlighting the fundamental differences between Federated MAS and traditional
FL. We then identify key challenges in developing Federated MAS, including: 1)
heterogeneous privacy protocols among agents, 2) structural differences in
multi-party conversations, and 3) dynamic conversational network structures. To
address these challenges, we propose Embedded Privacy-Enhancing Agents
(EPEAgent), an innovative solution that integrates seamlessly into the
Retrieval-Augmented Generation (RAG) phase and the context retrieval stage.
This solution minimizes data flows, ensuring that only task-relevant,
agent-specific information is shared. Additionally, we design and generate a
comprehensive dataset to evaluate the proposed paradigm. Extensive experiments
demonstrate that EPEAgent effectively enhances privacy protection while
maintaining strong system performance. The code will be availiable at
https://github.com/ZitongShi/EPEAgent</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08147v1' target='_blank'>FilmComposer: LLM-Driven Music Production for Silent Film Clips</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhifeng Xie, Qile He, Youjia Zhu, Qiwei He, Mengtian Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 08:05:11</h6>
<p class='card-text'>In this work, we implement music production for silent film clips using
LLM-driven method. Given the strong professional demands of film music
production, we propose the FilmComposer, simulating the actual workflows of
professional musicians. FilmComposer is the first to combine large generative
models with a multi-agent approach, leveraging the advantages of both waveform
music and symbolic music generation. Additionally, FilmComposer is the first to
focus on the three core elements of music production for film-audio quality,
musicality, and musical development-and introduces various controls, such as
rhythm, semantics, and visuals, to enhance these key aspects. Specifically,
FilmComposer consists of the visual processing module, rhythm-controllable
MusicGen, and multi-agent assessment, arrangement and mix. In addition, our
framework can seamlessly integrate into the actual music production pipeline
and allows user intervention in every step, providing strong interactivity and
a high degree of creative freedom. Furthermore, we propose MusicPro-7k which
includes 7,418 film clips, music, description, rhythm spots and main melody,
considering the lack of a professional and high-quality film music dataset.
Finally, both the standard metrics and the new specialized metrics we propose
demonstrate that the music generated by our model achieves state-of-the-art
performance in terms of quality, consistency with video, diversity, musicality,
and musical development. Project page:
https://apple-jun.github.io/FilmComposer.github.io/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08123v1' target='_blank'>LLM4MAC: An LLM-Driven Reinforcement Learning Framework for MAC Protocol
  Emergence</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Renxuan Tan, Rongpeng Li, Zhifeng Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 07:38:14</h6>
<p class='card-text'>With the advent of 6G systems, emerging hyper-connected ecosystems
necessitate agile and adaptive medium access control (MAC) protocols to contend
with network dynamics and diverse service requirements. We propose LLM4MAC, a
novel framework that harnesses large language models (LLMs) within a
reinforcement learning paradigm to drive MAC protocol emergence. By
reformulating uplink data transmission scheduling as a semantics-generalized
partially observable Markov game (POMG), LLM4MAC encodes network operations in
natural language, while proximal policy optimization (PPO) ensures continuous
alignment with the evolving network dynamics. A structured identity embedding
(SIE) mechanism further enables robust coordination among heterogeneous agents.
Extensive simulations demonstrate that on top of a compact LLM, which is
purposefully selected to balance performance with resource efficiency, the
protocol emerging from LLM4MAC outperforms comparative baselines in throughput
and generalization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08102v1' target='_blank'>AI-native Memory 2.0: Second Me</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiale Wei, Xiang Ying, Tao Gao, Felix Tao, Jingbo Shang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 07:05:52</h6>
<p class='card-text'>Human interaction with the external world fundamentally involves the exchange
of personal memory, whether with other individuals, websites, applications, or,
in the future, AI agents. A significant portion of this interaction is
redundant, requiring users to repeatedly provide the same information across
different contexts. Existing solutions, such as browser-stored credentials,
autofill mechanisms, and unified authentication systems, have aimed to mitigate
this redundancy by serving as intermediaries that store and retrieve commonly
used user data. The advent of large language models (LLMs) presents an
opportunity to redefine memory management through an AI-native paradigm: SECOND
ME. SECOND ME acts as an intelligent, persistent memory offload system that
retains, organizes, and dynamically utilizes user-specific knowledge. By
serving as an intermediary in user interactions, it can autonomously generate
context-aware responses, prefill required information, and facilitate seamless
communication with external systems, significantly reducing cognitive load and
interaction friction. Unlike traditional memory storage solutions, SECOND ME
extends beyond static data retention by leveraging LLM-based memory
parameterization. This enables structured organization, contextual reasoning,
and adaptive knowledge retrieval, facilitating a more systematic and
intelligent approach to memory management. As AI-driven personal agents like
SECOND ME become increasingly integrated into digital ecosystems, SECOND ME
further represents a critical step toward augmenting human-world interaction
with persistent, contextually aware, and self-optimizing memory systems. We
have open-sourced the fully localizable deployment system at GitHub:
https://github.com/Mindverse/Second-Me.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08026v1' target='_blank'>In Prospect and Retrospect: Reflective Memory Management for Long-term
  Personalized Dialogue Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhen Tan, Jun Yan, I-Hung Hsu, Rujun Han, Zifeng Wang, Long T. Le, Yiwen Song, Yanfei Chen, Hamid Palangi, George Lee, Anand Iyer, Tianlong Chen, Huan Liu, Chen-Yu Lee, Tomas Pfister</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 04:15:52</h6>
<p class='card-text'>Large Language Models (LLMs) have made significant progress in open-ended
dialogue, yet their inability to retain and retrieve relevant information from
long-term interactions limits their effectiveness in applications requiring
sustained personalization. External memory mechanisms have been proposed to
address this limitation, enabling LLMs to maintain conversational continuity.
However, existing approaches struggle with two key challenges. First, rigid
memory granularity fails to capture the natural semantic structure of
conversations, leading to fragmented and incomplete representations. Second,
fixed retrieval mechanisms cannot adapt to diverse dialogue contexts and user
interaction patterns. In this work, we propose Reflective Memory Management
(RMM), a novel mechanism for long-term dialogue agents, integrating forward-
and backward-looking reflections: (1) Prospective Reflection, which dynamically
summarizes interactions across granularities-utterances, turns, and
sessions-into a personalized memory bank for effective future retrieval, and
(2) Retrospective Reflection, which iteratively refines the retrieval in an
online reinforcement learning (RL) manner based on LLMs' cited evidence.
Experiments show that RMM demonstrates consistent improvement across various
metrics and benchmarks. For example, RMM shows more than 10% accuracy
improvement over the baseline without memory management on the LongMemEval
dataset.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07826v1' target='_blank'>Magnet: Multi-turn Tool-use Data Synthesis and Distillation via Graph
  Translation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fan Yin, Zifeng Wang, I-Hung Hsu, Jun Yan, Ke Jiang, Yanfei Chen, Jindong Gu, Long T. Le, Kai-Wei Chang, Chen-Yu Lee, Hamid Palangi, Tomas Pfister</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 20:13:07</h6>
<p class='card-text'>Large language models (LLMs) have exhibited the ability to effectively
utilize external tools to address user queries. However, their performance may
be limited in complex, multi-turn interactions involving users and multiple
tools. To address this, we propose Magnet, a principled framework for
synthesizing high-quality training trajectories to enhance the function calling
capability of large language model agents in multi-turn conversations with
humans. The framework is based on automatic and iterative translations from a
function signature path to a sequence of queries and executable function calls.
We model the complicated function interactions in multi-turn cases with graph
and design novel node operations to build reliable signature paths. Motivated
by context distillation, when guiding the generation of positive and negative
trajectories using a teacher model, we provide reference function call
sequences as positive hints in context and contrastive, incorrect function
calls as negative hints. Experiments show that training with the positive
trajectories with supervised fine-tuning and preference optimization against
negative trajectories, our 14B model, Magnet-14B-mDPO, obtains 68.01 on BFCL-v3
and 73.30 on ToolQuery, surpassing the performance of the teacher model
Gemini-1.5-pro-002 by a large margin in function calling.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07693v1' target='_blank'>Fully Autonomous Programming using Iterative Multi-Agent Debugging with
  Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Anastasiia Grishina, Vadim Liventsev, Aki Härmä, Leon Moonen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 16:56:51</h6>
<p class='card-text'>Program synthesis with Large Language Models (LLMs) suffers from a "near-miss
syndrome": the generated code closely resembles a correct solution but fails
unit tests due to minor errors. We address this with a multi-agent framework
called Synthesize, Execute, Instruct, Debug, and Repair (SEIDR). Effectively
applying SEIDR to instruction-tuned LLMs requires determining (a) optimal
prompts for LLMs, (b) what ranking algorithm selects the best programs in
debugging rounds, and (c) balancing the repair of unsuccessful programs with
the generation of new ones. We empirically explore these trade-offs by
comparing replace-focused, repair-focused, and hybrid debug strategies. We also
evaluate lexicase and tournament selection to rank candidates in each
generation. On Program Synthesis Benchmark 2 (PSB2), our framework outperforms
both conventional use of OpenAI Codex without a repair phase and traditional
genetic programming approaches. SEIDR outperforms the use of an LLM alone,
solving 18 problems in C++ and 20 in Python on PSB2 at least once across
experiments. To assess generalizability, we employ GPT-3.5 and Llama 3 on the
PSB2 and HumanEval-X benchmarks. Although SEIDR with these models does not
surpass current state-of-the-art methods on the Python benchmarks, the results
on HumanEval-C++ are promising. SEIDR with Llama 3-8B achieves an average
pass@100 of 84.2%. Across all SEIDR runs, 163 of 164 problems are solved at
least once with GPT-3.5 in HumanEval-C++, and 162 of 164 with the smaller Llama
3-8B. We conclude that SEIDR effectively overcomes the near-miss syndrome in
program synthesis with LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07459v1' target='_blank'>MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for
  Complex Medical Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiangru Tang, Daniel Shao, Jiwoong Sohn, Jiapeng Chen, Jiayi Zhang, Jinyu Xiang, Fang Wu, Yilun Zhao, Chenglin Wu, Wenqi Shi, Arman Cohan, Mark Gerstein</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 15:38:44</h6>
<p class='card-text'>Large Language Models (LLMs) have shown impressive performance on existing
medical question-answering benchmarks. This high performance makes it
increasingly difficult to meaningfully evaluate and differentiate advanced
methods. We present MedAgentsBench, a benchmark that focuses on challenging
medical questions requiring multi-step clinical reasoning, diagnosis
formulation, and treatment planning-scenarios where current models still
struggle despite their strong performance on standard tests. Drawing from seven
established medical datasets, our benchmark addresses three key limitations in
existing evaluations: (1) the prevalence of straightforward questions where
even base models achieve high performance, (2) inconsistent sampling and
evaluation protocols across studies, and (3) lack of systematic analysis of the
interplay between performance, cost, and inference time. Through experiments
with various base models and reasoning methods, we demonstrate that the latest
thinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in
complex medical reasoning tasks. Additionally, advanced search-based agent
methods offer promising performance-to-cost ratios compared to traditional
approaches. Our analysis reveals substantial performance gaps between model
families on complex questions and identifies optimal model selections for
different computational constraints. Our benchmark and evaluation framework are
publicly available at https://github.com/gersteinlab/medagents-benchmark.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07457v1' target='_blank'>LLMs syntactically adapt their language use to their conversational
  partner</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Florian Kandra, Vera Demberg, Alexander Koller</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 15:37:07</h6>
<p class='card-text'>It has been frequently observed that human speakers align their language use
with each other during conversations. In this paper, we study empirically
whether large language models (LLMs) exhibit the same behavior of
conversational adaptation. We construct a corpus of conversations between LLMs
and find that two LLM agents end up making more similar syntactic choices as
conversations go on, confirming that modern LLMs adapt their language use to
their conversational partners in at least a rudimentary way.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07323v1' target='_blank'>Dynamic Path Navigation for Motion Agents with LLM Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yubo Zhao, Qi Wu, Yifan Wang, Yu-Wing Tai, Chi-Keung Tang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 13:39:09</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated strong generalizable reasoning
and planning capabilities. However, their efficacies in spatial path planning
and obstacle-free trajectory generation remain underexplored. Leveraging LLMs
for navigation holds significant potential, given LLMs' ability to handle
unseen scenarios, support user-agent interactions, and provide global control
across complex systems, making them well-suited for agentic planning and
humanoid motion generation. As one of the first studies in this domain, we
explore the zero-shot navigation and path generation capabilities of LLMs by
constructing a dataset and proposing an evaluation protocol. Specifically, we
represent paths using anchor points connected by straight lines, enabling
movement in various directions. This approach offers greater flexibility and
practicality compared to previous methods while remaining simple and intuitive
for LLMs. We demonstrate that, when tasks are well-structured in this manner,
modern LLMs exhibit substantial planning proficiency in avoiding obstacles
while autonomously refining navigation with the generated motion to reach the
target. Further, this spatial reasoning ability of a single LLM motion agent
interacting in a static environment can be seamlessly generalized in
multi-motion agents coordination in dynamic environments. Unlike traditional
approaches that rely on single-step planning or local policies, our
training-free LLM-based method enables global, dynamic, closed-loop planning,
and autonomously resolving collision issues.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07320v1' target='_blank'>Experimental Exploration: Investigating Cooperative Interaction Behavior
  Between Humans and Large Language Model Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guanxuan Jiang, Yuyang Wang, Pan Hui</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 13:37:36</h6>
<p class='card-text'>With the rise of large language models (LLMs), AI agents as autonomous
decision-makers present significant opportunities and challenges for human-AI
cooperation. While many studies have explored human cooperation with AI as
tools, the role of LLM-augmented autonomous agents in competitive-cooperative
interactions remains under-examined. This study investigates human cooperative
behavior by engaging 30 participants who interacted with LLM agents exhibiting
different characteristics (purported human, purported rule-based AI agent, and
LLM agent) in repeated Prisoner's Dilemma games. Findings show significant
differences in cooperative behavior based on the agents' purported
characteristics and the interaction effect of participants' genders and
purported characteristics. We also analyzed human response patterns, including
game completion time, proactive favorable behavior, and acceptance of repair
efforts. These insights offer a new perspective on human interactions with LLM
agents in competitive cooperation contexts, such as virtual avatars or future
physical entities. The study underscores the importance of understanding human
biases toward AI agents and how observed behaviors can influence future
human-AI cooperation dynamics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07314v1' target='_blank'>Automated Movie Generation via Multi-Agent CoT Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weijia Wu, Zeyu Zhu, Mike Zheng Shou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 13:33:27</h6>
<p class='card-text'>Existing long-form video generation frameworks lack automated planning,
requiring manual input for storylines, scenes, cinematography, and character
interactions, resulting in high costs and inefficiencies. To address these
challenges, we present MovieAgent, an automated movie generation via
multi-agent Chain of Thought (CoT) planning. MovieAgent offers two key
advantages: 1) We firstly explore and define the paradigm of automated
movie/long-video generation. Given a script and character bank, our MovieAgent
can generates multi-scene, multi-shot long-form videos with a coherent
narrative, while ensuring character consistency, synchronized subtitles, and
stable audio throughout the film. 2) MovieAgent introduces a hierarchical
CoT-based reasoning process to automatically structure scenes, camera settings,
and cinematography, significantly reducing human effort. By employing multiple
LLM agents to simulate the roles of a director, screenwriter, storyboard
artist, and location manager, MovieAgent streamlines the production pipeline.
Experiments demonstrate that MovieAgent achieves new state-of-the-art results
in script faithfulness, character consistency, and narrative coherence. Our
hierarchical framework takes a step forward and provides new insights into
fully automated movie generation. The code and project website are available
at: https://github.com/showlab/MovieAgent and
https://weijiawu.github.io/MovieAgent.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07044v1' target='_blank'>DatawiseAgent: A Notebook-Centric LLM Agent Framework for Automated Data
  Science</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziming You, Yumiao Zhang, Dexuan Xu, Yiwei Lou, Yandong Yan, Wei Wang, Huaming Zhang, Yu Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 08:32:33</h6>
<p class='card-text'>Data Science tasks are multifaceted, dynamic, and often domain-specific.
Existing LLM-based approaches largely concentrate on isolated phases,
neglecting the interdependent nature of many data science tasks and limiting
their capacity for comprehensive end-to-end support. We propose DatawiseAgent,
a notebook-centric LLM agent framework that unifies interactions among user,
agent and the computational environment through markdown and executable code
cells, supporting flexible and adaptive automated data science. Built on a
Finite State Transducer(FST), DatawiseAgent orchestrates four stages, including
DSF-like planning, incremental execution, self-debugging, and post-filtering.
Specifically, the DFS-like planning stage systematically explores the solution
space, while incremental execution harnesses real-time feedback and
accommodates LLM's limited capabilities to progressively complete tasks. The
self-debugging and post-filtering modules further enhance reliability by
diagnosing and correcting errors and pruning extraneous information. Extensive
experiments on diverse tasks, including data analysis, visualization, and data
modeling, show that DatawiseAgent consistently outperforms or matches
state-of-the-art methods across multiple model settings. These results
highlight its potential to generalize across data science scenarios and lay the
groundwork for more efficient, fully automated workflows.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07020v1' target='_blank'>Combating Partial Perception Deficit in Autonomous Driving with
  Multimodal LLM Commonsense</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuting Hu, Chenhui Xu, Ruiyang Qin, Dancheng Liu, Amir Nassereldine, Yiyu Shi, Jinjun Xiong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 08:01:41</h6>
<p class='card-text'>Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07018v1' target='_blank'>Toward Multi-Session Personalized Conversation: A Large-Scale Dataset
  and Hierarchical Tree Framework for Implicit Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xintong Li, Jalend Bantupalli, Ria Dharmani, Yuwei Zhang, Jingbo Shang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 07:59:41</h6>
<p class='card-text'>There has been a surge in the use of large language models (LLM)
conversational agents to generate responses based on long-term history from
multiple sessions. However, existing long-term open-domain dialogue datasets
lack complex, real-world personalization and fail to capture implicit
reasoning-where relevant information is embedded in subtle, syntactic, or
semantically distant connections rather than explicit statements. In such
cases, traditional retrieval methods fail to capture relevant context, and
long-context modeling also becomes inefficient due to numerous complicated
persona-related details. To address this gap, we introduce ImplexConv, a
large-scale long-term dataset with 2,500 examples, each containing
approximately 100 conversation sessions, designed to study implicit reasoning
in personalized dialogues. Additionally, we propose TaciTree, a novel
hierarchical tree framework that structures conversation history into multiple
levels of summarization. Instead of brute-force searching all data, TaciTree
enables an efficient, level-based retrieval process where models refine their
search by progressively selecting relevant details. Our experiments demonstrate
that TaciTree significantly improves the ability of LLMs to reason over
long-term conversations with implicit contextual dependencies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07010v1' target='_blank'>ProjectEval: A Benchmark for Programming Agents Automated Evaluation on
  Project-Level Code Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kaiyuan Liu, Youcheng Pan, Jing Li, Daojing He, Yang Xiang, Yexing Du, Tianrun Gao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 07:47:27</h6>
<p class='card-text'>Recently, LLM agents have made rapid progress in improving their programming
capabilities. However, existing benchmarks lack the ability to automatically
evaluate from users' perspective, and also lack the explainability of the
results of LLM agents' code generation capabilities. Thus, we introduce
ProjectEval, a new benchmark for LLM agents project-level code generation's
automated evaluation by simulating user interaction. ProjectEval is constructed
by LLM with human reviewing. It has three different level inputs of natural
languages or code skeletons. ProjectEval can evaluate the generated projects by
user interaction simulation for execution, and by code similarity through
existing objective indicators. Through ProjectEval, we find that systematic
engineering project code, overall understanding of the project and
comprehensive analysis capability are the keys for LLM agents to achieve
practical projects. Our findings and benchmark provide valuable insights for
developing more effective programming agents that can be deployed in future
real-world production.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07675v1' target='_blank'>DynTaskMAS: A Dynamic Task Graph-driven Framework for Asynchronous and
  Parallel LLM-based Multi-Agent Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Junwei Yu, Yepeng Ding, Hiroyuki Sato</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 06:16:10</h6>
<p class='card-text'>The emergence of Large Language Models (LLMs) in Multi-Agent Systems (MAS)
has opened new possibilities for artificial intelligence, yet current
implementations face significant challenges in resource management, task
coordination, and system efficiency. While existing frameworks demonstrate the
potential of LLM-based agents in collaborative problem-solving, they often lack
sophisticated mechanisms for parallel execution and dynamic task management.
This paper introduces DynTaskMAS, a novel framework that orchestrates
asynchronous and parallel operations in LLM-based MAS through dynamic task
graphs. The framework features four key innovations: (1) a Dynamic Task Graph
Generator that intelligently decomposes complex tasks while maintaining logical
dependencies, (2) an Asynchronous Parallel Execution Engine that optimizes
resource utilization through efficient task scheduling, (3) a Semantic-Aware
Context Management System that enables efficient information sharing among
agents, and (4) an Adaptive Workflow Manager that dynamically optimizes system
performance. Experimental evaluations demonstrate that DynTaskMAS achieves
significant improvements over traditional approaches: a 21-33% reduction in
execution time across task complexities (with higher gains for more complex
tasks), a 35.4% improvement in resource utilization (from 65% to 88%), and
near-linear throughput scaling up to 16 concurrent agents (3.47X improvement
for 4X agents). Our framework establishes a foundation for building scalable,
high-performance LLM-based multi-agent systems capable of handling complex,
dynamic tasks efficiently.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06951v1' target='_blank'>ReAgent: Reversible Multi-Agent Reasoning for Knowledge-Enhanced
  Multi-Hop QA</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhao Xinjie, Fan Gao, Rui Yang, Yingjian Chen, Yuyang Wang, Ying Zhu, Jiacheng Tang, Irene Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 05:56:46</h6>
<p class='card-text'>Recent advances in large language models (LLMs) have significantly improved
multi-hop question answering (QA) through direct Chain-of-Thought (CoT)
reasoning. However, the irreversible nature of CoT leads to error accumulation,
making it challenging to correct mistakes in multi-hop reasoning. This paper
introduces ReAgent: a Reversible multi-Agent collaborative framework augmented
with explicit backtracking mechanisms, enabling reversible multi-hop reasoning.
By incorporating text-based retrieval, information aggregation and validation,
our system can detect and correct errors mid-reasoning, leading to more robust
and interpretable QA outcomes. The framework and experiments serve as a
foundation for future work on error-tolerant QA systems. Empirical evaluations
across three benchmarks indicate ReAgent's efficacy, yielding average about 6\%
improvements against baseline models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06911v1' target='_blank'>Beyond Code Generation: LLM-supported Exploration of the Program Design
  Space</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:J. D. Zamfirescu-Pereira, Eunice Jun, Michael Terry, Qian Yang, Björn Hartmann</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 04:25:30</h6>
<p class='card-text'>In this work, we explore explicit Large Language Model (LLM)-powered support
for the iterative design of computer programs. Program design, like other
design activity, is characterized by navigating a space of alternative problem
formulations and associated solutions in an iterative fashion. LLMs are
potentially powerful tools in helping this exploration; however, by default,
code-generation LLMs deliver code that represents a particular point solution.
This obscures the larger space of possible alternatives, many of which might be
preferable to the LLM's default interpretation and its generated code. We
contribute an IDE that supports program design through generating and showing
new ways to frame problems alongside alternative solutions, tracking design
decisions, and identifying implicit decisions made by either the programmer or
the LLM. In a user study, we find that with our IDE, users combine and
parallelize design phases to explore a broader design space -- but also
struggle to keep up with LLM-originated changes to code and other information
overload. These findings suggest a core challenge for future IDEs that support
program design through higher-level instructions given to LLM-based agents:
carefully managing attention and deciding what information agents should
surface to program designers and when.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06892v1' target='_blank'>SafePlan: Leveraging Formal Logic and Chain-of-Thought Reasoning for
  Enhanced Safety in LLM-based Robotic Task Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ike Obi, Vishnunandan L. N. Venkatesh, Weizheng Wang, Ruiqi Wang, Dayoon Suh, Temitope I. Amosa, Wonse Jo, Byung-Cheol Min</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 03:37:36</h6>
<p class='card-text'>Robotics researchers increasingly leverage large language models (LLM) in
robotics systems, using them as interfaces to receive task commands, generate
task plans, form team coalitions, and allocate tasks among multi-robot and
human agents. However, despite their benefits, the growing adoption of LLM in
robotics has raised several safety concerns, particularly regarding executing
malicious or unsafe natural language prompts. In addition, ensuring that task
plans, team formation, and task allocation outputs from LLMs are adequately
examined, refined, or rejected is crucial for maintaining system integrity. In
this paper, we introduce SafePlan, a multi-component framework that combines
formal logic and chain-of-thought reasoners for enhancing the safety of
LLM-based robotics systems. Using the components of SafePlan, including Prompt
Sanity COT Reasoner and Invariant, Precondition, and Postcondition COT
reasoners, we examined the safety of natural language task prompts, task plans,
and task allocation outputs generated by LLM-based robotic systems as means of
investigating and enhancing system safety profile. Our results show that
SafePlan outperforms baseline models by leading to 90.5% reduction in harmful
task prompt acceptance while still maintaining reasonable acceptance of safe
tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06832v1' target='_blank'>GUIDE-CoT: Goal-driven and User-Informed Dynamic Estimation for
  Pedestrian Trajectory using Chain-of-Thought</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sungsik Kim, Janghyun Baek, Jinkyu Kim, Jaekoo Lee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 01:39:24</h6>
<p class='card-text'>While Large Language Models (LLMs) have recently shown impressive results in
reasoning tasks, their application to pedestrian trajectory prediction remains
challenging due to two key limitations: insufficient use of visual information
and the difficulty of predicting entire trajectories. To address these
challenges, we propose Goal-driven and User-Informed Dynamic Estimation for
pedestrian trajectory using Chain-of-Thought (GUIDE-CoT). Our approach
integrates two innovative modules: (1) a goal-oriented visual prompt, which
enhances goal prediction accuracy combining visual prompts with a pretrained
visual encoder, and (2) a chain-of-thought (CoT) LLM for trajectory generation,
which generates realistic trajectories toward the predicted goal. Moreover, our
method introduces controllable trajectory generation, allowing for flexible and
user-guided modifications to the predicted paths. Through extensive experiments
on the ETH/UCY benchmark datasets, our method achieves state-of-the-art
performance, delivering both high accuracy and greater adaptability in
pedestrian trajectory prediction. Our code is publicly available at
https://github.com/ai-kmu/GUIDE-CoT.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06791v1' target='_blank'>AutoMisty: A Multi-Agent LLM Framework for Automated Code Generation in
  the Misty Social Robot</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiao Wang, Lu Dong, Sahana Rangasrinivasan, Ifeoma Nwogu, Srirangaraj Setlur, Venugopal Govindaraju</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-09 22:07:46</h6>
<p class='card-text'>The social robot's open API allows users to customize open-domain
interactions. However, it remains inaccessible to those without programming
experience. In this work, we introduce AutoMisty, the first multi-agent
collaboration framework powered by large language models (LLMs), to enable the
seamless generation of executable Misty robot code from natural language
instructions. AutoMisty incorporates four specialized agent modules to manage
task decomposition, assignment, problem-solving, and result synthesis. Each
agent incorporates a two-layer optimization mechanism, with self-reflection for
iterative refinement and human-in-the-loop for better alignment with user
preferences. AutoMisty ensures a transparent reasoning process, allowing users
to iteratively refine tasks through natural language feedback for precise
execution. To evaluate AutoMisty's effectiveness, we designed a benchmark task
set spanning four levels of complexity and conducted experiments in a real
Misty robot environment. Extensive evaluations demonstrate that AutoMisty not
only consistently generates high-quality code but also enables precise code
control, significantly outperforming direct reasoning with ChatGPT-4o and
ChatGPT-o1. All code, optimized APIs, and experimental videos will be publicly
released through the webpage: https://wangxiaoshawn.github.io/AutoMisty.html</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>