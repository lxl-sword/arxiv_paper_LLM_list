<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>LLM-planning - 2025-03-06</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>LLM-planning - 2025-03-06</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03556v1' target='_blank'>Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented
  Manipulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaomeng Zhu, Yuyang Li, Leiyao Cui, Pengfei Li, Huan-ang Gao, Yixin Zhu, Hao Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 14:44:53</h6>
<p class='card-text'>Object affordance reasoning, the ability to infer object functionalities
based on physical properties, is fundamental for task-oriented planning and
activities in both humans and Artificial Intelligence (AI). This capability,
required for planning and executing daily activities in a task-oriented manner,
relies on commonsense knowledge of object physics and functionalities,
extending beyond simple object recognition. Current computational models for
affordance reasoning from perception lack generalizability, limiting their
applicability in novel scenarios. Meanwhile, comprehensive Large Language
Models (LLMs) with emerging reasoning capabilities are challenging to deploy on
local devices for task-oriented manipulations. Here, we introduce LVIS-Aff, a
large-scale dataset comprising 1,496 tasks and 119k images, designed to enhance
the generalizability of affordance reasoning from perception. Utilizing this
dataset, we develop Afford-X, an end-to-end trainable affordance reasoning
model that incorporates Verb Attention and Bi-Fusion modules to improve
multi-modal understanding. This model achieves up to a 12.1% performance
improvement over the best-reported results from non-LLM methods, while also
demonstrating a 1.2% enhancement compared to our previous conference paper.
Additionally, it maintains a compact 187M parameter size and infers nearly 50
times faster than the GPT-4V API. Our work demonstrates the potential for
efficient, generalizable affordance reasoning models that can be deployed on
local devices for task-oriented manipulations. We showcase Afford-X's
effectiveness in enabling task-oriented manipulations for robots across various
tasks and environments, underscoring its efficiency and broad implications for
advancing robotics and AI systems in real-world applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03505v1' target='_blank'>Parallelized Planning-Acting for Efficient LLM-based Multi-Agent Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yaoru Li, Shunyu Liu, Tongya Zheng, Mingli Song</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 13:53:10</h6>
<p class='card-text'>Recent advancements in Large Language Model(LLM)-based Multi-Agent
Systems(MAS) have demonstrated remarkable potential for tackling complex
decision-making tasks. However, existing frameworks inevitably rely on
serialized execution paradigms, where agents must complete sequential LLM
planning before taking action. This fundamental constraint severely limits
real-time responsiveness and adaptation, which is crucial in dynamic
environments with ever-changing scenarios. In this paper, we propose a novel
parallelized planning-acting framework for LLM-based MAS, featuring a
dual-thread architecture with interruptible execution to enable concurrent
planning and acting. Specifically, our framework comprises two core threads:(1)
a planning thread driven by a centralized memory system, maintaining
synchronization of environmental states and agent communication to support
dynamic decision-making; and (2) an acting thread equipped with a comprehensive
skill library, enabling automated task execution through recursive
decomposition. Extensive experiments on challenging Minecraft demonstrate the
effectiveness of the proposed framework.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03459v1' target='_blank'>Unified Mind Model: Reimagining Autonomous Agents in the LLM Era</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pengbo Hu, Xiang Ying</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 12:49:44</h6>
<p class='card-text'>Large language models (LLMs) have recently demonstrated remarkable
capabilities across domains, tasks, and languages (e.g., ChatGPT and GPT-4),
reviving the research of general autonomous agents with human-like cognitive
abilities.Such human-level agents require semantic comprehension and
instruction-following capabilities, which exactly fall into the strengths of
LLMs.Although there have been several initial attempts to build human-level
agents based on LLMs, the theoretical foundation remains a challenging open
problem. In this paper, we propose a novel theoretical cognitive architecture,
the Unified Mind Model (UMM), which offers guidance to facilitate the rapid
creation of autonomous agents with human-level cognitive abilities.
Specifically, our UMM starts with the global workspace theory and further
leverage LLMs to enable the agent with various cognitive abilities, such as
multi-modal perception, planning, reasoning, tool use, learning, memory,
reflection and motivation. Building upon UMM, we then develop an agent-building
engine, MindOS, which allows users to quickly create domain-/task-specific
autonomous agents without any programming effort.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03074v1' target='_blank'>BEVDriver: Leveraging BEV Maps in LLMs for Robust Closed-Loop Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Katharina Winter, Mark Azer, Fabian B. Flohr</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 00:27:32</h6>
<p class='card-text'>Autonomous driving has the potential to set the stage for more efficient
future mobility, requiring the research domain to establish trust through safe,
reliable and transparent driving. Large Language Models (LLMs) possess
reasoning capabilities and natural language understanding, presenting the
potential to serve as generalized decision-makers for ego-motion planning that
can interact with humans and navigate environments designed for human drivers.
While this research avenue is promising, current autonomous driving approaches
are challenged by combining 3D spatial grounding and the reasoning and language
capabilities of LLMs. We introduce BEVDriver, an LLM-based model for end-to-end
closed-loop driving in CARLA that utilizes latent BEV features as perception
input. BEVDriver includes a BEV encoder to efficiently process multi-view
images and 3D LiDAR point clouds. Within a common latent space, the BEV
features are propagated through a Q-Former to align with natural language
instructions and passed to the LLM that predicts and plans precise future
trajectories while considering navigation instructions and critical scenarios.
On the LangAuto benchmark, our model reaches up to 18.9% higher performance on
the Driving Score compared to SoTA methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02756v1' target='_blank'>BatchGEMBA: Token-Efficient Machine Translation Evaluation with Batched
  Prompting and Prompt Compression</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Daniil Larionov, Steffen Eger</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 16:20:52</h6>
<p class='card-text'>Recent advancements in Large Language Model (LLM)-based Natural Language
Generation evaluation have largely focused on single-example prompting,
resulting in significant token overhead and computational inefficiencies. In
this work, we introduce BatchGEMBA-MQM, a framework that integrates batched
prompting with the GEMBA-MQM metric for machine translation evaluation. Our
approach aggregates multiple translation examples into a single prompt,
reducing token usage by 2-4 times (depending on the batch size) relative to
single-example prompting. Furthermore, we propose a batching-aware prompt
compression model that achieves an additional token reduction of 13-15% on
average while also showing ability to help mitigate batching-induced quality
degradation. Evaluations across several LLMs (GPT-4o, GPT-4o-mini, Mistral
Small, Phi4, and CommandR7B) and varying batch sizes reveal that while batching
generally negatively affects quality (but sometimes not substantially), prompt
compression does not degrade further, and in some cases, recovers quality loss.
For instance, GPT-4o retains over 90% of its baseline performance at a batch
size of 4 when compression is applied, compared to a 44.6% drop without
compression. We plan to release our code and trained models at
https://github.com/NL2G/batchgemba to support future research in this domain.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02698v1' target='_blank'>FlowPlan: Zero-Shot Task Planning with LLM Flow Engineering for Robotic
  Instruction Following</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zijun Lin, Chao Tang, Hanjing Ye, Hong Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 15:14:41</h6>
<p class='card-text'>Robotic instruction following tasks require seamless integration of visual
perception, task planning, target localization, and motion execution. However,
existing task planning methods for instruction following are either data-driven
or underperform in zero-shot scenarios due to difficulties in grounding lengthy
instructions into actionable plans under operational constraints. To address
this, we propose FlowPlan, a structured multi-stage LLM workflow that elevates
zero-shot pipeline and bridges the performance gap between zero-shot and
data-driven in-context learning methods. By decomposing the planning process
into modular stages--task information retrieval, language-level reasoning,
symbolic-level planning, and logical evaluation--FlowPlan generates logically
coherent action sequences while adhering to operational constraints and further
extracts contextual guidance for precise instance-level target localization.
Benchmarked on the ALFRED and validated in real-world applications, our method
achieves competitive performance relative to data-driven in-context learning
methods and demonstrates adaptability across diverse environments. This work
advances zero-shot task planning in robotic systems without reliance on labeled
data. Project website: https://instruction-following-project.github.io/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02682v1' target='_blank'>MPO: Boosting LLM Agents with Meta Plan Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weimin Xiong, Yifan Song, Qingxiu Dong, Bingchan Zhao, Feifan Song, Xun Wang, Sujian Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 14:54:45</h6>
<p class='card-text'>Recent advancements in large language models (LLMs) have enabled LLM-based
agents to successfully tackle interactive planning tasks. However, despite
their successes, existing approaches often suffer from planning hallucinations
and require retraining for each new agent. To address these challenges, we
propose the Meta Plan Optimization (MPO) framework, which enhances agent
planning capabilities by directly incorporating explicit guidance. Unlike
previous methods that rely on complex knowledge, which either require
significant human effort or lack quality assurance, MPO leverages high-level
general guidance through meta plans to assist agent planning and enables
continuous optimization of the meta plans based on feedback from the agent's
task execution. Our experiments conducted on two representative tasks
demonstrate that MPO significantly outperforms existing baselines. Moreover,
our analysis indicates that MPO provides a plug-and-play solution that enhances
both task completion efficiency and generalization capabilities in previous
unseen scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02465v1' target='_blank'>UAV-VLRR: Vision-Language Informed NMPC for Rapid Response in UAV Search
  and Rescue</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yasheerah Yaqoot, Muhammad Ahsan Mustafa, Oleg Sautenkov, Dzmitry Tsetserukou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 10:21:58</h6>
<p class='card-text'>Emergency search and rescue (SAR) operations often require rapid and precise
target identification in complex environments where traditional manual drone
control is inefficient. In order to address these scenarios, a rapid SAR
system, UAV-VLRR (Vision-Language-Rapid-Response), is developed in this
research. This system consists of two aspects: 1) A multimodal system which
harnesses the power of Visual Language Model (VLM) and the natural language
processing capabilities of ChatGPT-4o (LLM) for scene interpretation. 2) A
non-linearmodel predictive control (NMPC) with built-in obstacle avoidance for
rapid response by a drone to fly according to the output of the multimodal
system. This work aims at improving response times in emergency SAR operations
by providing a more intuitive and natural approach to the operator to plan the
SAR mission while allowing the drone to carry out that mission in a rapid and
safe manner. When tested, our approach was faster on an average by 33.75% when
compared with an off-the-shelf autopilot and 54.6% when compared with a human
pilot. Video of UAV-VLRR: https://youtu.be/KJqQGKKt1xY</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02238v1' target='_blank'>Haste Makes Waste: Evaluating Planning Abilities of LLMs for Efficient
  and Feasible Multitasking with Time Constraints Between Actions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zirui Wu, Xiao Liu, Jiayi Li, Lingpeng Kong, Yansong Feng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 03:27:02</h6>
<p class='card-text'>While Large Language Model-based agents have demonstrated substantial
progress in task completion, existing evaluation benchmarks tend to
overemphasize single-task performance, with insufficient attention given to the
crucial aspects of multitask planning and execution efficiency required in
real-world scenarios. To bridge this gap, we present Recipe2Plan, a novel
benchmark framework based on real-world cooking scenarios. Unlike conventional
benchmarks, Recipe2Plan challenges agents to optimize cooking time through
parallel task execution while respecting temporal constraints i.e. specific
actions need to be performed within a particular time intervals following the
preceding steps. Overly aggressive local parallelization may disrupt this
constraint, potentially compromising the entire cooking process. This strict
time constraint between actions raises a unique challenge for agents to balance
between maximizing concurrent operations and adhering to critical timing
constraints. Extensive experiments with state-of-the-art models reveal
challenges in maintaining this balance between efficiency and feasibility. The
results highlight the need for improved temporal awareness and global
multitasking capabilities in large language models. We open-source our
benchmark and code at https://github.com/WilliamZR/Recipe2Plan.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02197v1' target='_blank'>ATLaS: Agent Tuning via Learning Critical Steps</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhixun Chen, Ming Li, Yuxuan Huang, Yali Du, Meng Fang, Tianyi Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 02:14:55</h6>
<p class='card-text'>Large Language Model (LLM) agents have demonstrated remarkable generalization
capabilities across multi-domain tasks. Existing agent tuning approaches
typically employ supervised finetuning on entire expert trajectories. However,
behavior-cloning of full trajectories can introduce expert bias and weaken
generalization to states not covered by the expert data. Additionally, critical
steps, such as planning, complex reasoning for intermediate subtasks, and
strategic decision-making, are essential to success in agent tasks, so learning
these steps is the key to improving LLM agents. For more effective and
efficient agent tuning, we propose ATLaS that identifies the critical steps in
expert trajectories and finetunes LLMs solely on these steps with reduced
costs. By steering the training's focus to a few critical steps, our method
mitigates the risk of overfitting entire trajectories and promotes
generalization across different environments and tasks. In extensive
experiments, an LLM finetuned on only 30% critical steps selected by ATLaS
outperforms the LLM finetuned on all steps and recent open-source LLM agents.
ATLaS maintains and improves base LLM skills as generalist agents interacting
with diverse environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01804v1' target='_blank'>$\texttt{SEM-CTRL}$: Semantically Controlled Decoding</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mohammad Albinhassan, Pranava Madhyastha, Alessandra Russo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 18:33:46</h6>
<p class='card-text'>Ensuring both syntactic and semantic correctness in Large Language Model
(LLM) outputs remains a significant challenge, despite being critical for
real-world deployment. In this paper, we introduce $\texttt{SEM-CTRL}$, a
unified approach that enforces rich context-sensitive constraints and task- and
instance-specific semantics directly on an LLM decoder. Our approach integrates
token-level MCTS, which is guided by specific syntactic and semantic
constraints. The constraints over the desired outputs are expressed using
Answer Set Grammars -- a logic-based formalism that generalizes
context-sensitive grammars while incorporating background knowledge to
represent task-specific semantics. We show that our approach guarantees correct
completions for any off-the-shelf LLM without the need for fine-tuning. We
evaluate $\texttt{SEM-CTRL}$ on a range of tasks, including synthetic grammar
synthesis, combinatorial reasoning, and planning. Our results demonstrate that
$\texttt{SEM-CTRL}$ allows small pre-trained LLMs to efficiently outperform
larger variants and state-of-the-art reasoning models (e.g., o1-preview) while
simultaneously guaranteeing solution correctness.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01700v1' target='_blank'>Code-as-Symbolic-Planner: Foundation Model-Based Robot Planning via
  Symbolic Code Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yongchao Chen, Yilun Hao, Yang Zhang, Chuchu Fan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 16:13:41</h6>
<p class='card-text'>Recent works have shown great potentials of Large Language Models (LLMs) in
robot task and motion planning (TAMP). Current LLM approaches generate text- or
code-based reasoning chains with sub-goals and action plans. However, they do
not fully leverage LLMs' symbolic computing and code generation capabilities.
Many robot TAMP tasks involve complex optimization under multiple constraints,
where pure textual reasoning is insufficient. While augmenting LLMs with
predefined solvers and planners improves performance, it lacks generalization
across tasks. Given LLMs' growing coding proficiency, we enhance their TAMP
capabilities by steering them to generate code as symbolic planners for
optimization and constraint verification. Unlike prior work that uses code to
interface with robot action modules, we steer LLMs to generate code as solvers,
planners, and checkers for TAMP tasks requiring symbolic computing, while still
leveraging textual reasoning to incorporate common sense. With a multi-round
guidance and answer evolution framework, the proposed Code-as-Symbolic-Planner
improves success rates by average 24.1\% over best baseline methods across
seven typical TAMP tasks and three popular LLMs. Code-as-Symbolic-Planner shows
strong effectiveness and generalizability across discrete and continuous
environments, 2D/3D simulations and real-world settings, as well as single- and
multi-robot tasks with diverse requirements. See our project website
https://yongchao98.github.io/Code-Symbol-Planner/ for prompts, videos, and
code.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01490v1' target='_blank'>Improving Retrospective Language Agents via Joint Policy Gradient
  Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xueyang Feng, Bo Lan, Quanyu Dai, Lei Wang, Jiakai Tang, Xu Chen, Zhenhua Dong, Ji-Rong Wen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 12:54:54</h6>
<p class='card-text'>In recent research advancements within the community, large language models
(LLMs) have sparked great interest in creating autonomous agents. However,
current prompt-based agents often heavily rely on large-scale LLMs. Meanwhile,
although fine-tuning methods significantly enhance the capabilities of smaller
LLMs, the fine-tuned agents often lack the potential for self-reflection and
self-improvement. To address these challenges, we introduce a novel agent
framework named RetroAct, which is a framework that jointly optimizes both
task-planning and self-reflective evolution capabilities in language agents.
Specifically, we develop a two-stage joint optimization process that integrates
imitation learning and reinforcement learning, and design an off-policy joint
policy gradient optimization algorithm with imitation learning regularization
to enhance the data efficiency and training stability in agent tasks. RetroAct
significantly improves the performance of open-source models, reduces
dependency on closed-source LLMs, and enables fine-tuned agents to learn and
evolve continuously. We conduct extensive experiments across various testing
environments, demonstrating RetroAct has substantial improvements in task
performance and decision-making processes.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01236v1' target='_blank'>LLM-Advisor: An LLM Benchmark for Cost-efficient Path Planning across
  Multiple Terrains</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ling Xiao, Toshihiko Yamasaki</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 07:02:10</h6>
<p class='card-text'>Multi-terrain cost-efficient path planning is a crucial task in robot
navigation, requiring the identification of a path from the start to the goal
that not only avoids obstacles but also minimizes travel costs. This is
especially crucial for real-world applications where robots need to navigate
diverse terrains in outdoor environments, where recharging or refueling is
difficult. However, there is very limited research on this topic. In this
paper, we develop a prompt-based approach, LLM-Advisor, which leverages large
language models (LLMs) as effective advisors for path planning. The LLM-Advisor
selectively provides suggestions, demonstrating its ability to recognize when
no modifications are necessary. When suggestions are made, 70.59% of the paths
suggested for the A* algorithm, 69.47% for the RRT* algorithm, and 78.70% for
the LLM-A* algorithm achieve greater cost efficiency. Since LLM-Advisor may
occasionally lack common sense in their suggestions, we propose two
hallucination-mitigation strategies. Furthermore, we experimentally verified
that GPT-4o performs poorly in zero-shot path planning, even when terrain
descriptions are clearly provided, demonstrating its low spatial awareness. We
also experimentally demonstrate that using an LLM as an advisor is more
effective than directly integrating it into the path-planning loop. Since LLMs
may generate hallucinations, using LLMs in the loop of a search-based method
(such as A*) may lead to a higher number of failed paths, demonstrating that
our proposed LLM-Advisor is a better choice.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01935v1' target='_blank'>MultiAgentBench: Evaluating the Collaboration and Competition of LLM
  agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kunlun Zhu, Hongyi Du, Zhaochen Hong, Xiaocheng Yang, Shuyi Guo, Zhe Wang, Zhenhailong Wang, Cheng Qian, Xiangru Tang, Heng Ji, Jiaxuan You</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 05:18:50</h6>
<p class='card-text'>Large Language Models (LLMs) have shown remarkable capabilities as autonomous
agents, yet existing benchmarks either focus on single-agent tasks or are
confined to narrow domains, failing to capture the dynamics of multi-agent
coordination and competition. In this paper, we introduce MultiAgentBench, a
comprehensive benchmark designed to evaluate LLM-based multi-agent systems
across diverse, interactive scenarios. Our framework measures not only task
completion but also the quality of collaboration and competition using novel,
milestone-based key performance indicators. Moreover, we evaluate various
coordination protocols (including star, chain, tree, and graph topologies) and
innovative strategies such as group discussion and cognitive planning. Notably,
gpt-4o-mini reaches the average highest task score, graph structure performs
the best among coordination protocols in the research scenario, and cognitive
planning improves milestone achievement rates by 3%. Code and datasets are
public available at https://github.com/MultiagentBench/MARBLE.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01068v1' target='_blank'>Language-Guided Object Search in Agricultural Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Advaith Balaji, Saket Pradhan, Dmitry Berenson</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 00:15:45</h6>
<p class='card-text'>Creating robots that can assist in farms and gardens can help reduce the
mental and physical workload experienced by farm workers. We tackle the problem
of object search in a farm environment, providing a method that allows a robot
to semantically reason about the location of an unseen target object among a
set of previously seen objects in the environment using a Large Language Model
(LLM). We leverage object-to-object semantic relationships to plan a path
through the environment that will allow us to accurately and efficiently locate
our target object while also reducing the overall distance traveled, without
needing high-level room or area-level semantic relationships. During our
evaluations, we found that our method outperformed a current state-of-the-art
baseline and our ablations. Our offline testing yielded an average path
efficiency of 84%, reflecting how closely the predicted path aligns with the
ideal path. Upon deploying our system on the Boston Dynamics Spot robot in a
real-world farm environment, we found that our system had a success rate of
80%, with a success weighted by path length of 0.67, which demonstrates a
reasonable trade-off between task success and path efficiency under real-world
conditions. The project website can be viewed at
https://adi-balaji.github.io/losae/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01007v1' target='_blank'>From Vague Instructions to Task Plans: A Feedback-Driven HRC Task
  Planning Framework based on LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Afagh Mehri Shervedani, Matthew R. Walter, Milos Zefran</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-02 20:22:32</h6>
<p class='card-text'>Recent advances in large language models (LLMs) have demonstrated their
potential as planners in human-robot collaboration (HRC) scenarios, offering a
promising alternative to traditional planning methods. LLMs, which can generate
structured plans by reasoning over natural language inputs, have the ability to
generalize across diverse tasks and adapt to human instructions. This paper
investigates the potential of LLMs to facilitate planning in the context of
human-robot collaborative tasks, with a focus on their ability to reason from
high-level, vague human inputs, and fine-tune plans based on real-time
feedback. We propose a novel hybrid framework that combines LLMs with human
feedback to create dynamic, context-aware task plans. Our work also highlights
how a single, concise prompt can be used for a wide range of tasks and
environments, overcoming the limitations of long, detailed structured prompts
typically used in prior studies. By integrating user preferences into the
planning loop, we ensure that the generated plans are not only effective but
aligned with human intentions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.00767v1' target='_blank'>LLMs are everywhere: Ubiquitous Utilization of AI Models through Air
  Computing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Baris Yamansavascilar, Atay Ozgovde, Cem Ersoy</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-02 07:24:34</h6>
<p class='card-text'>We are witnessing a new era where problem-solving and cognitive tasks are
being increasingly delegated to Large Language Models (LLMs) across diverse
domains, ranging from code generation to holiday planning. This trend also
creates a demand for the ubiquitous execution of LLM-powered applications in a
wide variety of environments in which traditional terrestrial 2D networking
infrastructures may prove insufficient. A promising solution in this context is
to extend edge computing into a 3D setting to include aerial platforms
organized in multiple layers, a paradigm we refer to as air computing, to
augment local devices for running LLM and Generative AI (GenAI) applications.
This approach alleviates the strain on existing infrastructure while enhancing
service efficiency by offloading computational tasks to the corresponding air
units such as UAVs. Furthermore, the coordinated deployment of various air
units can significantly improve the Quality of Experience (QoE) by ensuring
seamless, adaptive, and resilient task execution. In this study, we investigate
the synergy between LLM-based applications and air computing, exploring their
potential across various use cases. Additionally, we present a disaster
response case study demonstrating how the collaborative utilization of LLMs and
air computing can significantly improve outcomes in critical situations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.00729v1' target='_blank'>CLEA: Closed-Loop Embodied Agent for Enhancing Task Execution in Dynamic
  Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mingcong Lei, Ge Wang, Yiming Zhao, Zhixin Mai, Qing Zhao, Yao Guo, Zhen Li, Shuguang Cui, Yatong Han, Jinke Ren</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-02 04:50:59</h6>
<p class='card-text'>Large Language Models (LLMs) exhibit remarkable capabilities in the
hierarchical decomposition of complex tasks through semantic reasoning.
However, their application in embodied systems faces challenges in ensuring
reliable execution of subtask sequences and achieving one-shot success in
long-term task completion. To address these limitations in dynamic
environments, we propose Closed-Loop Embodied Agent (CLEA) -- a novel
architecture incorporating four specialized open-source LLMs with functional
decoupling for closed-loop task management. The framework features two core
innovations: (1) Interactive task planner that dynamically generates executable
subtasks based on the environmental memory, and (2) Multimodal execution critic
employing an evaluation framework to conduct a probabilistic assessment of
action feasibility, triggering hierarchical re-planning mechanisms when
environmental perturbations exceed preset thresholds. To validate CLEA's
effectiveness, we conduct experiments in a real environment with manipulable
objects, using two heterogeneous robots for object search, manipulation, and
search-manipulation integration tasks. Across 12 task trials, CLEA outperforms
the baseline model, achieving a 67.3% improvement in success rate and a 52.8%
increase in task completion rate. These results demonstrate that CLEA
significantly enhances the robustness of task planning and execution in dynamic
environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.00717v1' target='_blank'>LLMDR: LLM-Driven Deadlock Detection and Resolution in Multi-Agent
  Pathfinding</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Seungbae Seo, Junghwan Kim, Minjeong Shin, Bongwon Suh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-02 03:49:15</h6>
<p class='card-text'>Multi-Agent Pathfinding (MAPF) is a core challenge in multi-agent systems.
Existing learning-based MAPF methods often struggle with scalability,
particularly when addressing complex scenarios that are prone to deadlocks. To
address these challenges, we introduce LLMDR (LLM-Driven Deadlock Detection and
Resolution), an approach designed to resolve deadlocks and improve the
performance of learnt MAPF models. LLMDR integrates the inference capabilities
of large language models (LLMs) with learnt MAPF models and prioritized
planning, enabling it to detect deadlocks and provide customized resolution
strategies. We evaluate LLMDR on standard MAPF benchmark maps with varying
agent numbers, measuring its performance when combined with several base
models. The results demonstrate that LLMDR improves the performance of learnt
MAPF models, particularly in deadlock-prone scenarios, with notable
improvements in success rates. These findings show the potential of integrating
LLMs to improve the scalability of learning-based MAPF methods.
  The source code for LLMDR is available at:
https://github.com/ssbacc/llmdr-dhc</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.00714v1' target='_blank'>Speculative Ad-hoc Querying</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haoyu Li, Srikanth Kandula, Maria Angels de Luis Balaguer, Aditya Akella, Venkat Arun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-02 03:44:31</h6>
<p class='card-text'>Analyzing large datasets requires responsive query execution, but executing
SQL queries on massive datasets can be slow. This paper explores whether query
execution can begin even before the user has finished typing, allowing results
to appear almost instantly. We propose SpeQL, a system that leverages Large
Language Models (LLMs) to predict likely queries based on the database schema,
the user's past queries, and their incomplete query. Since exact query
prediction is infeasible, SpeQL speculates on partial queries in two ways: 1)
it predicts the query structure to compile and plan queries in advance, and 2)
it precomputes smaller temporary tables that are much smaller than the original
database, but are still predicted to contain all information necessary to
answer the user's final query. Additionally, SpeQL continuously displays
results for speculated queries and subqueries in real time, aiding exploratory
analysis. A utility/user study showed that SpeQL improved task completion time,
and participants reported that its speculative display of results helped them
discover patterns in the data more quickly. In the study, SpeQL improves user's
query latency by up to $289\times$ and kept the overhead reasonable, at $\$4$
per hour.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.00624v1' target='_blank'>An evaluation of DeepSeek Models in Biomedical Natural Language
  Processing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zaifu Zhan, Shuang Zhou, Huixue Zhou, Jiawen Deng, Yu Hou, Jeremy Yeung, Rui Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-01 21:26:29</h6>
<p class='card-text'>The advancement of Large Language Models (LLMs) has significantly impacted
biomedical Natural Language Processing (NLP), enhancing tasks such as named
entity recognition, relation extraction, event extraction, and text
classification. In this context, the DeepSeek series of models have shown
promising potential in general NLP tasks, yet their capabilities in the
biomedical domain remain underexplored. This study evaluates multiple DeepSeek
models (Distilled-DeepSeek-R1 series and Deepseek-LLMs) across four key
biomedical NLP tasks using 12 datasets, benchmarking them against
state-of-the-art alternatives (Llama3-8B, Qwen2.5-7B, Mistral-7B, Phi-4-14B,
Gemma-2-9B). Our results reveal that while DeepSeek models perform
competitively in named entity recognition and text classification, challenges
persist in event and relation extraction due to precision-recall trade-offs. We
provide task-specific model recommendations and highlight future research
directions. This evaluation underscores the strengths and limitations of
DeepSeek models in biomedical NLP, guiding their future deployment and
optimization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.00203v2' target='_blank'>Llamarine: Open-source Maritime Industry-specific Large Language Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:William Nguyen, An Phan, Konobu Kimura, Hitoshi Maeno, Mika Tanaka, Quynh Le, William Poucher, Christopher Nguyen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-28 21:39:22</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated substantial potential in
addressing complex reasoning tasks, yet their general-purpose nature often
limits their effectiveness in specialized domains such as maritime navigation.
To bridge this gap, we introduce Llamarine, the first open-source LLM designed
specifically for maritime navigation. Llamarine 1.0 is developed through
continued pretraining and fine-tuning on a high-quality corpus comprising
maritime textbooks, research publications, and web text from Wikipedia. This
domain-specific training enables the model to acquire expert-level knowledge in
navigational principles, collision avoidance, route optimization, and
regulatory compliance. Our key contributions include (a) the curation of a
comprehensive maritime dataset from authoritative sources, ensuring depth and
reliability in the model's knowledge base; (b) the development of a
foundational model capable of reasoning about complex navigational challenges
with greater accuracy than general-purpose LLMs; and (c) the establishment of a
benchmark to evaluate performance in maritime-specific decision-making tasks.
Experimental results demonstrate that Llamarine outperforms both
general-purpose and commercial LLMs in critical navigation-related tasks, such
as trajectory planning, risk assessment, and compliance with maritime
regulations. By providing an open-source foundation model trained exclusively
on high-quality maritime literature, Llamarine paves the way for AI-driven
advancements in maritime safety, efficiency, and operational decision-making.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01908v1' target='_blank'>UDora: A Unified Red Teaming Framework against LLM Agents by Dynamically
  Hijacking Their Own Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiawei Zhang, Shuang Yang, Bo Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-28 21:30:28</h6>
<p class='card-text'>Large Language Model (LLM) agents equipped with external tools have become
increasingly powerful for handling complex tasks such as web shopping,
automated email replies, and financial trading. However, these advancements
also amplify the risks of adversarial attacks, particularly when LLM agents can
access sensitive external functionalities. Moreover, because LLM agents engage
in extensive reasoning or planning before executing final actions, manipulating
them into performing targeted malicious actions or invoking specific tools
remains a significant challenge. Consequently, directly embedding adversarial
strings in malicious instructions or injecting malicious prompts into tool
interactions has become less effective against modern LLM agents. In this work,
we present UDora, a unified red teaming framework designed for LLM Agents that
dynamically leverages the agent's own reasoning processes to compel it toward
malicious behavior. Specifically, UDora first samples the model's reasoning for
the given task, then automatically identifies multiple optimal positions within
these reasoning traces to insert targeted perturbations. Subsequently, it uses
the modified reasoning as the objective to optimize the adversarial strings. By
iteratively applying this process, the LLM agent will then be induced to
undertake designated malicious actions or to invoke specific malicious tools.
Our approach demonstrates superior effectiveness compared to existing methods
across three LLM agent datasets.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.00096v1' target='_blank'>BixBench: a Comprehensive Benchmark for LLM-based Agents in
  Computational Biology</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ludovico Mitchener, Jon M Laurent, Benjamin Tenmann, Siddharth Narayanan, Geemi P Wellawatte, Andrew White, Lorenzo Sani, Samuel G Rodriques</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-28 18:47:57</h6>
<p class='card-text'>Large Language Models (LLMs) and LLM-based agents show great promise in
accelerating scientific research. Existing benchmarks for measuring this
potential and guiding future development continue to evolve from pure recall
and rote knowledge tasks, towards more practical work such as literature review
and experimental planning. Bioinformatics is a domain where fully autonomous
AI-driven discovery may be near, but no extensive benchmarks for measuring
progress have been introduced to date. We therefore present the Bioinformatics
Benchmark (BixBench), a dataset comprising over 50 real-world scenarios of
practical biological data analysis with nearly 300 associated open-answer
questions designed to measure the ability of LLM-based agents to explore
biological datasets, perform long, multi-step analytical trajectories, and
interpret the nuanced results of those analyses. We evaluate the performance of
two frontier LLMs (GPT-4o and Claude 3.5 Sonnet) using a custom agent framework
we open source. We find that even the latest frontier models only achieve 17%
accuracy in the open-answer regime, and no better than random in a
multiple-choice setting. By exposing the current limitations of frontier
models, we hope BixBench can spur the development of agents capable of
conducting rigorous bioinformatic analysis and accelerate scientific discovery.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.21087v1' target='_blank'>PASemiQA: Plan-Assisted Agent for Question Answering on Semi-Structured
  Data with Text and Relational Information</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hansi Yang, Qi Zhang, Wei Jiang, Jianguo Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-28 14:26:47</h6>
<p class='card-text'>Large language models (LLMs) have shown impressive abilities in answering
questions across various domains, but they often encounter hallucination issues
on questions that require professional and up-to-date knowledge. To address
this limitation, retrieval-augmented generation (RAG) techniques have been
proposed, which retrieve relevant information from external sources to inform
their responses. However, existing RAG methods typically focus on a single type
of external data, such as vectorized text database or knowledge graphs, and
cannot well handle real-world questions on semi-structured data containing both
text and relational information. To bridge this gap, we introduce PASemiQA, a
novel approach that jointly leverages text and relational information in
semi-structured data to answer questions. PASemiQA first generates a plan to
identify relevant text and relational information to answer the question in
semi-structured data, and then uses an LLM agent to traverse the
semi-structured data and extract necessary information. Our empirical results
demonstrate the effectiveness of PASemiQA across different semi-structured
datasets from various domains, showcasing its potential to improve the accuracy
and reliability of question answering systems on semi-structured data.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20807v1' target='_blank'>Digital Player: Evaluating Large Language Models based Human-like Agent
  in Games</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiawei Wang, Kai Wang, Shaojie Lin, Runze Wu, Bihan Xu, Lingeng Jiang, Shiwei Zhao, Renyu Zhu, Haoyu Liu, Zhipeng Hu, Zhong Fan, Le Li, Tangjie Lyu, Changjie Fan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-28 07:46:55</h6>
<p class='card-text'>With the rapid advancement of Large Language Models (LLMs), LLM-based
autonomous agents have shown the potential to function as digital employees,
such as digital analysts, teachers, and programmers. In this paper, we develop
an application-level testbed based on the open-source strategy game "Unciv",
which has millions of active players, to enable researchers to build a "data
flywheel" for studying human-like agents in the "digital players" task. This
"Civilization"-like game features expansive decision-making spaces along with
rich linguistic interactions such as diplomatic negotiations and acts of
deception, posing significant challenges for LLM-based agents in terms of
numerical reasoning and long-term planning. Another challenge for "digital
players" is to generate human-like responses for social interaction,
collaboration, and negotiation with human players. The open-source project can
be found at https:/github.com/fuxiAIlab/CivAgent.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20795v1' target='_blank'>Plan2Align: Predictive Planning Based Test-Time Preference Alignment in
  Paragraph-Level Machine Translation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kuang-Da Wang, Teng-Ruei Chen, Yu Heng Hung, Shuoyang Ding, Yueh-Hua Wu, Yu-Chiang Frank Wang, Chao-Han Huck Yang, Wen-Chih Peng, Ping-Chun Hsieh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-28 07:24:33</h6>
<p class='card-text'>Machine Translation (MT) has been predominantly designed for sentence-level
translation using transformer-based architectures. While next-token prediction
based Large Language Models (LLMs) demonstrate strong capabilities in long-text
translation, non-extensive language models often suffer from omissions and
semantic inconsistencies when processing paragraphs. Existing preference
alignment methods improve sentence-level translation but fail to ensure
coherence over extended contexts due to the myopic nature of next-token
generation. We introduce Plan2Align, a test-time alignment framework that
treats translation as a predictive planning problem, adapting Model Predictive
Control to iteratively refine translation outputs. Experiments on WMT24
Discourse-Level Literary Translation show that Plan2Align significantly
improves paragraph-level translation, achieving performance surpassing or on
par with the existing training-time and test-time alignment methods on
LLaMA-3.1 8B.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20601v1' target='_blank'>NutriGen: Personalized Meal Plan Generator Leveraging Large Language
  Models to Enhance Dietary and Nutritional Adherence</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Saman Khamesian, Asiful Arefeen, Stephanie M. Carpenter, Hassan Ghasemzadeh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-28 00:05:49</h6>
<p class='card-text'>Maintaining a balanced diet is essential for overall health, yet many
individuals struggle with meal planning due to nutritional complexity, time
constraints, and lack of dietary knowledge. Personalized food recommendations
can help address these challenges by tailoring meal plans to individual
preferences, habits, and dietary restrictions. However, existing dietary
recommendation systems often lack adaptability, fail to consider real-world
constraints such as food ingredient availability, and require extensive user
input, making them impractical for sustainable and scalable daily use. To
address these limitations, we introduce NutriGen, a framework based on large
language models (LLM) designed to generate personalized meal plans that align
with user-defined dietary preferences and constraints. By building a
personalized nutrition database and leveraging prompt engineering, our approach
enables LLMs to incorporate reliable nutritional references like the USDA
nutrition database while maintaining flexibility and ease-of-use. We
demonstrate that LLMs have strong potential in generating accurate and
user-friendly food recommendations, addressing key limitations in existing
dietary recommendation systems by providing structured, practical, and scalable
meal plans. Our evaluation shows that Llama 3.1 8B and GPT-3.5 Turbo achieve
the lowest percentage errors of 1.55\% and 3.68\%, respectively, producing meal
plans that closely align with user-defined caloric targets while minimizing
deviation and improving precision. Additionally, we compared the performance of
DeepSeek V3 against several established models to evaluate its potential in
personalized nutrition planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20508v1' target='_blank'>TripCraft: A Benchmark for Spatio-Temporally Fine Grained Travel
  Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Soumyabrata Chaudhuri, Pranav Purkar, Ritwik Raghav, Shubhojit Mallick, Manish Gupta, Abhik Jana, Shreya Ghosh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 20:33:28</h6>
<p class='card-text'>Recent advancements in probing Large Language Models (LLMs) have explored
their latent potential as personalized travel planning agents, yet existing
benchmarks remain limited in real world applicability. Existing datasets, such
as TravelPlanner and TravelPlanner+, suffer from semi synthetic data reliance,
spatial inconsistencies, and a lack of key travel constraints, making them
inadequate for practical itinerary generation. To address these gaps, we
introduce TripCraft, a spatiotemporally coherent travel planning dataset that
integrates real world constraints, including public transit schedules, event
availability, diverse attraction categories, and user personas for enhanced
personalization. To evaluate LLM generated plans beyond existing binary
validation methods, we propose five continuous evaluation metrics, namely
Temporal Meal Score, Temporal Attraction Score, Spatial Score, Ordering Score,
and Persona Score which assess itinerary quality across multiple dimensions.
Our parameter informed setting significantly enhances meal scheduling,
improving the Temporal Meal Score from 61% to 80% in a 7 day scenario.
TripCraft establishes a new benchmark for LLM driven personalized travel
planning, offering a more realistic, constraint aware framework for itinerary
generation. Dataset and Codebase will be made publicly available upon
acceptance.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>