<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>LLM-agent - 2025-03-13</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>LLM-agent - 2025-03-13</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.09572v1' target='_blank'>Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lutfi Eren Erdogan, Nicholas Lee, Sehoon Kim, Suhong Moon, Hiroki Furuta, Gopala Anumanchipalli, Kurt Keutzer, Amir Gholami</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-12 17:40:52</h6>
<p class='card-text'>Large language models (LLMs) have shown remarkable advancements in enabling
language agents to tackle simple tasks. However, applying them for complex,
multi-step, long-horizon tasks remains a challenge. Recent work have found
success by separating high-level planning from low-level execution, which
enables the model to effectively balance high-level planning objectives and
low-level execution details. However, generating accurate plans remains
difficult since LLMs are not inherently trained for this task. To address this,
we propose Plan-and-Act, a novel framework that incorporates explicit planning
into LLM-based agents and introduces a scalable method to enhance plan
generation through a novel synthetic data generation method. Plan-and-Act
consists of a Planner model which generates structured, high-level plans to
achieve user goals, and an Executor model that translates these plans into
environment-specific actions. To train the Planner effectively, we introduce a
synthetic data generation method that annotates ground-truth trajectories with
feasible plans, augmented with diverse and extensive examples to enhance
generalization. We evaluate Plan-and-Act using web navigation as a
representative long-horizon planning environment, demonstrating a state-of
the-art 54% success rate on the WebArena-Lite benchmark.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.09533v1' target='_blank'>Large Language Models for Multi-Facility Location Mechanism Design</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nguyen Thach, Fei Liu, Houyu Zhou, Hau Chan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-12 16:49:56</h6>
<p class='card-text'>Designing strategyproof mechanisms for multi-facility location that optimize
social costs based on agent preferences had been challenging due to the
extensive domain knowledge required and poor worst-case guarantees. Recently,
deep learning models have been proposed as alternatives. However, these models
require some domain knowledge and extensive hyperparameter tuning as well as
lacking interpretability, which is crucial in practice when transparency of the
learned mechanisms is mandatory. In this paper, we introduce a novel approach,
named LLMMech, that addresses these limitations by incorporating large language
models (LLMs) into an evolutionary framework for generating interpretable,
hyperparameter-free, empirically strategyproof, and nearly optimal mechanisms.
Our experimental results, evaluated on various problem settings where the
social cost is arbitrarily weighted across agents and the agent preferences may
not be uniformly distributed, demonstrate that the LLM-generated mechanisms
generally outperform existing handcrafted baselines and deep learning models.
Furthermore, the mechanisms exhibit impressive generalizability to
out-of-distribution agent preferences and to larger instances with more agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.09501v1' target='_blank'>ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziyu Wan, Yunxiang Li, Yan Song, Hanjing Wang, Linyi Yang, Mark Schmidt, Jun Wang, Weinan Zhang, Shuyue Hu, Ying Wen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-12 16:05:31</h6>
<p class='card-text'>Recent research on Reasoning of Large Language Models (LLMs) has sought to
further enhance their performance by integrating meta-thinking -- enabling
models to monitor, evaluate, and control their reasoning processes for more
adaptive and effective problem-solving. However, current single-agent work
lacks a specialized design for acquiring meta-thinking, resulting in low
efficacy. To address this challenge, we introduce Reinforced Meta-thinking
Agents (ReMA), a novel framework that leverages Multi-Agent Reinforcement
Learning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think
about thinking. ReMA decouples the reasoning process into two hierarchical
agents: a high-level meta-thinking agent responsible for generating strategic
oversight and plans, and a low-level reasoning agent for detailed executions.
Through iterative reinforcement learning with aligned objectives, these agents
explore and learn collaboration, leading to improved generalization and
robustness. Experimental results demonstrate that ReMA outperforms single-agent
RL baselines on complex reasoning tasks, including competitive-level
mathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation
studies further illustrate the evolving dynamics of each distinct agent,
providing valuable insights into how the meta-thinking reasoning process
enhances the reasoning capabilities of LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.09263v1' target='_blank'>COLA: A Scalable Multi-Agent Framework For Windows UI Task Automation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Di Zhao, Longhui Ma, Siwei Wang, Miao Wang, Zhao Lv</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-12 11:03:27</h6>
<p class='card-text'>With the rapid advancements in Large Language Models (LLMs), an increasing
number of studies have leveraged LLMs as the cognitive core of agents to
address complex task decision-making challenges. Specially, recent research has
demonstrated the potential of LLM-based agents on automating Windows GUI
operations. However, existing methodologies exhibit two critical challenges:
(1) static agent architectures fail to dynamically adapt to the heterogeneous
requirements of OS-level tasks, leading to inadequate scenario
generalization;(2) the agent workflows lack fault tolerance mechanism,
necessitating complete process re-execution for UI agent decision error. To
address these limitations, we introduce \textit{COLA}, a collaborative
multi-agent framework for automating Windows UI operations. In this framework,
a scenario-aware agent Task Scheduler decomposes task requirements into atomic
capability units, dynamically selects the optimal agent from a decision agent
pool, effectively responds to the capability requirements of diverse scenarios.
The decision agent pool supports plug-and-play expansion for enhanced
flexibility. In addition, we design a memory unit equipped to all agents for
their self-evolution. Furthermore, we develop an interactive backtracking
mechanism that enables human to intervene to trigger state rollbacks for
non-destructive process repair. Our experimental results on the GAIA benchmark
demonstrates that the \textit{COLA} framework achieves state-of-the-art
performance with an average score of 31.89\%, significantly outperforming
baseline approaches without web API integration. Ablation studies further
validate the individual contributions of our dynamic scheduling. The code is
available at https://github.com/Alokia/COLA-demo.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.09150v1' target='_blank'>AdaptAI: A Personalized Solution to Sense Your Stress, Fix Your Mess,
  and Boost Productivity</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rushiraj Gadhvi, Soham Petkar, Priyansh Desai, Shreyas Ramachandran, Siddharth Siddharth</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-12 08:25:58</h6>
<p class='card-text'>Personalization is a critical yet often overlooked factor in boosting
productivity and wellbeing in knowledge-intensive workplaces to better address
individual preferences. Existing tools typically offer uniform guidance whether
auto-generating email responses or prompting break reminders without accounting
for individual behavioral patterns or stress triggers. We introduce AdaptAI, a
multimodal AI solution combining egocentric vision and audio, heart and motion
activities, and the agentic workflow of Large Language Models LLMs to deliver
highly personalized productivity support and context-aware well-being
interventions. AdaptAI not only automates peripheral tasks (e.g. drafting
succinct document summaries, replying to emails etc.) but also continuously
monitors the users unique physiological and situational indicators to
dynamically tailor interventions such as micro-break suggestions or exercise
prompts, at the exact point of need. In a preliminary study with 15
participants, AdaptAI demonstrated significant improvements in task throughput
and user satisfaction by anticipating user stressors and streamlining daily
workflows.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.09089v1' target='_blank'>LocAgent: Graph-Guided LLM Agents for Code Localization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhaoling Chen, Xiangru Tang, Gangda Deng, Fang Wu, Jialong Wu, Zhiwei Jiang, Viktor Prasanna, Arman Cohan, Xingyao Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-12 05:55:01</h6>
<p class='card-text'>Code localization--identifying precisely where in a codebase changes need to
be made--is a fundamental yet challenging task in software maintenance.
Existing approaches struggle to efficiently navigate complex codebases when
identifying relevant code sections. The challenge lies in bridging natural
language problem descriptions with the appropriate code elements, often
requiring reasoning across hierarchical structures and multiple dependencies.
We introduce LocAgent, a framework that addresses code localization through
graph-based representation. By parsing codebases into directed heterogeneous
graphs, LocAgent creates a lightweight representation that captures code
structures (files, classes, functions) and their dependencies (imports,
invocations, inheritance), enabling LLM agents to effectively search and locate
relevant entities through powerful multi-hop reasoning. Experimental results on
real-world benchmarks demonstrate that our approach significantly enhances
accuracy in code localization. Notably, our method with the fine-tuned
Qwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA
proprietary models at greatly reduced cost (approximately 86% reduction),
reaching up to 92.7% accuracy on file-level localization while improving
downstream GitHub issue resolution success rates by 12% for multiple attempts
(Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.09035v1' target='_blank'>ManeuverGPT Agentic Control for Safe Autonomous Stunt Maneuvers</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shawn Azdam, Pranav Doma, Aliasghar Moj Arab</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-12 03:51:41</h6>
<p class='card-text'>The next generation of active safety features in autonomous vehicles should
be capable of safely executing evasive hazard-avoidance maneuvers akin to those
performed by professional stunt drivers to achieve high-agility motion at the
limits of vehicle handling. This paper presents a novel framework, ManeuverGPT,
for generating and executing high-dynamic stunt maneuvers in autonomous
vehicles using large language model (LLM)-based agents as controllers. We
target aggressive maneuvers, such as J-turns, within the CARLA simulation
environment and demonstrate an iterative, prompt-based approach to refine
vehicle control parameters, starting tabula rasa without retraining model
weights. We propose an agentic architecture comprised of three specialized
agents (1) a Query Enricher Agent for contextualizing user commands, (2) a
Driver Agent for generating maneuver parameters, and (3) a Parameter Validator
Agent that enforces physics-based and safety constraints. Experimental results
demonstrate successful J-turn execution across multiple vehicle models through
textual prompts that adapt to differing vehicle dynamics. We evaluate
performance via established success criteria and discuss limitations regarding
numeric precision and scenario complexity. Our findings underscore the
potential of LLM-driven control for flexible, high-dynamic maneuvers, while
highlighting the importance of hybrid approaches that combine language-based
reasoning with algorithmic validation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08931v1' target='_blank'>ARCHED: A Human-Centered Framework for Transparent, Responsible, and
  Collaborative AI-Assisted Instructional Design</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hongming Li, Yizirui Fang, Shan Zhang, Seiyon M. Lee, Yiming Wang, Mark Trexler, Anthony F. Botelho</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 22:19:46</h6>
<p class='card-text'>Integrating Large Language Models (LLMs) in educational technology presents
unprecedented opportunities to improve instructional design (ID), yet existing
approaches often prioritize automation over pedagogical rigor and human agency.
This paper introduces ARCHED (AI for Responsible, Collaborative, Human-centered
Education Instructional Design), a structured multi-stage framework that
ensures human educators remain central in the design process while leveraging
AI capabilities. Unlike traditional AI-generated instructional materials that
lack transparency, ARCHED employs a cascaded workflow aligned with Bloom's
taxonomy. The framework integrates specialized AI agents - one generating
diverse pedagogical options and another evaluating alignment with learning
objectives - while maintaining educators as primary decision-makers. This
approach addresses key limitations in current AI-assisted instructional design,
ensuring transparency, pedagogical foundation, and meaningful human agency.
Empirical evaluations demonstrate that ARCHED enhances instructional design
quality while preserving educator oversight, marking a step forward in
responsible AI integration in education.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08683v1' target='_blank'>CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous
  Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Changxing Liu, Genjia Liu, Zijun Wang, Jinchang Yang, Siheng Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 17:58:42</h6>
<p class='card-text'>Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise
for improving safety by addressing the perception and prediction uncertainties
inherent in single-agent systems. However, traditional cooperative methods are
constrained by rigid collaboration protocols and limited generalization to
unseen interactive scenarios. While LLM-based approaches offer generalized
reasoning capabilities, their challenges in spatial planning and unstable
inference latency hinder their direct application in cooperative driving. To
address these limitations, we propose CoLMDriver, the first full-pipeline
LLM-based cooperative driving system, enabling effective language-based
negotiation and real-time driving control. CoLMDriver features a parallel
driving pipeline with two key components: (i) an LLM-based negotiation module
under an actor-critic paradigm, which continuously refines cooperation policies
through feedback from previous decisions of all vehicles; and (ii) an
intention-guided waypoint generator, which translates negotiation outcomes into
executable waypoints. Additionally, we introduce InterDrive, a CARLA-based
simulation benchmark comprising 10 challenging interactive driving scenarios
for evaluating V2V cooperation. Experimental results demonstrate that
CoLMDriver significantly outperforms existing approaches, achieving an 11%
higher success rate across diverse highly interactive V2V driving scenarios.
Code will be released on https://github.com/cxliu0314/CoLMDriver.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08604v1' target='_blank'>EMMOE: A Comprehensive Benchmark for Embodied Mobile Manipulation in
  Open Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dongping Li, Tielong Cai, Tianci Tang, Wenhao Chai, Katherine Rose Driggs-Campbell, Gaoang Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 16:42:36</h6>
<p class='card-text'>Developing autonomous home robots controlled by natural language has long
been a pursuit of human. While advancements in large language models (LLMs) and
embodied intelligence make this goal closer, several challenges persist: the
lack of a unified benchmark for more complex robot tasks, limited evaluation
methods and metrics, data incompatibility between LLMs and mobile manipulation
trajectories. To address these issues, we introduce Embodied Mobile
Manipulation in Open Environments (EMMOE), which requires agents to interpret
user instructions and execute long-horizon everyday tasks in continuous space.
EMMOE seamlessly integrates high-level and low-level embodied tasks into a
unified framework, along with three new metrics for more diverse assessment.
Additionally, we collect EMMOE-100, which features in various task attributes,
detailed process annotations, re-plans after failures, and two sub-datasets for
LLM training. Furthermore, we design HomieBot, a sophisticated agent system
consists of LLM with Direct Preference Optimization (DPO), light weighted
navigation and manipulation models, and multiple error detection mechanisms.
Finally, we demonstrate HomieBot's performance and the evaluation of different
models and policies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08525v1' target='_blank'>GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based
  VLM Agent Training</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tong Wei, Yijun Yang, Junliang Xing, Yuanchun Shi, Zongqing Lu, Deheng Ye</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 15:17:02</h6>
<p class='card-text'>Reinforcement learning with verifiable outcome rewards (RLVR) has effectively
scaled up chain-of-thought (CoT) reasoning in large language models (LLMs).
Yet, its efficacy in training vision-language model (VLM) agents for
goal-directed action reasoning in visual environments is less established. This
work investigates this problem through extensive experiments on complex card
games, such as 24 points, and embodied tasks from ALFWorld. We find that when
rewards are based solely on action outcomes, RL fails to incentivize CoT
reasoning in VLMs, instead leading to a phenomenon we termed thought collapse,
characterized by a rapid loss of diversity in the agent's thoughts,
state-irrelevant and incomplete reasoning, and subsequent invalid actions,
resulting in negative rewards. To counteract thought collapse, we highlight the
necessity of process guidance and propose an automated corrector that evaluates
and refines the agent's reasoning at each RL step. This simple and scalable GTR
(Guided Thought Reinforcement) framework trains reasoning and action
simultaneously without the need for dense, per-step human labeling. Our
experiments demonstrate that GTR significantly enhances the performance and
generalization of the LLaVA-7b model across various visual environments,
achieving 3-5 times higher task success rates compared to SoTA models with
notably smaller model sizes.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08506v1' target='_blank'>ReviewAgents: Bridging the Gap Between Human and AI-Generated Paper
  Reviews</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xian Gao, Jiacheng Ruan, Jingsheng Gao, Ting Liu, Yuzhuo Fu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 14:56:58</h6>
<p class='card-text'>Academic paper review is a critical yet time-consuming task within the
research community. With the increasing volume of academic publications,
automating the review process has become a significant challenge. The primary
issue lies in generating comprehensive, accurate, and reasoning-consistent
review comments that align with human reviewers' judgments. In this paper, we
address this challenge by proposing ReviewAgents, a framework that leverages
large language models (LLMs) to generate academic paper reviews. We first
introduce a novel dataset, Review-CoT, consisting of 142k review comments,
designed for training LLM agents. This dataset emulates the structured
reasoning process of human reviewers-summarizing the paper, referencing
relevant works, identifying strengths and weaknesses, and generating a review
conclusion. Building upon this, we train LLM reviewer agents capable of
structured reasoning using a relevant-paper-aware training method. Furthermore,
we construct ReviewAgents, a multi-role, multi-LLM agent review framework, to
enhance the review comment generation process. Additionally, we propose
ReviewBench, a benchmark for evaluating the review comments generated by LLMs.
Our experimental results on ReviewBench demonstrate that while existing LLMs
exhibit a certain degree of potential for automating the review process, there
remains a gap when compared to human-generated reviews. Moreover, our
ReviewAgents framework further narrows this gap, outperforming advanced LLMs in
generating review comments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08308v1' target='_blank'>Seeing and Reasoning with Confidence: Supercharging Multimodal LLMs with
  an Uncertainty-Aware Agentic Framework</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhuo Zhi, Chen Feng, Adam Daneshmend, Mine Orlu, Andreas Demosthenous, Lu Yin, Da Li, Ziquan Liu, Miguel R. D. Rodrigues</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 11:18:53</h6>
<p class='card-text'>Multimodal large language models (MLLMs) show promise in tasks like visual
question answering (VQA) but still face challenges in multimodal reasoning.
Recent works adapt agentic frameworks or chain-of-thought (CoT) reasoning to
improve performance. However, CoT-based multimodal reasoning often demands
costly data annotation and fine-tuning, while agentic approaches relying on
external tools risk introducing unreliable output from these tools. In this
paper, we propose Seeing and Reasoning with Confidence (SRICE), a training-free
multimodal reasoning framework that integrates external vision models with
uncertainty quantification (UQ) into an MLLM to address these challenges.
Specifically, SRICE guides the inference process by allowing MLLM to
autonomously select regions of interest through multi-stage interactions with
the help of external tools. We propose to use a conformal prediction-based
approach to calibrate the output of external tools and select the optimal tool
by estimating the uncertainty of an MLLM's output. Our experiment shows that
the average improvement of SRICE over the base MLLM is 4.6% on five datasets
and the performance on some datasets even outperforms fine-tuning-based
methods, revealing the significance of ensuring reliable tool use in an MLLM
agent.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08302v1' target='_blank'>General-Purpose Aerial Intelligent Agents Empowered by Large Language
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ji Zhao, Xiao Lin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 11:13:58</h6>
<p class='card-text'>The emergence of large language models (LLMs) opens new frontiers for
unmanned aerial vehicle (UAVs), yet existing systems remain confined to
predefined tasks due to hardware-software co-design challenges. This paper
presents the first aerial intelligent agent capable of open-world task
execution through tight integration of LLM-based reasoning and robotic
autonomy. Our hardware-software co-designed system addresses two fundamental
limitations: (1) Onboard LLM operation via an edge-optimized computing
platform, achieving 5-6 tokens/sec inference for 14B-parameter models at 220W
peak power; (2) A bidirectional cognitive architecture that synergizes slow
deliberative planning (LLM task planning) with fast reactive control (state
estimation, mapping, obstacle avoidance, and motion planning). Validated
through preliminary results using our prototype, the system demonstrates
reliable task planning and scene understanding in communication-constrained
environments, such as sugarcane monitoring, power grid inspection, mine tunnel
exploration, and biological observation applications. This work establishes a
novel framework for embodied aerial artificial intelligence, bridging the gap
between task planning and robotic autonomy in open environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08199v1' target='_blank'>A Cascading Cooperative Multi-agent Framework for On-ramp Merging
  Control Integrating Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Miao Zhang, Zhenlong Fang, Tianyi Wang, Qian Zhang, Shuai Lu, Junfeng Jiao, Tianyu Shi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 09:08:04</h6>
<p class='card-text'>Traditional Reinforcement Learning (RL) suffers from replicating human-like
behaviors, generalizing effectively in multi-agent scenarios, and overcoming
inherent interpretability issues.These tasks are compounded when deep
environment understanding, agent coordination and dynamic optimization are
required. While Large Language Model (LLM) enhanced methods have shown promise
in generalization and interoperability, they often neglect necessary
multi-agent coordination. Therefore, we introduce the Cascading Cooperative
Multi-agent (CCMA) framework, integrating RL for individual interactions, a
fine-tuned LLM for regional cooperation, a reward function for global
optimization, and the Retrieval-augmented Generation mechanism to dynamically
optimize decision-making across complex driving scenarios. Our experiments
demonstrate that the CCMA outperforms existing RL methods, demonstrating
significant improvements in both micro and macro-level performance in complex
driving environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08193v1' target='_blank'>Guess What I am Thinking: A Benchmark for Inner Thought Reasoning of
  Role-Playing Language Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rui Xu, MingYu Wang, XinTao Wang, Dakuan Lu, Xiaoyu Tan, Wei Chu, Yinghui Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 08:57:07</h6>
<p class='card-text'>Recent advances in LLM-based role-playing language agents (RPLAs) have
attracted broad attention in various applications. While chain-of-thought
reasoning has shown importance in many tasks for LLMs, the internal thinking
processes of RPLAs remain unexplored. Understanding characters' inner thoughts
is crucial for developing advanced RPLAs. In this paper, we introduce
ROLETHINK, a novel benchmark constructed from literature for evaluating
character thought generation. We propose the task of inner thought reasoning,
which includes two sets: the gold set that compares generated thoughts with
original character monologues, and the silver set that uses expert synthesized
character analyses as references. To address this challenge, we propose MIRROR,
a chain-of-thought approach that generates character thoughts by retrieving
memories, predicting character reactions, and synthesizing motivations. Through
extensive experiments, we demonstrate the importance of inner thought reasoning
for RPLAs, and MIRROR consistently outperforms existing methods. Resources are
available at https://github.com/airaer1998/RPA_Thought.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08175v1' target='_blank'>Privacy-Enhancing Paradigms within Federated Multi-Agent Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zitong Shi, Guancheng Wan, Wenke Huang, Guibin Zhang, Jiawei Shao, Mang Ye, Carl Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 08:38:45</h6>
<p class='card-text'>LLM-based Multi-Agent Systems (MAS) have proven highly effective in solving
complex problems by integrating multiple agents, each performing different
roles. However, in sensitive domains, they face emerging privacy protection
challenges. In this paper, we introduce the concept of Federated MAS,
highlighting the fundamental differences between Federated MAS and traditional
FL. We then identify key challenges in developing Federated MAS, including: 1)
heterogeneous privacy protocols among agents, 2) structural differences in
multi-party conversations, and 3) dynamic conversational network structures. To
address these challenges, we propose Embedded Privacy-Enhancing Agents
(EPEAgent), an innovative solution that integrates seamlessly into the
Retrieval-Augmented Generation (RAG) phase and the context retrieval stage.
This solution minimizes data flows, ensuring that only task-relevant,
agent-specific information is shared. Additionally, we design and generate a
comprehensive dataset to evaluate the proposed paradigm. Extensive experiments
demonstrate that EPEAgent effectively enhances privacy protection while
maintaining strong system performance. The code will be availiable at
https://github.com/ZitongShi/EPEAgent</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08147v1' target='_blank'>FilmComposer: LLM-Driven Music Production for Silent Film Clips</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhifeng Xie, Qile He, Youjia Zhu, Qiwei He, Mengtian Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 08:05:11</h6>
<p class='card-text'>In this work, we implement music production for silent film clips using
LLM-driven method. Given the strong professional demands of film music
production, we propose the FilmComposer, simulating the actual workflows of
professional musicians. FilmComposer is the first to combine large generative
models with a multi-agent approach, leveraging the advantages of both waveform
music and symbolic music generation. Additionally, FilmComposer is the first to
focus on the three core elements of music production for film-audio quality,
musicality, and musical development-and introduces various controls, such as
rhythm, semantics, and visuals, to enhance these key aspects. Specifically,
FilmComposer consists of the visual processing module, rhythm-controllable
MusicGen, and multi-agent assessment, arrangement and mix. In addition, our
framework can seamlessly integrate into the actual music production pipeline
and allows user intervention in every step, providing strong interactivity and
a high degree of creative freedom. Furthermore, we propose MusicPro-7k which
includes 7,418 film clips, music, description, rhythm spots and main melody,
considering the lack of a professional and high-quality film music dataset.
Finally, both the standard metrics and the new specialized metrics we propose
demonstrate that the music generated by our model achieves state-of-the-art
performance in terms of quality, consistency with video, diversity, musicality,
and musical development. Project page:
https://apple-jun.github.io/FilmComposer.github.io/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08123v1' target='_blank'>LLM4MAC: An LLM-Driven Reinforcement Learning Framework for MAC Protocol
  Emergence</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Renxuan Tan, Rongpeng Li, Zhifeng Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 07:38:14</h6>
<p class='card-text'>With the advent of 6G systems, emerging hyper-connected ecosystems
necessitate agile and adaptive medium access control (MAC) protocols to contend
with network dynamics and diverse service requirements. We propose LLM4MAC, a
novel framework that harnesses large language models (LLMs) within a
reinforcement learning paradigm to drive MAC protocol emergence. By
reformulating uplink data transmission scheduling as a semantics-generalized
partially observable Markov game (POMG), LLM4MAC encodes network operations in
natural language, while proximal policy optimization (PPO) ensures continuous
alignment with the evolving network dynamics. A structured identity embedding
(SIE) mechanism further enables robust coordination among heterogeneous agents.
Extensive simulations demonstrate that on top of a compact LLM, which is
purposefully selected to balance performance with resource efficiency, the
protocol emerging from LLM4MAC outperforms comparative baselines in throughput
and generalization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08102v2' target='_blank'>AI-native Memory 2.0: Second Me</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiale Wei, Xiang Ying, Tao Gao, Fangyi Bao, Felix Tao, Jingbo Shang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 07:05:52</h6>
<p class='card-text'>Human interaction with the external world fundamentally involves the exchange
of personal memory, whether with other individuals, websites, applications, or,
in the future, AI agents. A significant portion of this interaction is
redundant, requiring users to repeatedly provide the same information across
different contexts. Existing solutions, such as browser-stored credentials,
autofill mechanisms, and unified authentication systems, have aimed to mitigate
this redundancy by serving as intermediaries that store and retrieve commonly
used user data. The advent of large language models (LLMs) presents an
opportunity to redefine memory management through an AI-native paradigm: SECOND
ME. SECOND ME acts as an intelligent, persistent memory offload system that
retains, organizes, and dynamically utilizes user-specific knowledge. By
serving as an intermediary in user interactions, it can autonomously generate
context-aware responses, prefill required information, and facilitate seamless
communication with external systems, significantly reducing cognitive load and
interaction friction. Unlike traditional memory storage solutions, SECOND ME
extends beyond static data retention by leveraging LLM-based memory
parameterization. This enables structured organization, contextual reasoning,
and adaptive knowledge retrieval, facilitating a more systematic and
intelligent approach to memory management. As AI-driven personal agents like
SECOND ME become increasingly integrated into digital ecosystems, SECOND ME
further represents a critical step toward augmenting human-world interaction
with persistent, contextually aware, and self-optimizing memory systems. We
have open-sourced the fully localizable deployment system at GitHub:
https://github.com/Mindverse/Second-Me.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08026v1' target='_blank'>In Prospect and Retrospect: Reflective Memory Management for Long-term
  Personalized Dialogue Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhen Tan, Jun Yan, I-Hung Hsu, Rujun Han, Zifeng Wang, Long T. Le, Yiwen Song, Yanfei Chen, Hamid Palangi, George Lee, Anand Iyer, Tianlong Chen, Huan Liu, Chen-Yu Lee, Tomas Pfister</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 04:15:52</h6>
<p class='card-text'>Large Language Models (LLMs) have made significant progress in open-ended
dialogue, yet their inability to retain and retrieve relevant information from
long-term interactions limits their effectiveness in applications requiring
sustained personalization. External memory mechanisms have been proposed to
address this limitation, enabling LLMs to maintain conversational continuity.
However, existing approaches struggle with two key challenges. First, rigid
memory granularity fails to capture the natural semantic structure of
conversations, leading to fragmented and incomplete representations. Second,
fixed retrieval mechanisms cannot adapt to diverse dialogue contexts and user
interaction patterns. In this work, we propose Reflective Memory Management
(RMM), a novel mechanism for long-term dialogue agents, integrating forward-
and backward-looking reflections: (1) Prospective Reflection, which dynamically
summarizes interactions across granularities-utterances, turns, and
sessions-into a personalized memory bank for effective future retrieval, and
(2) Retrospective Reflection, which iteratively refines the retrieval in an
online reinforcement learning (RL) manner based on LLMs' cited evidence.
Experiments show that RMM demonstrates consistent improvement across various
metrics and benchmarks. For example, RMM shows more than 10% accuracy
improvement over the baseline without memory management on the LongMemEval
dataset.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07826v1' target='_blank'>Magnet: Multi-turn Tool-use Data Synthesis and Distillation via Graph
  Translation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fan Yin, Zifeng Wang, I-Hung Hsu, Jun Yan, Ke Jiang, Yanfei Chen, Jindong Gu, Long T. Le, Kai-Wei Chang, Chen-Yu Lee, Hamid Palangi, Tomas Pfister</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 20:13:07</h6>
<p class='card-text'>Large language models (LLMs) have exhibited the ability to effectively
utilize external tools to address user queries. However, their performance may
be limited in complex, multi-turn interactions involving users and multiple
tools. To address this, we propose Magnet, a principled framework for
synthesizing high-quality training trajectories to enhance the function calling
capability of large language model agents in multi-turn conversations with
humans. The framework is based on automatic and iterative translations from a
function signature path to a sequence of queries and executable function calls.
We model the complicated function interactions in multi-turn cases with graph
and design novel node operations to build reliable signature paths. Motivated
by context distillation, when guiding the generation of positive and negative
trajectories using a teacher model, we provide reference function call
sequences as positive hints in context and contrastive, incorrect function
calls as negative hints. Experiments show that training with the positive
trajectories with supervised fine-tuning and preference optimization against
negative trajectories, our 14B model, Magnet-14B-mDPO, obtains 68.01 on BFCL-v3
and 73.30 on ToolQuery, surpassing the performance of the teacher model
Gemini-1.5-pro-002 by a large margin in function calling.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07693v1' target='_blank'>Fully Autonomous Programming using Iterative Multi-Agent Debugging with
  Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Anastasiia Grishina, Vadim Liventsev, Aki Härmä, Leon Moonen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 16:56:51</h6>
<p class='card-text'>Program synthesis with Large Language Models (LLMs) suffers from a "near-miss
syndrome": the generated code closely resembles a correct solution but fails
unit tests due to minor errors. We address this with a multi-agent framework
called Synthesize, Execute, Instruct, Debug, and Repair (SEIDR). Effectively
applying SEIDR to instruction-tuned LLMs requires determining (a) optimal
prompts for LLMs, (b) what ranking algorithm selects the best programs in
debugging rounds, and (c) balancing the repair of unsuccessful programs with
the generation of new ones. We empirically explore these trade-offs by
comparing replace-focused, repair-focused, and hybrid debug strategies. We also
evaluate lexicase and tournament selection to rank candidates in each
generation. On Program Synthesis Benchmark 2 (PSB2), our framework outperforms
both conventional use of OpenAI Codex without a repair phase and traditional
genetic programming approaches. SEIDR outperforms the use of an LLM alone,
solving 18 problems in C++ and 20 in Python on PSB2 at least once across
experiments. To assess generalizability, we employ GPT-3.5 and Llama 3 on the
PSB2 and HumanEval-X benchmarks. Although SEIDR with these models does not
surpass current state-of-the-art methods on the Python benchmarks, the results
on HumanEval-C++ are promising. SEIDR with Llama 3-8B achieves an average
pass@100 of 84.2%. Across all SEIDR runs, 163 of 164 problems are solved at
least once with GPT-3.5 in HumanEval-C++, and 162 of 164 with the smaller Llama
3-8B. We conclude that SEIDR effectively overcomes the near-miss syndrome in
program synthesis with LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07459v1' target='_blank'>MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for
  Complex Medical Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiangru Tang, Daniel Shao, Jiwoong Sohn, Jiapeng Chen, Jiayi Zhang, Jinyu Xiang, Fang Wu, Yilun Zhao, Chenglin Wu, Wenqi Shi, Arman Cohan, Mark Gerstein</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 15:38:44</h6>
<p class='card-text'>Large Language Models (LLMs) have shown impressive performance on existing
medical question-answering benchmarks. This high performance makes it
increasingly difficult to meaningfully evaluate and differentiate advanced
methods. We present MedAgentsBench, a benchmark that focuses on challenging
medical questions requiring multi-step clinical reasoning, diagnosis
formulation, and treatment planning-scenarios where current models still
struggle despite their strong performance on standard tests. Drawing from seven
established medical datasets, our benchmark addresses three key limitations in
existing evaluations: (1) the prevalence of straightforward questions where
even base models achieve high performance, (2) inconsistent sampling and
evaluation protocols across studies, and (3) lack of systematic analysis of the
interplay between performance, cost, and inference time. Through experiments
with various base models and reasoning methods, we demonstrate that the latest
thinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in
complex medical reasoning tasks. Additionally, advanced search-based agent
methods offer promising performance-to-cost ratios compared to traditional
approaches. Our analysis reveals substantial performance gaps between model
families on complex questions and identifies optimal model selections for
different computational constraints. Our benchmark and evaluation framework are
publicly available at https://github.com/gersteinlab/medagents-benchmark.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07457v1' target='_blank'>LLMs syntactically adapt their language use to their conversational
  partner</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Florian Kandra, Vera Demberg, Alexander Koller</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 15:37:07</h6>
<p class='card-text'>It has been frequently observed that human speakers align their language use
with each other during conversations. In this paper, we study empirically
whether large language models (LLMs) exhibit the same behavior of
conversational adaptation. We construct a corpus of conversations between LLMs
and find that two LLM agents end up making more similar syntactic choices as
conversations go on, confirming that modern LLMs adapt their language use to
their conversational partners in at least a rudimentary way.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07323v1' target='_blank'>Dynamic Path Navigation for Motion Agents with LLM Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yubo Zhao, Qi Wu, Yifan Wang, Yu-Wing Tai, Chi-Keung Tang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 13:39:09</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated strong generalizable reasoning
and planning capabilities. However, their efficacies in spatial path planning
and obstacle-free trajectory generation remain underexplored. Leveraging LLMs
for navigation holds significant potential, given LLMs' ability to handle
unseen scenarios, support user-agent interactions, and provide global control
across complex systems, making them well-suited for agentic planning and
humanoid motion generation. As one of the first studies in this domain, we
explore the zero-shot navigation and path generation capabilities of LLMs by
constructing a dataset and proposing an evaluation protocol. Specifically, we
represent paths using anchor points connected by straight lines, enabling
movement in various directions. This approach offers greater flexibility and
practicality compared to previous methods while remaining simple and intuitive
for LLMs. We demonstrate that, when tasks are well-structured in this manner,
modern LLMs exhibit substantial planning proficiency in avoiding obstacles
while autonomously refining navigation with the generated motion to reach the
target. Further, this spatial reasoning ability of a single LLM motion agent
interacting in a static environment can be seamlessly generalized in
multi-motion agents coordination in dynamic environments. Unlike traditional
approaches that rely on single-step planning or local policies, our
training-free LLM-based method enables global, dynamic, closed-loop planning,
and autonomously resolving collision issues.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07320v1' target='_blank'>Experimental Exploration: Investigating Cooperative Interaction Behavior
  Between Humans and Large Language Model Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guanxuan Jiang, Yuyang Wang, Pan Hui</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 13:37:36</h6>
<p class='card-text'>With the rise of large language models (LLMs), AI agents as autonomous
decision-makers present significant opportunities and challenges for human-AI
cooperation. While many studies have explored human cooperation with AI as
tools, the role of LLM-augmented autonomous agents in competitive-cooperative
interactions remains under-examined. This study investigates human cooperative
behavior by engaging 30 participants who interacted with LLM agents exhibiting
different characteristics (purported human, purported rule-based AI agent, and
LLM agent) in repeated Prisoner's Dilemma games. Findings show significant
differences in cooperative behavior based on the agents' purported
characteristics and the interaction effect of participants' genders and
purported characteristics. We also analyzed human response patterns, including
game completion time, proactive favorable behavior, and acceptance of repair
efforts. These insights offer a new perspective on human interactions with LLM
agents in competitive cooperation contexts, such as virtual avatars or future
physical entities. The study underscores the importance of understanding human
biases toward AI agents and how observed behaviors can influence future
human-AI cooperation dynamics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07314v1' target='_blank'>Automated Movie Generation via Multi-Agent CoT Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weijia Wu, Zeyu Zhu, Mike Zheng Shou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 13:33:27</h6>
<p class='card-text'>Existing long-form video generation frameworks lack automated planning,
requiring manual input for storylines, scenes, cinematography, and character
interactions, resulting in high costs and inefficiencies. To address these
challenges, we present MovieAgent, an automated movie generation via
multi-agent Chain of Thought (CoT) planning. MovieAgent offers two key
advantages: 1) We firstly explore and define the paradigm of automated
movie/long-video generation. Given a script and character bank, our MovieAgent
can generates multi-scene, multi-shot long-form videos with a coherent
narrative, while ensuring character consistency, synchronized subtitles, and
stable audio throughout the film. 2) MovieAgent introduces a hierarchical
CoT-based reasoning process to automatically structure scenes, camera settings,
and cinematography, significantly reducing human effort. By employing multiple
LLM agents to simulate the roles of a director, screenwriter, storyboard
artist, and location manager, MovieAgent streamlines the production pipeline.
Experiments demonstrate that MovieAgent achieves new state-of-the-art results
in script faithfulness, character consistency, and narrative coherence. Our
hierarchical framework takes a step forward and provides new insights into
fully automated movie generation. The code and project website are available
at: https://github.com/showlab/MovieAgent and
https://weijiawu.github.io/MovieAgent.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07044v1' target='_blank'>DatawiseAgent: A Notebook-Centric LLM Agent Framework for Automated Data
  Science</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziming You, Yumiao Zhang, Dexuan Xu, Yiwei Lou, Yandong Yan, Wei Wang, Huaming Zhang, Yu Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 08:32:33</h6>
<p class='card-text'>Data Science tasks are multifaceted, dynamic, and often domain-specific.
Existing LLM-based approaches largely concentrate on isolated phases,
neglecting the interdependent nature of many data science tasks and limiting
their capacity for comprehensive end-to-end support. We propose DatawiseAgent,
a notebook-centric LLM agent framework that unifies interactions among user,
agent and the computational environment through markdown and executable code
cells, supporting flexible and adaptive automated data science. Built on a
Finite State Transducer(FST), DatawiseAgent orchestrates four stages, including
DSF-like planning, incremental execution, self-debugging, and post-filtering.
Specifically, the DFS-like planning stage systematically explores the solution
space, while incremental execution harnesses real-time feedback and
accommodates LLM's limited capabilities to progressively complete tasks. The
self-debugging and post-filtering modules further enhance reliability by
diagnosing and correcting errors and pruning extraneous information. Extensive
experiments on diverse tasks, including data analysis, visualization, and data
modeling, show that DatawiseAgent consistently outperforms or matches
state-of-the-art methods across multiple model settings. These results
highlight its potential to generalize across data science scenarios and lay the
groundwork for more efficient, fully automated workflows.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07020v1' target='_blank'>Combating Partial Perception Deficit in Autonomous Driving with
  Multimodal LLM Commonsense</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuting Hu, Chenhui Xu, Ruiyang Qin, Dancheng Liu, Amir Nassereldine, Yiyu Shi, Jinjun Xiong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 08:01:41</h6>
<p class='card-text'>Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>