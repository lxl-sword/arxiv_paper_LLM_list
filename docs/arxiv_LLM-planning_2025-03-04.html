<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>LLM-planning - 2025-03-04</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>LLM-planning - 2025-03-04</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.21087v1' target='_blank'>PASemiQA: Plan-Assisted Agent for Question Answering on Semi-Structured
  Data with Text and Relational Information</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hansi Yang, Qi Zhang, Wei Jiang, Jianguo Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-28 14:26:47</h6>
<p class='card-text'>Large language models (LLMs) have shown impressive abilities in answering
questions across various domains, but they often encounter hallucination issues
on questions that require professional and up-to-date knowledge. To address
this limitation, retrieval-augmented generation (RAG) techniques have been
proposed, which retrieve relevant information from external sources to inform
their responses. However, existing RAG methods typically focus on a single type
of external data, such as vectorized text database or knowledge graphs, and
cannot well handle real-world questions on semi-structured data containing both
text and relational information. To bridge this gap, we introduce PASemiQA, a
novel approach that jointly leverages text and relational information in
semi-structured data to answer questions. PASemiQA first generates a plan to
identify relevant text and relational information to answer the question in
semi-structured data, and then uses an LLM agent to traverse the
semi-structured data and extract necessary information. Our empirical results
demonstrate the effectiveness of PASemiQA across different semi-structured
datasets from various domains, showcasing its potential to improve the accuracy
and reliability of question answering systems on semi-structured data.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20807v1' target='_blank'>Digital Player: Evaluating Large Language Models based Human-like Agent
  in Games</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiawei Wang, Kai Wang, Shaojie Lin, Runze Wu, Bihan Xu, Lingeng Jiang, Shiwei Zhao, Renyu Zhu, Haoyu Liu, Zhipeng Hu, Zhong Fan, Le Li, Tangjie Lyu, Changjie Fan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-28 07:46:55</h6>
<p class='card-text'>With the rapid advancement of Large Language Models (LLMs), LLM-based
autonomous agents have shown the potential to function as digital employees,
such as digital analysts, teachers, and programmers. In this paper, we develop
an application-level testbed based on the open-source strategy game "Unciv",
which has millions of active players, to enable researchers to build a "data
flywheel" for studying human-like agents in the "digital players" task. This
"Civilization"-like game features expansive decision-making spaces along with
rich linguistic interactions such as diplomatic negotiations and acts of
deception, posing significant challenges for LLM-based agents in terms of
numerical reasoning and long-term planning. Another challenge for "digital
players" is to generate human-like responses for social interaction,
collaboration, and negotiation with human players. The open-source project can
be found at https:/github.com/fuxiAIlab/CivAgent.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20795v1' target='_blank'>Plan2Align: Predictive Planning Based Test-Time Preference Alignment in
  Paragraph-Level Machine Translation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kuang-Da Wang, Teng-Ruei Chen, Yu Heng Hung, Shuoyang Ding, Yueh-Hua Wu, Yu-Chiang Frank Wang, Chao-Han Huck Yang, Wen-Chih Peng, Ping-Chun Hsieh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-28 07:24:33</h6>
<p class='card-text'>Machine Translation (MT) has been predominantly designed for sentence-level
translation using transformer-based architectures. While next-token prediction
based Large Language Models (LLMs) demonstrate strong capabilities in long-text
translation, non-extensive language models often suffer from omissions and
semantic inconsistencies when processing paragraphs. Existing preference
alignment methods improve sentence-level translation but fail to ensure
coherence over extended contexts due to the myopic nature of next-token
generation. We introduce Plan2Align, a test-time alignment framework that
treats translation as a predictive planning problem, adapting Model Predictive
Control to iteratively refine translation outputs. Experiments on WMT24
Discourse-Level Literary Translation show that Plan2Align significantly
improves paragraph-level translation, achieving performance surpassing or on
par with the existing training-time and test-time alignment methods on
LLaMA-3.1 8B.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20601v1' target='_blank'>NutriGen: Personalized Meal Plan Generator Leveraging Large Language
  Models to Enhance Dietary and Nutritional Adherence</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Saman Khamesian, Asiful Arefeen, Stephanie M. Carpenter, Hassan Ghasemzadeh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-28 00:05:49</h6>
<p class='card-text'>Maintaining a balanced diet is essential for overall health, yet many
individuals struggle with meal planning due to nutritional complexity, time
constraints, and lack of dietary knowledge. Personalized food recommendations
can help address these challenges by tailoring meal plans to individual
preferences, habits, and dietary restrictions. However, existing dietary
recommendation systems often lack adaptability, fail to consider real-world
constraints such as food ingredient availability, and require extensive user
input, making them impractical for sustainable and scalable daily use. To
address these limitations, we introduce NutriGen, a framework based on large
language models (LLM) designed to generate personalized meal plans that align
with user-defined dietary preferences and constraints. By building a
personalized nutrition database and leveraging prompt engineering, our approach
enables LLMs to incorporate reliable nutritional references like the USDA
nutrition database while maintaining flexibility and ease-of-use. We
demonstrate that LLMs have strong potential in generating accurate and
user-friendly food recommendations, addressing key limitations in existing
dietary recommendation systems by providing structured, practical, and scalable
meal plans. Our evaluation shows that Llama 3.1 8B and GPT-3.5 Turbo achieve
the lowest percentage errors of 1.55\% and 3.68\%, respectively, producing meal
plans that closely align with user-defined caloric targets while minimizing
deviation and improving precision. Additionally, we compared the performance of
DeepSeek V3 against several established models to evaluate its potential in
personalized nutrition planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20508v1' target='_blank'>TripCraft: A Benchmark for Spatio-Temporally Fine Grained Travel
  Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Soumyabrata Chaudhuri, Pranav Purkar, Ritwik Raghav, Shubhojit Mallick, Manish Gupta, Abhik Jana, Shreya Ghosh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 20:33:28</h6>
<p class='card-text'>Recent advancements in probing Large Language Models (LLMs) have explored
their latent potential as personalized travel planning agents, yet existing
benchmarks remain limited in real world applicability. Existing datasets, such
as TravelPlanner and TravelPlanner+, suffer from semi synthetic data reliance,
spatial inconsistencies, and a lack of key travel constraints, making them
inadequate for practical itinerary generation. To address these gaps, we
introduce TripCraft, a spatiotemporally coherent travel planning dataset that
integrates real world constraints, including public transit schedules, event
availability, diverse attraction categories, and user personas for enhanced
personalization. To evaluate LLM generated plans beyond existing binary
validation methods, we propose five continuous evaluation metrics, namely
Temporal Meal Score, Temporal Attraction Score, Spatial Score, Ordering Score,
and Persona Score which assess itinerary quality across multiple dimensions.
Our parameter informed setting significantly enhances meal scheduling,
improving the Temporal Meal Score from 61% to 80% in a 7 day scenario.
TripCraft establishes a new benchmark for LLM driven personalized travel
planning, offering a more realistic, constraint aware framework for itinerary
generation. Dataset and Codebase will be made publicly available upon
acceptance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20284v1' target='_blank'>Evaluating Human Trust in LLM-Based Planners: A Preliminary Study</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shenghui Chen, Yunhao Yang, Kayla Boggess, Seongkook Heo, Lu Feng, Ufuk Topcu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 17:10:52</h6>
<p class='card-text'>Large Language Models (LLMs) are increasingly used for planning tasks,
offering unique capabilities not found in classical planners such as generating
explanations and iterative refinement. However, trust--a critical factor in the
adoption of planning systems--remains underexplored in the context of LLM-based
planning tasks. This study bridges this gap by comparing human trust in
LLM-based planners with classical planners through a user study in a Planning
Domain Definition Language (PDDL) domain. Combining subjective measures, such
as trust questionnaires, with objective metrics like evaluation accuracy, our
findings reveal that correctness is the primary driver of trust and
performance. Explanations provided by the LLM improved evaluation accuracy but
had limited impact on trust, while plan refinement showed potential for
increasing trust without significantly enhancing evaluation accuracy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20238v1' target='_blank'>FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through
  Reflective Puzzle Solving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guizhen Chen, Weiwen Xu, Hao Zhang, Hou Pong Chan, Chaoqun Liu, Lidong Bing, Deli Zhao, Anh Tuan Luu, Yu Rong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 16:23:25</h6>
<p class='card-text'>Many challenging reasoning tasks require not just rapid, intuitive responses,
but a more deliberate, multi-step approach. Recent progress in large language
models (LLMs) highlights an important shift from the "System 1" way of quick
reactions to the "System 2" style of reflection-and-correction problem solving.
However, current benchmarks heavily rely on the final-answer accuracy, leaving
much of a model's intermediate reasoning steps unexamined. This fails to assess
the model's ability to reflect and rectify mistakes within the reasoning
process. To bridge this gap, we introduce FINEREASON, a logic-puzzle benchmark
for fine-grained evaluation of LLMs' reasoning capabilities. Each puzzle can be
decomposed into atomic steps, making it ideal for rigorous validation of
intermediate correctness. Building on this, we introduce two tasks: state
checking, and state transition, for a comprehensive evaluation of how models
assess the current situation and plan the next move. To support broader
research, we also provide a puzzle training set aimed at enhancing performance
on general mathematical tasks. We show that models trained on our state
checking and transition data demonstrate gains in math reasoning by up to 5.1%
on GSM8K.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20175v1' target='_blank'>An Extensive Evaluation of PDDL Capabilities in off-the-shelf LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kaustubh Vyas, Damien Graux, Sébastien Montella, Pavlos Vougiouklis, Ruofei Lai, Keshuang Li, Yang Ren, Jeff Z. Pan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 15:13:07</h6>
<p class='card-text'>In recent advancements, large language models (LLMs) have exhibited
proficiency in code generation and chain-of-thought reasoning, laying the
groundwork for tackling automatic formal planning tasks. This study evaluates
the potential of LLMs to understand and generate Planning Domain Definition
Language (PDDL), an essential representation in artificial intelligence
planning. We conduct an extensive analysis across 20 distinct models spanning 7
major LLM families, both commercial and open-source. Our comprehensive
evaluation sheds light on the zero-shot LLM capabilities of parsing,
generating, and reasoning with PDDL. Our findings indicate that while some
models demonstrate notable effectiveness in handling PDDL, others pose
limitations in more complex scenarios requiring nuanced planning knowledge.
These results highlight the promise and current limitations of LLMs in formal
planning tasks, offering insights into their application and guiding future
efforts in AI-driven planning paradigms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19805v1' target='_blank'>Implicit Search via Discrete Diffusion: A Study on Chess</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiacheng Ye, Zhenyu Wu, Jiahui Gao, Zhiyong Wu, Xin Jiang, Zhenguo Li, Lingpeng Kong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 06:25:15</h6>
<p class='card-text'>In the post-AlphaGo era, there has been a renewed interest in search
techniques such as Monte Carlo Tree Search (MCTS), particularly in their
application to Large Language Models (LLMs). This renewed attention is driven
by the recognition that current next-token prediction models often lack the
ability for long-term planning. Is it possible to instill search-like abilities
within the models to enhance their planning abilities without relying on
explicit search? We propose DiffuSearch , a model that does \textit{implicit
search} by looking into the future world via discrete diffusion modeling. We
instantiate DiffuSearch on a classical board game, Chess, where explicit search
is known to be essential. Through extensive controlled experiments, we show
DiffuSearch outperforms both the searchless and explicit search-enhanced
policies. Specifically, DiffuSearch outperforms the one-step policy by 19.2%
and the MCTS-enhanced policy by 14% on action accuracy. Furthermore,
DiffuSearch demonstrates a notable 30% enhancement in puzzle-solving abilities
compared to explicit search-based policies, along with a significant 540 Elo
increase in game-playing strength assessment. These results indicate that
implicit search via discrete diffusion is a viable alternative to explicit
search over a one-step policy. All codes are publicly available at
\href{https://github.com/HKUNLP/DiffuSearch}{https://github.com/HKUNLP/DiffuSearch}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19500v1' target='_blank'>Conversational Planning for Personal Plans</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Konstantina Christakopoulou, Iris Qu, John Canny, Andrew Goodridge, Cj Adams, Minmin Chen, Maja Matarić</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 19:04:26</h6>
<p class='card-text'>The language generation and reasoning capabilities of large language models
(LLMs) have enabled conversational systems with impressive performance in a
variety of tasks, from code generation, to composing essays, to passing STEM
and legal exams, to a new paradigm for knowledge search. Besides those
short-term use applications, LLMs are increasingly used to help with real-life
goals or tasks that take a long time to complete, involving multiple sessions
across days, weeks, months, or even years. Thus to enable conversational
systems for long term interactions and tasks, we need language-based agents
that can plan for long horizons. Traditionally, such capabilities were
addressed by reinforcement learning agents with hierarchical planning
capabilities. In this work, we explore a novel architecture where the LLM acts
as the meta-controller deciding the agent's next macro-action, and tool use
augmented LLM-based option policies execute the selected macro-action. We
instantiate this framework for a specific set of macro-actions enabling
adaptive planning for users' personal plans through conversation and follow-up
questions collecting user feedback. We show how this paradigm can be applicable
in scenarios ranging from tutoring for academic and non-academic tasks to
conversational coaching for personal health plans.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19411v1' target='_blank'>Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and
  Reasoning-Driven Code Intelligence in LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dayu Yang, Tianyang Liu, Daoan Zhang, Antoine Simoulin, Xiaoyi Liu, Yuwei Cao, Zhaopu Teng, Xin Qian, Grey Yang, Jiebo Luo, Julian McAuley</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 18:55:42</h6>
<p class='card-text'>In large language models (LLMs), code and reasoning reinforce each other:
code offers an abstract, modular, and logic-driven structure that supports
reasoning, while reasoning translates high-level goals into smaller, executable
steps that drive more advanced code intelligence. In this study, we examine how
code serves as a structured medium for enhancing reasoning: it provides
verifiable execution paths, enforces logical decomposition, and enables runtime
validation. We also explore how improvements in reasoning have transformed code
intelligence from basic completion to advanced capabilities, enabling models to
address complex software engineering tasks through planning and debugging.
Finally, we identify key challenges and propose future research directions to
strengthen this synergy, ultimately improving LLM's performance in both areas.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19400v1' target='_blank'>TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem
  Understanding</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Max Ku, Thomas Chong, Jonathan Leung, Krish Shah, Alvin Yu, Wenhu Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 18:50:09</h6>
<p class='card-text'>Understanding domain-specific theorems often requires more than just
text-based reasoning; effective communication through structured visual
explanations is crucial for deeper comprehension. While large language models
(LLMs) demonstrate strong performance in text-based theorem reasoning, their
ability to generate coherent and pedagogically meaningful visual explanations
remains an open challenge. In this work, we introduce TheoremExplainAgent, an
agentic approach for generating long-form theorem explanation videos (over 5
minutes) using Manim animations. To systematically evaluate multimodal theorem
explanations, we propose TheoremExplainBench, a benchmark covering 240 theorems
across multiple STEM disciplines, along with 5 automated evaluation metrics.
Our results reveal that agentic planning is essential for generating detailed
long-form videos, and the o3-mini agent achieves a success rate of 93.8% and an
overall score of 0.77. However, our quantitative and qualitative studies show
that most of the videos produced exhibit minor issues with visual element
layout. Furthermore, multimodal explanations expose deeper reasoning flaws that
text-based explanations fail to reveal, highlighting the importance of
multimodal explanations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19295v1' target='_blank'>Complex LLM Planning via Automated Heuristics Discovery</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hongyi Ling, Shubham Parashar, Sambhav Khurana, Blake Olson, Anwesha Basu, Gaurangi Sinha, Zhengzhong Tu, James Caverlee, Shuiwang Ji</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 16:52:31</h6>
<p class='card-text'>We consider enhancing large language models (LLMs) for complex planning
tasks. While existing methods allow LLMs to explore intermediate steps to make
plans, they either depend on unreliable self-verification or external verifiers
to evaluate these steps, which demand significant data and computations. Here,
we propose automated heuristics discovery (AutoHD), a novel approach that
enables LLMs to explicitly generate heuristic functions to guide inference-time
search, allowing accurate evaluation of intermediate states. These heuristic
functions are further refined through a heuristic evolution process, improving
their robustness and effectiveness. Our proposed method requires no additional
model training or fine-tuning, and the explicit definition of heuristic
functions generated by the LLMs provides interpretability and insights into the
reasoning process. Extensive experiments across diverse benchmarks demonstrate
significant gains over multiple baselines, including nearly twice the accuracy
on some datasets, establishing our approach as a reliable and interpretable
solution for complex planning tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19135v1' target='_blank'>A Temporal Planning Framework for Multi-Agent Systems via LLM-Aided
  Knowledge Base Management</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Enrico Saccon, Ahmet Tikna, Davide De Martini, Edoardo Lamon, Luigi Palopoli, Marco Roveri</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 13:51:28</h6>
<p class='card-text'>This paper presents a novel framework, called PLANTOR (PLanning with Natural
language for Task-Oriented Robots), that integrates Large Language Models
(LLMs) with Prolog-based knowledge management and planning for multi-robot
tasks. The system employs a two-phase generation of a robot-oriented knowledge
base, ensuring reusability and compositional reasoning, as well as a three-step
planning procedure that handles temporal dependencies, resource constraints,
and parallel task execution via mixed-integer linear programming. The final
plan is converted into a Behaviour Tree for direct use in ROS2. We tested the
framework in multi-robot assembly tasks within a block world and an
arch-building scenario. Results demonstrate that LLMs can produce accurate
knowledge bases with modest human feedback, while Prolog guarantees formal
correctness and explainability. This approach underscores the potential of LLM
integration for advanced robotics tasks requiring flexible, scalable, and
human-understandable planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19103v1' target='_blank'>LongEval: A Comprehensive Analysis of Long-Text Generation Through a
  Plan-based Paradigm</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Siwei Wu, Yizhi Li, Xingwei Qu, Rishi Ravikumar, Yucheng Li, Tyler Loakman Shanghaoran Quan Xiaoyong Wei, Riza Batista-Navarro, Chenghua Lin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 12:46:36</h6>
<p class='card-text'>Large Language Models (LLMs) have achieved remarkable success in various
natural language processing tasks, yet their ability to generate long-form
content remains poorly understood and evaluated. Our analysis reveals that
current LLMs struggle with length requirements and information density in
long-text generation, with performance deteriorating as text length increases.
To quantitively locate such a performance degradation and provide further
insights on model development, we present LongEval, a benchmark that evaluates
long-text generation through both direct and plan-based generation paradigms,
inspired by cognitive and linguistic writing models. The comprehensive
experiments in this work reveal interesting findings such as that while model
size correlates with generation ability, the small-scale model (e.g.,
LongWriter), well-trained on long texts, has comparable performance. All code
and datasets are released in https://github.com/Wusiwei0410/LongEval.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.18836v1' target='_blank'>REALM-Bench: A Real-World Planning Benchmark for LLMs and Multi-Agent
  Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Longling Geng, Edward Y. Chang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 05:24:22</h6>
<p class='card-text'>This benchmark suite provides a comprehensive evaluation framework for
assessing both individual LLMs and multi-agent systems in real-world planning
scenarios. The suite encompasses eleven designed problems that progress from
basic to highly complex, incorporating key aspects such as multi-agent
coordination, inter-agent dependencies, and dynamic environmental disruptions.
Each problem can be scaled along three dimensions: the number of parallel
planning threads, the complexity of inter-dependencies, and the frequency of
unexpected disruptions requiring real-time adaptation. The benchmark includes
detailed specifications, evaluation metrics, and baseline implementations using
contemporary frameworks like LangGraph, enabling rigorous testing of both
single-agent and multi-agent planning capabilities. Through standardized
evaluation criteria and scalable complexity, this benchmark aims to drive
progress in developing more robust and adaptable AI planning systems for
real-world applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.18822v1' target='_blank'>Data-Efficient Multi-Agent Spatial Planning with LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Huangyuan Su, Aaron Walsman, Daniel Garces, Sham Kakade, Stephanie Gil</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 04:53:07</h6>
<p class='card-text'>In this project, our goal is to determine how to leverage the world-knowledge
of pretrained large language models for efficient and robust learning in
multiagent decision making. We examine this in a taxi routing and assignment
problem where agents must decide how to best pick up passengers in order to
minimize overall waiting time. While this problem is situated on a graphical
road network, we show that with the proper prompting zero-shot performance is
quite strong on this task. Furthermore, with limited fine-tuning along with the
one-at-a-time rollout algorithm for look ahead, LLMs can out-compete existing
approaches with 50 times fewer environmental interactions. We also explore the
benefits of various linguistic prompting approaches and show that including
certain easy-to-compute information in the prompt significantly improves
performance. Finally, we highlight the LLM's built-in semantic understanding,
showing its ability to adapt to environmental factors through simple prompts.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.18712v1' target='_blank'>TrajLLM: A Modular LLM-Enhanced Agent-Based Framework for Realistic
  Human Trajectory Simulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chenlu Ju, Jiaxin Liu, Shobhit Sinha, Hao Xue, Flora Salim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 00:13:26</h6>
<p class='card-text'>This work leverages Large Language Models (LLMs) to simulate human mobility,
addressing challenges like high costs and privacy concerns in traditional
models. Our hierarchical framework integrates persona generation, activity
selection, and destination prediction, using real-world demographic and
psychological data to create realistic movement patterns. Both physical models
and language models are employed to explore and demonstrate different
methodologies for human mobility simulation. By structuring data with
summarization and weighted density metrics, the system ensures scalable memory
management while retaining actionable insights. Preliminary results indicate
that LLM-driven simulations align with observed real-world patterns, offering
scalable, interpretable insights for social problems such as urban planning,
traffic management, and public health. The framework's ability to dynamically
generate personas and activities enables it to provide adaptable and realistic
daily routines. This study demonstrates the transformative potential of LLMs in
advancing mobility modeling for societal and urban applications. The source
code and interactive demo for our framework are available at
https://github.com/cju0/TrajLLM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.18690v1' target='_blank'>Hybrid Voting-Based Task Assignment in Role-Playing Games</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Daniel Weiner, Raj Korpan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-25 22:58:21</h6>
<p class='card-text'>In role-playing games (RPGs), the level of immersion is critical-especially
when an in-game agent conveys tasks, hints, or ideas to the player. For an
agent to accurately interpret the player's emotional state and contextual
nuances, a foundational level of understanding is required, which can be
achieved using a Large Language Model (LLM). Maintaining the LLM's focus across
multiple context changes, however, necessitates a more robust approach, such as
integrating the LLM with a dedicated task allocation model to guide its
performance throughout gameplay. In response to this need, we introduce
Voting-Based Task Assignment (VBTA), a framework inspired by human reasoning in
task allocation and completion. VBTA assigns capability profiles to agents and
task descriptions to tasks, then generates a suitability matrix that quantifies
the alignment between an agent's abilities and a task's requirements.
Leveraging six distinct voting methods, a pre-trained LLM, and integrating
conflict-based search (CBS) for path planning, VBTA efficiently identifies and
assigns the most suitable agent to each task. While existing approaches focus
on generating individual aspects of gameplay, such as single quests, or combat
encounters, our method shows promise when generating both unique combat
encounters and narratives because of its generalizable nature.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.18641v1' target='_blank'>WhatELSE: Shaping Narrative Spaces at Configurable Level of Abstraction
  for AI-bridged Interactive Storytelling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhuoran Lu, Qian Zhou, Yi Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-25 21:02:15</h6>
<p class='card-text'>Generative AI significantly enhances player agency in interactive narratives
(IN) by enabling just-in-time content generation that adapts to player actions.
While delegating generation to AI makes IN more interactive, it becomes
challenging for authors to control the space of possible narratives - within
which the final story experienced by the player emerges from their interaction
with AI. In this paper, we present WhatELSE, an AI-bridged IN authoring system
that creates narrative possibility spaces from example stories. WhatELSE
provides three views (narrative pivot, outline, and variants) to help authors
understand the narrative space and corresponding tools leveraging linguistic
abstraction to control the boundaries of the narrative space. Taking innovative
LLM-based narrative planning approaches, WhatELSE further unfolds the narrative
space into executable game events. Through a user study (N=12) and technical
evaluations, we found that WhatELSE enables authors to perceive and edit the
narrative space and generates engaging interactive narratives at play-time.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.18387v2' target='_blank'>How Far are LLMs from Real Search? A Comprehensive Study on Efficiency,
  Completeness, and Inherent Capabilities</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Minhua Lin, Hui Liu, Xianfeng Tang, Jingying Zeng, Zhenwei Dai, Chen Luo, Zheng Li, Xiang Zhang, Qi He, Suhang Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-25 17:30:40</h6>
<p class='card-text'>Search plays a fundamental role in problem-solving across various domains,
with most real-world decision-making problems being solvable through systematic
search. Drawing inspiration from recent discussions on search and learning, we
systematically explore the complementary relationship between search and Large
Language Models (LLMs) from three perspectives. First, we analyze how learning
can enhance search efficiency and propose Search via Learning (SeaL), a
framework that leverages LLMs for effective and efficient search. Second, we
further extend SeaL to SeaL-C to ensure rigorous completeness during search.
Our evaluation across three real-world planning tasks demonstrates that SeaL
achieves near-perfect accuracy while reducing search spaces by up to 99.1%
compared to traditional approaches. Finally, we explore how far LLMs are from
real search by investigating whether they can develop search capabilities
independently. Our analysis reveals that while current LLMs struggle with
efficient search in complex problems, incorporating systematic search
strategies significantly enhances their problem-solving capabilities. These
findings not only validate the effectiveness of our approach but also highlight
the need for improving LLMs' search abilities for real-world applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.18228v1' target='_blank'>Debt Collection Negotiations with Large Language Models: An Evaluation
  System and Optimizing Decision Making with Multi-Agent</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaofeng Wang, Zhixin Zhang, Jinguang Zheng, Yiming Ai, Rui Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-25 14:13:03</h6>
<p class='card-text'>Debt collection negotiations (DCN) are vital for managing non-performing
loans (NPLs) and reducing creditor losses. Traditional methods are
labor-intensive, while large language models (LLMs) offer promising automation
potential. However, prior systems lacked dynamic negotiation and real-time
decision-making capabilities. This paper explores LLMs in automating DCN and
proposes a novel evaluation framework with 13 metrics across 4 aspects. Our
experiments reveal that LLMs tend to over-concede compared to human
negotiators. To address this, we propose the Multi-Agent Debt Negotiation
(MADeN) framework, incorporating planning and judging modules to improve
decision rationality. We also apply post-training techniques, including DPO
with rejection sampling, to optimize performance. Our studies provide valuable
insights for practitioners and researchers seeking to enhance efficiency and
outcomes in this domain.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.18139v1' target='_blank'>LevelRAG: Enhancing Retrieval-Augmented Generation with Multi-hop Logic
  Planning over Rewriting Augmented Searchers</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhuocheng Zhang, Yang Feng, Min Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-25 12:09:16</h6>
<p class='card-text'>Retrieval-Augmented Generation (RAG) is a crucial method for mitigating
hallucinations in Large Language Models (LLMs) and integrating external
knowledge into their responses. Existing RAG methods typically employ query
rewriting to clarify the user intent and manage multi-hop logic, while using
hybrid retrieval to expand search scope. However, the tight coupling of query
rewriting to the dense retriever limits its compatibility with hybrid
retrieval, impeding further RAG performance improvements. To address this
challenge, we introduce a high-level searcher that decomposes complex queries
into atomic queries, independent of any retriever-specific optimizations.
Additionally, to harness the strengths of sparse retrievers for precise keyword
retrieval, we have developed a new sparse searcher that employs Lucene syntax
to enhance retrieval accuracy.Alongside web and dense searchers, these
components seamlessly collaborate within our proposed method,
\textbf{LevelRAG}. In LevelRAG, the high-level searcher orchestrates the
retrieval logic, while the low-level searchers (sparse, web, and dense) refine
the queries for optimal retrieval. This approach enhances both the completeness
and accuracy of the retrieval process, overcoming challenges associated with
current query rewriting techniques in hybrid retrieval scenarios. Empirical
experiments conducted on five datasets, encompassing both single-hop and
multi-hop question answering tasks, demonstrate the superior performance of
LevelRAG compared to existing RAG methods. Notably, LevelRAG outperforms the
state-of-the-art proprietary model, GPT4o, underscoring its effectiveness and
potential impact on the RAG field.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.18072v1' target='_blank'>MRBTP: Efficient Multi-Robot Behavior Tree Planning and Collaboration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yishuai Cai, Xinglin Chen, Zhongxuan Cai, Yunxin Mao, Minglong Li, Wenjing Yang, Ji Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-25 10:39:28</h6>
<p class='card-text'>Multi-robot task planning and collaboration are critical challenges in
robotics. While Behavior Trees (BTs) have been established as a popular control
architecture and are plannable for a single robot, the development of effective
multi-robot BT planning algorithms remains challenging due to the complexity of
coordinating diverse action spaces. We propose the Multi-Robot Behavior Tree
Planning (MRBTP) algorithm, with theoretical guarantees of both soundness and
completeness. MRBTP features cross-tree expansion to coordinate heterogeneous
actions across different BTs to achieve the team's goal. For homogeneous
actions, we retain backup structures among BTs to ensure robustness and prevent
redundant execution through intention sharing. While MRBTP is capable of
generating BTs for both homogeneous and heterogeneous robot teams, its
efficiency can be further improved. We then propose an optional plugin for
MRBTP when Large Language Models (LLMs) are available to reason goal-related
actions for each robot. These relevant actions can be pre-planned to form
long-horizon subtrees, significantly enhancing the planning speed and
collaboration efficiency of MRBTP. We evaluate our algorithm in warehouse
management and everyday service scenarios. Results demonstrate MRBTP's
robustness and execution efficiency under varying settings, as well as the
ability of the pre-trained LLM to generate effective task-specific subtrees for
MRBTP.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.17910v1' target='_blank'>Scaling LLM Pre-training with Vocabulary Curriculum</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fangyuan Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-25 07:18:29</h6>
<p class='card-text'>Modern language models rely on static vocabularies, fixed before pretraining,
in contrast to the adaptive vocabulary acquisition observed in human language
learning. To bridge this gap, we introduce vocabulary curriculum learning, an
approach that improves pretraining efficiency with log-linear scaling gains
relative to vocabulary size. Our method alternates between entropy-guided
vocabulary expansion and model optimization, enabling models to learn
transferable representations across diverse tokenization granularities. This
approach naturally gives rise to an optimal computation allocation pattern:
longer tokens capture predictable content, while shorter tokens focus on more
complex, harder-to-predict contexts. Experiments on small-scale GPT models
demonstrate improved scaling efficiency, reinforcing the effectiveness of
dynamic tokenization. We release our code to support further research and plan
to extend our experiments to larger models and diverse domains.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.17898v1' target='_blank'>VeriPlan: Integrating Formal Verification and LLMs into End-User
  Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Christine Lee, David Porfirio, Xinyu Jessica Wang, Kevin Zhao, Bilge Mutlu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-25 06:53:00</h6>
<p class='card-text'>Automated planning is traditionally the domain of experts, utilized in fields
like manufacturing and healthcare with the aid of expert planning tools. Recent
advancements in LLMs have made planning more accessible to everyday users due
to their potential to assist users with complex planning tasks. However, LLMs
face several application challenges within end-user planning, including
consistency, accuracy, and user trust issues. This paper introduces VeriPlan, a
system that applies formal verification techniques, specifically model
checking, to enhance the reliability and flexibility of LLMs for end-user
planning. In addition to the LLM planner, VeriPlan includes three additional
core features -- a rule translator, flexibility sliders, and a model checker --
that engage users in the verification process. Through a user study (n=12), we
evaluate VeriPlan, demonstrating improvements in the perceived quality,
usability, and user satisfaction of LLMs. Our work shows the effective
integration of formal verification and user-control features with LLMs for
end-user planning tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.17132v1' target='_blank'>Applications of Large Models in Medicine</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:YunHe Su, Zhengyang Lu, Junhui Liu, Ke Pang, Haoran Dai, Sa Liu Yuxin Jia, Lujia Ge, Jing-min Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-24 13:21:30</h6>
<p class='card-text'>This paper explores the advancements and applications of large-scale models
in the medical field, with a particular focus on Medical Large Models (MedLMs).
These models, encompassing Large Language Models (LLMs), Vision Models, 3D
Large Models, and Multimodal Models, are revolutionizing healthcare by
enhancing disease prediction, diagnostic assistance, personalized treatment
planning, and drug discovery. The integration of graph neural networks in
medical knowledge graphs and drug discovery highlights the potential of Large
Graph Models (LGMs) in understanding complex biomedical relationships. The
study also emphasizes the transformative role of Vision-Language Models (VLMs)
and 3D Large Models in medical image analysis, anatomical modeling, and
prosthetic design. Despite the challenges, these technologies are setting new
benchmarks in medical innovation, improving diagnostic accuracy, and paving the
way for personalized healthcare solutions. This paper aims to provide a
comprehensive overview of the current state and future directions of large
models in medicine, underscoring their significance in advancing global health.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.16804v1' target='_blank'>Multi-Agent Autonomous Driving Systems with Large Language Models: A
  Survey of Recent Advances</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yaozu Wu, Dongyuan Li, Yankai Chen, Renhe Jiang, Henry Peng Zou, Liancheng Fang, Zhen Wang, Philip S. Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-24 03:26:13</h6>
<p class='card-text'>Autonomous Driving Systems (ADSs) are revolutionizing transportation by
reducing human intervention, improving operational efficiency, and enhancing
safety. Large Language Models (LLMs), known for their exceptional planning and
reasoning capabilities, have been integrated into ADSs to assist with driving
decision-making. However, LLM-based single-agent ADSs face three major
challenges: limited perception, insufficient collaboration, and high
computational demands. To address these issues, recent advancements in
LLM-based multi-agent ADSs have focused on improving inter-agent communication
and cooperation. This paper provides a frontier survey of LLM-based multi-agent
ADSs. We begin with a background introduction to related concepts, followed by
a categorization of existing LLM-based approaches based on different agent
interaction modes. We then discuss agent-human interactions in scenarios where
LLM-based agents engage with humans. Finally, we summarize key applications,
datasets, and challenges in this field to support future research
(https://anonymous.4open.science/r/LLM-based_Multi-agent_ADS-3A5C/README.md).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.16730v1' target='_blank'>RapidPen: Fully Automated IP-to-Shell Penetration Testing with LLM-based
  Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sho Nakatani</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-23 21:57:46</h6>
<p class='card-text'>We present RapidPen, a fully automated penetration testing (pentesting)
framework that addresses
  the challenge of achieving an initial foothold (IP-to-Shell) without human
intervention. Unlike prior
  approaches that focus primarily on post-exploitation or require a
human-in-the-loop, RapidPen
  leverages large language models (LLMs) to autonomously discover and exploit
vulnerabilities, starting from
  a single IP address. By integrating advanced ReAct-style task planning (Re)
with retrieval-augmented
  knowledge bases of successful exploits, along with a command-generation and
direct execution feedback loop
  (Act), RapidPen systematically scans services, identifies viable attack
vectors, and executes targeted
  exploits in a fully automated manner.
  In our evaluation against a vulnerable target from the Hack The Box platform,
RapidPen achieved shell
  access within 200-400 seconds at a per-run cost of approximately \$0.3-\$0.6,
demonstrating a
  60\% success rate when reusing prior "success-case" data. These results
underscore the potential
  of truly autonomous pentesting for both security novices and seasoned
professionals. Organizations
  without dedicated security teams can leverage RapidPen to quickly identify
critical vulnerabilities,
  while expert pentesters can offload repetitive tasks and focus on complex
challenges.
  Ultimately, our work aims to make penetration testing more accessible and
cost-efficient,
  thereby enhancing the overall security posture of modern software ecosystems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.16515v1' target='_blank'>Path Planning using Instruction-Guided Probabilistic Roadmaps</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiaqi Bao, Ryo Yonetani</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-23 09:26:20</h6>
<p class='card-text'>This work presents a novel data-driven path planning algorithm named
Instruction-Guided Probabilistic Roadmap (IG-PRM). Despite the recent
development and widespread use of mobile robot navigation, the safe and
effective travels of mobile robots still require significant engineering effort
to take into account the constraints of robots and their tasks. With IG-PRM, we
aim to address this problem by allowing robot operators to specify such
constraints through natural language instructions, such as ``aim for wider
paths'' or ``mind small gaps''. The key idea is to convert such instructions
into embedding vectors using large-language models (LLMs) and use the vectors
as a condition to predict instruction-guided cost maps from occupancy maps. By
constructing a roadmap based on the predicted costs, we can find
instruction-guided paths via the standard shortest path search. Experimental
results demonstrate the effectiveness of our approach on both synthetic and
real-world indoor navigation environments.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>