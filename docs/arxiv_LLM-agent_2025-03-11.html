<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>LLM-agent - 2025-03-11</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>LLM-agent - 2025-03-11</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07459v1' target='_blank'>MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for
  Complex Medical Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiangru Tang, Daniel Shao, Jiwoong Sohn, Jiapeng Chen, Jiayi Zhang, Jinyu Xiang, Fang Wu, Yilun Zhao, Chenglin Wu, Wenqi Shi, Arman Cohan, Mark Gerstein</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 15:38:44</h6>
<p class='card-text'>Large Language Models (LLMs) have shown impressive performance on existing
medical question-answering benchmarks. This high performance makes it
increasingly difficult to meaningfully evaluate and differentiate advanced
methods. We present MedAgentsBench, a benchmark that focuses on challenging
medical questions requiring multi-step clinical reasoning, diagnosis
formulation, and treatment planning-scenarios where current models still
struggle despite their strong performance on standard tests. Drawing from seven
established medical datasets, our benchmark addresses three key limitations in
existing evaluations: (1) the prevalence of straightforward questions where
even base models achieve high performance, (2) inconsistent sampling and
evaluation protocols across studies, and (3) lack of systematic analysis of the
interplay between performance, cost, and inference time. Through experiments
with various base models and reasoning methods, we demonstrate that the latest
thinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in
complex medical reasoning tasks. Additionally, advanced search-based agent
methods offer promising performance-to-cost ratios compared to traditional
approaches. Our analysis reveals substantial performance gaps between model
families on complex questions and identifies optimal model selections for
different computational constraints. Our benchmark and evaluation framework are
publicly available at https://github.com/gersteinlab/medagents-benchmark.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07457v1' target='_blank'>LLMs syntactically adapt their language use to their conversational
  partner</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Florian Kandra, Vera Demberg, Alexander Koller</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 15:37:07</h6>
<p class='card-text'>It has been frequently observed that human speakers align their language use
with each other during conversations. In this paper, we study empirically
whether large language models (LLMs) exhibit the same behavior of
conversational adaptation. We construct a corpus of conversations between LLMs
and find that two LLM agents end up making more similar syntactic choices as
conversations go on, confirming that modern LLMs adapt their language use to
their conversational partners in at least a rudimentary way.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07323v1' target='_blank'>Dynamic Path Navigation for Motion Agents with LLM Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yubo Zhao, Qi Wu, Yifan Wang, Yu-Wing Tai, Chi-Keung Tang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 13:39:09</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated strong generalizable reasoning
and planning capabilities. However, their efficacies in spatial path planning
and obstacle-free trajectory generation remain underexplored. Leveraging LLMs
for navigation holds significant potential, given LLMs' ability to handle
unseen scenarios, support user-agent interactions, and provide global control
across complex systems, making them well-suited for agentic planning and
humanoid motion generation. As one of the first studies in this domain, we
explore the zero-shot navigation and path generation capabilities of LLMs by
constructing a dataset and proposing an evaluation protocol. Specifically, we
represent paths using anchor points connected by straight lines, enabling
movement in various directions. This approach offers greater flexibility and
practicality compared to previous methods while remaining simple and intuitive
for LLMs. We demonstrate that, when tasks are well-structured in this manner,
modern LLMs exhibit substantial planning proficiency in avoiding obstacles
while autonomously refining navigation with the generated motion to reach the
target. Further, this spatial reasoning ability of a single LLM motion agent
interacting in a static environment can be seamlessly generalized in
multi-motion agents coordination in dynamic environments. Unlike traditional
approaches that rely on single-step planning or local policies, our
training-free LLM-based method enables global, dynamic, closed-loop planning,
and autonomously resolving collision issues.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07320v1' target='_blank'>Experimental Exploration: Investigating Cooperative Interaction Behavior
  Between Humans and Large Language Model Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guanxuan Jiang, Yuyang Wang, Pan Hui</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 13:37:36</h6>
<p class='card-text'>With the rise of large language models (LLMs), AI agents as autonomous
decision-makers present significant opportunities and challenges for human-AI
cooperation. While many studies have explored human cooperation with AI as
tools, the role of LLM-augmented autonomous agents in competitive-cooperative
interactions remains under-examined. This study investigates human cooperative
behavior by engaging 30 participants who interacted with LLM agents exhibiting
different characteristics (purported human, purported rule-based AI agent, and
LLM agent) in repeated Prisoner's Dilemma games. Findings show significant
differences in cooperative behavior based on the agents' purported
characteristics and the interaction effect of participants' genders and
purported characteristics. We also analyzed human response patterns, including
game completion time, proactive favorable behavior, and acceptance of repair
efforts. These insights offer a new perspective on human interactions with LLM
agents in competitive cooperation contexts, such as virtual avatars or future
physical entities. The study underscores the importance of understanding human
biases toward AI agents and how observed behaviors can influence future
human-AI cooperation dynamics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07314v1' target='_blank'>Automated Movie Generation via Multi-Agent CoT Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weijia Wu, Zeyu Zhu, Mike Zheng Shou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 13:33:27</h6>
<p class='card-text'>Existing long-form video generation frameworks lack automated planning,
requiring manual input for storylines, scenes, cinematography, and character
interactions, resulting in high costs and inefficiencies. To address these
challenges, we present MovieAgent, an automated movie generation via
multi-agent Chain of Thought (CoT) planning. MovieAgent offers two key
advantages: 1) We firstly explore and define the paradigm of automated
movie/long-video generation. Given a script and character bank, our MovieAgent
can generates multi-scene, multi-shot long-form videos with a coherent
narrative, while ensuring character consistency, synchronized subtitles, and
stable audio throughout the film. 2) MovieAgent introduces a hierarchical
CoT-based reasoning process to automatically structure scenes, camera settings,
and cinematography, significantly reducing human effort. By employing multiple
LLM agents to simulate the roles of a director, screenwriter, storyboard
artist, and location manager, MovieAgent streamlines the production pipeline.
Experiments demonstrate that MovieAgent achieves new state-of-the-art results
in script faithfulness, character consistency, and narrative coherence. Our
hierarchical framework takes a step forward and provides new insights into
fully automated movie generation. The code and project website are available
at: https://github.com/showlab/MovieAgent and
https://weijiawu.github.io/MovieAgent.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07044v1' target='_blank'>DatawiseAgent: A Notebook-Centric LLM Agent Framework for Automated Data
  Science</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziming You, Yumiao Zhang, Dexuan Xu, Yiwei Lou, Yandong Yan, Wei Wang, Huaming Zhang, Yu Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 08:32:33</h6>
<p class='card-text'>Data Science tasks are multifaceted, dynamic, and often domain-specific.
Existing LLM-based approaches largely concentrate on isolated phases,
neglecting the interdependent nature of many data science tasks and limiting
their capacity for comprehensive end-to-end support. We propose DatawiseAgent,
a notebook-centric LLM agent framework that unifies interactions among user,
agent and the computational environment through markdown and executable code
cells, supporting flexible and adaptive automated data science. Built on a
Finite State Transducer(FST), DatawiseAgent orchestrates four stages, including
DSF-like planning, incremental execution, self-debugging, and post-filtering.
Specifically, the DFS-like planning stage systematically explores the solution
space, while incremental execution harnesses real-time feedback and
accommodates LLM's limited capabilities to progressively complete tasks. The
self-debugging and post-filtering modules further enhance reliability by
diagnosing and correcting errors and pruning extraneous information. Extensive
experiments on diverse tasks, including data analysis, visualization, and data
modeling, show that DatawiseAgent consistently outperforms or matches
state-of-the-art methods across multiple model settings. These results
highlight its potential to generalize across data science scenarios and lay the
groundwork for more efficient, fully automated workflows.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07020v1' target='_blank'>Combating Partial Perception Deficit in Autonomous Driving with
  Multimodal LLM Commonsense</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuting Hu, Chenhui Xu, Ruiyang Qin, Dancheng Liu, Amir Nassereldine, Yiyu Shi, Jinjun Xiong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 08:01:41</h6>
<p class='card-text'>Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07018v1' target='_blank'>Toward Multi-Session Personalized Conversation: A Large-Scale Dataset
  and Hierarchical Tree Framework for Implicit Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xintong Li, Jalend Bantupalli, Ria Dharmani, Yuwei Zhang, Jingbo Shang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 07:59:41</h6>
<p class='card-text'>There has been a surge in the use of large language models (LLM)
conversational agents to generate responses based on long-term history from
multiple sessions. However, existing long-term open-domain dialogue datasets
lack complex, real-world personalization and fail to capture implicit
reasoning-where relevant information is embedded in subtle, syntactic, or
semantically distant connections rather than explicit statements. In such
cases, traditional retrieval methods fail to capture relevant context, and
long-context modeling also becomes inefficient due to numerous complicated
persona-related details. To address this gap, we introduce ImplexConv, a
large-scale long-term dataset with 2,500 examples, each containing
approximately 100 conversation sessions, designed to study implicit reasoning
in personalized dialogues. Additionally, we propose TaciTree, a novel
hierarchical tree framework that structures conversation history into multiple
levels of summarization. Instead of brute-force searching all data, TaciTree
enables an efficient, level-based retrieval process where models refine their
search by progressively selecting relevant details. Our experiments demonstrate
that TaciTree significantly improves the ability of LLMs to reason over
long-term conversations with implicit contextual dependencies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07010v1' target='_blank'>ProjectEval: A Benchmark for Programming Agents Automated Evaluation on
  Project-Level Code Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kaiyuan Liu, Youcheng Pan, Jing Li, Daojing He, Yang Xiang, Yexing Du, Tianrun Gao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 07:47:27</h6>
<p class='card-text'>Recently, LLM agents have made rapid progress in improving their programming
capabilities. However, existing benchmarks lack the ability to automatically
evaluate from users' perspective, and also lack the explainability of the
results of LLM agents' code generation capabilities. Thus, we introduce
ProjectEval, a new benchmark for LLM agents project-level code generation's
automated evaluation by simulating user interaction. ProjectEval is constructed
by LLM with human reviewing. It has three different level inputs of natural
languages or code skeletons. ProjectEval can evaluate the generated projects by
user interaction simulation for execution, and by code similarity through
existing objective indicators. Through ProjectEval, we find that systematic
engineering project code, overall understanding of the project and
comprehensive analysis capability are the keys for LLM agents to achieve
practical projects. Our findings and benchmark provide valuable insights for
developing more effective programming agents that can be deployed in future
real-world production.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06951v1' target='_blank'>ReAgent: Reversible Multi-Agent Reasoning for Knowledge-Enhanced
  Multi-Hop QA</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhao Xinjie, Fan Gao, Rui Yang, Yingjian Chen, Yuyang Wang, Ying Zhu, Jiacheng Tang, Irene Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 05:56:46</h6>
<p class='card-text'>Recent advances in large language models (LLMs) have significantly improved
multi-hop question answering (QA) through direct Chain-of-Thought (CoT)
reasoning. However, the irreversible nature of CoT leads to error accumulation,
making it challenging to correct mistakes in multi-hop reasoning. This paper
introduces ReAgent: a Reversible multi-Agent collaborative framework augmented
with explicit backtracking mechanisms, enabling reversible multi-hop reasoning.
By incorporating text-based retrieval, information aggregation and validation,
our system can detect and correct errors mid-reasoning, leading to more robust
and interpretable QA outcomes. The framework and experiments serve as a
foundation for future work on error-tolerant QA systems. Empirical evaluations
across three benchmarks indicate ReAgent's efficacy, yielding average about 6\%
improvements against baseline models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06911v1' target='_blank'>Beyond Code Generation: LLM-supported Exploration of the Program Design
  Space</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:J. D. Zamfirescu-Pereira, Eunice Jun, Michael Terry, Qian Yang, Björn Hartmann</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 04:25:30</h6>
<p class='card-text'>In this work, we explore explicit Large Language Model (LLM)-powered support
for the iterative design of computer programs. Program design, like other
design activity, is characterized by navigating a space of alternative problem
formulations and associated solutions in an iterative fashion. LLMs are
potentially powerful tools in helping this exploration; however, by default,
code-generation LLMs deliver code that represents a particular point solution.
This obscures the larger space of possible alternatives, many of which might be
preferable to the LLM's default interpretation and its generated code. We
contribute an IDE that supports program design through generating and showing
new ways to frame problems alongside alternative solutions, tracking design
decisions, and identifying implicit decisions made by either the programmer or
the LLM. In a user study, we find that with our IDE, users combine and
parallelize design phases to explore a broader design space -- but also
struggle to keep up with LLM-originated changes to code and other information
overload. These findings suggest a core challenge for future IDEs that support
program design through higher-level instructions given to LLM-based agents:
carefully managing attention and deciding what information agents should
surface to program designers and when.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06892v1' target='_blank'>SafePlan: Leveraging Formal Logic and Chain-of-Thought Reasoning for
  Enhanced Safety in LLM-based Robotic Task Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ike Obi, Vishnunandan L. N. Venkatesh, Weizheng Wang, Ruiqi Wang, Dayoon Suh, Temitope I. Amosa, Wonse Jo, Byung-Cheol Min</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 03:37:36</h6>
<p class='card-text'>Robotics researchers increasingly leverage large language models (LLM) in
robotics systems, using them as interfaces to receive task commands, generate
task plans, form team coalitions, and allocate tasks among multi-robot and
human agents. However, despite their benefits, the growing adoption of LLM in
robotics has raised several safety concerns, particularly regarding executing
malicious or unsafe natural language prompts. In addition, ensuring that task
plans, team formation, and task allocation outputs from LLMs are adequately
examined, refined, or rejected is crucial for maintaining system integrity. In
this paper, we introduce SafePlan, a multi-component framework that combines
formal logic and chain-of-thought reasoners for enhancing the safety of
LLM-based robotics systems. Using the components of SafePlan, including Prompt
Sanity COT Reasoner and Invariant, Precondition, and Postcondition COT
reasoners, we examined the safety of natural language task prompts, task plans,
and task allocation outputs generated by LLM-based robotic systems as means of
investigating and enhancing system safety profile. Our results show that
SafePlan outperforms baseline models by leading to 90.5% reduction in harmful
task prompt acceptance while still maintaining reasonable acceptance of safe
tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06832v1' target='_blank'>GUIDE-CoT: Goal-driven and User-Informed Dynamic Estimation for
  Pedestrian Trajectory using Chain-of-Thought</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sungsik Kim, Janghyun Baek, Jinkyu Kim, Jaekoo Lee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 01:39:24</h6>
<p class='card-text'>While Large Language Models (LLMs) have recently shown impressive results in
reasoning tasks, their application to pedestrian trajectory prediction remains
challenging due to two key limitations: insufficient use of visual information
and the difficulty of predicting entire trajectories. To address these
challenges, we propose Goal-driven and User-Informed Dynamic Estimation for
pedestrian trajectory using Chain-of-Thought (GUIDE-CoT). Our approach
integrates two innovative modules: (1) a goal-oriented visual prompt, which
enhances goal prediction accuracy combining visual prompts with a pretrained
visual encoder, and (2) a chain-of-thought (CoT) LLM for trajectory generation,
which generates realistic trajectories toward the predicted goal. Moreover, our
method introduces controllable trajectory generation, allowing for flexible and
user-guided modifications to the predicted paths. Through extensive experiments
on the ETH/UCY benchmark datasets, our method achieves state-of-the-art
performance, delivering both high accuracy and greater adaptability in
pedestrian trajectory prediction. Our code is publicly available at
https://github.com/ai-kmu/GUIDE-CoT.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06791v1' target='_blank'>AutoMisty: A Multi-Agent LLM Framework for Automated Code Generation in
  the Misty Social Robot</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiao Wang, Lu Dong, Sahana Rangasrinivasan, Ifeoma Nwogu, Srirangaraj Setlur, Venugopal Govindaraju</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-09 22:07:46</h6>
<p class='card-text'>The social robot's open API allows users to customize open-domain
interactions. However, it remains inaccessible to those without programming
experience. In this work, we introduce AutoMisty, the first multi-agent
collaboration framework powered by large language models (LLMs), to enable the
seamless generation of executable Misty robot code from natural language
instructions. AutoMisty incorporates four specialized agent modules to manage
task decomposition, assignment, problem-solving, and result synthesis. Each
agent incorporates a two-layer optimization mechanism, with self-reflection for
iterative refinement and human-in-the-loop for better alignment with user
preferences. AutoMisty ensures a transparent reasoning process, allowing users
to iteratively refine tasks through natural language feedback for precise
execution. To evaluate AutoMisty's effectiveness, we designed a benchmark task
set spanning four levels of complexity and conducted experiments in a real
Misty robot environment. Extensive evaluations demonstrate that AutoMisty not
only consistently generates high-quality code but also enables precise code
control, significantly outperforming direct reasoning with ChatGPT-4o and
ChatGPT-o1. All code, optimized APIs, and experimental videos will be publicly
released through the webpage: https://wangxiaoshawn.github.io/AutoMisty.html</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06709v1' target='_blank'>Delusions of Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hongshen Xu, Zixv yang, Zichen Zhu, Kunyao Lan, Zihan Wang, Mengyue Wu, Ziwei Ji, Lu Chen, Pascale Fung, Kai Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-09 17:59:16</h6>
<p class='card-text'>Large Language Models often generate factually incorrect but plausible
outputs, known as hallucinations. We identify a more insidious phenomenon, LLM
delusion, defined as high belief hallucinations, incorrect outputs with
abnormally high confidence, making them harder to detect and mitigate. Unlike
ordinary hallucinations, delusions persist with low uncertainty, posing
significant challenges to model reliability. Through empirical analysis across
different model families and sizes on several Question Answering tasks, we show
that delusions are prevalent and distinct from hallucinations. LLMs exhibit
lower honesty with delusions, which are harder to override via finetuning or
self reflection. We link delusion formation with training dynamics and dataset
noise and explore mitigation strategies such as retrieval augmented generation
and multi agent debating to mitigate delusions. By systematically investigating
the nature, prevalence, and mitigation of LLM delusions, our study provides
insights into the underlying causes of this phenomenon and outlines future
directions for improving model reliability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06664v1' target='_blank'>Exploring LLM Agents for Cleaning Tabular Machine Learning Datasets</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tommaso Bendinelli, Artur Dox, Christian Holz</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-09 15:29:46</h6>
<p class='card-text'>High-quality, error-free datasets are a key ingredient in building reliable,
accurate, and unbiased machine learning (ML) models. However, real world
datasets often suffer from errors due to sensor malfunctions, data entry
mistakes, or improper data integration across multiple sources that can
severely degrade model performance. Detecting and correcting these issues
typically require tailor-made solutions and demand extensive domain expertise.
Consequently, automation is challenging, rendering the process labor-intensive
and tedious. In this study, we investigate whether Large Language Models (LLMs)
can help alleviate the burden of manual data cleaning. We set up an experiment
in which an LLM, paired with Python, is tasked with cleaning the training
dataset to improve the performance of a learning algorithm without having the
ability to modify the training pipeline or perform any feature engineering. We
run this experiment on multiple Kaggle datasets that have been intentionally
corrupted with errors. Our results show that LLMs can identify and correct
erroneous entries, such as illogical values or outlier, by leveraging
contextual information from other features within the same row, as well as
feedback from previous iterations. However, they struggle to detect more
complex errors that require understanding data distribution across multiple
rows, such as trends and biases.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06416v1' target='_blank'>Advancing AI Negotiations: New Theory and Evidence from a Large-Scale
  Autonomous Negotiations Competition</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Michelle Vaccaro, Michael Caoson, Harang Ju, Sinan Aral, Jared R. Curhan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-09 03:25:48</h6>
<p class='card-text'>Despite the rapid proliferation of artificial intelligence (AI) negotiation
agents, there has been limited integration of computer science research and
established negotiation theory to develop new theories of AI negotiation. To
bridge this gap, we conducted an International AI Negotiations Competition in
which participants iteratively designed and refined prompts for large language
model (LLM) negotiation agents. We then facilitated over 120,000 negotiations
between these agents across multiple scenarios with diverse characteristics and
objectives. Our findings revealed that fundamental principles from established
human-human negotiation theory remain crucial in AI-AI negotiations.
Specifically, agents exhibiting high warmth fostered higher counterpart
subjective value and reached deals more frequently, which enabled them to
create and claim more value in integrative settings. However, conditional on
reaching a deal, warm agents claimed less value while dominant agents claimed
more value. These results align with classic negotiation theory emphasizing
relationship-building, assertiveness, and preparation. Our analysis also
revealed unique dynamics in AI-AI negotiations not fully explained by
negotiation theory, particularly regarding the effectiveness of AI-specific
strategies like chain-of-thought reasoning and prompt injection. The agent that
won our competition implemented an approach that blended traditional
negotiation preparation frameworks with AI-specific methods. Together, these
results suggest the importance of establishing a new theory of AI negotiations
that integrates established negotiation theory with AI-specific strategies to
optimize agent performance. Our research suggests this new theory must account
for the unique characteristics of autonomous agents and establish the
conditions under which traditional negotiation theory applies in automated
settings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06410v1' target='_blank'>Performant LLM Agentic Framework for Conversational AI</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alex Casella, Wayne Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-09 02:58:34</h6>
<p class='card-text'>The rise of Agentic applications and automation in the Voice AI industry has
led to an increased reliance on Large Language Models (LLMs) to navigate
graph-based logic workflows composed of nodes and edges. However, existing
methods face challenges such as alignment errors in complex workflows and
hallucinations caused by excessive context size. To address these limitations,
we introduce the Performant Agentic Framework (PAF), a novel system that
assists LLMs in selecting appropriate nodes and executing actions in order when
traversing complex graphs. PAF combines LLM-based reasoning with a
mathematically grounded vector scoring mechanism, achieving both higher
accuracy and reduced latency. Our approach dynamically balances strict
adherence to predefined paths with flexible node jumps to handle various user
inputs efficiently. Experiments demonstrate that PAF significantly outperforms
baseline methods, paving the way for scalable, real-time Conversational AI
systems in complex business environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06074v1' target='_blank'>Towards Conversational AI for Disease Management</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Anil Palepu, Valentin Liévin, Wei-Hung Weng, Khaled Saab, David Stutz, Yong Cheng, Kavita Kulkarni, S. Sara Mahdavi, Joëlle Barral, Dale R. Webster, Katherine Chou, Avinatan Hassidim, Yossi Matias, James Manyika, Ryutaro Tanno, Vivek Natarajan, Adam Rodman, Tao Tu, Alan Karthikesalingam, Mike Schaekermann</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-08 05:48:58</h6>
<p class='card-text'>While large language models (LLMs) have shown promise in diagnostic dialogue,
their capabilities for effective management reasoning - including disease
progression, therapeutic response, and safe medication prescription - remain
under-explored. We advance the previously demonstrated diagnostic capabilities
of the Articulate Medical Intelligence Explorer (AMIE) through a new LLM-based
agentic system optimised for clinical management and dialogue, incorporating
reasoning over the evolution of disease and multiple patient visit encounters,
response to therapy, and professional competence in medication prescription. To
ground its reasoning in authoritative clinical knowledge, AMIE leverages
Gemini's long-context capabilities, combining in-context retrieval with
structured reasoning to align its output with relevant and up-to-date clinical
practice guidelines and drug formularies. In a randomized, blinded virtual
Objective Structured Clinical Examination (OSCE) study, AMIE was compared to 21
primary care physicians (PCPs) across 100 multi-visit case scenarios designed
to reflect UK NICE Guidance and BMJ Best Practice guidelines. AMIE was
non-inferior to PCPs in management reasoning as assessed by specialist
physicians and scored better in both preciseness of treatments and
investigations, and in its alignment with and grounding of management plans in
clinical guidelines. To benchmark medication reasoning, we developed RxQA, a
multiple-choice question benchmark derived from two national drug formularies
(US, UK) and validated by board-certified pharmacists. While AMIE and PCPs both
benefited from the ability to access external drug information, AMIE
outperformed PCPs on higher difficulty questions. While further research would
be needed before real-world translation, AMIE's strong performance across
evaluations marks a significant step towards conversational AI as a tool in
disease management.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06047v1' target='_blank'>DSGBench: A Diverse Strategic Game Benchmark for Evaluating LLM-based
  Agents in Complex Decision-Making Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenjie Tang, Yuan Zhou, Erqiang Xu, Keyan Cheng, Minne Li, Liquan Xiao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-08 04:17:23</h6>
<p class='card-text'>Large Language Model~(LLM) based agents have been increasingly popular in
solving complex and dynamic tasks, which requires proper evaluation systems to
assess their capabilities. Nevertheless, existing benchmarks usually either
focus on single-objective tasks or use overly broad assessing metrics, failing
to provide a comprehensive inspection of the actual capabilities of LLM-based
agents in complicated decision-making tasks. To address these issues, we
introduce DSGBench, a more rigorous evaluation platform for strategic
decision-making. Firstly, it incorporates six complex strategic games which
serve as ideal testbeds due to their long-term and multi-dimensional
decision-making demands and flexibility in customizing tasks of various
difficulty levels or multiple targets. Secondly, DSGBench employs a
fine-grained evaluation scoring system which examines the decision-making
capabilities by looking into the performance in five specific dimensions and
offering a comprehensive assessment in a well-designed way. Furthermore,
DSGBench also incorporates an automated decision-tracking mechanism which
enables in-depth analysis of agent behaviour patterns and the changes in their
strategies. We demonstrate the advances of DSGBench by applying it to multiple
popular LLM-based agents and our results suggest that DSGBench provides
valuable insights in choosing LLM-based agents as well as improving their
future development. DSGBench is available at
https://github.com/DeciBrain-Group/DSGBench.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.05944v1' target='_blank'>Enhancing Reasoning with Collaboration and Memory</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Julie Michelman, Nasrin Baratalipour, Matthew Abueg</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-07 21:19:21</h6>
<p class='card-text'>We envision a continuous collaborative learning system where groups of LLM
agents work together to solve reasoning problems, drawing on memory they
collectively build to improve performance as they gain experience. This work
establishes the foundations for such a system by studying the interoperability
of chain-of-thought reasoning styles, multi-agent collaboration, and memory
banks. Extending beyond the identical agents of self-consistency, we introduce
varied-context agents with diverse exemplars and a summarizer agent in place of
voting. We generate frozen and continuously learned memory banks of exemplars
and pair them with fixed, random, and similarity-based retrieval mechanisms.
Our systematic study reveals where various methods contribute to reasoning
performance of two LLMs on three grounded reasoning tasks, showing that random
exemplar selection can often beat more principled approaches, and in some
tasks, inclusion of any exemplars serves only to distract both weak and strong
models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.05891v1' target='_blank'>MastermindEval: A Simple But Scalable Reasoning Benchmark</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jonas Golde, Patrick Haller, Fabio Barth, Alan Akbik</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-07 19:24:59</h6>
<p class='card-text'>Recent advancements in large language models (LLMs) have led to remarkable
performance across a wide range of language understanding and mathematical
tasks. As a result, increasing attention has been given to assessing the true
reasoning capabilities of LLMs, driving research into commonsense, numerical,
logical, and qualitative reasoning. However, with the rapid progress of
reasoning-focused models such as OpenAI's o1 and DeepSeek's R1, there has been
a growing demand for reasoning benchmarks that can keep pace with ongoing model
developments. In this paper, we introduce MastermindEval, a simple, scalable,
and interpretable deductive reasoning benchmark inspired by the board game
Mastermind. Our benchmark supports two evaluation paradigms: (1) agentic
evaluation, in which the model autonomously plays the game, and (2) deductive
reasoning evaluation, in which the model is given a pre-played game state with
only one possible valid code to infer. In our experimental results we (1) find
that even easy Mastermind instances are difficult for current models and (2)
demonstrate that the benchmark is scalable to possibly more advanced models in
the future Furthermore, we investigate possible reasons why models cannot
deduce the final solution and find that current models are limited in deducing
the concealed code as the number of statement to combine information from is
increasing.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.05659v1' target='_blank'>A Survey of Large Language Model Empowered Agents for Recommendation and
  Search: Towards Next-Generation Information Retrieval</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yu Zhang, Shutong Qiao, Jiaqi Zhang, Tzu-Heng Lin, Chen Gao, Yong Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-07 18:20:30</h6>
<p class='card-text'>Information technology has profoundly altered the way humans interact with
information. The vast amount of content created, shared, and disseminated
online has made it increasingly difficult to access relevant information. Over
the past two decades, search and recommendation systems (collectively referred
to as information retrieval systems) have evolved significantly to address
these challenges. Recent advances in large language models (LLMs) have
demonstrated capabilities that surpass human performance in various
language-related tasks and exhibit general understanding, reasoning, and
decision-making abilities. This paper explores the transformative potential of
large language model agents in enhancing search and recommendation systems. We
discuss the motivations and roles of LLM agents, and establish a classification
framework to elaborate on the existing research. We highlight the immense
potential of LLM agents in addressing current challenges in search and
recommendation, providing insights into future research directions. This paper
is the first to systematically review and classify the research on LLM agents
in these domains, offering a novel perspective on leveraging this advanced AI
technology for information retrieval. To help understand the existing works, we
list the existing papers on agent-based simulation with large language models
at this link:
https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.05641v1' target='_blank'>Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for
  Heterogeneous Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Justin Chih-Yao Chen, Sukwon Yun, Elias Stengel-Eskin, Tianlong Chen, Mohit Bansal</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-07 18:03:13</h6>
<p class='card-text'>Combining existing pre-trained expert LLMs is a promising avenue for scalably
tackling large-scale and diverse tasks. However, selecting experts at the task
level is often too coarse-grained, as heterogeneous tasks may require different
expertise for each instance. To enable adaptive instance-level mixing of
pre-trained LLM experts, we propose Symbolic-MoE, a symbolic, text-based, and
gradient-free Mixture-of-Experts framework. Symbolic-MoE takes a fine-grained
approach to selection by emphasizing skills, e.g., algebra in math or molecular
biology in biomedical reasoning. We propose a skill-based recruiting strategy
that dynamically selects the most relevant set of expert LLMs for diverse
reasoning tasks based on their strengths. Each selected expert then generates
its own reasoning, resulting in k outputs from k experts, which are then
synthesized into a final high-quality response by an aggregator chosen based on
its ability to integrate diverse reasoning outputs. We show that Symbolic-MoE's
instance-level expert selection improves performance by a large margin but --
when implemented naively -- can introduce a high computational overhead due to
the need for constant model loading and offloading. To address this, we
implement a batch inference strategy that groups instances based on their
assigned experts, loading each model only once. This allows us to integrate 16
expert models on 1 GPU with a time cost comparable to or better than prior
multi-agent baselines using 4 GPUs. Through extensive evaluations on diverse
benchmarks (MMLU-Pro, GPQA, AIME, and MedMCQA), we demonstrate that
Symbolic-MoE outperforms strong LLMs like GPT4o-mini, as well as multi-agent
approaches, with an absolute average improvement of 8.15% over the best
multi-agent baseline. Moreover, Symbolic-MoE removes the need for expensive
multi-round discussions, outperforming discussion baselines with less
computation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.05856v1' target='_blank'>This Is Your Doge, If It Please You: Exploring Deception and Robustness
  in Mixture of LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lorenz Wolf, Sangwoong Yoon, Ilija Bogunovic</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-07 14:46:39</h6>
<p class='card-text'>Mixture of large language model (LLMs) Agents (MoA) architectures achieve
state-of-the-art performance on prominent benchmarks like AlpacaEval 2.0 by
leveraging the collaboration of multiple LLMs at inference time. Despite these
successes, an evaluation of the safety and reliability of MoA is missing. We
present the first comprehensive study of MoA's robustness against deceptive LLM
agents that deliberately provide misleading responses. We examine factors like
the propagation of deceptive information, model size, and information
availability, and uncover critical vulnerabilities. On AlpacaEval 2.0, the
popular LLaMA 3.1-70B model achieves a length-controlled Win Rate (LC WR) of
49.2% when coupled with 3-layer MoA (6 LLM agents). However, we demonstrate
that introducing only a $\textit{single}$ carefully-instructed deceptive agent
into the MoA can reduce performance to 37.9%, effectively nullifying all MoA
gains. On QuALITY, a multiple-choice comprehension task, the impact is also
severe, with accuracy plummeting by a staggering 48.5%. Inspired in part by the
historical Doge of Venice voting process, designed to minimize influence and
deception, we propose a range of unsupervised defense mechanisms that recover
most of the lost performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.05854v1' target='_blank'>Accelerating Earth Science Discovery via Multi-Agent LLM Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dmitrii Pantiukhin, Boris Shapkin, Ivan Kuznetsov, Antonia Anna Jost, Nikolay Koldunov</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-07 13:25:56</h6>
<p class='card-text'>This Perspective explores the transformative potential of Multi-Agent Systems
(MAS) powered by Large Language Models (LLMs) in the geosciences. Users of
geoscientific data repositories face challenges due to the complexity and
diversity of data formats, inconsistent metadata practices, and a considerable
number of unprocessed datasets. MAS possesses transformative potential for
improving scientists' interaction with geoscientific data by enabling
intelligent data processing, natural language interfaces, and collaborative
problem-solving capabilities. We illustrate this approach with "PANGAEA GPT", a
specialized MAS pipeline integrated with the diverse PANGAEA database for Earth
and Environmental Science, demonstrating how MAS-driven workflows can
effectively manage complex datasets and accelerate scientific discovery. We
discuss how MAS can address current data challenges in geosciences, highlight
advancements in other scientific fields, and propose future directions for
integrating MAS into geoscientific data processing pipelines. In this
Perspective, we show how MAS can fundamentally improve data accessibility,
promote cross-disciplinary collaboration, and accelerate geoscientific
discoveries.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.05347v1' target='_blank'>GEMA-Score: Granular Explainable Multi-Agent Score for Radiology Report
  Evaluation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhenxuan Zhang, Kinhei Lee, Weihang Deng, Huichi Zhou, Zihao Jin, Jiahao Huang, Zhifan Gao, Dominic C Marshall, Yingying Fang, Guang Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-07 11:42:22</h6>
<p class='card-text'>Automatic medical report generation supports clinical diagnosis, reduces the
workload of radiologists, and holds the promise of improving diagnosis
consistency. However, existing evaluation metrics primarily assess the accuracy
of key medical information coverage in generated reports compared to
human-written reports, while overlooking crucial details such as the location
and certainty of reported abnormalities. These limitations hinder the
comprehensive assessment of the reliability of generated reports and pose risks
in their selection for clinical use. Therefore, we propose a Granular
Explainable Multi-Agent Score (GEMA-Score) in this paper, which conducts both
objective quantification and subjective evaluation through a large language
model-based multi-agent workflow. Our GEMA-Score parses structured reports and
employs NER-F1 calculations through interactive exchanges of information among
agents to assess disease diagnosis, location, severity, and uncertainty.
Additionally, an LLM-based scoring agent evaluates completeness, readability,
and clinical terminology while providing explanatory feedback. Extensive
experiments validate that GEMA-Score achieves the highest correlation with
human expert evaluations on a public dataset, demonstrating its effectiveness
in clinical scoring (Kendall coefficient = 0.70 for Rexval dataset and Kendall
coefficient = 0.54 for RadEvalX dataset). The anonymous project demo is
available at: https://github.com/Zhenxuan-Zhang/GEMA_score.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.05242v1' target='_blank'>MM-StoryAgent: Immersive Narrated Storybook Video Generation with a
  Multi-Agent Paradigm across Text, Image and Audio</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xuenan Xu, Jiahao Mei, Chenliang Li, Yuning Wu, Ming Yan, Shaopeng Lai, Ji Zhang, Mengyue Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-07 08:53:10</h6>
<p class='card-text'>The rapid advancement of large language models (LLMs) and artificial
intelligence-generated content (AIGC) has accelerated AI-native applications,
such as AI-based storybooks that automate engaging story production for
children. However, challenges remain in improving story attractiveness,
enriching storytelling expressiveness, and developing open-source evaluation
benchmarks and frameworks. Therefore, we propose and opensource MM-StoryAgent,
which creates immersive narrated video storybooks with refined plots,
role-consistent images, and multi-channel audio. MM-StoryAgent designs a
multi-agent framework that employs LLMs and diverse expert tools (generative
models and APIs) across several modalities to produce expressive storytelling
videos. The framework enhances story attractiveness through a multi-stage
writing pipeline. In addition, it improves the immersive storytelling
experience by integrating sound effects with visual, music and narrative
assets. MM-StoryAgent offers a flexible, open-source platform for further
development, where generative modules can be substituted. Both objective and
subjective evaluation regarding textual story quality and alignment between
modalities validate the effectiveness of our proposed MM-StoryAgent system. The
demo and source code are available.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.05200v1' target='_blank'>ORANSight-2.0: Foundational LLMs for O-RAN</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pranshav Gajjar, Vijay K. Shah</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-07 07:44:31</h6>
<p class='card-text'>Despite the transformative impact of Large Language Models (LLMs) across
critical domains such as healthcare, customer service, and business marketing,
their integration into Open Radio Access Networks (O-RAN) remains limited. This
gap is primarily due to the absence of domain-specific foundational models,
with existing solutions often relying on general-purpose LLMs that fail to
address the unique challenges and technical intricacies of O-RAN. To bridge
this gap, we introduce ORANSight-2.0 (O-RAN Insights), a pioneering initiative
aimed at developing specialized foundational LLMs tailored for O-RAN. Built on
18 LLMs spanning five open-source LLM frameworks, ORANSight-2.0 fine-tunes
models ranging from 1 to 70B parameters, significantly reducing reliance on
proprietary, closed-source models while enhancing performance for O-RAN. At the
core of ORANSight-2.0 is RANSTRUCT, a novel Retrieval-Augmented Generation
(RAG) based instruction-tuning framework that employs two LLM agents to create
high-quality instruction-tuning datasets. The generated dataset is then used to
fine-tune the 18 pre-trained open-source LLMs via QLoRA. To evaluate
ORANSight-2.0, we introduce srsRANBench, a novel benchmark designed for code
generation and codebase understanding in the context of srsRAN, a widely used
5G O-RAN stack. We also leverage ORANBench13K, an existing benchmark for
assessing O-RAN-specific knowledge. Our comprehensive evaluations demonstrate
that ORANSight-2.0 models outperform general-purpose and closed-source models,
such as ChatGPT-4o and Gemini, by 5.421% on ORANBench and 18.465% on
srsRANBench, achieving superior performance while maintaining lower
computational and energy costs. We also experiment with RAG-augmented variants
of ORANSight-2.0 LLMs and thoroughly evaluate their energy characteristics,
demonstrating costs for training, standard inference, and RAG-augmented
inference.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.05164v1' target='_blank'>A Comprehensive LLM-powered Framework for Driving Intelligence
  Evaluation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shanhe You, Xuewen Luo, Xinhe Liang, Jiashu Yu, Chen Zheng, Jiangtao Gong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-07 06:03:02</h6>
<p class='card-text'>Evaluation methods for autonomous driving are crucial for algorithm
optimization. However, due to the complexity of driving intelligence, there is
currently no comprehensive evaluation method for the level of autonomous
driving intelligence. In this paper, we propose an evaluation framework for
driving behavior intelligence in complex traffic environments, aiming to fill
this gap. We constructed a natural language evaluation dataset of human
professional drivers and passengers through naturalistic driving experiments
and post-driving behavior evaluation interviews. Based on this dataset, we
developed an LLM-powered driving evaluation framework. The effectiveness of
this framework was validated through simulated experiments in the CARLA urban
traffic simulator and further corroborated by human assessment. Our research
provides valuable insights for evaluating and designing more intelligent,
human-like autonomous driving agents. The implementation details of the
framework and detailed information about the dataset can be found at Github.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>