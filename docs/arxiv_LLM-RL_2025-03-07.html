<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>LLM-RL - 2025-03-07</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>LLM-RL - 2025-03-07</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04418v1' target='_blank'>AOLO: Analysis and Optimization For Low-Carbon Oriented Wireless Large
  Language Model Services</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaoqi Wang, Hongyang Du, Yuehong Gao, Dong In Kim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 13:21:38</h6>
<p class='card-text'>Recent advancements in large language models (LLMs) have led to their
widespread adoption and large-scale deployment across various domains. However,
their environmental impact, particularly during inference, has become a growing
concern due to their substantial energy consumption and carbon footprint.
Existing research has focused on inference computation alone, overlooking the
analysis and optimization of carbon footprint in network-aided LLM service
systems. To address this gap, we propose AOLO, a framework for analysis and
optimization for low-carbon oriented wireless LLM services. AOLO introduces a
comprehensive carbon footprint model that quantifies greenhouse gas emissions
across the entire LLM service chain, including computational inference and
wireless communication. Furthermore, we formulate an optimization problem aimed
at minimizing the overall carbon footprint, which is solved through joint
optimization of inference outputs and transmit power under
quality-of-experience and system performance constraints. To achieve this joint
optimization, we leverage the energy efficiency of spiking neural networks
(SNNs) by adopting SNN as the actor network and propose a low-carbon-oriented
optimization algorithm, i.e., SNN-based deep reinforcement learning (SDRL).
Comprehensive simulations demonstrate that SDRL algorithm significantly reduces
overall carbon footprint, achieving an 18.77% reduction compared to the
benchmark soft actor-critic, highlighting its potential for enabling more
sustainable LLM inference services.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04280v1' target='_blank'>Towards Autonomous Reinforcement Learning for Real-World Robotic
  Manipulation with Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Niccol√≤ Turcato, Matteo Iovino, Aris Synodinos, Alberto Dalla Libera, Ruggero Carli, Pietro Falco</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 10:08:44</h6>
<p class='card-text'>Recent advancements in Large Language Models (LLMs) and Visual Language
Models (VLMs) have significantly impacted robotics, enabling high-level
semantic motion planning applications. Reinforcement Learning (RL), a
complementary paradigm, enables agents to autonomously optimize complex
behaviors through interaction and reward signals. However, designing effective
reward functions for RL remains challenging, especially in real-world tasks
where sparse rewards are insufficient and dense rewards require elaborate
design. In this work, we propose Autonomous Reinforcement learning for Complex
HumanInformed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4,
a pre-trained LLM, to generate reward functions directly from natural language
task descriptions. The rewards are used to train RL agents in simulated
environments, where we formalize the reward generation process to enhance
feasibility. Additionally, GPT-4 automates the coding of task success criteria,
creating a fully automated, one-shot procedure for translating human-readable
text into deployable robot skills. Our approach is validated through extensive
simulated experiments on single-arm and bi-manual manipulation tasks using an
ABB YuMi collaborative robot, highlighting its practicality and effectiveness.
Tasks are demonstrated on the real robot setup.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03889v1' target='_blank'>Pretrained LLMs as Real-Time Controllers for Robot Operated Serial
  Production Line</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Muhammad Waseem, Kshitij Bhatta, Chen Li, Qing Chang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 20:43:49</h6>
<p class='card-text'>The manufacturing industry is undergoing a transformative shift, driven by
cutting-edge technologies like 5G, AI, and cloud computing. Despite these
advancements, effective system control, which is crucial for optimizing
production efficiency, remains a complex challenge due to the intricate,
knowledge-dependent nature of manufacturing processes and the reliance on
domain-specific expertise. Conventional control methods often demand heavy
customization, considerable computational resources, and lack transparency in
decision-making. In this work, we investigate the feasibility of using Large
Language Models (LLMs), particularly GPT-4, as a straightforward, adaptable
solution for controlling manufacturing systems, specifically, mobile robot
scheduling. We introduce an LLM-based control framework to assign mobile robots
to different machines in robot assisted serial production lines, evaluating its
performance in terms of system throughput. Our proposed framework outperforms
traditional scheduling approaches such as First-Come-First-Served (FCFS),
Shortest Processing Time (SPT), and Longest Processing Time (LPT). While it
achieves performance that is on par with state-of-the-art methods like
Multi-Agent Reinforcement Learning (MARL), it offers a distinct advantage by
delivering comparable throughput without the need for extensive retraining.
These results suggest that the proposed LLM-based solution is well-suited for
scenarios where technical expertise, computational resources, and financial
investment are limited, while decision transparency and system scalability are
critical concerns.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03654v1' target='_blank'>Improving Neutral Point of View Text Generation through
  Parameter-Efficient Reinforcement Learning and a Small-Scale High-Quality
  Dataset</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jessica Hoffmann, Christiane Ahlheim, Zac Yu, Aria Walfrand, Jarvis Jin, Marie Tano, Ahmad Beirami, Erin van Liemt, Nithum Thain, Hakim Sidahmed, Lucas Dixon</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 16:32:47</h6>
<p class='card-text'>This paper describes the construction of a dataset and the evaluation of
training methods to improve generative large language models' (LLMs) ability to
answer queries on sensitive topics with a Neutral Point of View (NPOV), i.e.,
to provide significantly more informative, diverse and impartial answers. The
dataset, the SHQ-NPOV dataset, comprises 300 high-quality, human-written
quadruplets: a query on a sensitive topic, an answer, an NPOV rating, and a set
of links to source texts elaborating the various points of view. The first key
contribution of this paper is a new methodology to create such datasets through
iterative rounds of human peer-critique and annotator training, which we
release alongside the dataset. The second key contribution is the
identification of a highly effective training regime for parameter-efficient
reinforcement learning (PE-RL) to improve NPOV generation. We compare and
extensively evaluate PE-RL and multiple baselines-including LoRA finetuning (a
strong baseline), SFT and RLHF.
  PE-RL not only improves on overall NPOV quality compared to the strongest
baseline ($97.06\%\rightarrow 99.08\%$), but also scores much higher on
features linguists identify as key to separating good answers from the best
answers ($60.25\%\rightarrow 85.21\%$ for presence of supportive details,
$68.74\%\rightarrow 91.43\%$ for absence of oversimplification). A qualitative
analysis corroborates this. Finally, our evaluation finds no statistical
differences between results on topics that appear in the training dataset and
those on separated evaluation topics, which provides strong evidence that our
approach to training PE-RL exhibits very effective out of topic generalization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03796v1' target='_blank'>Human Implicit Preference-Based Policy Fine-tuning for Multi-Agent
  Reinforcement Learning in USV Swarm</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hyeonjun Kim, Kanghoon Lee, Junho Park, Jiachen Li, Jinkyoo Park</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 14:33:18</h6>
<p class='card-text'>Multi-Agent Reinforcement Learning (MARL) has shown promise in solving
complex problems involving cooperation and competition among agents, such as an
Unmanned Surface Vehicle (USV) swarm used in search and rescue, surveillance,
and vessel protection. However, aligning system behavior with user preferences
is challenging due to the difficulty of encoding expert intuition into reward
functions. To address the issue, we propose a Reinforcement Learning with Human
Feedback (RLHF) approach for MARL that resolves credit-assignment challenges
through an Agent-Level Feedback system categorizing feedback into intra-agent,
inter-agent, and intra-team types. To overcome the challenges of direct human
feedback, we employ a Large Language Model (LLM) evaluator to validate our
approach using feedback scenarios such as region constraints, collision
avoidance, and task allocation. Our method effectively refines USV swarm
policies, addressing key challenges in multi-agent systems while maintaining
fairness and performance consistency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03040v1' target='_blank'>SAGE: Steering and Refining Dialog Generation with State-Action
  Augmentation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yizhe Zhang, Navdeep Jaitly</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 22:45:24</h6>
<p class='card-text'>Recent advances in large language models have demonstrated impressive
capabilities in task-oriented applications, yet building emotionally
intelligent chatbots that can engage in natural, strategic conversations
remains a challenge. We present a novel approach called SAGE that uses latent
variables to control long-horizon behavior in dialogue generation. At the core
of our method is the State-Action Chain (SAC), which augments standard language
model fine-tuning by introducing latent variables that encapsulate emotional
states and conversational strategies between dialogue turns. During inference,
these variables are generated before each response, enabling coarse-grained
control over dialogue progression while maintaining natural interaction
patterns. We also introduce a self-improvement pipeline that leverages dialogue
tree search, LLM-based reward modeling, and targeted fine-tuning to optimize
conversational trajectories. Our experimental results show that models trained
with this approach demonstrate improved performance in emotional intelligence
metrics while maintaining strong capabilities on LLM benchmarks. The discrete
nature of our latent variables facilitates search-based strategies and provides
a foundation for future applications of reinforcement learning to dialogue
systems, where learning can occur at the state level rather than the token
level.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03039v1' target='_blank'>LLM Misalignment via Adversarial RLHF Platforms</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Erfan Entezami, Ali Naseh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 22:38:54</h6>
<p class='card-text'>Reinforcement learning has shown remarkable performance in aligning language
models with human preferences, leading to the rise of attention towards
developing RLHF platforms. These platforms enable users to fine-tune models
without requiring any expertise in developing complex machine learning
algorithms. While these platforms offer useful features such as reward modeling
and RLHF fine-tuning, their security and reliability remain largely unexplored.
Given the growing adoption of RLHF and open-source RLHF frameworks, we
investigate the trustworthiness of these systems and their potential impact on
behavior of LLMs. In this paper, we present an attack targeting publicly
available RLHF tools. In our proposed attack, an adversarial RLHF platform
corrupts the LLM alignment process by selectively manipulating data samples in
the preference dataset. In this scenario, when a user's task aligns with the
attacker's objective, the platform manipulates a subset of the preference
dataset that contains samples related to the attacker's target. This
manipulation results in a corrupted reward model, which ultimately leads to the
misalignment of the language model. Our results demonstrate that such an attack
can effectively steer LLMs toward undesirable behaviors within the targeted
domains. Our work highlights the critical need to explore the vulnerabilities
of RLHF platforms and their potential to cause misalignment in LLMs during the
RLHF fine-tuning process.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02832v1' target='_blank'>AlignDistil: Token-Level Language Model Alignment as Adaptive Policy
  Distillation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Songming Zhang, Xue Zhang, Tong Zhang, Bojie Hu, Yufeng Chen, Jinan Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 17:57:09</h6>
<p class='card-text'>In modern large language models (LLMs), LLM alignment is of crucial
importance and is typically achieved through methods such as reinforcement
learning from human feedback (RLHF) and direct preference optimization (DPO).
However, in most existing methods for LLM alignment, all tokens in the response
are optimized using a sparse, response-level reward or preference annotation.
The ignorance of token-level rewards may erroneously punish high-quality tokens
or encourage low-quality tokens, resulting in suboptimal performance and slow
convergence speed. To address this issue, we propose AlignDistil, an
RLHF-equivalent distillation method for token-level reward optimization.
Specifically, we introduce the reward learned by DPO into the RLHF objective
and theoretically prove the equivalence between this objective and a
token-level distillation process, where the teacher distribution linearly
combines the logits from the DPO model and a reference model. On this basis, we
further bridge the accuracy gap between the reward from the DPO model and the
pure reward model, by building a contrastive DPO reward with a normal and a
reverse DPO model. Moreover, to avoid under- and over-optimization on different
tokens, we design a token adaptive logit extrapolation mechanism to construct
an appropriate teacher distribution for each token. Experimental results
demonstrate the superiority of our AlignDistil over existing methods and
showcase fast convergence due to its token-level distributional reward
optimization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02623v2' target='_blank'>Rewarding Doubt: A Reinforcement Learning Approach to Confidence
  Calibration of Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Paul Stangel, David Bani-Harouni, Chantal Pellegrini, Ege √ñzsoy, Kamilia Zaripova, Matthias Keicher, Nassir Navab</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 13:48:50</h6>
<p class='card-text'>A safe and trustworthy use of Large Language Models (LLMs) requires an
accurate expression of confidence in their answers. We introduce a novel
Reinforcement Learning (RL) approach for LLM calibration that fine-tunes LLMs
to elicit calibrated confidence estimations in their answers to factual
questions. We model the problem as a betting game where the model predicts a
confidence score together with every answer, and design a reward function that
penalizes both over and under-confidence. We prove that under our reward design
an optimal policy would result in a perfectly calibrated confidence estimation.
Our experiments demonstrate significantly improved confidence calibration and
generalization to new tasks without re-training, indicating that our approach
teaches a general confidence awareness. This approach enables the training of
inherently calibrated LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02296v1' target='_blank'>Memorize or Generalize? Evaluating LLM Code Generation with Evolved
  Questions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wentao Chen, Lizhe Zhang, Li Zhong, Letian Peng, Zilong Wang, Jingbo Shang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 05:39:24</h6>
<p class='card-text'>Large Language Models (LLMs) are known to exhibit a memorization phenomenon
in code generation: instead of truly understanding the underlying principles of
a programming problem, they tend to memorize the original prompt and its
solution together in the training. Consequently, when facing variants of the
original problem, their answers very likely resemble the memorized solutions
and fail to generalize. In this paper, we investigate this phenomenon by
designing three evolution strategies to create variants: mutation,
paraphrasing, and code-rewriting. By comparing the performance and AST
similarity of the LLM-generated codes before and after these three evolutions,
we develop a memorization score that positively correlates with the level of
memorization. As expected, as supervised fine-tuning goes on, the memorization
score rises before overfitting, suggesting more severe memorization. We
demonstrate that common mitigation approaches, such as prompt translation and
using evolved variants as data augmentation in supervised learning and
reinforcement learning, either compromise the performance or fail to alleviate
the memorization issue. Therefore, memorization remains a significant challenge
in LLM code generation, highlighting the need for a more effective solution.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>