<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>LLM-planning - 2025-03-11</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>LLM-planning - 2025-03-11</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07459v1' target='_blank'>MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for
  Complex Medical Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiangru Tang, Daniel Shao, Jiwoong Sohn, Jiapeng Chen, Jiayi Zhang, Jinyu Xiang, Fang Wu, Yilun Zhao, Chenglin Wu, Wenqi Shi, Arman Cohan, Mark Gerstein</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 15:38:44</h6>
<p class='card-text'>Large Language Models (LLMs) have shown impressive performance on existing
medical question-answering benchmarks. This high performance makes it
increasingly difficult to meaningfully evaluate and differentiate advanced
methods. We present MedAgentsBench, a benchmark that focuses on challenging
medical questions requiring multi-step clinical reasoning, diagnosis
formulation, and treatment planning-scenarios where current models still
struggle despite their strong performance on standard tests. Drawing from seven
established medical datasets, our benchmark addresses three key limitations in
existing evaluations: (1) the prevalence of straightforward questions where
even base models achieve high performance, (2) inconsistent sampling and
evaluation protocols across studies, and (3) lack of systematic analysis of the
interplay between performance, cost, and inference time. Through experiments
with various base models and reasoning methods, we demonstrate that the latest
thinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in
complex medical reasoning tasks. Additionally, advanced search-based agent
methods offer promising performance-to-cost ratios compared to traditional
approaches. Our analysis reveals substantial performance gaps between model
families on complex questions and identifies optimal model selections for
different computational constraints. Our benchmark and evaluation framework are
publicly available at https://github.com/gersteinlab/medagents-benchmark.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07323v1' target='_blank'>Dynamic Path Navigation for Motion Agents with LLM Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yubo Zhao, Qi Wu, Yifan Wang, Yu-Wing Tai, Chi-Keung Tang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 13:39:09</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated strong generalizable reasoning
and planning capabilities. However, their efficacies in spatial path planning
and obstacle-free trajectory generation remain underexplored. Leveraging LLMs
for navigation holds significant potential, given LLMs' ability to handle
unseen scenarios, support user-agent interactions, and provide global control
across complex systems, making them well-suited for agentic planning and
humanoid motion generation. As one of the first studies in this domain, we
explore the zero-shot navigation and path generation capabilities of LLMs by
constructing a dataset and proposing an evaluation protocol. Specifically, we
represent paths using anchor points connected by straight lines, enabling
movement in various directions. This approach offers greater flexibility and
practicality compared to previous methods while remaining simple and intuitive
for LLMs. We demonstrate that, when tasks are well-structured in this manner,
modern LLMs exhibit substantial planning proficiency in avoiding obstacles
while autonomously refining navigation with the generated motion to reach the
target. Further, this spatial reasoning ability of a single LLM motion agent
interacting in a static environment can be seamlessly generalized in
multi-motion agents coordination in dynamic environments. Unlike traditional
approaches that rely on single-step planning or local policies, our
training-free LLM-based method enables global, dynamic, closed-loop planning,
and autonomously resolving collision issues.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07317v1' target='_blank'>Self-Corrective Task Planning by Inverse Prompting with Large Language
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiho Lee, Hayun Lee, Jonghyeon Kim, Kyungjae Lee, Eunwoo Kim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 13:35:51</h6>
<p class='card-text'>In robot task planning, large language models (LLMs) have shown significant
promise in generating complex and long-horizon action sequences. However, it is
observed that LLMs often produce responses that sound plausible but are not
accurate. To address these problems, existing methods typically employ
predefined error sets or external knowledge sources, requiring human efforts
and computation resources. Recently, self-correction approaches have emerged,
where LLM generates and refines plans, identifying errors by itself. Despite
their effectiveness, they are more prone to failures in correction due to
insufficient reasoning. In this paper, we introduce InversePrompt, a novel
self-corrective task planning approach that leverages inverse prompting to
enhance interpretability. Our method incorporates reasoning steps to provide
clear, interpretable feedback. It generates inverse actions corresponding to
the initially generated actions and verifies whether these inverse actions can
restore the system to its original state, explicitly validating the logical
coherence of the generated plans.The results on benchmark datasets show an
average 16.3% higher success rate over existing LLM-based task planning
methods. Our approach offers clearer justifications for feedback in real-world
environments, resulting in more successful task completion than existing
self-correction approaches across various scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07314v1' target='_blank'>Automated Movie Generation via Multi-Agent CoT Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weijia Wu, Zeyu Zhu, Mike Zheng Shou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 13:33:27</h6>
<p class='card-text'>Existing long-form video generation frameworks lack automated planning,
requiring manual input for storylines, scenes, cinematography, and character
interactions, resulting in high costs and inefficiencies. To address these
challenges, we present MovieAgent, an automated movie generation via
multi-agent Chain of Thought (CoT) planning. MovieAgent offers two key
advantages: 1) We firstly explore and define the paradigm of automated
movie/long-video generation. Given a script and character bank, our MovieAgent
can generates multi-scene, multi-shot long-form videos with a coherent
narrative, while ensuring character consistency, synchronized subtitles, and
stable audio throughout the film. 2) MovieAgent introduces a hierarchical
CoT-based reasoning process to automatically structure scenes, camera settings,
and cinematography, significantly reducing human effort. By employing multiple
LLM agents to simulate the roles of a director, screenwriter, storyboard
artist, and location manager, MovieAgent streamlines the production pipeline.
Experiments demonstrate that MovieAgent achieves new state-of-the-art results
in script faithfulness, character consistency, and narrative coherence. Our
hierarchical framework takes a step forward and provides new insights into
fully automated movie generation. The code and project website are available
at: https://github.com/showlab/MovieAgent and
https://weijiawu.github.io/MovieAgent.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07282v1' target='_blank'>A Graph-based Verification Framework for Fact-Checking</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yani Huang, Richong Zhang, Zhijie Nie, Junfan Chen, Xuefeng Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 13:02:29</h6>
<p class='card-text'>Fact-checking plays a crucial role in combating misinformation. Existing
methods using large language models (LLMs) for claim decomposition face two key
limitations: (1) insufficient decomposition, introducing unnecessary complexity
to the verification process, and (2) ambiguity of mentions, leading to
incorrect verification results. To address these challenges, we suggest
introducing a claim graph consisting of triplets to address the insufficient
decomposition problem and reduce mention ambiguity through graph structure.
Based on this core idea, we propose a graph-based framework, GraphFC, for
fact-checking. The framework features three key components: graph construction,
which builds both claim and evidence graphs; graph-guided planning, which
prioritizes the triplet verification order; and graph-guided checking, which
verifies the triples one by one between claim and evidence graphs. Extensive
experiments show that GraphFC enables fine-grained decomposition while
resolving referential ambiguities through relational constraints, achieving
state-of-the-art performance across three datasets.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07044v1' target='_blank'>DatawiseAgent: A Notebook-Centric LLM Agent Framework for Automated Data
  Science</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziming You, Yumiao Zhang, Dexuan Xu, Yiwei Lou, Yandong Yan, Wei Wang, Huaming Zhang, Yu Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 08:32:33</h6>
<p class='card-text'>Data Science tasks are multifaceted, dynamic, and often domain-specific.
Existing LLM-based approaches largely concentrate on isolated phases,
neglecting the interdependent nature of many data science tasks and limiting
their capacity for comprehensive end-to-end support. We propose DatawiseAgent,
a notebook-centric LLM agent framework that unifies interactions among user,
agent and the computational environment through markdown and executable code
cells, supporting flexible and adaptive automated data science. Built on a
Finite State Transducer(FST), DatawiseAgent orchestrates four stages, including
DSF-like planning, incremental execution, self-debugging, and post-filtering.
Specifically, the DFS-like planning stage systematically explores the solution
space, while incremental execution harnesses real-time feedback and
accommodates LLM's limited capabilities to progressively complete tasks. The
self-debugging and post-filtering modules further enhance reliability by
diagnosing and correcting errors and pruning extraneous information. Extensive
experiments on diverse tasks, including data analysis, visualization, and data
modeling, show that DatawiseAgent consistently outperforms or matches
state-of-the-art methods across multiple model settings. These results
highlight its potential to generalize across data science scenarios and lay the
groundwork for more efficient, fully automated workflows.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07020v1' target='_blank'>Combating Partial Perception Deficit in Autonomous Driving with
  Multimodal LLM Commonsense</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuting Hu, Chenhui Xu, Ruiyang Qin, Dancheng Liu, Amir Nassereldine, Yiyu Shi, Jinjun Xiong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 08:01:41</h6>
<p class='card-text'>Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07006v1' target='_blank'>HELM: Human-Preferred Exploration with Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shuhao Liao, Xuxin Lv, Yuhong Cao, Jeric Lew, Wenjun Wu, Guillaume Sartoretti</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 07:40:01</h6>
<p class='card-text'>In autonomous exploration tasks, robots are required to explore and map
unknown environments while efficiently planning in dynamic and uncertain
conditions. Given the significant variability of environments, human operators
often have specific preference requirements for exploration, such as
prioritizing certain areas or optimizing for different aspects of efficiency.
However, existing methods struggle to accommodate these human preferences
adaptively, often requiring extensive parameter tuning or network retraining.
With the recent advancements in Large Language Models (LLMs), which have been
widely applied to text-based planning and complex reasoning, their potential
for enhancing autonomous exploration is becoming increasingly promising.
Motivated by this, we propose an LLM-based human-preferred exploration
framework that seamlessly integrates a mobile robot system with LLMs. By
leveraging the reasoning and adaptability of LLMs, our approach enables
intuitive and flexible preference control through natural language while
maintaining a task success rate comparable to state-of-the-art traditional
methods. Experimental results demonstrate that our framework effectively
bridges the gap between human intent and policy preference in autonomous
exploration, offering a more user-friendly and adaptable solution for
real-world robotic applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06902v1' target='_blank'>A Query Optimization Method Utilizing Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiming Yao, Haoyang Li, Jing Zhang, Cuiping Li, Hong Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 04:07:56</h6>
<p class='card-text'>Query optimization is a critical task in database systems, focused on
determining the most efficient way to execute a query from an enormous set of
possible strategies. Traditional approaches rely on heuristic search methods
and cost predictions, but these often struggle with the complexity of the
search space and inaccuracies in performance estimation, leading to suboptimal
plan choices. This paper presents LLMOpt, a novel framework that leverages
Large Language Models (LLMs) to address these challenges through two innovative
components: (1) LLM for Plan Candidate Generation (LLMOpt(G)), which eliminates
heuristic search by utilizing the reasoning abilities of LLMs to directly
generate high-quality query plans, and (2) LLM for Plan Candidate Selection
(LLMOpt(S)), a list-wise cost model that compares candidates globally to
enhance selection accuracy. To adapt LLMs for query optimization, we propose
fine-tuning pre-trained models using optimization data collected offline.
Experimental results on the JOB, JOB-EXT, and Stack benchmarks show that
LLMOpt(G) and LLMOpt(S) outperform state-of-the-art methods, including
PostgreSQL, BAO, and HybridQO. Notably, LLMOpt(S) achieves the best practical
performance, striking a balance between plan quality and inference efficiency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06892v1' target='_blank'>SafePlan: Leveraging Formal Logic and Chain-of-Thought Reasoning for
  Enhanced Safety in LLM-based Robotic Task Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ike Obi, Vishnunandan L. N. Venkatesh, Weizheng Wang, Ruiqi Wang, Dayoon Suh, Temitope I. Amosa, Wonse Jo, Byung-Cheol Min</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 03:37:36</h6>
<p class='card-text'>Robotics researchers increasingly leverage large language models (LLM) in
robotics systems, using them as interfaces to receive task commands, generate
task plans, form team coalitions, and allocate tasks among multi-robot and
human agents. However, despite their benefits, the growing adoption of LLM in
robotics has raised several safety concerns, particularly regarding executing
malicious or unsafe natural language prompts. In addition, ensuring that task
plans, team formation, and task allocation outputs from LLMs are adequately
examined, refined, or rejected is crucial for maintaining system integrity. In
this paper, we introduce SafePlan, a multi-component framework that combines
formal logic and chain-of-thought reasoners for enhancing the safety of
LLM-based robotics systems. Using the components of SafePlan, including Prompt
Sanity COT Reasoner and Invariant, Precondition, and Postcondition COT
reasoners, we examined the safety of natural language task prompts, task plans,
and task allocation outputs generated by LLM-based robotic systems as means of
investigating and enhancing system safety profile. Our results show that
SafePlan outperforms baseline models by leading to 90.5% reduction in harmful
task prompt acceptance while still maintaining reasonable acceptance of safe
tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06866v1' target='_blank'>Graphormer-Guided Task Planning: Beyond Static Rules with LLM Safety
  Perception</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wanjing Huang, Tongjie Pan, Yalan Ye</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 02:43:54</h6>
<p class='card-text'>Recent advancements in large language models (LLMs) have expanded their role
in robotic task planning. However, while LLMs have been explored for generating
feasible task sequences, their ability to ensure safe task execution remains
underdeveloped. Existing methods struggle with structured risk perception,
making them inadequate for safety-critical applications where low-latency
hazard adaptation is required. To address this limitation, we propose a
Graphormer-enhanced risk-aware task planning framework that combines LLM-based
decision-making with structured safety modeling. Our approach constructs a
dynamic spatio-semantic safety graph, capturing spatial and contextual risk
factors to enable online hazard detection and adaptive task refinement. Unlike
existing methods that rely on predefined safety constraints, our framework
introduces a context-aware risk perception module that continuously refines
safety predictions based on real-time task execution. This enables a more
flexible and scalable approach to robotic planning, allowing for adaptive
safety compliance beyond static rules. To validate our framework, we conduct
experiments in the AI2-THOR environment. The experiments results validates
improvements in risk detection accuracy, rising safety notice, and task
adaptability of our framework in continuous environments compared to static
rule-based and LLM-only baselines. Our project is available at
https://github.com/hwj20/GGTP</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06552v1' target='_blank'>Multimodal Programming in Computer Science with Interactive Assistance
  Powered by Large Language Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rajan Das Gupta, Md. Tanzib Hosain, M. F. Mridha, Salah Uddin Ahmed</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-09 10:48:47</h6>
<p class='card-text'>LLM chatbot interfaces allow students to get instant, interactive assistance
with homework, but doing so carelessly may not advance educational objectives.
In this study, an interactive homework help system based on DeepSeek R1 is
developed and first implemented for students enrolled in a large computer
science beginning programming course. In addition to an assist button in a
well-known code editor, our assistant also has a feedback option in our
command-line automatic evaluator. It wraps student work in a personalized
prompt that advances our educational objectives without offering answers
straight away. We have discovered that our assistant can recognize students'
conceptual difficulties and provide ideas, plans, and template code in
pedagogically appropriate ways. However, among other mistakes, it occasionally
incorrectly labels the correct student code as incorrect or encourages students
to use correct-but-lesson-inappropriate approaches, which can lead to long and
frustrating journeys for the students. After discussing many development and
deployment issues, we provide our conclusions and future actions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06216v1' target='_blank'>A Novel Distributed PV Power Forecasting Approach Based on Time-LLM</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Huapeng Lin, Miao Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-08 13:37:31</h6>
<p class='card-text'>Distributed photovoltaic (DPV) systems are essential for advancing renewable
energy applications and achieving energy independence. Accurate DPV power
forecasting can optimize power system planning and scheduling while
significantly reducing energy loss, thus enhancing overall system efficiency
and reliability. However, solar energy's intermittent nature and DPV systems'
spatial distribution create significant forecasting challenges. Traditional
methods often rely on costly external data, such as numerical weather
prediction (NWP) and satellite images, which are difficult to scale for smaller
DPV systems. To tackle this issue, this study has introduced an advanced large
language model (LLM)-based time series forecasting framework Time-LLM to
improve the DPV power forecasting accuracy and generalization ability. By
reprogramming, the framework aligns historical power data with natural language
modalities, facilitating efficient modeling of time-series data. Then
Qwen2.5-3B model is integrated as the backbone LLM to process input data by
leveraging its pattern recognition and inference abilities, achieving a balance
between efficiency and performance. Finally, by using a flatten and linear
projection layer, the LLM's high-dimensional output is transformed into the
final forecasts. Experimental results indicate that Time-LLM outperforms
leading recent advanced time series forecasting models, such as
Transformer-based methods and MLP-based models, achieving superior accuracy in
both short-term and long-term forecasting. Time-LLM also demonstrates
exceptional adaptability in few-shot and zero-shot learning scenarios. To the
best of the authors' knowledge, this study is the first attempt to explore the
application of LLMs to DPV power forecasting, which can offer a scalable
solution that eliminates reliance on costly external data sources and improve
real-world forecasting accuracy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06074v1' target='_blank'>Towards Conversational AI for Disease Management</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Anil Palepu, Valentin Liévin, Wei-Hung Weng, Khaled Saab, David Stutz, Yong Cheng, Kavita Kulkarni, S. Sara Mahdavi, Joëlle Barral, Dale R. Webster, Katherine Chou, Avinatan Hassidim, Yossi Matias, James Manyika, Ryutaro Tanno, Vivek Natarajan, Adam Rodman, Tao Tu, Alan Karthikesalingam, Mike Schaekermann</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-08 05:48:58</h6>
<p class='card-text'>While large language models (LLMs) have shown promise in diagnostic dialogue,
their capabilities for effective management reasoning - including disease
progression, therapeutic response, and safe medication prescription - remain
under-explored. We advance the previously demonstrated diagnostic capabilities
of the Articulate Medical Intelligence Explorer (AMIE) through a new LLM-based
agentic system optimised for clinical management and dialogue, incorporating
reasoning over the evolution of disease and multiple patient visit encounters,
response to therapy, and professional competence in medication prescription. To
ground its reasoning in authoritative clinical knowledge, AMIE leverages
Gemini's long-context capabilities, combining in-context retrieval with
structured reasoning to align its output with relevant and up-to-date clinical
practice guidelines and drug formularies. In a randomized, blinded virtual
Objective Structured Clinical Examination (OSCE) study, AMIE was compared to 21
primary care physicians (PCPs) across 100 multi-visit case scenarios designed
to reflect UK NICE Guidance and BMJ Best Practice guidelines. AMIE was
non-inferior to PCPs in management reasoning as assessed by specialist
physicians and scored better in both preciseness of treatments and
investigations, and in its alignment with and grounding of management plans in
clinical guidelines. To benchmark medication reasoning, we developed RxQA, a
multiple-choice question benchmark derived from two national drug formularies
(US, UK) and validated by board-certified pharmacists. While AMIE and PCPs both
benefited from the ability to access external drug information, AMIE
outperformed PCPs on higher difficulty questions. While further research would
be needed before real-world translation, AMIE's strong performance across
evaluations marks a significant step towards conversational AI as a tool in
disease management.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.05193v1' target='_blank'>Memory-augmented Query Reconstruction for LLM-based Knowledge Graph
  Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mufan Xu, Gewen Liang, Kehai Chen, Wei Wang, Xun Zhou, Muyun Yang, Tiejun Zhao, Min Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-07 07:28:32</h6>
<p class='card-text'>Large language models (LLMs) have achieved remarkable performance on
knowledge graph question answering (KGQA) tasks by planning and interacting
with knowledge graphs. However, existing methods often confuse tool utilization
with knowledge reasoning, harming readability of model outputs and giving rise
to hallucinatory tool invocations, which hinder the advancement of KGQA. To
address this issue, we propose Memory-augmented Query Reconstruction for
LLM-based Knowledge Graph Reasoning (MemQ) to decouple LLM from tool invocation
tasks using LLM-built query memory. By establishing a memory module with
explicit descriptions of query statements, the proposed MemQ facilitates the
KGQA process with natural language reasoning and memory-augmented query
reconstruction. Meanwhile, we design an effective and readable reasoning to
enhance the LLM's reasoning capability in KGQA. Experimental results that MemQ
achieves state-of-the-art performance on widely used benchmarks WebQSP and CWQ.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04723v2' target='_blank'>Shifting Long-Context LLMs Research from Input to Output</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuhao Wu, Yushi Bai, Zhiqing Hu, Shangqing Tu, Ming Shan Hee, Juanzi Li, Roy Ka-Wei Lee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 18:59:37</h6>
<p class='card-text'>Recent advancements in long-context Large Language Models (LLMs) have
primarily concentrated on processing extended input contexts, resulting in
significant strides in long-context comprehension. However, the equally
critical aspect of generating long-form outputs has received comparatively less
attention. This paper advocates for a paradigm shift in NLP research toward
addressing the challenges of long-output generation. Tasks such as novel
writing, long-term planning, and complex reasoning require models to understand
extensive contexts and produce coherent, contextually rich, and logically
consistent extended text. These demands highlight a critical gap in current LLM
capabilities. We underscore the importance of this under-explored domain and
call for focused efforts to develop foundational LLMs tailored for generating
high-quality, long-form outputs, which hold immense potential for real-world
applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04691v2' target='_blank'>Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pengcheng Qiu, Chaoyi Wu, Shuyu Liu, Weike Zhao, Zhuoxia Chen, Hongfei Gu, Chuanjin Peng, Ya Zhang, Yanfeng Wang, Weidi Xie</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 18:35:39</h6>
<p class='card-text'>Recent advancements in reasoning-enhanced large language models (LLMs), such
as DeepSeek-R1 and OpenAI-o3, have demonstrated significant progress. However,
their application in professional medical contexts remains underexplored,
particularly in evaluating the quality of their reasoning processes alongside
final outputs. Here, we introduce MedR-Bench, a benchmarking dataset of 1,453
structured patient cases, annotated with reasoning references derived from
clinical case reports. Spanning 13 body systems and 10 specialties, it includes
both common and rare diseases. To comprehensively evaluate LLM performance, we
propose a framework encompassing three critical examination recommendation,
diagnostic decision-making, and treatment planning, simulating the entire
patient care journey. To assess reasoning quality, we present the Reasoning
Evaluator, a novel automated system that objectively scores free-text reasoning
responses based on efficiency, actuality, and completeness using dynamic
cross-referencing and evidence checks. Using this benchmark, we evaluate five
state-of-the-art reasoning LLMs, including DeepSeek-R1, OpenAI-o3-mini, and
Gemini-2.0-Flash Thinking, etc. Our results show that current LLMs achieve over
85% accuracy in relatively simple diagnostic tasks when provided with
sufficient examination results. However, performance declines in more complex
tasks, such as examination recommendation and treatment planning. While
reasoning outputs are generally reliable, with factuality scores exceeding 90%,
critical reasoning steps are frequently missed. These findings underscore both
the progress and limitations of clinical LLMs. Notably, open-source models like
DeepSeek-R1 are narrowing the gap with proprietary systems, highlighting their
potential to drive accessible and equitable advancements in healthcare.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04675v1' target='_blank'>LLM-guided Plan and Retrieval: A Strategic Alignment for Interpretable
  User Satisfaction Estimation in Dialogue</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sangyeop Kim, Sohhyung Park, Jaewon Jung, Jinseok Kim, Sungzoon Cho</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 18:12:33</h6>
<p class='card-text'>Understanding user satisfaction with conversational systems, known as User
Satisfaction Estimation (USE), is essential for assessing dialogue quality and
enhancing user experiences. However, existing methods for USE face challenges
due to limited understanding of underlying reasons for user dissatisfaction and
the high costs of annotating user intentions. To address these challenges, we
propose PRAISE (Plan and Retrieval Alignment for Interpretable Satisfaction
Estimation), an interpretable framework for effective user satisfaction
prediction. PRAISE operates through three key modules. The Strategy Planner
develops strategies, which are natural language criteria for classifying user
satisfaction. The Feature Retriever then incorporates knowledge on user
satisfaction from Large Language Models (LLMs) and retrieves relevance features
from utterances. Finally, the Score Analyzer evaluates strategy predictions and
classifies user satisfaction. Experimental results demonstrate that PRAISE
achieves state-of-the-art performance on three benchmarks for the USE task.
Beyond its superior performance, PRAISE offers additional benefits. It enhances
interpretability by providing instance-level explanations through effective
alignment of utterances with strategies. Moreover, PRAISE operates more
efficiently than existing approaches by eliminating the need for LLMs during
the inference phase.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04280v2' target='_blank'>Towards Autonomous Reinforcement Learning for Real-World Robotic
  Manipulation with Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Niccolò Turcato, Matteo Iovino, Aris Synodinos, Alberto Dalla Libera, Ruggero Carli, Pietro Falco</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 10:08:44</h6>
<p class='card-text'>Recent advancements in Large Language Models (LLMs) and Visual Language
Models (VLMs) have significantly impacted robotics, enabling high-level
semantic motion planning applications. Reinforcement Learning (RL), a
complementary paradigm, enables agents to autonomously optimize complex
behaviors through interaction and reward signals. However, designing effective
reward functions for RL remains challenging, especially in real-world tasks
where sparse rewards are insufficient and dense rewards require elaborate
design. In this work, we propose Autonomous Reinforcement learning for Complex
HumanInformed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4,
a pre-trained LLM, to generate reward functions directly from natural language
task descriptions. The rewards are used to train RL agents in simulated
environments, where we formalize the reward generation process to enhance
feasibility. Additionally, GPT-4 automates the coding of task success criteria,
creating a fully automated, one-shot procedure for translating human-readable
text into deployable robot skills. Our approach is validated through extensive
simulated experiments on single-arm and bi-manual manipulation tasks using an
ABB YuMi collaborative robot, highlighting its practicality and effectiveness.
Tasks are demonstrated on the real robot setup.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04250v1' target='_blank'>An Egocentric Vision-Language Model based Portable Real-time Smart
  Assistant</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yifei Huang, Jilan Xu, Baoqi Pei, Yuping He, Guo Chen, Mingfang Zhang, Lijin Yang, Zheng Nie, Jinyao Liu, Guoshun Fan, Dechen Lin, Fang Fang, Kunpeng Li, Chang Yuan, Xinyuan Chen, Yaohui Wang, Yali Wang, Yu Qiao, Limin Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 09:33:46</h6>
<p class='card-text'>We present Vinci, a vision-language system designed to provide real-time,
comprehensive AI assistance on portable devices. At its core, Vinci leverages
EgoVideo-VL, a novel model that integrates an egocentric vision foundation
model with a large language model (LLM), enabling advanced functionalities such
as scene understanding, temporal grounding, video summarization, and future
planning. To enhance its utility, Vinci incorporates a memory module for
processing long video streams in real time while retaining contextual history,
a generation module for producing visual action demonstrations, and a retrieval
module that bridges egocentric and third-person perspectives to provide
relevant how-to videos for skill acquisition. Unlike existing systems that
often depend on specialized hardware, Vinci is hardware-agnostic, supporting
deployment across a wide range of devices, including smartphones and wearable
cameras. In our experiments, we first demonstrate the superior performance of
EgoVideo-VL on multiple public benchmarks, showcasing its vision-language
reasoning and contextual understanding capabilities. We then conduct a series
of user studies to evaluate the real-world effectiveness of Vinci, highlighting
its adaptability and usability in diverse scenarios. We hope Vinci can
establish a new framework for portable, real-time egocentric AI systems,
empowering users with contextual and actionable insights. Including the
frontend, backend, and models, all codes of Vinci are available at
https://github.com/OpenGVLab/vinci.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03911v1' target='_blank'>Safe LLM-Controlled Robots with Formal Guarantees via Reachability
  Analysis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ahmad Hafez, Alireza Naderi Akhormeh, Amr Hegazy, Amr Alanwar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 21:23:15</h6>
<p class='card-text'>The deployment of Large Language Models (LLMs) in robotic systems presents
unique safety challenges, particularly in unpredictable environments. Although
LLMs, leveraging zero-shot learning, enhance human-robot interaction and
decision-making capabilities, their inherent probabilistic nature and lack of
formal guarantees raise significant concerns for safety-critical applications.
Traditional model-based verification approaches often rely on precise system
models, which are difficult to obtain for real-world robotic systems and may
not be fully trusted due to modeling inaccuracies, unmodeled dynamics, or
environmental uncertainties. To address these challenges, this paper introduces
a safety assurance framework for LLM-controlled robots based on data-driven
reachability analysis, a formal verification technique that ensures all
possible system trajectories remain within safe operational limits. Our
framework specifically investigates the problem of instructing an LLM to
navigate the robot to a specified goal and assesses its ability to generate
low-level control actions that successfully guide the robot safely toward that
goal. By leveraging historical data to construct reachable sets of states for
the robot-LLM system, our approach provides rigorous safety guarantees against
unsafe behaviors without relying on explicit analytical models. We validate the
framework through experimental case studies in autonomous navigation and task
planning, demonstrating its effectiveness in mitigating risks associated with
LLM-generated commands. This work advances the integration of formal methods
into LLM-based robotics, offering a principled and practical approach to
ensuring safety in next-generation autonomous systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04839v1' target='_blank'>Advancing Multimodal In-Context Learning in Large Vision-Language Models
  with Task-aware Demonstrations</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yanshu Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 16:33:10</h6>
<p class='card-text'>Multimodal in-context learning (ICL) has emerged as a key capability of Large
Vision-Language Models (LVLMs), driven by their increasing scale and
applicability. Despite its promise, effective ICL in the multimodal setting
remains challenging due to the inherent complexity of image-text inputs and the
high sensitivity of ICL performance to input configurations. In this work, we
shed light on the core mechanism underlying multimodal ICL, identifying task
mapping as a crucial factor in configuring robust in-context demonstration
(ICD) sequences. Building on these insights, we propose \textit{SabER}, a
lightweight yet powerful decoder-only transformer equipped with task-aware
attention, which intelligently selects and arranges ICDs from a demonstration
library in an autoregressive fashion. This design enables fine-grained feature
extraction and cross-modal reasoning, iteratively refining task mapping to
generate high-quality ICD sequences. Through extensive experiments covering
five LVLMs and nine benchmark datasets, SabER not only demonstrates strong
empirical performance, but also provides deeper understanding of how task
semantics interact with multimodal ICDs. Our findings highlight the importance
of principled ICD sequence configuration and open new avenues to enhance
multimodal ICL in a wide range of real-world scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03556v1' target='_blank'>Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented
  Manipulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaomeng Zhu, Yuyang Li, Leiyao Cui, Pengfei Li, Huan-ang Gao, Yixin Zhu, Hao Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 14:44:53</h6>
<p class='card-text'>Object affordance reasoning, the ability to infer object functionalities
based on physical properties, is fundamental for task-oriented planning and
activities in both humans and Artificial Intelligence (AI). This capability,
required for planning and executing daily activities in a task-oriented manner,
relies on commonsense knowledge of object physics and functionalities,
extending beyond simple object recognition. Current computational models for
affordance reasoning from perception lack generalizability, limiting their
applicability in novel scenarios. Meanwhile, comprehensive Large Language
Models (LLMs) with emerging reasoning capabilities are challenging to deploy on
local devices for task-oriented manipulations. Here, we introduce LVIS-Aff, a
large-scale dataset comprising 1,496 tasks and 119k images, designed to enhance
the generalizability of affordance reasoning from perception. Utilizing this
dataset, we develop Afford-X, an end-to-end trainable affordance reasoning
model that incorporates Verb Attention and Bi-Fusion modules to improve
multi-modal understanding. This model achieves up to a 12.1% performance
improvement over the best-reported results from non-LLM methods, while also
demonstrating a 1.2% enhancement compared to our previous conference paper.
Additionally, it maintains a compact 187M parameter size and infers nearly 50
times faster than the GPT-4V API. Our work demonstrates the potential for
efficient, generalizable affordance reasoning models that can be deployed on
local devices for task-oriented manipulations. We showcase Afford-X's
effectiveness in enabling task-oriented manipulations for robots across various
tasks and environments, underscoring its efficiency and broad implications for
advancing robotics and AI systems in real-world applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03505v1' target='_blank'>Parallelized Planning-Acting for Efficient LLM-based Multi-Agent Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yaoru Li, Shunyu Liu, Tongya Zheng, Mingli Song</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 13:53:10</h6>
<p class='card-text'>Recent advancements in Large Language Model(LLM)-based Multi-Agent
Systems(MAS) have demonstrated remarkable potential for tackling complex
decision-making tasks. However, existing frameworks inevitably rely on
serialized execution paradigms, where agents must complete sequential LLM
planning before taking action. This fundamental constraint severely limits
real-time responsiveness and adaptation, which is crucial in dynamic
environments with ever-changing scenarios. In this paper, we propose a novel
parallelized planning-acting framework for LLM-based MAS, featuring a
dual-thread architecture with interruptible execution to enable concurrent
planning and acting. Specifically, our framework comprises two core threads:(1)
a planning thread driven by a centralized memory system, maintaining
synchronization of environmental states and agent communication to support
dynamic decision-making; and (2) an acting thread equipped with a comprehensive
skill library, enabling automated task execution through recursive
decomposition. Extensive experiments on challenging Minecraft demonstrate the
effectiveness of the proposed framework.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03459v2' target='_blank'>Unified Mind Model: Reimagining Autonomous Agents in the LLM Era</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pengbo Hu, Xiang Ying</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 12:49:44</h6>
<p class='card-text'>Large language models (LLMs) have recently demonstrated remarkable
capabilities across domains, tasks, and languages (e.g., ChatGPT and GPT-4),
reviving the research of general autonomous agents with human-like cognitive
abilities. Such human-level agents require semantic comprehension and
instruction-following capabilities, which exactly fall into the strengths of
LLMs. Although there have been several initial attempts to build human-level
agents based on LLMs, the theoretical foundation remains a challenging open
problem. In this paper, we propose a novel theoretical cognitive architecture,
the Unified Mind Model (UMM), which offers guidance to facilitate the rapid
creation of autonomous agents with human-level cognitive abilities.
Specifically, our UMM starts with the global workspace theory and further
leverage LLMs to enable the agent with various cognitive abilities, such as
multi-modal perception, planning, reasoning, tool use, learning, memory,
reflection and motivation. Building upon UMM, we then develop an agent-building
engine, MindOS, which allows users to quickly create domain-/task-specific
autonomous agents without any programming effort.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03074v1' target='_blank'>BEVDriver: Leveraging BEV Maps in LLMs for Robust Closed-Loop Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Katharina Winter, Mark Azer, Fabian B. Flohr</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 00:27:32</h6>
<p class='card-text'>Autonomous driving has the potential to set the stage for more efficient
future mobility, requiring the research domain to establish trust through safe,
reliable and transparent driving. Large Language Models (LLMs) possess
reasoning capabilities and natural language understanding, presenting the
potential to serve as generalized decision-makers for ego-motion planning that
can interact with humans and navigate environments designed for human drivers.
While this research avenue is promising, current autonomous driving approaches
are challenged by combining 3D spatial grounding and the reasoning and language
capabilities of LLMs. We introduce BEVDriver, an LLM-based model for end-to-end
closed-loop driving in CARLA that utilizes latent BEV features as perception
input. BEVDriver includes a BEV encoder to efficiently process multi-view
images and 3D LiDAR point clouds. Within a common latent space, the BEV
features are propagated through a Q-Former to align with natural language
instructions and passed to the LLM that predicts and plans precise future
trajectories while considering navigation instructions and critical scenarios.
On the LangAuto benchmark, our model reaches up to 18.9% higher performance on
the Driving Score compared to SoTA methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02756v1' target='_blank'>BatchGEMBA: Token-Efficient Machine Translation Evaluation with Batched
  Prompting and Prompt Compression</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Daniil Larionov, Steffen Eger</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 16:20:52</h6>
<p class='card-text'>Recent advancements in Large Language Model (LLM)-based Natural Language
Generation evaluation have largely focused on single-example prompting,
resulting in significant token overhead and computational inefficiencies. In
this work, we introduce BatchGEMBA-MQM, a framework that integrates batched
prompting with the GEMBA-MQM metric for machine translation evaluation. Our
approach aggregates multiple translation examples into a single prompt,
reducing token usage by 2-4 times (depending on the batch size) relative to
single-example prompting. Furthermore, we propose a batching-aware prompt
compression model that achieves an additional token reduction of 13-15% on
average while also showing ability to help mitigate batching-induced quality
degradation. Evaluations across several LLMs (GPT-4o, GPT-4o-mini, Mistral
Small, Phi4, and CommandR7B) and varying batch sizes reveal that while batching
generally negatively affects quality (but sometimes not substantially), prompt
compression does not degrade further, and in some cases, recovers quality loss.
For instance, GPT-4o retains over 90% of its baseline performance at a batch
size of 4 when compression is applied, compared to a 44.6% drop without
compression. We plan to release our code and trained models at
https://github.com/NL2G/batchgemba to support future research in this domain.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02698v1' target='_blank'>FlowPlan: Zero-Shot Task Planning with LLM Flow Engineering for Robotic
  Instruction Following</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zijun Lin, Chao Tang, Hanjing Ye, Hong Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 15:14:41</h6>
<p class='card-text'>Robotic instruction following tasks require seamless integration of visual
perception, task planning, target localization, and motion execution. However,
existing task planning methods for instruction following are either data-driven
or underperform in zero-shot scenarios due to difficulties in grounding lengthy
instructions into actionable plans under operational constraints. To address
this, we propose FlowPlan, a structured multi-stage LLM workflow that elevates
zero-shot pipeline and bridges the performance gap between zero-shot and
data-driven in-context learning methods. By decomposing the planning process
into modular stages--task information retrieval, language-level reasoning,
symbolic-level planning, and logical evaluation--FlowPlan generates logically
coherent action sequences while adhering to operational constraints and further
extracts contextual guidance for precise instance-level target localization.
Benchmarked on the ALFRED and validated in real-world applications, our method
achieves competitive performance relative to data-driven in-context learning
methods and demonstrates adaptability across diverse environments. This work
advances zero-shot task planning in robotic systems without reliance on labeled
data. Project website: https://instruction-following-project.github.io/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02682v1' target='_blank'>MPO: Boosting LLM Agents with Meta Plan Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weimin Xiong, Yifan Song, Qingxiu Dong, Bingchan Zhao, Feifan Song, Xun Wang, Sujian Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 14:54:45</h6>
<p class='card-text'>Recent advancements in large language models (LLMs) have enabled LLM-based
agents to successfully tackle interactive planning tasks. However, despite
their successes, existing approaches often suffer from planning hallucinations
and require retraining for each new agent. To address these challenges, we
propose the Meta Plan Optimization (MPO) framework, which enhances agent
planning capabilities by directly incorporating explicit guidance. Unlike
previous methods that rely on complex knowledge, which either require
significant human effort or lack quality assurance, MPO leverages high-level
general guidance through meta plans to assist agent planning and enables
continuous optimization of the meta plans based on feedback from the agent's
task execution. Our experiments conducted on two representative tasks
demonstrate that MPO significantly outperforms existing baselines. Moreover,
our analysis indicates that MPO provides a plug-and-play solution that enhances
both task completion efficiency and generalization capabilities in previous
unseen scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02465v1' target='_blank'>UAV-VLRR: Vision-Language Informed NMPC for Rapid Response in UAV Search
  and Rescue</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yasheerah Yaqoot, Muhammad Ahsan Mustafa, Oleg Sautenkov, Dzmitry Tsetserukou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 10:21:58</h6>
<p class='card-text'>Emergency search and rescue (SAR) operations often require rapid and precise
target identification in complex environments where traditional manual drone
control is inefficient. In order to address these scenarios, a rapid SAR
system, UAV-VLRR (Vision-Language-Rapid-Response), is developed in this
research. This system consists of two aspects: 1) A multimodal system which
harnesses the power of Visual Language Model (VLM) and the natural language
processing capabilities of ChatGPT-4o (LLM) for scene interpretation. 2) A
non-linearmodel predictive control (NMPC) with built-in obstacle avoidance for
rapid response by a drone to fly according to the output of the multimodal
system. This work aims at improving response times in emergency SAR operations
by providing a more intuitive and natural approach to the operator to plan the
SAR mission while allowing the drone to carry out that mission in a rapid and
safe manner. When tested, our approach was faster on an average by 33.75% when
compared with an off-the-shelf autopilot and 54.6% when compared with a human
pilot. Video of UAV-VLRR: https://youtu.be/KJqQGKKt1xY</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>