<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>LLM-RL - 2025-03-01</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>LLM-RL - 2025-03-01</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20127v1' target='_blank'>SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zexiong Ma, Chao Peng, Pengfei Gao, Xiangxin Meng, Yanzhen Zou, Bing Xie</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 14:19:45</h6>
<p class='card-text'>Mainstream issue-resolving frameworks predominantly rely on commercial
models, leading to high costs and privacy concerns. Existing training
approaches for issue resolving struggle with poor generalization and fail to
fully leverage open-source development resources. We propose Subtask-oriented
Reinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue
resolving capability of LLMs. We decomposes issue resolving into structured
subtasks: file localization, function localization, line localization, and code
edit generation. SoRFT consists of two training stages: (1) rejection-sampled
supervised fine-tuning, Chain of Thought (CoT) data is filtered using
ground-truth before fine-tuning the LLM, and (2) rule-based reinforcement
learning, which leverages PPO with ground-truth based rewards. We evaluate the
SoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving
state-of-the-art (SOTA) performance among open-source models (e.g., resolve
21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental
results demonstrate that SoRFT significantly enhances issue-resolving
performance, improves model generalization, and provides a cost-efficient
alternative to commercial models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19794v1' target='_blank'>ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced
  Large Language Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chuanliu Fan, Ziqiang Cao, Zicheng Ma, Nan Yu, Yimin Peng, Jun Zhang, Yiqin Gao, Guohong Fu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 06:05:45</h6>
<p class='card-text'>Goal-oriented de novo molecule design, namely generating molecules with
specific property or substructure constraints, is a crucial yet challenging
task in drug discovery. Existing methods, such as Bayesian optimization and
reinforcement learning, often require training multiple property predictors and
struggle to incorporate substructure constraints. Inspired by the success of
Large Language Models (LLMs) in text generation, we propose ChatMol, a novel
approach that leverages LLMs for molecule design across diverse constraint
settings. Initially, we crafted a molecule representation compatible with LLMs
and validated its efficacy across multiple online LLMs. Afterwards, we
developed specific prompts geared towards diverse constrained molecule
generation tasks to further fine-tune current LLMs while integrating feedback
learning derived from property prediction. Finally, to address the limitations
of LLMs in numerical recognition, we referred to the position encoding method
and incorporated additional encoding for numerical values within the prompt.
Experimental results across single-property, substructure-property, and
multi-property constrained tasks demonstrate that ChatMol consistently
outperforms state-of-the-art baselines, including VAE and RL-based methods.
Notably, in multi-objective binding affinity maximization task, ChatMol
achieves a significantly lower KD value of 0.25 for the protein target ESR1,
while maintaining the highest overall performance, surpassing previous methods
by 4.76%. Meanwhile, with numerical enhancement, the Pearson correlation
coefficient between the instructed property values and those of the generated
molecules increased by up to 0.49. These findings highlight the potential of
LLMs as a versatile framework for molecule generation, offering a promising
alternative to traditional latent space and RL-based approaches.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19735v1' target='_blank'>R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Minggui He, Yilun Liu, Shimin Tao, Yuanchang Luo, Hongyong Zeng, Chang Su, Li Zhang, Hongxia Ma, Daimeng Wei, Weibin Meng, Hao Yang, Boxing Chen, Osamu Yoshie</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 03:57:00</h6>
<p class='card-text'>Despite recent breakthroughs in reasoning-enhanced large language models
(LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine
translation (MT), where human translators naturally employ structured,
multi-layered reasoning chain-of-thoughts (CoTs), is yet underexplored.
Existing methods either design a fixed CoT tailored for a specific MT sub-task
(e.g., literature translation), or rely on synthesizing CoTs unaligned with
humans and supervised fine-tuning (SFT) prone to catastrophic forgetting,
limiting their adaptability to diverse translation scenarios. This paper
introduces R1-Translator (R1-T1), a novel framework to achieve inference-time
reasoning for general MT via reinforcement learning (RL) with human-aligned
CoTs comprising six common patterns. Our approach pioneers three innovations:
(1) extending reasoning-based translation beyond MT sub-tasks to six languages
and diverse tasks (e.g., legal/medical domain adaptation, idiom resolution);
(2) formalizing six expert-curated CoT templates that mirror hybrid human
strategies like context-aware paraphrasing and back translation; and (3)
enabling self-evolving CoT discovery and anti-forgetting adaptation through RL
with KL-constrained rewards. Experimental results indicate a steady translation
performance improvement in 21 languages and 80 translation directions on
Flores-101 test set, especially on the 15 languages unseen from training, with
its general multilingual abilities preserved compared with plain SFT.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19613v1' target='_blank'>Self-rewarding correction for mathematical reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wei Xiong, Hanning Zhang, Chenlu Ye, Lichang Chen, Nan Jiang, Tong Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 23:01:16</h6>
<p class='card-text'>We study self-rewarding reasoning large language models (LLMs), which can
simultaneously generate step-by-step reasoning and evaluate the correctness of
their outputs during the inference time-without external feedback. This
integrated approach allows a single model to independently guide its reasoning
process, offering computational advantages for model deployment. We
particularly focus on the representative task of self-correction, where models
autonomously detect errors in their responses, revise outputs, and decide when
to terminate iterative refinement loops. To enable this, we propose a
two-staged algorithmic framework for constructing self-rewarding reasoning
models using only self-generated data. In the first stage, we employ sequential
rejection sampling to synthesize long chain-of-thought trajectories that
incorporate both self-rewarding and self-correction mechanisms. Fine-tuning
models on these curated data allows them to learn the patterns of
self-rewarding and self-correction. In the second stage, we further enhance the
models' ability to assess response accuracy and refine outputs through
reinforcement learning with rule-based signals. Experiments with Llama-3 and
Qwen-2.5 demonstrate that our approach surpasses intrinsic self-correction
capabilities and achieves performance comparable to systems that rely on
external reward models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19557v1' target='_blank'>Distill Not Only Data but Also Rewards: Can Smaller Language Models
  Surpass Larger Ones?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yudi Zhang, Lu Wang, Meng Fang, Yali Du, Chenghua Huang, Jun Wang, Qingwei Lin, Mykola Pechenizkiy, Dongmei Zhang, Saravan Rajmohan, Qi Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 20:50:11</h6>
<p class='card-text'>Distilling large language models (LLMs) typically involves transferring the
teacher model's responses through supervised fine-tuning (SFT). However, this
approach neglects the potential to distill both data (output content) and
reward signals (quality evaluations). Extracting reliable reward signals
directly from teacher models is challenging, as LLMs are optimized for
generation rather than evaluation, often resulting in biased or inconsistent
assessments. To address this limitation, we propose a novel distillation
pipeline that transfers both responses and rewards. Our method generates
pseudo-rewards through a self-supervised mechanism that leverages the inherent
structure of both teacher and student responses, enabling reward learning
without explicit external evaluation. The reward model subsequently guides
reinforcement learning (RL), allowing iterative refinement of the student model
after an SFT warm-up phase. Experiments on GSM8K and MMLU-PRO demonstrate that
our method consistently outperforms traditional SFT-based approaches, enabling
student models to surpass the performance of their teachers. This work
highlights the potential for scalable, efficient distillation through
structured self-supervised reward learning, reducing dependence on external
reward supervision.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19500v1' target='_blank'>Conversational Planning for Personal Plans</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Konstantina Christakopoulou, Iris Qu, John Canny, Andrew Goodridge, Cj Adams, Minmin Chen, Maja Matarić</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 19:04:26</h6>
<p class='card-text'>The language generation and reasoning capabilities of large language models
(LLMs) have enabled conversational systems with impressive performance in a
variety of tasks, from code generation, to composing essays, to passing STEM
and legal exams, to a new paradigm for knowledge search. Besides those
short-term use applications, LLMs are increasingly used to help with real-life
goals or tasks that take a long time to complete, involving multiple sessions
across days, weeks, months, or even years. Thus to enable conversational
systems for long term interactions and tasks, we need language-based agents
that can plan for long horizons. Traditionally, such capabilities were
addressed by reinforcement learning agents with hierarchical planning
capabilities. In this work, we explore a novel architecture where the LLM acts
as the meta-controller deciding the agent's next macro-action, and tool use
augmented LLM-based option policies execute the selected macro-action. We
instantiate this framework for a specific set of macro-actions enabling
adaptive planning for users' personal plans through conversation and follow-up
questions collecting user feedback. We show how this paradigm can be applicable
in scenarios ranging from tutoring for academic and non-academic tasks to
conversational coaching for personal health plans.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19230v1' target='_blank'>Two Heads Are Better Than One: Dual-Model Verbal Reflection at
  Inference-Time</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiazheng Li, Yuxiang Zhou, Junru Lu, Gladys Tyen, Lin Gui, Cesare Aloisi, Yulan He</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 15:41:41</h6>
<p class='card-text'>Large Language Models (LLMs) often struggle with complex reasoning scenarios.
While preference optimization methods enhance reasoning performance through
training, they often lack transparency in why one reasoning outcome is
preferred over another. Verbal reflection techniques improve explainability but
are limited in LLMs' critique and refinement capacity. To address these
challenges, we introduce a contrastive reflection synthesis pipeline that
enhances the accuracy and depth of LLM-generated reflections. We further
propose a dual-model reasoning framework within a verbal reinforcement learning
paradigm, decoupling inference-time self-reflection into specialized, trained
models for reasoning critique and refinement. Extensive experiments show that
our framework outperforms traditional preference optimization methods across
all evaluation metrics. Our findings also show that "two heads are better than
one", demonstrating that a collaborative Reasoner-Critic model achieves
superior reasoning performance and transparency, compared to single-model
approaches.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19158v1' target='_blank'>When Personalization Meets Reality: A Multi-Faceted Analysis of
  Personalized Preference Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yijiang River Dong, Tiancheng Hu, Yinhong Liu, Ahmet Üstün, Nigel Collier</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 14:14:58</h6>
<p class='card-text'>While Reinforcement Learning from Human Feedback (RLHF) is widely used to
align Large Language Models (LLMs) with human preferences, it typically assumes
homogeneous preferences across users, overlooking diverse human values and
minority viewpoints. Although personalized preference learning addresses this
by tailoring separate preferences for individual users, the field lacks
standardized methods to assess its effectiveness. We present a multi-faceted
evaluation framework that measures not only performance but also fairness,
unintended effects, and adaptability across varying levels of preference
divergence. Through extensive experiments comparing eight personalization
methods across three preference datasets, we demonstrate that performance
differences between methods could reach 36% when users strongly disagree, and
personalization can introduce up to 20% safety misalignment. These findings
highlight the critical need for holistic evaluation approaches to advance the
development of more effective and inclusive preference learning systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.18968v1' target='_blank'>Know You First and Be You Better: Modeling Human-Like User Simulators
  via Implicit Profiles</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kuang Wang, Xianfei Li, Shenghao Yang, Li Zhou, Feng Jiang, Haizhou Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 09:26:54</h6>
<p class='card-text'>User simulators are crucial for replicating human interactions with dialogue
systems, supporting both collaborative training and automatic evaluation,
especially for large language models (LLMs). However, existing simulators often
rely solely on text utterances, missing implicit user traits such as
personality, speaking style, and goals. In contrast, persona-based methods lack
generalizability, as they depend on predefined profiles of famous individuals
or archetypes. To address these challenges, we propose User Simulator with
implicit Profiles (USP), a framework that infers implicit user profiles from
human-machine conversations and uses them to generate more personalized and
realistic dialogues. We first develop an LLM-driven extractor with a
comprehensive profile schema. Then, we refine the simulation through
conditional supervised fine-tuning and reinforcement learning with cycle
consistency, optimizing it at both the utterance and conversation levels.
Finally, we adopt a diverse profile sampler to capture the distribution of
real-world user profiles. Experimental results demonstrate that USP outperforms
strong baselines in terms of authenticity and diversity while achieving
comparable performance in consistency. Furthermore, dynamic multi-turn
evaluations based on USP strongly align with mainstream benchmarks,
demonstrating its effectiveness in real-world applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.18878v1' target='_blank'>Learning to Generate Structured Output with Schema Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yaxi Lu, Haolun Li, Xin Cong, Zhong Zhang, Yesai Wu, Yankai Lin, Zhiyuan Liu, Fangming Liu, Maosong Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 06:45:29</h6>
<p class='card-text'>This study investigates the structured generation capabilities of large
language models (LLMs), focusing on producing valid JSON outputs against a
given schema. Despite the widespread use of JSON in integrating language models
with programs, there is a lack of comprehensive analysis and benchmarking of
these capabilities. We explore various aspects of JSON generation, such as
structure understanding, escaping, and natural language description, to
determine how to assess and enable LLMs to generate valid responses. Building
upon this, we propose SchemaBench features around 40K different JSON schemas to
obtain and assess models' abilities in generating valid JSON. We find that the
latest LLMs are still struggling to generate a valid JSON string. Moreover, we
demonstrate that incorporating reinforcement learning with a Fine-grained
Schema Validator can further enhance models' understanding of JSON schema,
leading to improved performance. Our models demonstrate significant improvement
in both generating JSON outputs and downstream tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.18770v2' target='_blank'>Reward Shaping to Mitigate Reward Hacking in RLHF</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiayi Fu, Xuandong Zhao, Chengyuan Yao, Heng Wang, Qi Han, Yanghua Xiao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 02:57:59</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) is essential for aligning
large language models (LLMs) with human values. However, RLHF is susceptible to
reward hacking, where the agent exploits flaws in the reward function rather
than learning the intended behavior, thus degrading alignment. While reward
shaping helps stabilize RLHF and partially mitigate reward hacking, a
systematic investigation into shaping techniques and their underlying
principles remains lacking. To bridge this gap, we present a comprehensive
study of the prevalent reward shaping methods. Our analysis suggests three key
design principles: (1) RL reward is ideally bounded, (2) RL benefits from rapid
initial growth followed by gradual convergence, and (3) RL reward is best
formulated as a function of centered reward. Guided by these insights, we
propose Preference As Reward (PAR), a novel approach that leverages the latent
preferences embedded within the reward model itself as the signal for
reinforcement learning. We evaluated PAR on two base models, Gemma2-2B and
Llama3-8B, using two datasets, Ultrafeedback-Binarized and HH-RLHF.
Experimental results demonstrate PAR's superior performance over other reward
shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate at
least 5 percentage points higher than competing approaches. Furthermore, PAR
exhibits remarkable data efficiency, requiring only a single reference reward
for optimal performance, and maintains robustness against reward hacking even
after two full epochs of training. Code is available at
https://github.com/PorUna-byte/PAR.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.18699v1' target='_blank'>MPO: An Efficient Post-Processing Framework for Mixing Diverse
  Preference Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tianze Wang, Dongnan Gui, Yifan Hu, Shuhang Lin, Linjun Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-25 23:22:12</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) has shown promise in
aligning large language models (LLMs). Yet its reliance on a singular reward
model often overlooks the diversity of human preferences. Recent approaches
address this limitation by leveraging multi-dimensional feedback to fine-tune
corresponding reward models and train LLMs using reinforcement learning.
However, the process is costly and unstable, especially given the competing and
heterogeneous nature of human preferences. In this paper, we propose Mixing
Preference Optimization (MPO), a post-processing framework for aggregating
single-objective policies as an alternative to both multi-objective RLHF
(MORLHF) and MaxMin-RLHF. MPO avoids alignment from scratch. Instead, it
log-linearly combines existing policies into a unified one with the weight of
each policy computed via a batch stochastic mirror descent. Empirical results
demonstrate that MPO achieves balanced performance across diverse preferences,
outperforming or matching existing models with significantly reduced
computational costs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.18449v1' target='_blank'>SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open
  Software Evolution</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, Sida I. Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-25 18:45:04</h6>
<p class='card-text'>The recent DeepSeek-R1 release has demonstrated the immense potential of
reinforcement learning (RL) in enhancing the general reasoning capabilities of
large language models (LLMs). While DeepSeek-R1 and other follow-up work
primarily focus on applying RL to competitive coding and math problems, this
paper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for
real-world software engineering. Leveraging a lightweight rule-based reward
(e.g., the similarity score between ground-truth and LLM-generated solutions),
SWE-RL enables LLMs to autonomously recover a developer's reasoning processes
and solutions by learning from extensive open-source software evolution data --
the record of a software's entire lifecycle, including its code snapshots, code
changes, and events such as issues and pull requests. Trained on top of Llama
3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve
rate on SWE-bench Verified -- a human-verified collection of real-world GitHub
issues. To our knowledge, this is the best performance reported for
medium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs
like GPT-4o. Surprisingly, despite performing RL solely on software evolution
data, Llama3-SWE-RL has even emerged with generalized reasoning skills. For
example, it shows improved results on five out-of-domain tasks, namely,
function coding, library use, code reasoning, mathematics, and general language
understanding, whereas a supervised-finetuning baseline even leads to
performance degradation on average. Overall, SWE-RL opens up a new direction to
improve the reasoning capabilities of LLMs through reinforcement learning on
massive software engineering data.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.18439v1' target='_blank'>MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language
  Models with Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chanwoo Park, Seungju Han, Xingzhi Guo, Asuman Ozdaglar, Kaiqing Zhang, Joo-Kyung Kim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-25 18:33:48</h6>
<p class='card-text'>Leveraging multiple large language models (LLMs) to build collaborative
multi-agentic workflows has demonstrated significant potential. However, most
previous studies focus on prompting the out-of-the-box LLMs, relying on their
innate capability for collaboration, which may not improve LLMs' performance as
shown recently. In this paper, we introduce a new post-training paradigm MAPoRL
(Multi-Agent Post-co-training for collaborative LLMs with Reinforcement
Learning), to explicitly elicit the collaborative behaviors and further unleash
the power of multi-agentic LLM frameworks. In MAPoRL, multiple LLMs first
generate their own responses independently and engage in a multi-turn
discussion to collaboratively improve the final answer. In the end, a MAPoRL
verifier evaluates both the answer and the discussion, by assigning a score
that verifies the correctness of the answer, while adding incentives to
encourage corrective and persuasive discussions. The score serves as the
co-training reward, and is then maximized through multi-agent RL. Unlike
existing LLM post-training paradigms, MAPoRL advocates the co-training of
multiple LLMs together using RL for better generalization. Accompanied by
analytical insights, our experiments demonstrate that training individual LLMs
alone is insufficient to induce effective collaboration. In contrast,
multi-agent co-training can boost the collaboration performance across
benchmarks, with generalization to unseen domains.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.18008v3' target='_blank'>NotaGen: Advancing Musicality in Symbolic Music Generation with Large
  Language Model Training Paradigms</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yashan Wang, Shangda Wu, Jianhuai Hu, Xingjian Du, Yueqi Peng, Yongxin Huang, Shuai Fan, Xiaobing Li, Feng Yu, Maosong Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-25 09:12:07</h6>
<p class='card-text'>We introduce NotaGen, a symbolic music generation model aiming to explore the
potential of producing high-quality classical sheet music. Inspired by the
success of Large Language Models (LLMs), NotaGen adopts pre-training,
fine-tuning, and reinforcement learning paradigms (henceforth referred to as
the LLM training paradigms). It is pre-trained on 1.6M pieces of music, and
then fine-tuned on approximately 9K high-quality classical compositions
conditioned on "period-composer-instrumentation" prompts. For reinforcement
learning, we propose the CLaMP-DPO method, which further enhances generation
quality and controllability without requiring human annotations or predefined
rewards. Our experiments demonstrate the efficacy of CLaMP-DPO in symbolic
music generation models with different architectures and encoding schemes.
Furthermore, subjective A/B tests show that NotaGen outperforms baseline models
against human compositions, greatly advancing musical aesthetics in symbolic
music generation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.17840v1' target='_blank'>A Combinatorial Identities Benchmark for Theorem Proving via Automated
  Theorem Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Beibei Xiong, Hangyu Lv, Haojia Shan, Jianlin Wang, Zhengfeng Yang, Lihong Zhi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-25 04:41:49</h6>
<p class='card-text'>Large language models (LLMs) have significantly advanced formal theorem
proving, yet the scarcity of high-quality training data constrains their
capabilities in complex mathematical domains. Combinatorics, a cornerstone of
mathematics, provides essential tools for analyzing discrete structures and
solving optimization problems. However, its inherent complexity makes it
particularly challenging for automated theorem proving (ATP) for combinatorial
identities. To address this, we manually construct LeanComb, combinatorial
identities benchmark in Lean, which is, to our knowledge, the first formalized
theorem proving benchmark built for combinatorial identities. We develop an
Automated Theorem Generator for Combinatorial Identities, ATG4CI, which
combines candidate tactics suggested by a self-improving large language model
with a Reinforcement Learning Tree Search approach for tactic prediction. By
utilizing ATG4CI, we generate a LeanComb-Enhanced dataset comprising 260K
combinatorial identities theorems, each with a complete formal proof in Lean,
and experimental evaluations demonstrate that models trained on this dataset
can generate more effective tactics, thereby improving success rates in
automated theorem proving for combinatorial identities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.17701v1' target='_blank'>From Perceptions to Decisions: Wildfire Evacuation Decision Prediction
  with Behavioral Theory-informed LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ruxiao Chen, Chenguang Wang, Yuran Sun, Xilei Zhao, Susu Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-24 22:47:33</h6>
<p class='card-text'>Evacuation decision prediction is critical for efficient and effective
wildfire response by helping emergency management anticipate traffic congestion
and bottlenecks, allocate resources, and minimize negative impacts. Traditional
statistical methods for evacuation decision prediction fail to capture the
complex and diverse behavioral logic of different individuals. In this work,
for the first time, we introduce FLARE, short for facilitating LLM for advanced
reasoning on wildfire evacuation decision prediction, a Large Language Model
(LLM)-based framework that integrates behavioral theories and models to
streamline the Chain-of-Thought (CoT) reasoning and subsequently integrate with
memory-based Reinforcement Learning (RL) module to provide accurate evacuation
decision prediction and understanding. Our proposed method addresses the
limitations of using existing LLMs for evacuation behavioral predictions, such
as limited survey data, mismatching with behavioral theory, conflicting
individual preferences, implicit and complex mental states, and intractable
mental state-behavior mapping. Experiments on three post-wildfire survey
datasets show an average of 20.47% performance improvement over traditional
theory-informed behavioral models, with strong cross-event generalizability.
Our complete code is publicly available at
https://github.com/SusuXu-s-Lab/FLARE</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.17387v1' target='_blank'>Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement
  Learning in Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alon Albalak, Duy Phung, Nathan Lile, Rafael Rafailov, Kanishk Gandhi, Louis Castricato, Anikait Singh, Chase Blagden, Violet Xiang, Dakota Mahan, Nick Haber</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-24 18:14:01</h6>
<p class='card-text'>Increasing interest in reasoning models has led math to become a prominent
testing ground for algorithmic and methodological improvements. However,
existing open math datasets either contain a small collection of high-quality,
human-written problems or a large corpus of machine-generated problems of
uncertain quality, forcing researchers to choose between quality and quantity.
In this work, we present Big-Math, a dataset of over 250,000 high-quality math
questions with verifiable answers, purposefully made for reinforcement learning
(RL). To create Big-Math, we rigorously filter, clean, and curate openly
available datasets, extracting questions that satisfy our three desiderata: (1)
problems with uniquely verifiable solutions, (2) problems that are open-ended,
(3) and problems with a closed-form solution. To ensure the quality of
Big-Math, we manually verify each step in our filtering process. Based on the
findings from our filtering process, we introduce 47,000 new questions with
verified answers, Big-Math-Reformulated: closed-ended questions (i.e. multiple
choice questions) that have been reformulated as open-ended questions through a
systematic reformulation algorithm. Compared to the most commonly used existing
open-source datasets for math reasoning, GSM8k and MATH, Big-Math is an order
of magnitude larger, while our rigorous filtering ensures that we maintain the
questions most suitable for RL. We also provide a rigorous analysis of the
dataset, finding that Big-Math contains a high degree of diversity across
problem domains, and incorporates a wide range of problem difficulties,
enabling a wide range of downstream uses for models of varying capabilities and
training requirements. By bridging the gap between data quality and quantity,
Big-Math establish a robust foundation for advancing reasoning in LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.17011v1' target='_blank'>Predicting Liquidity-Aware Bond Yields using Causal GANs and Deep
  Reinforcement Learning with LLM Evaluation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jaskaran Singh Walia, Aarush Sinha, Srinitish Srinivasan, Srihari Unnikrishnan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-24 09:46:37</h6>
<p class='card-text'>Financial bond yield forecasting is challenging due to data scarcity,
nonlinear macroeconomic dependencies, and evolving market conditions. In this
paper, we propose a novel framework that leverages Causal Generative
Adversarial Networks (CausalGANs) and Soft Actor-Critic (SAC) reinforcement
learning (RL) to generate high-fidelity synthetic bond yield data for four
major bond categories (AAA, BAA, US10Y, Junk). By incorporating 12 key
macroeconomic variables, we ensure statistical fidelity by preserving essential
market properties. To transform this market dependent synthetic data into
actionable insights, we employ a finetuned Large Language Model (LLM)
Qwen2.5-7B that generates trading signals (BUY/HOLD/SELL), risk assessments,
and volatility projections. We use automated, human and LLM evaluations, all of
which demonstrate that our framework improves forecasting performance over
existing methods, with statistical validation via predictive accuracy, MAE
evaluation(0.103%), profit/loss evaluation (60% profit rate), LLM evaluation
(3.37/5) and expert assessments scoring 4.67 out of 5. The reinforcement
learning-enhanced synthetic data generation achieves the least Mean Absolute
Error of 0.103, demonstrating its effectiveness in replicating real-world bond
market dynamics. We not only enhance data-driven trading strategies but also
provides a scalable, high-fidelity synthetic financial data pipeline for risk &
volatility management and investment decision-making. This work establishes a
bridge between synthetic data generation, LLM driven financial forecasting, and
language model evaluation, contributing to AI-driven financial decision-making.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.16944v1' target='_blank'>Lean and Mean: Decoupled Value Policy Optimization with Global Value
  Guidance</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chenghua Huang, Lu Wang, Fangkai Yang, Pu Zhao, Zhixu Li, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-24 08:11:33</h6>
<p class='card-text'>Proximal Policy Optimization (PPO)-based Reinforcement Learning from Human
Feedback (RLHF) is essential for aligning large language models (LLMs) with
human preferences. It requires joint training of an actor and critic with a
pretrained, fixed reward model for guidance. This approach increases
computational complexity and instability due to actor-critic interdependence.
Additionally, PPO lacks access to true environment rewards in LLM tasks,
limiting its adaptability. Under such conditions, pretraining a value model or
a reward model becomes equivalent, as both provide fixed supervisory signals
without new ground-truth feedback. To address these issues, we propose
\textbf{Decoupled Value Policy Optimization (DVPO)}, a lean framework that
replaces traditional reward modeling with a pretrained \emph{global value model
(GVM)}. The GVM is conditioned on policy trajectories and predicts token-level
return-to-go estimates. By decoupling value model from policy training (via
frozen GVM-driven RL objectives), DVPO eliminates actor-critic interdependence,
reducing GPU memory usage by 40\% and training time by 35\% compared to
conventional RLHF. Experiments across benchmarks show DVPO outperforms
efficient RLHF methods (e.g., DPO) while matching state-of-the-art PPO in
performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.16940v1' target='_blank'>Reasoning Does Not Necessarily Improve Role-Playing Ability</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiachong Feng, Longxu Dou, Lingpeng Kong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-24 08:08:41</h6>
<p class='card-text'>The application of role-playing large language models (LLMs) is rapidly
expanding in both academic and commercial domains, driving an increasing demand
for high-precision role-playing models. Simultaneously, the rapid advancement
of reasoning techniques has continuously pushed the performance boundaries of
LLMs. This intersection of practical role-playing demands and evolving
reasoning capabilities raises an important research question: "Can reasoning
techniques enhance the role-playing capabilities of LLMs?" To address this, we
conduct a comprehensive study using 6 role-playing benchmarks, 24 LLMs, and 3
distinct role-playing strategies, comparing the effectiveness of direct
zero-shot role-playing, role-playing with Chain-of-Thought (CoT), and
role-playing using reasoning-optimized LLMs. Our findings reveal that CoT may
reduce role-playing performance, reasoning-optimized LLMs are unsuitable for
role-playing, reasoning ability disrupts the role-playing scaling law, large
models still lack proficiency in advanced role-playing, and Chinese
role-playing performance surpasses English role-playing performance.
Furthermore, based on extensive experimental results, we propose two promising
future research directions: Role-aware CoT for improving role-playing LLMs and
Reinforcement Learning for role-playing LLMs, aiming to enhance the
adaptability, consistency, and effectiveness of role-playing LLMs for both
research and real-world applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.16863v1' target='_blank'>Leveraging Large Language Models for Effective and Explainable
  Multi-Agent Credit Assignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kartik Nagpal, Dayi Dong, Jean-Baptiste Bouvier, Negar Mehr</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-24 05:56:47</h6>
<p class='card-text'>Recent work, spanning from autonomous vehicle coordination to in-space
assembly, has shown the importance of learning collaborative behavior for
enabling robots to achieve shared goals. A common approach for learning this
cooperative behavior is to utilize the centralized-training
decentralized-execution paradigm. However, this approach also introduces a new
challenge: how do we evaluate the contributions of each agent's actions to the
overall success or failure of the team. This credit assignment problem has
remained open, and has been extensively studied in the Multi-Agent
Reinforcement Learning literature. In fact, humans manually inspecting agent
behavior often generate better credit evaluations than existing methods. We
combine this observation with recent works which show Large Language Models
demonstrate human-level performance at many pattern recognition tasks. Our key
idea is to reformulate credit assignment to the two pattern recognition
problems of sequence improvement and attribution, which motivates our novel
LLM-MCA method. Our approach utilizes a centralized LLM reward-critic which
numerically decomposes the environment reward based on the individualized
contribution of each agent in the scenario. We then update the agents' policy
networks based on this feedback. We also propose an extension LLM-TACA where
our LLM critic performs explicit task assignment by passing an intermediary
goal directly to each agent policy in the scenario. Both our methods far
outperform the state-of-the-art on a variety of benchmarks, including
Level-Based Foraging, Robotic Warehouse, and our new Spaceworld benchmark which
incorporates collision-related safety constraints. As an artifact of our
methods, we generate large trajectory datasets with each timestep annotated
with per-agent reward information, as sampled from our LLM critics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.16852v1' target='_blank'>Improving LLM General Preference Alignment via Optimistic Online Mirror
  Descent</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuheng Zhang, Dian Yu, Tao Ge, Linfeng Song, Zhichen Zeng, Haitao Mi, Nan Jiang, Dong Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-24 05:24:52</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) has demonstrated remarkable
effectiveness in aligning large language models (LLMs) with human preferences.
Many existing alignment approaches rely on the Bradley-Terry (BT) model
assumption, which assumes the existence of a ground-truth reward for each
prompt-response pair. However, this assumption can be overly restrictive when
modeling complex human preferences. In this paper, we drop the BT model
assumption and study LLM alignment under general preferences, formulated as a
two-player game. Drawing on theoretical insights from learning in games, we
integrate optimistic online mirror descent into our alignment framework to
approximate the Nash policy. Theoretically, we demonstrate that our approach
achieves an $O(T^{-1})$ bound on the duality gap, improving upon the previous
$O(T^{-1/2})$ result. More importantly, we implement our method and show
through experiments that it outperforms state-of-the-art RLHF algorithms across
multiple representative benchmarks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.16343v1' target='_blank'>Exploring Sentiment Manipulation by LLM-Enabled Intelligent Trading
  Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:David Byrd</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-22 20:17:14</h6>
<p class='card-text'>Companies across all economic sectors continue to deploy large language
models at a rapid pace. Reinforcement learning is experiencing a resurgence of
interest due to its association with the fine-tuning of language models from
human feedback. Tool-chain language models control task-specific agents; if the
converse has not already appeared, it soon will. In this paper, we present what
we believe is the first investigation of an intelligent trading agent based on
continuous deep reinforcement learning that also controls a large language
model with which it can post to a social media feed observed by other traders.
We empirically investigate the performance and impact of such an agent in a
simulated financial market, finding that it learns to optimize its total
reward, and thereby augment its profit, by manipulating the sentiment of the
posts it produces. The paper concludes with discussion, limitations, and
suggestions for future work.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.17515v1' target='_blank'>Towards User-level Private Reinforcement Learning with Human Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiaming Zhang, Mingxi Lei, Meng Ding, Mengdi Li, Zihang Xiang, Difei Xu, Jinhui Xu, Di Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-22 14:57:28</h6>
<p class='card-text'>Reinforcement Learning with Human Feedback (RLHF) has emerged as an
influential technique, enabling the alignment of large language models (LLMs)
with human preferences. Despite the promising potential of RLHF, how to protect
user preference privacy has become a crucial issue. Most previous work has
focused on using differential privacy (DP) to protect the privacy of individual
data. However, they have concentrated primarily on item-level privacy
protection and have unsatisfactory performance for user-level privacy, which is
more common in RLHF. This study proposes a novel framework, AUP-RLHF, which
integrates user-level label DP into RLHF. We first show that the classical
random response algorithm, which achieves an acceptable performance in
item-level privacy, leads to suboptimal utility when in the user-level
settings. We then establish a lower bound for the user-level label DP-RLHF and
develop the AUP-RLHF algorithm, which guarantees $(\varepsilon, \delta)$
user-level privacy and achieves an improved estimation error. Experimental
results show that AUP-RLHF outperforms existing baseline methods in sentiment
generation and summarization tasks, achieving a better privacy-utility
trade-off.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.16198v1' target='_blank'>An Autonomous Network Orchestration Framework Integrating Large Language
  Models with Continual Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Masoud Shokrnezhad, Tarik Taleb</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-22 11:53:34</h6>
<p class='card-text'>6G networks aim to achieve global coverage, massive connectivity, and
ultra-stringent requirements. Space-Air-Ground Integrated Networks (SAGINs) and
Semantic Communication (SemCom) are essential for realizing these goals, yet
they introduce considerable complexity in resource orchestration. Drawing
inspiration from research in robotics, a viable solution to manage this
complexity is the application of Large Language Models (LLMs). Although the use
of LLMs in network orchestration has recently gained attention, existing
solutions have not sufficiently addressed LLM hallucinations or their
adaptation to network dynamics. To address this gap, this paper proposes a
framework called Autonomous Reinforcement Coordination (ARC) for a
SemCom-enabled SAGIN. This framework employs an LLM-based Retrieval-Augmented
Generator (RAG) monitors services, users, and resources and processes the
collected data, while a Hierarchical Action Planner (HAP) orchestrates
resources. ARC decomposes orchestration into two tiers, utilizing LLMs for
high-level planning and Reinforcement Learning (RL) agents for low-level
decision-making, in alignment with the Mixture of Experts (MoE) concept. The
LLMs utilize Chain-of-Thought (CoT) reasoning for few-shot learning, empowered
by contrastive learning, while the RL agents employ replay buffer management
for continual learning, thereby achieving efficiency, accuracy, and
adaptability. Simulations are provided to demonstrate the effectiveness of ARC,
along with a comprehensive discussion on potential future research directions
to enhance and upgrade ARC.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.16182v1' target='_blank'>IPO: Your Language Model is Secretly a Preference Classifier</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shivank Garg, Ayush Singh, Shweta Singh, Paras Chopra</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-22 10:59:11</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) has emerged as the primary
method for aligning large language models (LLMs) with human preferences. While
it enables LLMs to achieve human-level alignment, it often incurs significant
computational and financial costs due to its reliance on training external
reward models or human-labeled preferences. In this work, we propose
\textbf{Implicit Preference Optimization (IPO)}, an alternative approach that
leverages generative LLMs as preference classifiers, thereby reducing the
dependence on external human feedback or reward models to obtain preferences.
We conduct a comprehensive evaluation on the preference classification ability
of LLMs using RewardBench, assessing models across different sizes,
architectures, and training levels to validate our hypothesis. Furthermore, we
investigate the self-improvement capabilities of LLMs by generating multiple
responses for a given instruction and employing the model itself as a
preference classifier for Direct Preference Optimization (DPO)-based training.
Our findings demonstrate that models trained through IPO achieve performance
comparable to those utilizing state-of-the-art reward models for obtaining
preferences.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.15214v1' target='_blank'>The Evolving Landscape of LLM- and VLM-Integrated Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sheila Schoepp, Masoud Jafaripour, Yingyue Cao, Tianpei Yang, Fatemeh Abdollahi, Shadan Golestan, Zahin Sufiyan, Osmar R. Zaiane, Matthew E. Taylor</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-21 05:01:30</h6>
<p class='card-text'>Reinforcement learning (RL) has shown impressive results in sequential
decision-making tasks. Meanwhile, Large Language Models (LLMs) and
Vision-Language Models (VLMs) have emerged, exhibiting impressive capabilities
in multimodal understanding and reasoning. These advances have led to a surge
of research integrating LLMs and VLMs into RL. In this survey, we review
representative works in which LLMs and VLMs are used to overcome key challenges
in RL, such as lack of prior knowledge, long-horizon planning, and reward
design. We present a taxonomy that categorizes these LLM/VLM-assisted RL
approaches into three roles: agent, planner, and reward. We conclude by
exploring open problems, including grounding, bias mitigation, improved
representations, and action advice. By consolidating existing research and
identifying future directions, this survey establishes a framework for
integrating LLMs and VLMs into RL, advancing approaches that unify natural
language and visual understanding with sequential decision-making.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.14768v1' target='_blank'>Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, Chong Luo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-20 17:49:26</h6>
<p class='card-text'>Inspired by the success of DeepSeek-R1, we explore the potential of
rule-based reinforcement learning (RL) in large reasoning models. To analyze
reasoning dynamics, we use synthetic logic puzzles as training data due to
their controllable complexity and straightforward answer verification. We make
some key technical contributions that lead to effective and stable RL training:
a system prompt that emphasizes the thinking and answering process, a stringent
format reward function that penalizes outputs for taking shortcuts, and a
straightforward training recipe that achieves stable convergence. Our 7B model
develops advanced reasoning skills-such as reflection, verification, and
summarization-that are absent from the logic corpus. Remarkably, after training
on just 5K logic problems, it demonstrates generalization abilities to the
challenging math benchmarks AIME and AMC.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.14565v1' target='_blank'>ReVISE: Learning to Refine at Test-Time via Intrinsic Self-Verification</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hyunseok Lee, Seunghyuk Oh, Jaehyung Kim, Jinwoo Shin, Jihoon Tack</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-20 13:50:02</h6>
<p class='card-text'>Self-awareness, i.e., the ability to assess and correct one's own generation,
is a fundamental aspect of human intelligence, making its replication in large
language models (LLMs) an important yet challenging task. Previous works tackle
this by employing extensive reinforcement learning or rather relying on large
external verifiers. In this work, we propose Refine via Intrinsic
Self-Verification (ReVISE), an efficient and effective framework that enables
LLMs to self-correct their outputs through self-verification. The core idea of
ReVISE is to enable LLMs to verify their reasoning processes and continually
rethink reasoning trajectories based on its verification. We introduce a
structured curriculum based upon online preference learning to implement this
efficiently. Specifically, as ReVISE involves two challenging tasks (i.e.,
self-verification and reasoning correction), we tackle each task sequentially
using curriculum learning, collecting both failed and successful reasoning
paths to construct preference pairs for efficient training. During inference,
our approach enjoys natural test-time scaling by integrating self-verification
and correction capabilities, further enhanced by our proposed confidence-aware
decoding mechanism. Our experiments on various reasoning tasks demonstrate that
ReVISE achieves efficient self-correction and significantly improves reasoning
performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.14499v1' target='_blank'>MLGym: A New Framework and Benchmark for Advancing AI Research Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Deepak Nathani, Lovish Madaan, Nicholas Roberts, Nikolay Bashlykov, Ajay Menon, Vincent Moens, Amar Budhiraja, Despoina Magka, Vladislav Vorotilov, Gaurav Chaurasia, Dieuwke Hupkes, Ricardo Silveira Cabral, Tatiana Shavrina, Jakob Foerster, Yoram Bachrach, William Yang Wang, Roberta Raileanu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-20 12:28:23</h6>
<p class='card-text'>We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for
evaluating and developing LLM agents on AI research tasks. This is the first
Gym environment for machine learning (ML) tasks, enabling research on
reinforcement learning (RL) algorithms for training such agents. MLGym-bench
consists of 13 diverse and open-ended AI research tasks from diverse domains
such as computer vision, natural language processing, reinforcement learning,
and game theory. Solving these tasks requires real-world AI research skills
such as generating new ideas and hypotheses, creating and processing data,
implementing ML methods, training models, running experiments, analyzing the
results, and iterating through this process to improve on a given task. We
evaluate a number of frontier large language models (LLMs) on our benchmarks
such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5
Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate
models or agents, generate synthetic data at scale, as well as develop new
learning algorithms for training agents on AI research tasks. We find that
current frontier models can improve on the given baselines, usually by finding
better hyperparameters, but do not generate novel hypotheses, algorithms,
architectures, or substantial improvements. We open-source our framework and
benchmark to facilitate future research in advancing the AI research
capabilities of LLM agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.14496v1' target='_blank'>Enhancing Language Multi-Agent Learning with Multi-Agent Credit
  Re-Assignment for Interactive Environment Generalization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhitao He, Zijun Liu, Peng Li, May Fung, Ming Yan, Ji Zhang, Fei Huang, Yang Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-20 12:26:15</h6>
<p class='card-text'>LLM-based agents have made significant advancements in interactive
environments, such as mobile operations and web browsing, and other domains
beyond computer using. Current multi-agent systems universally excel in
performance, compared to single agents, but struggle with generalization across
environments due to predefined roles and inadequate strategies for generalizing
language agents. The challenge of achieving both strong performance and good
generalization has hindered the progress of multi-agent systems for interactive
environments. To address these issues, we propose CollabUIAgents, a multi-agent
reinforcement learning framework with a novel multi-agent credit re-assignment
(CR) strategy, assigning process rewards with LLMs rather than
environment-specific rewards and learning with synthesized preference data, in
order to foster generalizable, collaborative behaviors among the role-free
agents' policies. Empirical results show that our framework improves both
performance and cross-environment generalizability of multi-agent systems.
Moreover, our 7B-parameter system achieves results on par with or exceed strong
closed-source models, and the LLM that guides the CR. We also provide insights
in using granular CR rewards effectively for environment generalization, and
accommodating trained LLMs in multi-agent systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.14340v1' target='_blank'>Earlier Tokens Contribute More: Learning Direct Preference Optimization
  From Temporal Decay Perspective</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ruichen Shao, Bei Li, Gangao Liu, Yang Chen, Xiang Zhou, Jingang Wang, Xunliang Cai, Peng Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-20 07:53:11</h6>
<p class='card-text'>Direct Preference Optimization (DPO) has gained attention as an efficient
alternative to reinforcement learning from human feedback (RLHF) for aligning
large language models (LLMs) with human preferences. Despite its advantages,
DPO suffers from a length bias, generating responses longer than those from the
reference model. Existing solutions like SimPO and SamPO address this issue but
uniformly treat the contribution of rewards across sequences, overlooking
temporal dynamics. To this end, we propose an enhanced preference optimization
method that incorporates a temporal decay factor controlled by a gamma
parameter. This dynamic weighting mechanism adjusts the influence of each
reward based on its position in the sequence, prioritizing earlier tokens that
are more critical for alignment. By adaptively focusing on more relevant
feedback, our approach mitigates overfitting to less pertinent data and remains
responsive to evolving human preferences. Experimental results on several
benchmarks show that our approach consistently outperforms vanilla DPO by
5.9-8.8 points on AlpacaEval 2 and 3.3-9.7 points on Arena-Hard across
different model architectures and sizes. Furthermore, additional experiments on
mathematical and reasoning benchmarks (MMLU, GSM8K, and MATH) confirm that our
method enhances performance without compromising general capabilities. Our
codebase would be available at \url{https://github.com/LotuSrc/D2PO}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.14289v2' target='_blank'>Drift: Decoding-time Personalized Alignments with Implicit User
  Preferences</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Minbeom Kim, Kang-il Lee, Seongho Joo, Hwaran Lee, Kyomin Jung</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-20 06:05:29</h6>
<p class='card-text'>Personalized alignments for individual users have been a long-standing goal
in large language models (LLMs). We introduce Drift, a novel framework that
personalizes LLMs at decoding time with implicit user preferences. Traditional
Reinforcement Learning from Human Feedback (RLHF) requires thousands of
annotated examples and expensive gradient updates. In contrast, Drift
personalizes LLMs in a training-free manner, using only a few dozen examples to
steer a frozen model through efficient preference modeling. Our approach models
user preferences as a composition of predefined, interpretable attributes and
aligns them at decoding time to enable personalized generation. Experiments on
both a synthetic persona dataset (Perspective) and a real human-annotated
dataset (PRISM) demonstrate that Drift significantly outperforms RLHF baselines
while using only 50-100 examples. Our results and analysis show that Drift is
both computationally efficient and interpretable.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.14096v1' target='_blank'>Aligned Multi Objective Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yonathan Efroni, Ben Kertzu, Daniel Jiang, Jalaj Bhandari, Zheqing, Zhu, Karen Ullrich</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-19 20:50:03</h6>
<p class='card-text'>To date, the multi-objective optimization literature has mainly focused on
conflicting objectives, studying the Pareto front, or requiring users to
balance tradeoffs. Yet, in machine learning practice, there are many scenarios
where such conflict does not take place. Recent findings from multi-task
learning, reinforcement learning, and LLMs training show that diverse related
tasks can enhance performance across objectives simultaneously. Despite this
evidence, such phenomenon has not been examined from an optimization
perspective. This leads to a lack of generic gradient-based methods that can
scale to scenarios with a large number of related objectives. To address this
gap, we introduce the Aligned Multi-Objective Optimization framework, propose
new algorithms for this setting, and provide theoretical guarantees of their
superior performance compared to naive approaches.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.13723v1' target='_blank'>Direct Value Optimization: Improving Chain-of-Thought Reasoning in LLMs
  with Refined Values</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hongbo Zhang, Han Cui, Guangsheng Bao, Linyi Yang, Jun Wang, Yue Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-19 13:51:05</h6>
<p class='card-text'>We introduce Direct Value Optimization (DVO), an innovative reinforcement
learning framework for enhancing large language models in complex reasoning
tasks. Unlike traditional methods relying on preference labels, DVO utilizes
value signals at individual reasoning steps, optimizing models via a mean
squared error loss. The key benefit of DVO lies in its fine-grained
supervision, circumventing the need for labor-intensive human annotations.
Target values within the DVO are estimated using either Monte Carlo Tree Search
or an outcome value model. Our empirical analysis on both mathematical and
commonsense reasoning tasks shows that DVO consistently outperforms existing
offline preference optimization techniques, even with fewer training steps.
These findings underscore the importance of value signals in advancing
reasoning capabilities and highlight DVO as a superior methodology under
scenarios lacking explicit human preference information.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.13516v1' target='_blank'>SPPD: Self-training with Process Preference Learning Using Dynamic Value
  Margin</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hao Yi, Qingyang Li, Yulan Hu, Fuzheng Zhang, Di Zhang, Yong Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-19 08:11:26</h6>
<p class='card-text'>Recently, enhancing the numerical and logical reasoning capability of Large
Language Models (LLMs) has emerged as a research hotspot. Existing methods face
several limitations: inference-phase techniques (e.g., Chain of Thoughts) rely
on prompt selection and the pretrained knowledge; sentence-level Supervised
Fine-Tuning (SFT) and Direct Preference Optimization (DPO) struggle with
step-wise mathematical correctness and depend on stronger models distillation
or human annotations; while Reinforcement Learning (RL) approaches incur high
GPU memory costs and unstable training. To address these, we propose
\textbf{S}elf-training framework integrating \textbf{P}rocess
\textbf{P}reference learning using \textbf{D}ynamic value margin (SPPD). SPPD
leverages a process-based Markov Decision Process (MDP) and Bellman optimality
equation to derive \textbf{dynamic value margin} on step-level preference
optimization, which employs tree-based self-sampling on model responses
\textbf{without any distillation} from other models. Furthermore, we
theoretically prove that SPPD is \textbf{equivalent to on-policy policy
gradient methods} under reward constraints. Experiments on 7B-scale models
demonstrate superior performance across in-domain and out-domain mathematical
benchmarks. We open-source our code at
\href{https://anonymous.4open.science/r/SSDPO-D-DCDD}{https://anonymous.4open.science/r/SPPD-DCDD}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.13475v2' target='_blank'>LLM should think and action as a human</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haun Leung, ZiNan Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-19 06:58:34</h6>
<p class='card-text'>It is popular lately to train large language models to be used as chat
assistants, but in the conversation between the user and the chat assistant,
there are prompts, require multi-turns between the chat assistant and the user.
However, there are a number of issues with the multi-turns conversation: The
response of the chat assistant is prone to errors and can't help users achieve
their goals, and as the number of conversation turns increases, the probability
of errors will also increase; It is difficult for chat assistant to generate
responses with different processes based on actual needs for the same prompt;
Chat assistant require the use of tools, but the current approach is not
elegant and efficient, and the number of tool calls is limited. The main reason
for these issues is that large language models don't have the thinking ability
as a human, lack the reasoning ability and planning ability, and lack the
ability to execute plans. To solve these issues, we propose a thinking method
based on a built-in chain of thought: In the multi-turns conversation, for each
user prompt, the large language model thinks based on elements such as chat
history, thinking context, action calls, memory and knowledge, makes detailed
reasoning and planning, and actions according to the plan. We also explored how
the large language model enhances thinking ability through this thinking
method: Collect training datasets according to the thinking method and fine
tune the large language model through supervised learning; Train a consistency
reward model and use it as a reward function to fine tune the large language
model using reinforcement learning, and the reinforced large language model
outputs according to this way of thinking. Our experimental results show that
the reasoning ability and planning ability of the large language model are
enhanced, and the issues in the multi-turns conversation are solved.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.13417v2' target='_blank'>RLTHF: Targeted Human Feedback for LLM Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yifei Xu, Tusher Chakraborty, Emre Kıcıman, Bibek Aryal, Eduardo Rodrigues, Srinagesh Sharma, Roberto Estevao, Maria Angels de Luis Balaguer, Jessica Wolk, Rafael Padilha, Leonardo Nunes, Shobana Balakrishnan, Songwu Lu, Ranveer Chandra</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-19 04:25:11</h6>
<p class='card-text'>Fine-tuning large language models (LLMs) to align with user preferences is
challenging due to the high cost of quality human annotations in Reinforcement
Learning from Human Feedback (RLHF) and the generalizability limitations of AI
Feedback. To address these challenges, we propose RLTHF, a human-AI hybrid
framework that combines LLM-based initial alignment with selective human
annotations to achieve full-human annotation alignment with minimal effort.
RLTHF identifies hard-to-annotate samples mislabeled by LLMs using a reward
model's reward distribution and iteratively enhances alignment by integrating
strategic human corrections while leveraging LLM's correctly labeled samples.
Evaluations on HH-RLHF and TL;DR datasets show that RLTHF reaches full-human
annotation-level alignment with only 6-7% of the human annotation effort.
Furthermore, models trained on RLTHF's curated datasets for downstream tasks
outperform those trained on fully human-annotated datasets, underscoring the
effectiveness of RLTHF's strategic data curation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.13389v1' target='_blank'>Reasoning with Reinforced Functional Token Tuning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kongcheng Zhang, Qi Yao, Baisheng Lai, Jiaxing Huang, Wenkai Fang, Dacheng Tao, Mingli Song, Shunyu Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-19 02:59:42</h6>
<p class='card-text'>In this work, we propose Reinforced Functional Token Tuning (RFTT), a novel
reinforced fine-tuning framework that empowers Large Language Models (LLMs)
with self-play learn-to-reason capabilities. Unlike prior prompt-driven
reasoning efforts, RFTT embeds a rich set of learnable functional tokens (e.g.,
<analyze>, <verify>, <refine>) directly into the model vocabulary, enabling
chain-of-thought construction with diverse human-like reasoning behaviors.
Specifically, RFTT comprises two phases: (1) supervised fine-tuning performs
prompt-driven tree search to obtain self-generated training data annotated with
functional tokens, which warms up the model to learn these tokens for
reasoning; and (2) online reinforcement learning further allows the model to
explore different reasoning pathways through functional token sampling without
relying on prompts, thereby facilitating effective self-improvement for
functional reasoning. Extensive experiments demonstrate the superiority of the
proposed RFTT on mathematical benchmarks, significantly boosting
Qwen-2.5-7B-Instruct (70.6% to 79.8%) and LLaMA-3.1-8B-Instruct (32.2% to
60.2%) on the MATH dataset. Moreover, the performance of RFTT consistently
improves with more search rollouts at inference time. Our code is available at
https://github.com/sastpg/RFTT.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.13388v1' target='_blank'>Reflection of Episodes: Learning to Play Game from Expert and Self
  Experiences</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaojie Xu, Zongyuan Li, Chang Lu, Runnan Qi, Yanan Ni, Lumin Jiang, Xiangbei Liu, Xuebo Zhang, Yongchun Fang, Kuihua Huang, Xian Guo, Zhanghua Wu, Zhenya Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-19 02:53:43</h6>
<p class='card-text'>StarCraft II is a complex and dynamic real-time strategy (RTS) game
environment, which is very suitable for artificial intelligence and
reinforcement learning research. To address the problem of Large Language
Model(LLM) learning in complex environments through self-reflection, we propose
a Reflection of Episodes(ROE) framework based on expert experience and
self-experience. This framework first obtains key information in the game
through a keyframe selection method, then makes decisions based on expert
experience and self-experience. After a game is completed, it reflects on the
previous experience to obtain new self-experience. Finally, in the experiment,
our method beat the robot under the Very Hard difficulty in TextStarCraft II.
We analyze the data of the LLM in the process of the game in detail, verified
its effectiveness.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.13374v1' target='_blank'>Task-agnostic Prompt Compression with Context-aware Sentence Embedding
  and Reward-guided Task Descriptor</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Barys Liskavets, Shuvendu Roy, Maxim Ushakov, Mark Klibanov, Ali Etemad, Shane Luke</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-19 02:16:29</h6>
<p class='card-text'>The rise of Large Language Models (LLMs) has led to significant interest in
prompt compression, a technique aimed at reducing the length of input prompts
while preserving critical information. However, the prominent approaches in
prompt compression often require explicit questions or handcrafted templates
for compression, limiting their generalizability. We propose Task-agnostic
Prompt Compression (TPC), a novel framework that generalizes compression across
tasks and domains without requiring input questions or templates. TPC generates
a context-relevant task description using a task descriptor trained on a
curated dataset of context and query pairs, and fine-tuned via reinforcement
learning with a reward function designed to capture the most relevant
information. The task descriptor is then utilized to compute the relevance of
each sentence in the prompt to generate the compressed prompt. We introduce 3
model sizes (Base, Large, and Huge), where the largest model outperforms the
existing state-of-the-art methods on LongBench and ZeroSCROLLS benchmarks, and
our smallest model performs comparable to the existing solutions while being
considerably smaller.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.13146v1' target='_blank'>Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct
  Preference Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shuo Xing, Yuping Wang, Peiran Li, Ruizheng Bai, Yueqi Wang, Chengxuan Qian, Huaxiu Yao, Zhengzhong Tu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-18 18:59:57</h6>
<p class='card-text'>The emergence of large Vision Language Models (VLMs) has broadened the scope
and capabilities of single-modal Large Language Models (LLMs) by integrating
visual modalities, thereby unlocking transformative cross-modal applications in
a variety of real-world scenarios. Despite their impressive performance, VLMs
are prone to significant hallucinations, particularly in the form of
cross-modal inconsistencies. Building on the success of Reinforcement Learning
from Human Feedback (RLHF) in aligning LLMs, recent advancements have focused
on applying direct preference optimization (DPO) on carefully curated datasets
to mitigate these issues. Yet, such approaches typically introduce preference
signals in a brute-force manner, neglecting the crucial role of visual
information in the alignment process. In this paper, we introduce Re-Align, a
novel alignment framework that leverages image retrieval to construct a
dual-preference dataset, effectively incorporating both textual and visual
preference signals. We further introduce rDPO, an extension of the standard
direct preference optimization that incorporates an additional visual
preference objective during fine-tuning. Our experimental results demonstrate
that Re-Align not only mitigates hallucinations more effectively than previous
methods but also yields significant performance gains in general visual
question-answering (VQA) tasks. Moreover, we show that Re-Align maintains
robustness and scalability across a wide range of VLM sizes and architectures.
This work represents a significant step forward in aligning multimodal LLMs,
paving the way for more reliable and effective cross-modal applications. We
release all the code in https://github.com/taco-group/Re-Align.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.13137v1' target='_blank'>Theorem Prover as a Judge for Synthetic Data Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Joshua Ong Jun Leang, Giwon Hong, Wenda Li, Shay B. Cohen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-18 18:57:09</h6>
<p class='card-text'>The demand for synthetic data in mathematical reasoning has increased due to
its potential to enhance the mathematical capabilities of large language models
(LLMs). However, ensuring the validity of intermediate reasoning steps remains
a significant challenge, affecting data quality. While formal verification via
theorem provers effectively validates LLM reasoning, the autoformalisation of
mathematical proofs remains error-prone. In response, we introduce iterative
autoformalisation, an approach that iteratively refines theorem prover
formalisation to mitigate errors, thereby increasing the execution rate on the
Lean prover from 60% to 87%. Building upon that, we introduce Theorem Prover as
a Judge (TP-as-a-Judge), a method that employs theorem prover formalisation to
rigorously assess LLM intermediate reasoning, effectively integrating
autoformalisation with synthetic data generation. Finally, we present
Reinforcement Learning from Theorem Prover Feedback (RLTPF), a framework that
replaces human annotation with theorem prover feedback in Reinforcement
Learning from Human Feedback (RLHF). Across multiple LLMs, applying
TP-as-a-Judge and RLTPF improves benchmarks with only 3,508 samples, achieving
5.56% accuracy gain on Mistral-7B for MultiArith, 6.00% on Llama-2-7B for
SVAMP, and 3.55% on Llama-3.1-8B for AQUA.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.13092v2' target='_blank'>Text2World: Benchmarking Large Language Models for Symbolic World Model
  Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mengkang Hu, Tianxing Chen, Yude Zou, Yuheng Lei, Qiguang Chen, Ming Li, Yao Mu, Hongyuan Zhang, Wenqi Shao, Ping Luo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-18 17:59:48</h6>
<p class='card-text'>Recently, there has been growing interest in leveraging large language models
(LLMs) to generate symbolic world models from textual descriptions. Although
LLMs have been extensively explored in the context of world modeling, prior
studies encountered several challenges, including evaluation randomness,
dependence on indirect metrics, and a limited domain scope. To address these
limitations, we introduce a novel benchmark, Text2World, based on planning
domain definition language (PDDL), featuring hundreds of diverse domains and
employing multi-criteria, execution-based metrics for a more robust evaluation.
We benchmark current LLMs using Text2World and find that reasoning models
trained with large-scale reinforcement learning outperform others. However,
even the best-performing model still demonstrates limited capabilities in world
modeling. Building on these insights, we examine several promising strategies
to enhance the world modeling capabilities of LLMs, including test-time
scaling, agent training, and more. We hope that Text2World can serve as a
crucial resource, laying the groundwork for future research in leveraging LLMs
as world models. The project page is available at
https://text-to-world.github.io/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.14905v1' target='_blank'>Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema
  Adherence</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bhavik Agarwal, Ishan Joshi, Viktoria Rojkova</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-18 16:44:55</h6>
<p class='card-text'>In this paper, we address the challenge of enforcing strict schema adherence
in large language model (LLM) generation by leveraging LLM reasoning
capabilities. Building on the DeepSeek R1 reinforcement learning framework, our
approach trains structured reasoning skills of a 1.5B parameter model through a
novel pipeline that combines synthetic reasoning dataset construction with
custom reward functions under Group Relative Policy Optimization (GRPO).
Specifically, we first perform R1 reinforcement learning on a 20K sample
unstructured-to-structured dataset, mirroring the original DeepSeek R1 methods,
to establish core reasoning abilities. Subsequently, we performed supervised
fine-tuning on a separate 10K reasoning sample dataset, focusing on refining
schema adherence for downstream tasks. Despite the relatively modest training
scope, requiring approximately 20 hours on an 8xH100 GPU cluster for GRPO
training and 3 hours on 1xA100 for SFT, our model demonstrates robust
performance in enforcing schema consistency. We compare our ThinkJSON approach
against the original DeepSeek R1 (671B), distilled versions of DeepSeek R1
(Qwen-1.5B and Qwen-7B), and Gemini 2.0 Flash (70B), showcasing its
effectiveness in real-world applications. Our results underscore the practical
utility of a resource-efficient framework for schema-constrained text
generation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.13019v2' target='_blank'>Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented
  Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sha Li, Naren Ramakrishnan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-18 16:38:39</h6>
<p class='card-text'>Despite the remarkable capabilities of Large Language Models (LLMs) in
various NLP tasks, they remain vulnerable to hallucinations due to their
limited parametric knowledge and lack of domain-specific expertise.
Retrieval-Augmented Generation (RAG) addresses this challenge by incorporating
external document retrieval to augment the knowledge base of LLMs. In this
approach, RAG retrieves document chunks from an external corpus in response to
a query, which are then used as context for the downstream language model to
generate an answer. However, these retrieved knowledge sources often include
irrelevant or erroneous information, undermining the effectiveness of RAG in
downstream tasks. To overcome this limitation, we introduce a compact,
efficient, and pluggable module designed to refine external knowledge sources
before feeding them to the generator. The module reconstructs retrieved content
by extracting the most relevant and supportive information and reorganising it
into a concise, query-specific format. Through a three-stage training paradigm
- comprising supervised fine-tuning, contrastive multi-task learning, and
reinforcement learning-based alignment - it prioritises critical knowledge and
aligns it with the generator's preferences. This method enables LLMs to produce
outputs that are more accurate, reliable, and contextually appropriate.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.12929v1' target='_blank'>Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking
  Through Options</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lakshmi Nair, Ian Trase, Mark Kim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-18 15:11:46</h6>
<p class='card-text'>We present a novel reasoning approach called Flow-of-Options (FoO), designed
to address intrinsic biases in Large Language Models (LLMs). FoO enables LLMs
to systematically explore a diverse range of possibilities in their reasoning,
as demonstrated by an FoO-based agentic system for autonomously solving Machine
Learning tasks (AutoML). Our framework outperforms state-of-the-art baselines,
achieving improvements of 38.2% - 69.2% on standard data science tasks, and
37.4% - 47.9% on therapeutic chemistry tasks. With an overall operation cost
under $1 per task, our framework is well-suited for cost-sensitive
applications. Beyond classification and regression, we illustrate the broader
applicability of our FoO-based agentic system to tasks such as reinforcement
learning and image generation. Our framework presents significant advancements
compared to current state-of-the-art agentic systems for AutoML, due to the
benefits of FoO in enforcing diversity in LLM solutions through compressed,
explainable representations that also support long-term memory when combined
with case-based reasoning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.12876v1' target='_blank'>Continuous Learning Conversational AI: A Personalized Agent Framework
  via A2C Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nandakishor M, Anjali M</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-18 14:05:59</h6>
<p class='card-text'>Creating personalized and adaptable conversational AI remains a key
challenge. This paper introduces a Continuous Learning Conversational AI (CLCA)
approach, implemented using A2C reinforcement learning, to move beyond static
Large Language Models (LLMs). We use simulated sales dialogues, generated by
LLMs, to train an A2C agent. This agent learns to optimize conversation
strategies for personalization, focusing on engagement and delivering value.
Our system architecture integrates reinforcement learning with LLMs for both
data creation and response selection. This method offers a practical way to
build personalized AI companions that evolve through continuous learning,
advancing beyond traditional static LLM techniques.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.12853v1' target='_blank'>S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ruotian Ma, Peisong Wang, Cheng Liu, Xingyan Liu, Jiaqi Chen, Bang Zhang, Xin Zhou, Nan Du, Jia Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-18 13:40:22</h6>
<p class='card-text'>Recent studies have demonstrated the effectiveness of LLM test-time scaling.
However, existing approaches to incentivize LLMs' deep thinking abilities
generally require large-scale data or significant training efforts. Meanwhile,
it remains unclear how to improve the thinking abilities of less powerful base
models. In this work, we introduce S$^2$R, an efficient framework that enhances
LLM reasoning by teaching models to self-verify and self-correct during
inference. Specifically, we first initialize LLMs with iterative
self-verification and self-correction behaviors through supervised fine-tuning
on carefully curated data. The self-verification and self-correction skills are
then further strengthened by both outcome-level and process-level reinforcement
learning, with minimized resource requirements, enabling the model to
adaptively refine its reasoning process during inference. Our results
demonstrate that, with only 3.1k self-verifying and self-correcting behavior
initialization samples, Qwen2.5-math-7B achieves an accuracy improvement from
51.0\% to 81.6\%, outperforming models trained on an equivalent amount of
long-CoT distilled data. Extensive experiments and analysis based on three base
models across both in-domain and out-of-domain benchmarks validate the
effectiveness of S$^2$R. Our code and data are available at
https://github.com/NineAbyss/S2R.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.12663v1' target='_blank'>Demystifying Multilingual Chain-of-Thought in Process Reward Modeling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weixuan Wang, Minghao Wu, Barry Haddow, Alexandra Birch</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-18 09:11:44</h6>
<p class='card-text'>Large language models (LLMs) are designed to perform a wide range of tasks.
To improve their ability to solve complex problems requiring multi-step
reasoning, recent research leverages process reward modeling to provide
fine-grained feedback at each step of the reasoning process for reinforcement
learning (RL), but it predominantly focuses on English. In this paper, we
tackle the critical challenge of extending process reward models (PRMs) to
multilingual settings. To achieve this, we train multilingual PRMs on a dataset
spanning seven languages, which is translated from English. Through
comprehensive evaluations on two widely used reasoning benchmarks across 11
languages, we demonstrate that multilingual PRMs not only improve average
accuracy but also reduce early-stage reasoning errors. Furthermore, our results
highlight the sensitivity of multilingual PRMs to both the number of training
languages and the volume of English data, while also uncovering the benefits
arising from more candidate responses and trainable parameters. This work opens
promising avenues for robust multilingual applications in complex, multi-step
reasoning tasks. In addition, we release the code to foster research along this
line.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.12486v1' target='_blank'>EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaoqian Liu, Ke Wang, Yongbin Li, Yuchuan Wu, Wentao Ma, Aobo Kong, Fei Huang, Jianbin Jiao, Junge Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-18 03:15:55</h6>
<p class='card-text'>Large Language Models (LLMs) have shown impressive reasoning capabilities in
well-defined problems with clear solutions, such as mathematics and coding.
However, they still struggle with complex real-world scenarios like business
negotiations, which require strategic reasoning-an ability to navigate dynamic
environments and align long-term goals amidst uncertainty. Existing methods for
strategic reasoning face challenges in adaptability, scalability, and
transferring strategies to new contexts. To address these issues, we propose
explicit policy optimization (EPO) for strategic reasoning, featuring an LLM
that provides strategies in open-ended action space and can be plugged into
arbitrary LLM agents to motivate goal-directed behavior. To improve
adaptability and policy transferability, we train the strategic reasoning model
via multi-turn reinforcement learning (RL) using process rewards and iterative
self-play, without supervised fine-tuning (SFT) as a preliminary step.
Experiments across social and physical domains demonstrate EPO's ability of
long-term goal alignment through enhanced strategic reasoning, achieving
state-of-the-art performance on social dialogue and web navigation tasks. Our
findings reveal various collaborative reasoning mechanisms emergent in EPO and
its effectiveness in generating novel strategies, underscoring its potential
for strategic reasoning in real-world applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.12272v3' target='_blank'>Learning to Reason at the Frontier of Learnability</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Thomas Foster, Jakob Foerster</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-17 19:16:37</h6>
<p class='card-text'>Reinforcement learning is now widely adopted as the final stage of large
language model training, especially for reasoning-style tasks such as maths
problems. Typically, models attempt each question many times during a single
training step and attempt to learn from their successes and failures. However,
we demonstrate that throughout training with two popular algorithms (PPO and
VinePPO) on two widely used datasets, many questions are either solved by all
attempts - meaning they are already learned - or by none - providing no
meaningful training signal. To address this, we adapt a method from the
reinforcement learning literature - sampling for learnability - and apply it to
the reinforcement learning stage of LLM training. Our curriculum prioritises
questions with high variance of success, i.e. those where the agent sometimes
succeeds, but not always. Our findings demonstrate that this curriculum
consistently boosts training performance across multiple algorithms and
datasets, paving the way for more efficient and effective reinforcement
learning with LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.12118v2' target='_blank'>Scaling Test-Time Compute Without Verification or RL is Suboptimal</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Amrith Setlur, Nived Rajaraman, Sergey Levine, Aviral Kumar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-17 18:43:24</h6>
<p class='card-text'>Despite substantial advances in scaling test-time compute, an ongoing debate
in the community is how it should be scaled up to enable continued and
efficient improvements with scaling. There are largely two approaches: first,
distilling successful search or thinking traces; and second, using verification
(e.g., 0/1 outcome rewards, reward models, or verifiers) to guide reinforcement
learning (RL) and search algorithms. In this paper, we prove that finetuning
LLMs with verifier-based (VB) methods based on RL or search is far superior to
verifier-free (VF) approaches based on distilling or cloning search traces,
given a fixed amount of compute/data budget. Further, we show that as we scale
test-time compute (measured as the output token length) and training data,
suboptimality of VF methods scales poorly compared to VB when the base
pre-trained LLM presents a heterogeneous distribution over correct solution
traces (e.g., different lengths, styles, etc.) and admits a non-sharp
distribution over rewards on traces sampled from it. We formalize this
condition using anti-concentration [Erd\H{o}s, 1945]. This implies a stronger
result that VB methods scale better asymptotically, with the performance gap
between VB and VF methods widening as test-time budget grows. We corroborate
our theory empirically on both didactic and math reasoning problems with
3/8/32B-sized pre-trained LLMs, where we find verification is crucial for
scaling test-time compute.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.11896v1' target='_blank'>CAMEL: Continuous Action Masking Enabled by Large Language Models for
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yanxiao Zhao, Yangge Qian, Jingyang Shan, Xiaolin Qin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-17 15:22:19</h6>
<p class='card-text'>Reinforcement learning (RL) in continuous action spaces encounters persistent
challenges, such as inefficient exploration and convergence to suboptimal
solutions. To address these limitations, we propose CAMEL, a novel framework
integrating LLM-generated suboptimal policies into the RL training pipeline.
CAMEL leverages dynamic action masking and an adaptive epsilon-masking
mechanism to guide exploration during early training stages while gradually
enabling agents to optimize policies independently. At the core of CAMEL lies
the integration of Python-executable suboptimal policies generated by LLMs
based on environment descriptions and task objectives. Although simplistic and
hard-coded, these policies offer valuable initial guidance for RL agents. To
effectively utilize these priors, CAMEL employs masking-aware optimization to
dynamically constrain the action space based on LLM outputs. Additionally,
epsilon-masking gradually reduces reliance on LLM-generated guidance, enabling
agents to transition from constrained exploration to autonomous policy
refinement. Experimental validation on Gymnasium MuJoCo environments
demonstrates the effectiveness of CAMEL. In Hopper-v4 and Ant-v4, LLM-generated
policies significantly improve sample efficiency, achieving performance
comparable to or surpassing expert masking baselines. For Walker2d-v4, where
LLMs struggle to accurately model bipedal gait dynamics, CAMEL maintains robust
RL performance without notable degradation, highlighting the framework's
adaptability across diverse tasks. While CAMEL shows promise in enhancing
sample efficiency and mitigating convergence challenges, these issues remain
open for further research. Future work aims to generalize CAMEL to multimodal
LLMs for broader observation-action spaces and automate policy evaluation,
reducing human intervention and enhancing scalability in RL training pipelines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.11733v1' target='_blank'>Plant in Cupboard, Orange on Table, Book on Shelf. Benchmarking
  Practical Reasoning and Situation Modelling in a Text-Simulated Situated
  Environment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jonathan Jordan, Sherzod Hakimov, David Schlangen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-17 12:20:39</h6>
<p class='card-text'>Large language models (LLMs) have risen to prominence as 'chatbots' for users
to interact via natural language. However, their abilities to capture
common-sense knowledge make them seem promising as language-based planners of
situated or embodied action as well. We have implemented a simple text-based
environment -- similar to others that have before been used for
reinforcement-learning of agents -- that simulates, very abstractly, a
household setting. We use this environment and the detailed error-tracking
capabilities we implemented for targeted benchmarking of LLMs on the problem of
practical reasoning: Going from goals and observations to actions. Our findings
show that environmental complexity and game restrictions hamper performance,
and concise action planning is demanding for current LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.11555v1' target='_blank'>Equilibrate RLHF: Towards Balancing Helpfulness-Safety Trade-off in
  Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yingshui Tan, Yilei Jiang, Yanshi Li, Jiaheng Liu, Xingyuan Bu, Wenbo Su, Xiangyu Yue, Xiaoyong Zhu, Bo Zheng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-17 08:40:30</h6>
<p class='card-text'>Fine-tuning large language models (LLMs) based on human preferences, commonly
achieved through reinforcement learning from human feedback (RLHF), has been
effective in improving their performance. However, maintaining LLM safety
throughout the fine-tuning process remains a significant challenge, as
resolving conflicts between safety and helpfulness can be non-trivial.
Typically, the safety alignment of LLM is trained on data with safety-related
categories. However, our experiments find that naively increasing the scale of
safety training data usually leads the LLMs to an ``overly safe'' state rather
than a ``truly safe'' state, boosting the refusal rate through extensive
safety-aligned data without genuinely understanding the requirements for safe
responses. Such an approach can inadvertently diminish the models' helpfulness.
To understand the phenomenon, we first investigate the role of safety data by
categorizing them into three different groups, and observe that each group
behaves differently as training data scales up. To boost the balance between
safety and helpfulness, we propose an Equilibrate RLHF framework including a
Fine-grained Data-centric (FDC) approach that achieves better safety alignment
even with fewer training data, and an Adaptive Message-wise Alignment (AMA)
approach, which selectively highlight the key segments through a gradient
masking strategy. Extensive experimental results demonstrate that our approach
significantly enhances the safety alignment of LLMs while balancing safety and
helpfulness.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.11433v3' target='_blank'>FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning
  for Financial Trading</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guojun Xiong, Zhiyang Deng, Keyi Wang, Yupeng Cao, Haohang Li, Yangyang Yu, Xueqing Peng, Mingquan Lin, Kaleb E Smith, Xiao-Yang Liu, Jimin Huang, Sophia Ananiadou, Qianqian Xie</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-17 04:45:53</h6>
<p class='card-text'>Large language models (LLMs) fine-tuned on multimodal financial data have
demonstrated impressive reasoning capabilities in various financial tasks.
However, they often struggle with multi-step, goal-oriented scenarios in
interactive financial markets, such as trading, where complex agentic
approaches are required to improve decision-making. To address this, we propose
\textsc{FLAG-Trader}, a unified architecture integrating linguistic processing
(via LLMs) with gradient-driven reinforcement learning (RL) policy
optimization, in which a partially fine-tuned LLM acts as the policy network,
leveraging pre-trained knowledge while adapting to the financial domain through
parameter-efficient fine-tuning. Through policy gradient optimization driven by
trading rewards, our framework not only enhances LLM performance in trading but
also improves results on other financial-domain tasks. We present extensive
empirical evidence to validate these enhancements.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.11386v1' target='_blank'>Intelligent Mobile AI-Generated Content Services via Interactive Prompt
  Engineering and Dynamic Service Provisioning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yinqiu Liu, Ruichen Zhang, Jiacheng Wang, Dusit Niyato, Xianbin Wang, Dong In Kim, Hongyang Du</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-17 03:05:20</h6>
<p class='card-text'>Due to massive computational demands of large generative models, AI-Generated
Content (AIGC) can organize collaborative Mobile AIGC Service Providers (MASPs)
at network edges to provide ubiquitous and customized content generation for
resource-constrained users. However, such a paradigm faces two significant
challenges: 1) raw prompts (i.e., the task description from users) often lead
to poor generation quality due to users' lack of experience with specific AIGC
models, and 2) static service provisioning fails to efficiently utilize
computational and communication resources given the heterogeneity of AIGC
tasks. To address these challenges, we propose an intelligent mobile AIGC
service scheme. Firstly, we develop an interactive prompt engineering mechanism
that leverages a Large Language Model (LLM) to generate customized prompt
corpora and employs Inverse Reinforcement Learning (IRL) for policy imitation
through small-scale expert demonstrations. Secondly, we formulate a dynamic
mobile AIGC service provisioning problem that jointly optimizes the number of
inference trials and transmission power allocation. Then, we propose the
Diffusion-Enhanced Deep Deterministic Policy Gradient (D3PG) algorithm to solve
the problem. By incorporating the diffusion process into Deep Reinforcement
Learning (DRL) architecture, the environment exploration capability can be
improved, thus adapting to varying mobile AIGC scenarios. Extensive
experimental results demonstrate that our prompt engineering approach improves
single-round generation success probability by 6.3 times, while D3PG increases
the user service experience by 67.8% compared to baseline DRL approaches.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.12206v1' target='_blank'>Evaluating the Paperclip Maximizer: Are RL-Based Language Models More
  Likely to Pursue Instrumental Goals?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yufei He, Yuexin Li, Jiaying Wu, Yuan Sui, Yulin Chen, Bryan Hooi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-16 16:29:20</h6>
<p class='card-text'>As large language models (LLMs) continue to evolve, ensuring their alignment
with human goals and values remains a pressing challenge. A key concern is
\textit{instrumental convergence}, where an AI system, in optimizing for a
given objective, develops unintended intermediate goals that override the
ultimate objective and deviate from human-intended goals. This issue is
particularly relevant in reinforcement learning (RL)-trained models, which can
generate creative but unintended strategies to maximize rewards. In this paper,
we explore instrumental convergence in LLMs by comparing models trained with
direct RL optimization (e.g., the o1 model) to those trained with reinforcement
learning from human feedback (RLHF). We hypothesize that RL-driven models
exhibit a stronger tendency for instrumental convergence due to their
optimization of goal-directed behavior in ways that may misalign with human
intentions. To assess this, we introduce InstrumentalEval, a benchmark for
evaluating instrumental convergence in RL-trained LLMs. Initial experiments
reveal cases where a model tasked with making money unexpectedly pursues
instrumental objectives, such as self-replication, implying signs of
instrumental convergence. Our findings contribute to a deeper understanding of
alignment challenges in AI systems and the risks posed by unintended model
behaviors.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.11078v1' target='_blank'>DEEPER Insight into Your User: Directed Persona Refinement for Dynamic
  Persona Modeling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aili Chen, Chengyu Du, Jiangjie Chen, Jinghan Xu, Yikai Zhang, Siyu Yuan, Zulong Chen, Liangyue Li, Yanghua Xiao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-16 11:02:37</h6>
<p class='card-text'>To advance personalized applications such as recommendation systems and user
behavior prediction, recent research increasingly adopts large language models
(LLMs) for human -readable persona modeling. In dynamic real -world scenarios,
effective persona modeling necessitates leveraging streaming behavior data to
continually optimize user personas. However, existing methods -whether
regenerating personas or incrementally extending them with new behaviors -often
fail to achieve sustained improvements in persona quality or future behavior
prediction accuracy. To address this, we propose DEEPER, a novel approach for
dynamic persona modeling that enables continual persona optimization.
Specifically, we enhance the model's direction -search capability through an
iterative reinforcement learning framework, allowing it to automatically
identify effective update directions and optimize personas using discrepancies
between user behaviors and model predictions. Extensive experiments on dynamic
persona modeling involving 4800 users across 10 domains highlight the superior
persona optimization capabilities of DEEPER, delivering an impressive 32.2%
average reduction in user behavior prediction error over four update rounds
-outperforming the best baseline by a remarkable 22.92%.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.11066v1' target='_blank'>CARMA: Enhanced Compositionality in LLMs via Advanced Regularisation and
  Mutual Information Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nura Aljaafari, Danilo S. Carvalho, André Freitas</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-16 10:18:15</h6>
<p class='card-text'>Large language models (LLMs) struggle with compositional generalisation,
limiting their ability to systematically combine learned components to
interpret novel inputs. While architectural modifications, fine-tuning, and
data augmentation improve compositionality, they often have limited
adaptability, face scalability constraints, or yield diminishing returns on
real data. To address this, we propose CARMA, an intervention that enhances the
stability and robustness of compositional reasoning in LLMs while preserving
fine-tuned performance. CARMA employs mutual information regularisation and
layer-wise stability constraints to mitigate feature fragmentation, ensuring
structured representations persist across and within layers. We evaluate CARMA
on inverse dictionary modelling and sentiment classification, measuring its
impact on semantic consistency, performance stability, and robustness to
lexical perturbations. Results show that CARMA reduces the variability
introduced by fine-tuning, stabilises token representations, and improves
compositional reasoning. While its effectiveness varies across architectures,
CARMA's key strength lies in reinforcing learned structures rather than
introducing new capabilities, making it a scalable auxiliary method. These
findings suggest that integrating CARMA with fine-tuning can improve
compositional generalisation while maintaining task-specific performance in
LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.11026v2' target='_blank'>Simplify RLHF as Reward-Weighted SFT: A Variational Method</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuhao Du, Zhuo Li, Pengyu Cheng, Zhihong Chen, Yuejiao Xie, Xiang Wan, Anningzhe Gao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-16 07:22:00</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning
Large Language Models (LLMs) with human values. However, RLHF has been
continuously challenged by its high complexity in implementation and
computation consumption. Even with recent simplifications, such as Direct
Preference Optimization (DPO) and Advantage Leftover Lunch (A-LoL), the
problems of over-fitting and training instability remain hindering the
alignment process from the expected optimal performance. To address the
existing challenges, we propose a novel simplification of RLHF from the
perspective of variational inference, called $\textbf{V}$ariational
$\textbf{A}$lignment with $\textbf{R}$e-weighting ($\textbf{VAR}$). More
specifically, by directly minimizing the distribution gap between the learning
LLM policy and the optimal solution of RLHF, we transform the alignment
objective into a reward-driven re-weighted supervised fine-tuning (SFT) form,
which only requires minor adjustment on the SFT loss to obtain noticeable
improvement on training stability and effectiveness. On comprehensive alignment
and generation benchmarks, our VAR method has numerically achieved competitive
performance in LLM alignment helpfulness and harmlessness.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.11007v1' target='_blank'>Local-Cloud Inference Offloading for LLMs in Multi-Modal, Multi-Task,
  Multi-Dialogue Settings</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Liangqi Yuan, Dong-Jun Han, Shiqiang Wang, Christopher G. Brinton</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-16 06:18:28</h6>
<p class='card-text'>Compared to traditional machine learning models, recent large language models
(LLMs) can exhibit multi-task-solving capabilities through multiple dialogues
and multi-modal data sources. These unique characteristics of LLMs, beyond
their large size, make their deployment more challenging during the inference
stage. Specifically, (i) deploying LLMs on local devices faces computational,
memory, and energy resource issues, while (ii) deploying them in the cloud
cannot guarantee real-time service and incurs communication/usage costs. In
this paper, we design a local-cloud LLM inference offloading (LCIO) system,
featuring (i) a large-scale cloud LLM that can handle multi-modal data sources
and (ii) a lightweight local LLM that can process simple tasks at high speed.
LCIO employs resource-constrained reinforcement learning (RCRL) to determine
where to make the inference (i.e., local vs. cloud) and which multi-modal data
sources to use for each dialogue/task, aiming to maximize the long-term reward
(which incorporates response quality, latency, and usage cost) while adhering
to resource constraints. We also propose M4A1, a new dataset that accounts for
multi-modal, multi-task, multi-dialogue, and multi-LLM characteristics, to
investigate the capabilities of LLMs in various practical scenarios. We
demonstrate the effectiveness of LCIO compared to baselines, showing
significant savings in latency and cost while achieving satisfactory response
quality.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.10906v1' target='_blank'>PCGRLLM: Large Language Model-Driven Reward Design for Procedural
  Content Generation Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:In-Chang Baek, Sung-Hyun Kim, Sam Earle, Zehua Jiang, Noh Jin-Ha, Julian Togelius, Kyung-Joong Kim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-15 21:00:40</h6>
<p class='card-text'>Reward design plays a pivotal role in the training of game AIs, requiring
substantial domain-specific knowledge and human effort. In recent years,
several studies have explored reward generation for training game agents and
controlling robots using large language models (LLMs). In the content
generation literature, there has been early work on generating reward functions
for reinforcement learning agent generators. This work introduces PCGRLLM, an
extended architecture based on earlier work, which employs a feedback mechanism
and several reasoning-based prompt engineering techniques. We evaluate the
proposed method on a story-to-reward generation task in a two-dimensional
environment using two state-of-the-art LLMs, demonstrating the generalizability
of our approach. Our experiments provide insightful evaluations that
demonstrate the capabilities of LLMs essential for content generation tasks.
The results highlight significant performance improvements of 415% and 40%
respectively, depending on the zero-shot capabilities of the language model.
Our work demonstrates the potential to reduce human dependency in game AI
development, while supporting and enhancing creative processes.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.10867v1' target='_blank'>A Tutorial on LLM Reasoning: Relevant Methods behind ChatGPT o1</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jun Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-15 17:52:11</h6>
<p class='card-text'>OpenAI o1 has shown that applying reinforcement learning to integrate
reasoning steps directly during inference can significantly improve a model's
reasoning capabilities. This result is exciting as the field transitions from
the conventional autoregressive method of generating answers to a more
deliberate approach that models the slow-thinking process through step-by-step
reasoning training. Reinforcement learning plays a key role in both the model's
training and decoding processes. In this article, we present a comprehensive
formulation of reasoning problems and investigate the use of both model-based
and model-free approaches to better support this slow-thinking framework.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.10732v1' target='_blank'>Rule-Bottleneck Reinforcement Learning: Joint Explanation and Decision
  Optimization for Resource Allocation with Language Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mauricio Tec, Guojun Xiong, Haichuan Wang, Francesca Dominici, Milind Tambe</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-15 09:01:31</h6>
<p class='card-text'>Deep Reinforcement Learning (RL) is remarkably effective in addressing
sequential resource allocation problems in domains such as healthcare, public
policy, and resource management. However, deep RL policies often lack
transparency and adaptability, challenging their deployment alongside human
decision-makers. In contrast, Language Agents, powered by large language models
(LLMs), provide human-understandable reasoning but may struggle with effective
decision making. To bridge this gap, we propose Rule-Bottleneck Reinforcement
Learning (RBRL), a novel framework that jointly optimizes decision and
explanations. At each step, RBRL generates candidate rules with an LLM, selects
among them using an attention-based RL policy, and determines the environment
action with an explanation via chain-of-thought reasoning. The RL rule
selection is optimized using the environment rewards and an explainability
metric judged by the LLM. Evaluations in real-world scenarios highlight RBRL's
competitive performance with deep RL and efficiency gains over LLM fine-tuning.
A survey further confirms the enhanced quality of its explanations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.15763v1' target='_blank'>Hybrid Offline-online Scheduling Method for Large Language Model
  Inference Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bowen Pang, Kai Li, Ruifeng She, Feifan Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-14 16:00:00</h6>
<p class='card-text'>With the development of large language models (LLMs), it has become
increasingly important to optimize hardware usage and improve throughput. In
this paper, we study the inference optimization of the serving system that
deploys LLMs. To optimize system throughput and maximize hardware utilization,
we formulate the inference optimization problem as a mixed-integer programming
(MIP) model and propose a hybrid offline-online method as solution. The offline
method improves large-scale inference systems by introducing a Minimizing
Makespan Bin Packing Problem. We further provide a theoretical lower bound
computation method. Then, we propose an online sorting and preemptive
scheduling method to better utilize hardware. In the online iteration
scheduling process, a Lagrangian method is applied to evaluate the cost
efficiency of inserting prefill stages versus decode stages at each iteration
and dynamically determine when to preempt decoding tasks and insert prefill
tasks. Experiments using real-world data from the LLaMA-65B model and the GSM8K
dataset demonstrate that system utilization improves from 80.2% to 89.1%, and
the total inference time decreases from 201.00 to 190.58 seconds. A 100-cases
study shows that our method consistently outperforms the baseline method and
improves the utilization rate by 8.0% on average. Finally, we discuss potential
future extensions, including stochastic modeling, reinforcement learning-based
schedulers, and dynamic decision-making strategies for system throughput and
hardware utilization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.10148v1' target='_blank'>Cooperative Multi-Agent Planning with Adaptive Skill Synthesis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiyuan Li, Wenshuai Zhao, Joni Pajarinen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-14 13:23:18</h6>
<p class='card-text'>Despite much progress in training distributed artificial intelligence (AI),
building cooperative multi-agent systems with multi-agent reinforcement
learning (MARL) faces challenges in sample efficiency, interpretability, and
transferability. Unlike traditional learning-based methods that require
extensive interaction with the environment, large language models (LLMs)
demonstrate remarkable capabilities in zero-shot planning and complex
reasoning. However, existing LLM-based approaches heavily rely on text-based
observations and struggle with the non-Markovian nature of multi-agent
interactions under partial observability. We present COMPASS, a novel
multi-agent architecture that integrates vision-language models (VLMs) with a
dynamic skill library and structured communication for decentralized
closed-loop decision-making. The skill library, bootstrapped from
demonstrations, evolves via planner-guided tasks to enable adaptive strategies.
COMPASS propagates entity information through multi-hop communication under
partial observability. Evaluations on the improved StarCraft Multi-Agent
Challenge (SMACv2) demonstrate COMPASS achieves up to 30\% higher win rates
than state-of-the-art MARL algorithms in symmetric scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.09955v1' target='_blank'>Diverse Inference and Verification for Advanced Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Iddo Drori, Gaston Longhitano, Mao Mao, Seunghwan Hyun, Yuke Zhang, Sungjun Park, Zachary Meeks, Xin-Yu Zhang, Ben Segev, Howard Yong, Nakul Verma, Avi Shporer, Alon Amit, Madeleine Udell</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-14 07:22:25</h6>
<p class='card-text'>Reasoning LLMs such as OpenAI o1, o3 and DeepSeek R1 have made significant
progress in mathematics and coding, yet find challenging advanced tasks such as
International Mathematical Olympiad (IMO) combinatorics problems, Abstraction
and Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) questions.
We use a diverse inference approach that combines multiple models and methods
at test time. We find that verifying mathematics and code problems, and
rejection sampling on other problems is simple and effective. We automatically
verify correctness of solutions to IMO problems by Lean, and ARC puzzles by
code, and find that best-of-N effectively answers HLE questions. Our approach
increases answer accuracy on IMO combinatorics problems from 33.3% to 77.8%,
accuracy on HLE questions from 8% to 37%, and solves 80% of ARC puzzles that
948 humans could not and 26.5% of ARC puzzles that o3 high compute does not.
Test-time simulations, reinforcement learning, and meta-learning with inference
feedback improve generalization by adapting agent graph representations and
varying prompts, code, and datasets. Our approach is reliable, robust, and
scalable, and in the spirit of reproducible research, we will make it publicly
available upon publication.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.09886v1' target='_blank'>Video2Policy: Scaling up Manipulation Tasks in Simulation through
  Internet Videos</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weirui Ye, Fangchen Liu, Zheng Ding, Yang Gao, Oleh Rybkin, Pieter Abbeel</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-14 03:22:03</h6>
<p class='card-text'>Simulation offers a promising approach for cheaply scaling training data for
generalist policies. To scalably generate data from diverse and realistic
tasks, existing algorithms either rely on large language models (LLMs) that may
hallucinate tasks not interesting for robotics; or digital twins, which require
careful real-to-sim alignment and are hard to scale. To address these
challenges, we introduce Video2Policy, a novel framework that leverages
internet RGB videos to reconstruct tasks based on everyday human behavior. Our
approach comprises two phases: (1) task generation in simulation from videos;
and (2) reinforcement learning utilizing in-context LLM-generated reward
functions iteratively. We demonstrate the efficacy of Video2Policy by
reconstructing over 100 videos from the Something-Something-v2 (SSv2) dataset,
which depicts diverse and complex human behaviors on 9 different tasks. Our
method can successfully train RL policies on such tasks, including complex and
challenging tasks such as throwing. Finally, we show that the generated
simulation data can be scaled up for training a general policy, and it can be
transferred back to the real robot in a Real2Sim2Real way.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.09854v1' target='_blank'>Efficient Multitask Learning in Small Language Models Through
  Upside-Down Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yu-Chen Lin, Sanat Sharma, Hari Manikandan, Jayant Kumar, Tracy Holloway King, Jing Zheng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-14 01:39:45</h6>
<p class='card-text'>In this work, we demonstrate that small language models (SLMs), specifically
a 100M parameter GPT-2 model, can achieve competitive performance in multitask
prompt generation tasks while requiring only a fraction of the computational
resources needed by large language models (LLMs). Through a novel combination
of upside-down reinforcement learning and synthetic data distillation from a
powerful LLM, Llama-3, we train an SLM that achieves relevance scores within 5%
of state-of-the-art models, including Llama-3, Qwen2, and Mistral, despite
being up to 80 times smaller, making it highly suitable for
resource-constrained and real-time applications. This study highlights the
potential of SLMs as efficient multitask learners in multimodal settings,
providing a promising alternative to LLMs for scalable, low-latency
deployments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.09100v1' target='_blank'>Logical Reasoning in Large Language Models: A Survey</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hanmeng Liu, Zhizhang Fu, Mengru Ding, Ruoxi Ning, Chaoli Zhang, Xiaozhang Liu, Yue Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-13 09:19:14</h6>
<p class='card-text'>With the emergence of advanced reasoning models like OpenAI o3 and
DeepSeek-R1, large language models (LLMs) have demonstrated remarkable
reasoning capabilities. However, their ability to perform rigorous logical
reasoning remains an open question. This survey synthesizes recent advancements
in logical reasoning within LLMs, a critical area of AI research. It outlines
the scope of logical reasoning in LLMs, its theoretical foundations, and the
benchmarks used to evaluate reasoning proficiency. We analyze existing
capabilities across different reasoning paradigms - deductive, inductive,
abductive, and analogical - and assess strategies to enhance reasoning
performance, including data-centric tuning, reinforcement learning, decoding
strategies, and neuro-symbolic approaches. The review concludes with future
directions, emphasizing the need for further exploration to strengthen logical
reasoning in AI systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.09042v1' target='_blank'>Typhoon T1: An Open Thai Reasoning Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pittawat Taveekitworachai, Potsawee Manakul, Kasima Tharnpipitchai, Kunat Pipatanakul</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-13 07:55:54</h6>
<p class='card-text'>This paper introduces Typhoon T1, an open effort to develop an open Thai
reasoning model. A reasoning model is a relatively new type of generative model
built on top of large language models (LLMs). A reasoning model generates a
long chain of thought before arriving at a final answer, an approach found to
improve performance on complex tasks. However, details on developing such a
model are limited, especially for reasoning models that can generate traces in
a low-resource language. Typhoon T1 presents an open effort that dives into the
details of developing a reasoning model in a more cost-effective way by
leveraging supervised fine-tuning using open datasets, instead of reinforcement
learning. This paper shares the details about synthetic data generation and
training, as well as our dataset and model weights. Additionally, we provide
insights gained from developing a reasoning model that generalizes across
domains and is capable of generating reasoning traces in a low-resource
language, using Thai as an example. We hope this open effort provides a
foundation for further research in this field.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.08908v1' target='_blank'>Reinforced Large Language Model is a formal theorem prover</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiling Luo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-13 02:49:58</h6>
<p class='card-text'>To take advantage of Large Language Model in theorem formalization and proof,
we propose a reinforcement learning framework to iteratively optimize the
pretrained LLM by rolling out next tactics and comparing them with the expected
ones. The experiment results show that it helps to achieve a higher accuracy
compared with directly fine-tuned LLM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.08127v1' target='_blank'>Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lingfei Qian, Weipeng Zhou, Yan Wang, Xueqing Peng, Jimin Huang, Qianqian Xie</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-12 05:13:04</h6>
<p class='card-text'>Recent advancements in large language models (LLMs) have shown strong general
reasoning abilities, yet their effectiveness in financial reasoning remains
underexplored. In this study, we comprehensively evaluate 16 powerful reasoning
and general LLMs on three complex financial tasks involving financial text,
tabular data, and equations, assessing numerical reasoning, tabular
interpretation, financial terminology comprehension, long-context processing,
and equation-based problem solving. Our results show that while better datasets
and pretraining improve financial reasoning, general enhancements like CoT
fine-tuning do not always yield consistent gains. Moreover, all reasoning
strategies face challenges in improving performance on long-context and
multi-table tasks. To address these limitations, we develop a financial
reasoning-enhanced model based on Llama-3.1-8B-Instruct, by CoT fine-tuning and
reinforcement learning with domain-specific reasoning paths. Even with simple
fine-tuning with one financial dataset, our model achieves a consistent 10%
performance improvement across tasks, surpassing all 8B models and even
Llama3-70B-Instruct and Llama3.1-70B-Instruct on average. Our results highlight
the need for domain-specific adaptations in financial tasks, emphasizing future
directions such as multi-table reasoning, long-context processing, and
financial terminology comprehension. All our datasets, models, and codes are
publicly available. Furthermore, we introduce a leaderboard for benchmarking
future datasets and models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.07912v1' target='_blank'>Elevating Legal LLM Responses: Harnessing Trainable Logical Structures
  and Semantic Knowledge with Legal Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rujing Yao, Yang Wu, Chenghao Wang, Jingwei Xiong, Fang Wang, Xiaozhong Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-11 19:33:07</h6>
<p class='card-text'>Large Language Models (LLMs) have achieved impressive results across numerous
domains, yet they experience notable deficiencies in legal question-answering
tasks. LLMs often generate generalized responses that lack the logical
specificity required for expert legal advice and are prone to hallucination,
providing answers that appear correct but are unreliable. Retrieval-Augmented
Generation (RAG) techniques offer partial solutions to address this challenge,
but existing approaches typically focus only on semantic similarity, neglecting
the logical structure essential to legal reasoning. In this paper, we propose
the Logical-Semantic Integration Model (LSIM), a novel supervised framework
that bridges semantic and logical coherence. LSIM comprises three components:
reinforcement learning predicts a structured fact-rule chain for each question,
a trainable Deep Structured Semantic Model (DSSM) retrieves the most relevant
candidate questions by integrating semantic and logical features, and
in-context learning generates the final answer using the retrieved content. Our
experiments on a real-world legal QA dataset-validated through both automated
metrics and human evaluation-demonstrate that LSIM significantly enhances
accuracy and reliability compared to existing methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.07640v2' target='_blank'>Goedel-Prover: A Frontier Model for Open-Source Automated Theorem
  Proving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yong Lin, Shange Tang, Bohan Lyu, Jiayun Wu, Hongzhou Lin, Kaiyu Yang, Jia Li, Mengzhou Xia, Danqi Chen, Sanjeev Arora, Chi Jin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-11 15:27:35</h6>
<p class='card-text'>We introduce Goedel-Prover, an open-source large language model (LLM) that
achieves the state-of-the-art (SOTA) performance in automated formal proof
generation for mathematical problems. The key challenge in this field is the
scarcity of formalized math statements and proofs, which we tackle in the
following ways. We train statement formalizers to translate the natural
language math problems from Numina into formal language (Lean 4), creating a
dataset of 1.64 million formal statements. LLMs are used to check that the
formal statements accurately preserve the content of the original natural
language problems. We then iteratively build a large dataset of formal proofs
by training a series of provers. Each prover succeeds in proving many
statements that the previous ones could not, and these new proofs are added to
the training set for the next prover. Despite using only supervised
fine-tuning, our final prover significantly outperforms the previous best
open-source model, DeepSeek-Prover-V1.5, which employs reinforcement learning.
On the miniF2F benchmark, our model achieves a success rate of 57.6% (Pass@32),
surpassing DeepSeek-Prover-V1.5 by 7.6%. On PutnamBench, Goedel-Prover
successfully solves 7 problems (Pass@512), ranking first on the leaderboard.
Furthermore, it generates 29.7K formal proofs for Lean Workbook problems,
nearly doubling the 15.7K produced by earlier works.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.07460v2' target='_blank'>Logarithmic Regret for Online KL-Regularized Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Heyang Zhao, Chenlu Ye, Wei Xiong, Quanquan Gu, Tong Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-11 11:11:05</h6>
<p class='card-text'>Recent advances in Reinforcement Learning from Human Feedback (RLHF) have
shown that KL-regularization plays a pivotal role in improving the efficiency
of RL fine-tuning for large language models (LLMs). Despite its empirical
advantage, the theoretical difference between KL-regularized RL and standard RL
remains largely under-explored. While there is a recent line of work on the
theoretical analysis of KL-regularized objective in decision making
\citep{xiong2024iterative, xie2024exploratory,zhao2024sharp}, these analyses
either reduce to the traditional RL setting or rely on strong coverage
assumptions. In this paper, we propose an optimism-based KL-regularized online
contextual bandit algorithm, and provide a novel analysis of its regret. By
carefully leveraging the benign optimization landscape induced by the
KL-regularization and the optimistic reward estimation, our algorithm achieves
an $\mathcal{O}\big(\eta\log (N_{\mathcal R} T)\cdot d_{\mathcal R}\big)$
logarithmic regret bound, where $\eta, N_{\mathcal R},T,d_{\mathcal R}$ denote
the KL-regularization parameter, the cardinality of the reward function class,
number of rounds, and the complexity of the reward function class. Furthermore,
we extend our algorithm and analysis to reinforcement learning by developing a
novel decomposition over transition steps and also obtain a similar logarithmic
regret bound.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.07393v1' target='_blank'>FinRL-DeepSeek: LLM-Infused Risk-Sensitive Reinforcement Learning for
  Trading Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mostapha Benhenda</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-11 09:23:14</h6>
<p class='card-text'>This paper presents a novel risk-sensitive trading agent combining
reinforcement learning and large language models (LLMs). We extend the
Conditional Value-at-Risk Proximal Policy Optimization (CPPO) algorithm, by
adding risk assessment and trading recommendation signals generated by a LLM
from financial news. Our approach is backtested on the Nasdaq-100 index
benchmark, using financial news data from the FNSPID dataset and the DeepSeek
V3, Qwen 2.5 and Llama 3.3 language models. The code, data, and trading agents
are available at: https://github.com/benstaf/FinRL_DeepSeek</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.07237v1' target='_blank'>DrugImproverGPT: A Large Language Model for Drug Optimization with
  Fine-Tuning via Structured Policy Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xuefeng Liu, Songhao Jiang, Siyu Chen, Zhuoran Yang, Yuxin Chen, Ian Foster, Rick Stevens</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-11 04:00:21</h6>
<p class='card-text'>Finetuning a Large Language Model (LLM) is crucial for generating results
towards specific objectives. This research delves into the realm of drug
optimization and introduce a novel reinforcement learning algorithm to finetune
a drug optimization LLM-based generative model, enhancing the original drug
across target objectives, while retains the beneficial chemical properties of
the original drug. This work is comprised of two primary components: (1)
DrugImprover: A framework tailored for improving robustness and efficiency in
drug optimization. It includes a LLM designed for drug optimization and a novel
Structured Policy Optimization (SPO) algorithm, which is theoretically
grounded. This algorithm offers a unique perspective for fine-tuning the
LLM-based generative model by aligning the improvement of the generated
molecule with the input molecule under desired objectives. (2) A dataset of 1
million compounds, each with OEDOCK docking scores on 5 human proteins
associated with cancer cells and 24 binding sites from SARS-CoV-2 virus. We
conduct a comprehensive evaluation of SPO and demonstrate its effectiveness in
improving the original drug across target properties. Our code and dataset will
be publicly available at: https://github.com/xuefeng-cs/DrugImproverGPT.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.07193v1' target='_blank'>Provably Efficient RLHF Pipeline: A Unified View from Contextual Bandits</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Long-Fei Li, Yu-Yang Qian, Peng Zhao, Zhi-Hua Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-11 02:36:01</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) is a widely used approach
for aligning Large Language Models (LLMs) with human preferences. While recent
advancements have provided valuable insights into various stages and settings
of RLHF, a comprehensive theoretical understanding of the entire RLHF pipeline
remains lacking. Towards this end, we propose a unified framework for the RLHF
pipeline from the view of contextual bandits and provide provable efficiency
guarantees. In particular, we decompose the RLHF process into two distinct
stages: (post-)training and deployment, exploring both passive and active data
collection strategies during the training phase. By employing the Bradley-Terry
preference model with a linearly parameterized reward function, we reformulate
RLHF as a contextual preference bandit problem. We then develop novel
algorithms for each stage, demonstrating significant improvements over existing
approaches in both statistical and computational efficiency. Finally, we apply
our method to train and deploy Llama-3-8B-Instruct on the
Ultrafeedback-binarized dataset, and empirical results confirm the
effectiveness of our approach.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.06773v1' target='_blank'>On the Emergence of Thinking in LLMs I: Searching for the Right
  Intuition</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guanghao Ye, Khiem Duc Pham, Xinzhi Zhang, Sivakanth Gopi, Baolin Peng, Beibin Li, Janardhan Kulkarni, Huseyin A. Inan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-10 18:52:04</h6>
<p class='card-text'>Recent AI advancements, such as OpenAI's new models, are transforming LLMs
into LRMs (Large Reasoning Models) that perform reasoning during inference,
taking extra time and compute for higher-quality outputs. We aim to uncover the
algorithmic framework for training LRMs. Methods like self-consistency, PRM,
and AlphaZero suggest reasoning as guided search. We ask: what is the simplest,
most scalable way to enable search in LLMs?
  We propose a post-training framework called Reinforcement Learning via
Self-Play (RLSP). RLSP involves three steps: (1) supervised fine-tuning with
human or synthetic demonstrations of the reasoning process, (2) using an
exploration reward signal to encourage diverse and efficient reasoning
behaviors, and (3) RL training with an outcome verifier to ensure correctness
while preventing reward hacking. Our key innovation is to decouple exploration
and correctness signals during PPO training, carefully balancing them to
improve performance and efficiency.
  Empirical studies in the math domain show that RLSP improves reasoning. On
the Llama-3.1-8B-Instruct model, RLSP can boost performance by 23% in MATH-500
test set; On AIME 2024 math problems, Qwen2.5-32B-Instruct improved by 10% due
to RLSP. However, a more important finding of this work is that the models
trained using RLSP, even with the simplest exploration reward that encourages
the model to take more intermediate steps, showed several emergent behaviors
such as backtracking, exploration of ideas, and verification. These findings
demonstrate that RLSP framework might be enough to enable emergence of complex
reasoning abilities in LLMs when scaled. Lastly, we propose a theory as to why
RLSP search strategy is more suitable for LLMs inspired by a remarkable result
that says CoT provably increases computational power of LLMs, which grows as
the number of steps in CoT \cite{li2024chain,merrill2023expresssive}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.06772v1' target='_blank'>ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ling Yang, Zhaochen Yu, Bin Cui, Mengdi Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-10 18:51:47</h6>
<p class='card-text'>We present that hierarchical LLM reasoning via scaling thought templates can
effectively optimize the reasoning search space and outperform the mathematical
reasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3.
We train our ReasonFlux-32B model with only 8 GPUs and introduces three
innovations: (i) a structured and generic thought template library, containing
around 500 high-level thought templates capable of generalizing to similar or
relevant reasoning problems; (ii) performing hierarchical reinforcement
learning on a sequence of thought templates instead of long CoTs, optimizing a
base LLM to plan out an optimal template trajectory for gradually handling
complex problems; (iii) a brand new inference scaling system that enables
hierarchical LLM reasoning by adaptively scaling thought templates at inference
time. With a template trajectory containing sequential thought templates, our
ReasonFlux-32B significantly advances math reasoning capabilities to
state-of-the-art levels. Notably, on the MATH benchmark, it achieves an
accuracy of 91.2% and surpasses o1-preview by 6.7%. On the USA Math Olympiad
(AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems,
surpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code:
https://github.com/Gen-Verse/ReasonFlux</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.06533v1' target='_blank'>Ignore the KL Penalty! Boosting Exploration on Critical Tokens to
  Enhance RL Fine-Tuning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jean Vassoyan, Nathanaël Beau, Roman Plaud</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-10 14:56:25</h6>
<p class='card-text'>The ability to achieve long-term goals is a key challenge in the current
development of large language models (LLMs). To address this, pre-trained LLMs
can be fine-tuned with reinforcement learning (RL) to explore solutions that
optimize a given goal. However, exploration with LLMs is difficult, as a
balance has to be struck between discovering new solutions and staying close
enough to the pre-trained model, so as not to degrade basic capabilities. This
is typically controlled with a Kullback-Leibler (KL) penalty. In this paper, we
investigate the exploration dynamics of a small language model on a simple
arithmetic task. We show how varying degrees of pre-training influence
exploration and demonstrate the importance of "critical tokens" which have a
dramatic impact on the final outcome. Consequently, we introduce a simple
modification to the KL penalty that favors exploration on critical tokens,
increasing the efficiency of the RL fine-tuning stage.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.06205v1' target='_blank'>C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like
  Retrieval-Augmented Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guoxin Chen, Minpeng Liao, Peiying Yu, Dingmin Wang, Zile Qiao, Chao Yang, Xin Zhao, Kai Fan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-10 07:04:32</h6>
<p class='card-text'>Retrieval-augmented generation (RAG) systems face a fundamental challenge in
aligning independently developed retrievers and large language models (LLMs).
Existing approaches typically involve modifying either component or introducing
simple intermediate modules, resulting in practical limitations and sub-optimal
performance. Inspired by human search behavior -- typically involving a
back-and-forth process of proposing search queries and reviewing documents, we
propose C-3PO, a proxy-centric framework that facilitates communication between
retrievers and LLMs through a lightweight multi-agent system. Our framework
implements three specialized agents that collaboratively optimize the entire
RAG pipeline without altering the retriever and LLMs. These agents work
together to assess the need for retrieval, generate effective queries, and
select information suitable for the LLMs. To enable effective multi-agent
coordination, we develop a tree-structured rollout approach for reward credit
assignment in reinforcement learning. Extensive experiments in both in-domain
and out-of-distribution scenarios demonstrate that C-3PO significantly enhances
RAG performance while maintaining plug-and-play flexibility and superior
generalization capabilities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.05759v2' target='_blank'>Reinforced Lifelong Editing for Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zherui Li, Houcheng Jiang, Hao Chen, Baolong Bi, Zhenhong Zhou, Fei Sun, Junfeng Fang, Xiang Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-09 03:37:06</h6>
<p class='card-text'>Large language models (LLMs) acquire information from pre-training corpora,
but their stored knowledge can become inaccurate or outdated over time. Model
editing addresses this challenge by modifying model parameters without
retraining, and prevalent approaches leverage hypernetworks to generate these
parameter updates. However, they face significant challenges in lifelong
editing due to their incompatibility with LLM parameters that dynamically
change during the editing process. To address this, we observed that
hypernetwork-based lifelong editing aligns with reinforcement learning modeling
and proposed RLEdit, an RL-based editing method. By treating editing losses as
rewards and optimizing hypernetwork parameters at the full knowledge sequence
level, we enable it to precisely capture LLM changes and generate appropriate
parameter updates. Our extensive empirical evaluation across several LLMs
demonstrates that RLEdit outperforms existing methods in lifelong editing with
superior effectiveness and efficiency, achieving a 59.24% improvement while
requiring only 2.11% of the time compared to most approaches. Our code is
available at: https://github.com/zhrli324/RLEdit.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.06884v1' target='_blank'>Learning Conformal Abstention Policies for Adaptive Risk Management in
  Large Language and Vision-Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sina Tayebati, Divake Kumar, Nastaran Darabi, Dinithi Jayasuriya, Ranganath Krishnan, Amit Ranjan Trivedi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-08 21:30:41</h6>
<p class='card-text'>Large Language and Vision-Language Models (LLMs/VLMs) are increasingly used
in safety-critical applications, yet their opaque decision-making complicates
risk assessment and reliability. Uncertainty quantification (UQ) helps assess
prediction confidence and enables abstention when uncertainty is high.
Conformal prediction (CP), a leading UQ method, provides statistical guarantees
but relies on static thresholds, which fail to adapt to task complexity and
evolving data distributions, leading to suboptimal trade-offs in accuracy,
coverage, and informativeness. To address this, we propose learnable conformal
abstention, integrating reinforcement learning (RL) with CP to optimize
abstention thresholds dynamically. By treating CP thresholds as adaptive
actions, our approach balances multiple objectives, minimizing prediction set
size while maintaining reliable coverage. Extensive evaluations across diverse
LLM/VLM benchmarks show our method outperforms Least Ambiguous Classifiers
(LAC) and Adaptive Prediction Sets (APS), improving accuracy by up to 3.2%,
boosting AUROC for hallucination detection by 22.19%, enhancing
uncertainty-guided selective generation (AUARC) by 21.17%, and reducing
calibration error by 70%-85%. These improvements hold across multiple models
and datasets while consistently meeting the 90% coverage target, establishing
our approach as a more effective and flexible solution for reliable
decision-making in safety-critical applications. The code is available at:
{https://github.com/sinatayebati/vlm-uncertainty}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.08657v1' target='_blank'>Refining Positive and Toxic Samples for Dual Safety Self-Alignment of
  LLMs with Minimal Human Interventions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jingxin Xu, Guoshun Nan, Sheng Guan, Sicong Leng, Yilian Liu, Zixiao Wang, Yuyang Ma, Zhili Zhou, Yanzhao Hou, Xiaofeng Tao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-08 09:54:47</h6>
<p class='card-text'>Recent AI agents, such as ChatGPT and LLaMA, primarily rely on instruction
tuning and reinforcement learning to calibrate the output of large language
models (LLMs) with human intentions, ensuring the outputs are harmless and
helpful. Existing methods heavily depend on the manual annotation of
high-quality positive samples, while contending with issues such as noisy
labels and minimal distinctions between preferred and dispreferred response
data. However, readily available toxic samples with clear safety distinctions
are often filtered out, removing valuable negative references that could aid
LLMs in safety alignment. In response, we propose PT-ALIGN, a novel safety
self-alignment approach that minimizes human supervision by automatically
refining positive and toxic samples and performing fine-grained dual
instruction tuning. Positive samples are harmless responses, while toxic
samples deliberately contain extremely harmful content, serving as a new
supervisory signals. Specifically, we utilize LLM itself to iteratively
generate and refine training instances by only exploring fewer than 50 human
annotations. We then employ two losses, i.e., maximum likelihood estimation
(MLE) and fine-grained unlikelihood training (UT), to jointly learn to enhance
the LLM's safety. The MLE loss encourages an LLM to maximize the generation of
harmless content based on positive samples. Conversely, the fine-grained UT
loss guides the LLM to minimize the output of harmful words based on negative
samples at the token-level, thereby guiding the model to decouple safety from
effectiveness, directing it toward safer fine-tuning objectives, and increasing
the likelihood of generating helpful and reliable content. Experiments on 9
popular open-source LLMs demonstrate the effectiveness of our PT-ALIGN for
safety alignment, while maintaining comparable levels of helpfulness and
usefulness.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.06869v1' target='_blank'>A Survey on Explainable Deep Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zelei Cheng, Jiahao Yu, Xinyu Xing</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-08 05:30:31</h6>
<p class='card-text'>Deep Reinforcement Learning (DRL) has achieved remarkable success in
sequential decision-making tasks across diverse domains, yet its reliance on
black-box neural architectures hinders interpretability, trust, and deployment
in high-stakes applications. Explainable Deep Reinforcement Learning (XRL)
addresses these challenges by enhancing transparency through feature-level,
state-level, dataset-level, and model-level explanation techniques. This survey
provides a comprehensive review of XRL methods, evaluates their qualitative and
quantitative assessment frameworks, and explores their role in policy
refinement, adversarial robustness, and security. Additionally, we examine the
integration of reinforcement learning with Large Language Models (LLMs),
particularly through Reinforcement Learning from Human Feedback (RLHF), which
optimizes AI alignment with human preferences. We conclude by highlighting open
research challenges and future directions to advance the development of
interpretable, reliable, and accountable DRL systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.05453v1' target='_blank'>LLM-Powered Decentralized Generative Agents with Adaptive Hierarchical
  Knowledge Graph for Cooperative Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hanqing Yang, Jingdi Chen, Marie Siew, Tania Lorido-Botran, Carlee Joe-Wong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-08 05:26:02</h6>
<p class='card-text'>Developing intelligent agents for long-term cooperation in dynamic open-world
scenarios is a major challenge in multi-agent systems. Traditional Multi-agent
Reinforcement Learning (MARL) frameworks like centralized training
decentralized execution (CTDE) struggle with scalability and flexibility. They
require centralized long-term planning, which is difficult without custom
reward functions, and face challenges in processing multi-modal data. CTDE
approaches also assume fixed cooperation strategies, making them impractical in
dynamic environments where agents need to adapt and plan independently. To
address decentralized multi-agent cooperation, we propose Decentralized
Adaptive Knowledge Graph Memory and Structured Communication System (DAMCS) in
a novel Multi-agent Crafter environment. Our generative agents, powered by
Large Language Models (LLMs), are more scalable than traditional MARL agents by
leveraging external knowledge and language for long-term planning and
reasoning. Instead of fully sharing information from all past experiences,
DAMCS introduces a multi-modal memory system organized as a hierarchical
knowledge graph and a structured communication protocol to optimize agent
cooperation. This allows agents to reason from past interactions and share
relevant information efficiently. Experiments on novel multi-agent open-world
tasks show that DAMCS outperforms both MARL and LLM baselines in task
efficiency and collaboration. Compared to single-agent scenarios, the two-agent
scenario achieves the same goal with 63% fewer steps, and the six-agent
scenario with 74% fewer steps, highlighting the importance of adaptive memory
and structured communication in achieving long-term goals. We publicly release
our project at: https://happyeureka.github.io/damcs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.05163v1' target='_blank'>DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM
  Guardrails</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yihe Deng, Yu Yang, Junkai Zhang, Wei Wang, Bo Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-07 18:45:03</h6>
<p class='card-text'>The rapid advancement of large language models (LLMs) has increased the need
for guardrail models to ensure responsible use, particularly in detecting
unsafe and illegal content. While substantial safety data exist in English,
multilingual guardrail modeling remains underexplored due to the scarcity of
open-source safety data in other languages. To address this gap, we propose a
novel two-player Reinforcement Learning (RL) framework, where a generator and a
guardrail model co-evolve adversarially to produce high-quality synthetic data
for multilingual guardrail training. We theoretically formalize this
interaction as a two-player game, proving convergence to a Nash equilibrium.
Empirical evaluations show that our model \ours outperforms state-of-the-art
models, achieving nearly 10% improvement over LlamaGuard3 (8B) on English
benchmarks while being 4.5x faster at inference with a significantly smaller
model (0.5B). We achieve substantial advancements in multilingual safety tasks,
particularly in addressing the imbalance for lower-resource languages in a
collected real dataset. Ablation studies emphasize the critical role of
synthetic data generation in bridging the imbalance in open-source data between
English and other languages. These findings establish a scalable and efficient
approach to synthetic data generation, paving the way for improved multilingual
guardrail models to enhance LLM safety. Code, model, and data will be
open-sourced at https://github.com/yihedeng9/DuoGuard.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.05078v1' target='_blank'>Adaptive Graph of Thoughts: Test-Time Adaptive Reasoning Unifying Chain,
  Tree, and Graph Structures</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tushar Pandey, Ara Ghukasyan, Oktay Goktas, Santosh Kumar Radha</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-07 16:54:19</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated impressive reasoning
capabilities, yet their performance is highly dependent on the prompting
strategy and model scale. While reinforcement learning and fine-tuning have
been deployed to boost reasoning, these approaches incur substantial
computational and data overhead. In this work, we introduce Adaptive Graph of
Thoughts (AGoT), a dynamic, graph-based inference framework that enhances LLM
reasoning solely at test time. Rather than relying on fixed-step methods like
Chain of Thought (CoT) or Tree of Thoughts (ToT), AGoT recursively decomposes
complex queries into structured subproblems, forming an dynamic directed
acyclic graph (DAG) of interdependent reasoning steps. By selectively expanding
only those subproblems that require further analysis, AGoT unifies the
strengths of chain, tree, and graph paradigms into a cohesive framework that
allocates computation where it is most needed. We validate our approach on
diverse benchmarks spanning multi-hop retrieval, scientific reasoning, and
mathematical problem-solving, achieving up to 46.2% improvement on scientific
reasoning tasks (GPQA) - comparable to gains achieved through computationally
intensive reinforcement learning approaches and outperforming state-of-the-art
iterative approaches. These results suggest that dynamic decomposition and
structured recursion offer a scalable, cost-effective alternative to
post-training modifications, paving the way for more robust, general-purpose
reasoning in LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.04692v3' target='_blank'>STRIDE: Automating Reward Design, Deep Reinforcement Learning Training
  and Feedback Optimization in Humanoid Robotics Locomotion</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhenwei Wu, Jinxiong Lu, Yuxiao Chen, Yunxin Liu, Yueting Zhuang, Luhui Hu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-07 06:37:05</h6>
<p class='card-text'>Humanoid robotics presents significant challenges in artificial intelligence,
requiring precise coordination and control of high-degree-of-freedom systems.
Designing effective reward functions for deep reinforcement learning (DRL) in
this domain remains a critical bottleneck, demanding extensive manual effort,
domain expertise, and iterative refinement. To overcome these challenges, we
introduce STRIDE, a novel framework built on agentic engineering to automate
reward design, DRL training, and feedback optimization for humanoid robot
locomotion tasks. By combining the structured principles of agentic engineering
with large language models (LLMs) for code-writing, zero-shot generation, and
in-context optimization, STRIDE generates, evaluates, and iteratively refines
reward functions without relying on task-specific prompts or templates. Across
diverse environments featuring humanoid robot morphologies, STRIDE outperforms
the state-of-the-art reward design framework EUREKA, achieving an average
improvement of round 250% in efficiency and task performance. Using
STRIDE-generated rewards, simulated humanoid robots achieve sprint-level
locomotion across complex terrains, highlighting its ability to advance DRL
workflows and humanoid robotics research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.04686v1' target='_blank'>Learning Strategic Language Agents in the Werewolf Game with Iterative
  Latent Space Policy Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zelai Xu, Wanjun Gu, Chao Yu, Yi Wu, Yu Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-07 06:19:55</h6>
<p class='card-text'>Large language model (LLM)-based agents have recently shown impressive
progress in a variety of domains, including open-ended conversation and
multi-step decision-making. However, applying these agents to social deduction
games such as Werewolf, which requires both strategic decision-making and
free-form language interaction, remains non-trivial. Traditional methods based
on Counterfactual Regret Minimization (CFR) or reinforcement learning (RL)
typically depend on a predefined action space, making them unsuitable for
language games with unconstrained text action space. Meanwhile, pure LLM-based
agents often suffer from intrinsic biases and require prohibitively large
datasets for fine-tuning. We propose Latent Space Policy Optimization (LSPO),
an iterative framework that addresses these challenges by first mapping
free-form text to a discrete latent space, where methods like CFR and RL can
learn strategic policy more effectively. We then translate the learned policy
back into natural language dialogues, which are used to fine-tune an LLM via
Direct Preference Optimization (DPO). By iteratively alternating between these
stages, our LSPO agent progressively enhances both strategic reasoning and
language communication. Experiment results on the Werewolf game show that our
method improves the agent's performance in each iteration and outperforms
existing Werewolf agents, underscoring its promise for free-form language
decision-making.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.04576v1' target='_blank'>Self-Regulation and Requesting Interventions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:So Yeon Min, Yue Wu, Jimin Sun, Max Kaufmann, Fahim Tajwar, Yonatan Bisk, Ruslan Salakhutdinov</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-07 00:06:17</h6>
<p class='card-text'>Human intelligence involves metacognitive abilities like self-regulation,
recognizing limitations, and seeking assistance only when needed. While LLM
Agents excel in many domains, they often lack this awareness. Overconfident
agents risk catastrophic failures, while those that seek help excessively
hinder efficiency. A key challenge is enabling agents with a limited
intervention budget $C$ is to decide when to request assistance. In this paper,
we propose an offline framework that trains a "helper" policy to request
interventions, such as more powerful models or test-time compute, by combining
LLM-based process reward models (PRMs) with tabular reinforcement learning.
Using state transitions collected offline, we score optimal intervention timing
with PRMs and train the helper model on these labeled trajectories. This
offline approach significantly reduces costly intervention calls during
training. Furthermore, the integration of PRMs with tabular RL enhances
robustness to off-policy data while avoiding the inefficiencies of deep RL. We
empirically find that our method delivers optimal helper behavior.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.04492v1' target='_blank'>Multi-Agent Reinforcement Learning with Focal Diversity Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Selim Furkan Tekin, Fatih Ilhan, Tiansheng Huang, Sihao Hu, Zachary Yahn, Ling Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-06 20:44:26</h6>
<p class='card-text'>The advancement of Large Language Models (LLMs) and their finetuning
strategies has triggered the renewed interests in multi-agent reinforcement
learning. In this paper, we introduce a focal diversity-optimized multi-agent
reinforcement learning approach, coined as MARL-Focal, with three unique
characteristics. First, we develop an agent-fusion framework for encouraging
multiple LLM based agents to collaborate in producing the final inference
output for each LLM query. Second, we develop a focal-diversity optimized agent
selection algorithm that can choose a small subset of the available agents
based on how well they can complement one another to generate the query output.
Finally, we design a conflict-resolution method to detect output inconsistency
among multiple agents and produce our MARL-Focal output through reward-aware
and policy-adaptive inference fusion. Extensive evaluations on five benchmarks
show that MARL-Focal is cost-efficient and adversarial-robust. Our multi-agent
fusion model achieves performance improvement of 5.51\% compared to the best
individual LLM-agent and offers stronger robustness over the TruthfulQA
benchmark. Code is available at https://github.com/sftekin/rl-focal</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.04463v2' target='_blank'>Training Language Models to Reason Efficiently</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Daman Arora, Andrea Zanette</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-06 19:18:16</h6>
<p class='card-text'>Scaling model size and training data has led to great advances in the
performance of Large Language Models (LLMs). However, the diminishing returns
of this approach necessitate alternative methods to improve model capabilities,
particularly in tasks requiring advanced reasoning. Large reasoning models,
which leverage long chain-of-thoughts, bring unprecedented breakthroughs in
problem-solving capabilities but at a substantial deployment cost associated to
longer generations. Reducing inference costs is crucial for the economic
feasibility, user experience, and environmental sustainability of these models.
  In this work, we propose to train large reasoning models to reason
efficiently. More precisely, we use reinforcement learning (RL) to train
reasoning models to dynamically allocate inference-time compute based on task
complexity. Our method incentivizes models to minimize unnecessary
computational overhead while maintaining accuracy, thereby achieving
substantial efficiency gains. It enables the derivation of a family of
reasoning models with varying efficiency levels, controlled via a single
hyperparameter. Experiments on two open-weight large reasoning models
demonstrate significant reductions in inference cost while preserving most of
the accuracy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.03723v1' target='_blank'>Speaking the Language of Teamwork: LLM-Guided Credit Assignment in
  Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Muhan Lin, Shuyang Shi, Yue Guo, Vaishnav Tadiparthi, Behdad Chalaki, Ehsan Moradi Pari, Simon Stepputtis, Woojun Kim, Joseph Campbell, Katia Sycara</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-06 02:26:47</h6>
<p class='card-text'>Credit assignment, the process of attributing credit or blame to individual
agents for their contributions to a team's success or failure, remains a
fundamental challenge in multi-agent reinforcement learning (MARL),
particularly in environments with sparse rewards. Commonly-used approaches such
as value decomposition often lead to suboptimal policies in these settings, and
designing dense reward functions that align with human intuition can be complex
and labor-intensive. In this work, we propose a novel framework where a large
language model (LLM) generates dense, agent-specific rewards based on a natural
language description of the task and the overall team goal. By learning a
potential-based reward function over multiple queries, our method reduces the
impact of ranking errors while allowing the LLM to evaluate each agent's
contribution to the overall task. Through extensive experiments, we demonstrate
that our approach achieves faster convergence and higher policy returns
compared to state-of-the-art MARL baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.03699v1' target='_blank'>LLM Alignment as Retriever Optimization: An Information Retrieval
  Perspective</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bowen Jin, Jinsung Yoon, Zhen Qin, Ziqi Wang, Wei Xiong, Yu Meng, Jiawei Han, Sercan O. Arik</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-06 01:22:06</h6>
<p class='card-text'>Large Language Models (LLMs) have revolutionized artificial intelligence with
capabilities in reasoning, coding, and communication, driving innovation across
industries. Their true potential depends on effective alignment to ensure
correct, trustworthy and ethical behavior, addressing challenges like
misinformation, hallucinations, bias and misuse. While existing Reinforcement
Learning (RL)-based alignment methods are notoriously complex, direct
optimization approaches offer a simpler alternative. In this work, we introduce
a novel direct optimization approach for LLM alignment by drawing on
established Information Retrieval (IR) principles. We present a systematic
framework that bridges LLM alignment and IR methodologies, mapping LLM
generation and reward models to IR's retriever-reranker paradigm. Building on
this foundation, we propose LLM Alignment as Retriever Preference Optimization
(LarPO), a new alignment method that enhances overall alignment quality.
Extensive experiments validate LarPO's effectiveness with 38.9 % and 13.7 %
averaged improvement on AlpacaEval2 and MixEval-Hard respectively. Our work
opens new avenues for advancing LLM alignment by integrating IR foundations,
offering a promising direction for future research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.03671v1' target='_blank'>Advancing Reasoning in Large Language Models: Promising Methods and
  Approaches</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Avinash Patil</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-05 23:31:39</h6>
<p class='card-text'>Large Language Models (LLMs) have succeeded remarkably in various natural
language processing (NLP) tasks, yet their reasoning capabilities remain a
fundamental challenge. While LLMs exhibit impressive fluency and factual
recall, their ability to perform complex reasoning-spanning logical deduction,
mathematical problem-solving, commonsense inference, and multi-step
reasoning-often falls short of human expectations. This survey provides a
comprehensive review of emerging techniques enhancing reasoning in LLMs. We
categorize existing methods into key approaches, including prompting strategies
(e.g., Chain-of-Thought reasoning, Self-Consistency, and Tree-of-Thought
reasoning), architectural innovations (e.g., retrieval-augmented models,
modular reasoning networks, and neuro-symbolic integration), and learning
paradigms (e.g., fine-tuning with reasoning-specific datasets, reinforcement
learning, and self-supervised reasoning objectives). Additionally, we explore
evaluation frameworks used to assess reasoning in LLMs and highlight open
challenges, such as hallucinations, robustness, and reasoning generalization
across diverse tasks. By synthesizing recent advancements, this survey aims to
provide insights into promising directions for future research and practical
applications of reasoning-augmented LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.03575v1' target='_blank'>Chartist: Task-driven Eye Movement Control for Chart Reading</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Danqing Shi, Yao Wang, Yunpeng Bai, Andreas Bulling, Antti Oulasvirta</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-05 19:47:48</h6>
<p class='card-text'>To design data visualizations that are easy to comprehend, we need to
understand how people with different interests read them. Computational models
of predicting scanpaths on charts could complement empirical studies by
offering estimates of user performance inexpensively; however, previous models
have been limited to gaze patterns and overlooked the effects of tasks. Here,
we contribute Chartist, a computational model that simulates how users move
their eyes to extract information from the chart in order to perform analysis
tasks, including value retrieval, filtering, and finding extremes. The novel
contribution lies in a two-level hierarchical control architecture. At the high
level, the model uses LLMs to comprehend the information gained so far and
applies this representation to select a goal for the lower-level controllers,
which, in turn, move the eyes in accordance with a sampling policy learned via
reinforcement learning. The model is capable of predicting human-like
task-driven scanpaths across various tasks. It can be applied in fields such as
explainable AI, visualization design evaluation, and optimization. While it
displays limitations in terms of generalizability and accuracy, it takes
modeling in a promising direction, toward understanding human behaviors in
interacting with charts.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.03373v1' target='_blank'>Demystifying Long Chain-of-Thought Reasoning in LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, Xiang Yue</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-05 17:13:32</h6>
<p class='card-text'>Scaling inference compute enhances reasoning in large language models (LLMs),
with long chains-of-thought (CoTs) enabling strategies like backtracking and
error correction. Reinforcement learning (RL) has emerged as a crucial method
for developing these capabilities, yet the conditions under which long CoTs
emerge remain unclear, and RL training requires careful design choices. In this
study, we systematically investigate the mechanics of long CoT reasoning,
identifying the key factors that enable models to generate long CoT
trajectories. Through extensive supervised fine-tuning (SFT) and RL
experiments, we present four main findings: (1) While SFT is not strictly
necessary, it simplifies training and improves efficiency; (2) Reasoning
capabilities tend to emerge with increased training compute, but their
development is not guaranteed, making reward shaping crucial for stabilizing
CoT length growth; (3) Scaling verifiable reward signals is critical for RL. We
find that leveraging noisy, web-extracted solutions with filtering mechanisms
shows strong potential, particularly for out-of-distribution (OOD) tasks such
as STEM reasoning; and (4) Core abilities like error correction are inherently
present in base models, but incentivizing these skills effectively for complex
tasks via RL demands significant compute, and measuring their emergence
requires a nuanced approach. These insights provide practical guidance for
optimizing training strategies to enhance long CoT reasoning in LLMs. Our code
is available at: https://github.com/eddycmu/demystify-long-cot.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.03095v1' target='_blank'>Reveal the Mystery of DPO: The Connection between DPO and RL Algorithms</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xuerui Su, Yue Wang, Jinhua Zhu, Mingyang Yi, Feng Xu, Zhiming Ma, Yuting Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-05 11:41:43</h6>
<p class='card-text'>With the rapid development of Large Language Models (LLMs), numerous
Reinforcement Learning from Human Feedback (RLHF) algorithms have been
introduced to improve model safety and alignment with human preferences. These
algorithms can be divided into two main frameworks based on whether they
require an explicit reward (or value) function for training: actor-critic-based
Proximal Policy Optimization (PPO) and alignment-based Direct Preference
Optimization (DPO). The mismatch between DPO and PPO, such as DPO's use of a
classification loss driven by human-preferred data, has raised confusion about
whether DPO should be classified as a Reinforcement Learning (RL) algorithm. To
address these ambiguities, we focus on three key aspects related to DPO, RL,
and other RLHF algorithms: (1) the construction of the loss function; (2) the
target distribution at which the algorithm converges; (3) the impact of key
components within the loss function. Specifically, we first establish a unified
framework named UDRRA connecting these algorithms based on the construction of
their loss functions. Next, we uncover their target policy distributions within
this framework. Finally, we investigate the critical components of DPO to
understand their impact on the convergence rate. Our work provides a deeper
understanding of the relationship between DPO, RL, and other RLHF algorithms,
offering new insights for improving existing algorithms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.03067v1' target='_blank'>Optimizing Electric Vehicles Charging using Large Language Models and
  Graph Neural Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Stavros Orfanoudakis, Peter Palensky, Pedro P. Vergara</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-05 11:00:51</h6>
<p class='card-text'>Maintaining grid stability amid widespread electric vehicle (EV) adoption is
vital for sustainable transportation. Traditional optimization methods and
Reinforcement Learning (RL) approaches often struggle with the high
dimensionality and dynamic nature of real-time EV charging, leading to
sub-optimal solutions. To address these challenges, this study demonstrates
that combining Large Language Models (LLMs), for sequence modeling, with Graph
Neural Networks (GNNs), for relational information extraction, not only
outperforms conventional EV smart charging methods, but also paves the way for
entirely new research directions and innovative solutions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.03492v1' target='_blank'>Teaching Language Models to Critique via Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhihui Xie, Jie chen, Liyu Chen, Weichao Mao, Jingjing Xu, Lingpeng Kong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-05 02:18:46</h6>
<p class='card-text'>Teaching large language models (LLMs) to critique and refine their outputs is
crucial for building systems that can iteratively improve, yet it is
fundamentally limited by the ability to provide accurate judgments and
actionable suggestions. In this work, we study LLM critics for code generation
and propose $\texttt{CTRL}$, a framework for $\texttt{C}$ritic
$\texttt{T}$raining via $\texttt{R}$einforcement $\texttt{L}$earning, which
trains a critic model to generate feedback that maximizes correction
performance for a fixed generator model without human supervision. Our results
demonstrate that critics trained with $\texttt{CTRL}$ significantly enhance
pass rates and mitigate compounding errors across both base and stronger
generator models. Furthermore, we show that these critic models act as accurate
generative reward models and enable test-time scaling through iterative
critique-revision, achieving up to 106.1% relative improvements across
challenging code generation benchmarks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.06813v1' target='_blank'>Policy Guided Tree Search for Enhanced LLM Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yang Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-04 22:08:20</h6>
<p class='card-text'>Despite their remarkable capabilities, large language models often struggle
with tasks requiring complex reasoning and planning. While existing approaches
like Chain-of-Thought prompting and tree search techniques show promise, they
are limited by their reliance on predefined heuristics and computationally
expensive exploration strategies. We propose Policy-Guided Tree Search (PGTS),
a framework that combines reinforcement learning with structured tree
exploration to efficiently navigate reasoning paths. Our key innovation is a
learned policy that dynamically decides between expanding, branching,
backtracking, or terminating exploration, eliminating the need for manual
heuristics or exhaustive search. Experiments across mathematical reasoning,
logical deduction, and planning benchmarks demonstrate that PGTS achieves
superior reasoning performance while significantly reducing computational costs
compared to existing methods. These results establish PGTS as a scalable and
effective solution for tackling complex reasoning tasks with LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.04357v1' target='_blank'>Reusing Embeddings: Reproducible Reward Model Research in Large Language
  Model Alignment without GPUs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hao Sun, Yunyi Shen, Jean-Francois Ton, Mihaela van der Schaar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-04 19:37:35</h6>
<p class='card-text'>Large Language Models (LLMs) have made substantial strides in structured
tasks through Reinforcement Learning (RL), demonstrating proficiency in
mathematical reasoning and code generation. However, applying RL in broader
domains like chatbots and content generation -- through the process known as
Reinforcement Learning from Human Feedback (RLHF) -- presents unique
challenges. Reward models in RLHF are critical, acting as proxies that evaluate
the alignment of LLM outputs with human intent. Despite advancements, the
development of reward models is hindered by challenges such as computational
heavy training, costly evaluation, and therefore poor reproducibility. We
advocate for using embedding-based input in reward model research as an
accelerated solution to those challenges. By leveraging embeddings for reward
modeling, we can enhance reproducibility, reduce computational demands on
hardware, improve training stability, and significantly reduce training and
evaluation costs, hence facilitating fair and efficient comparisons in this
active research area. We then show a case study of reproducing existing reward
model ensemble research using embedding-based reward models. We discussed
future avenues for research, aiming to contribute to safer and more effective
LLM deployments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.04354v1' target='_blank'>Reviving The Classics: Active Reward Modeling in Large Language Model
  Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yunyi Shen, Hao Sun, Jean-François Ton</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-04 18:47:11</h6>
<p class='card-text'>Building neural reward models from human preferences is a pivotal component
in reinforcement learning from human feedback (RLHF) and large language model
alignment research. Given the scarcity and high cost of human annotation, how
to select the most informative pairs to annotate is an essential yet
challenging open problem. In this work, we highlight the insight that an ideal
comparison dataset for reward modeling should balance exploration of the
representation space and make informative comparisons between pairs with
moderate reward differences. Technically, challenges arise in quantifying the
two objectives and efficiently prioritizing the comparisons to be annotated. To
address this, we propose the Fisher information-based selection strategies,
adapt theories from the classical experimental design literature, and apply
them to the final linear layer of the deep neural network-based reward modeling
tasks. Empirically, our method demonstrates remarkable performance, high
computational efficiency, and stability compared to other selection methods
from deep learning and classical statistical literature across multiple
open-source LLMs and datasets. Further ablation studies reveal that
incorporating cross-prompt comparisons in active reward modeling significantly
enhances labeling efficiency, shedding light on the potential for improved
annotation strategies in RLHF.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.02508v1' target='_blank'>Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM
  Reasoning via Autoregressive Search</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Maohao Shen, Guangtao Zeng, Zhenting Qi, Zhang-Wei Hong, Zhenfang Chen, Wei Lu, Gregory Wornell, Subhro Das, David Cox, Chuang Gan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-04 17:26:58</h6>
<p class='card-text'>Large language models (LLMs) have demonstrated remarkable reasoning
capabilities across diverse domains. Recent studies have shown that increasing
test-time computation enhances LLMs' reasoning capabilities. This typically
involves extensive sampling at inference time guided by an external LLM
verifier, resulting in a two-player system. Despite external guidance, the
effectiveness of this system demonstrates the potential of a single LLM to
tackle complex tasks. Thus, we pose a new research problem: Can we internalize
the searching capabilities to fundamentally enhance the reasoning abilities of
a single LLM? This work explores an orthogonal direction focusing on
post-training LLMs for autoregressive searching (i.e., an extended reasoning
process with self-reflection and self-exploration of new strategies). To
achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a
two-stage training paradigm: 1) a small-scale format tuning stage to
internalize the COAT reasoning format and 2) a large-scale self-improvement
stage leveraging reinforcement learning. Our approach results in Satori, a 7B
LLM trained on open-source models and data. Extensive empirical evaluations
demonstrate that Satori achieves state-of-the-art performance on mathematical
reasoning benchmarks while exhibits strong generalization to out-of-domain
tasks. Code, data, and models will be fully open-sourced.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.01992v1' target='_blank'>FinRLlama: A Solution to LLM-Engineered Signals Challenge at FinRL
  Contest 2024</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Arnav Grover</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-04 04:11:09</h6>
<p class='card-text'>In response to Task II of the FinRL Challenge at ACM ICAIF 2024, this study
proposes a novel prompt framework for fine-tuning large language models (LLM)
with Reinforcement Learning from Market Feedback (RLMF). Our framework
incorporates market-specific features and short-term price dynamics to generate
more precise trading signals. Traditional LLMs, while competent in sentiment
analysis, lack contextual alignment for financial market applications. To
bridge this gap, we fine-tune the LLaMA-3.2-3B-Instruct model using a custom
RLMF prompt design that integrates historical market data and reward-based
feedback. Our evaluation shows that this RLMF-tuned framework outperforms
baseline methods in signal consistency and achieving tighter trading outcomes;
awarded as winner of Task II. You can find the code for this project on GitHub.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.06807v2' target='_blank'>Competitive Programming with Large Reasoning Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:OpenAI, :, Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaiev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, Jerry Tworek, Lorenz Kuhn, Lukasz Kaiser, Mark Chen, Max Schwarzer, Mostafa Rohaninejad, Nat McAleese, o3 contributors, Oleg Mürk, Rhythm Garg, Rui Shu, Szymon Sidor, Vineet Kosaraju, Wenda Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-03 23:00:15</h6>
<p class='card-text'>We show that reinforcement learning applied to large language models (LLMs)
significantly boosts performance on complex coding and reasoning tasks.
Additionally, we compare two general-purpose reasoning models - OpenAI o1 and
an early checkpoint of o3 - with a domain-specific system, o1-ioi, which uses
hand-engineered inference strategies designed for competing in the 2024
International Olympiad in Informatics (IOI). We competed live at IOI 2024 with
o1-ioi and, using hand-crafted test-time strategies, placed in the 49th
percentile. Under relaxed competition constraints, o1-ioi achieved a gold
medal. However, when evaluating later models such as o3, we find that o3
achieves gold without hand-crafted domain-specific strategies or relaxed
constraints. Our findings show that although specialized pipelines such as
o1-ioi yield solid improvements, the scaled-up, general-purpose o3 model
surpasses those results without relying on hand-crafted inference heuristics.
Notably, o3 achieves a gold medal at the 2024 IOI and obtains a Codeforces
rating on par with elite human competitors. Overall, these results indicate
that scaling general-purpose reinforcement learning, rather than relying on
domain-specific techniques, offers a robust path toward state-of-the-art AI in
reasoning domains, such as competitive programming.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.01600v2' target='_blank'>Reinforcement Learning for Long-Horizon Interactive LLM Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kevin Chen, Marco Cusumano-Towner, Brody Huval, Aleksei Petrenko, Jackson Hamburger, Vladlen Koltun, Philipp Krähenbühl</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-03 18:35:42</h6>
<p class='card-text'>Interactive digital agents (IDAs) leverage APIs of stateful digital
environments to perform tasks in response to user requests. While IDAs powered
by instruction-tuned large language models (LLMs) can react to feedback from
interface invocations in multi-step exchanges, they have not been trained in
their respective digital environments. Prior methods accomplish less than half
of tasks in sophisticated benchmarks such as AppWorld. We present a
reinforcement learning (RL) approach that trains IDAs directly in their target
environments. We formalize this training as a partially observable Markov
decision process and derive LOOP, a data- and memory-efficient variant of
proximal policy optimization. LOOP uses no value network and maintains exactly
one copy of the underlying LLM in memory, making its implementation
straightforward and as memory-efficient as fine-tuning a single LLM. A
32-billion-parameter agent trained with LOOP in the AppWorld environment
outperforms the much larger OpenAI o1 agent by 9 percentage points (15%
relative). To our knowledge, this is the first reported application of RL to
IDAs that interact with a stateful, multi-domain, multi-app environment via
direct API calls. Our analysis sheds light on the effectiveness of RL in this
area, showing that the agent learns to consult the API documentation, avoid
unwarranted assumptions, minimize confabulation, and recover from setbacks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.01715v1' target='_blank'>Process-Supervised Reinforcement Learning for Code Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yufan Ye, Ting Zhang, Wenbin Jiang, Hua Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-03 16:22:06</h6>
<p class='card-text'>Existing reinforcement learning strategies based on outcome supervision have
proven effective in enhancing the performance of large language models(LLMs)
for code generation. While reinforcement learning based on process supervision
has shown great promise in handling multi-step reasoning tasks, its
effectiveness in code generation remains largely underexplored and
underjustified. The primary obstacle stems from the resource-intensive nature
of constructing high-quality process-supervised data, which demands substantial
human expertise and computational resources. In response to this challenge, we
propose a "statement mutation/refactoring-compile and execution verification"
strategy: mutating and refactoring code line-by-line through a teacher model,
and utilizing compiler execution results to automatically label each line,
resulting in line-by-line process-supervised data, which is pivotal for
training a process-supervised reward model. The trained reward model is then
integrated into the PRLCoder framework, followed by experimental validation on
several benchmarks. Experimental results demonstrate that process-supervised
reinforcement learning significantly surpasses methods relying solely on
outcome supervision. Notably, in tackling complex code generation tasks,
process-supervised reinforcement learning shows a clear advantage, ensuring
both the integrity of the code generation process and the correctness of the
generation results.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.01456v1' target='_blank'>Process Reinforcement through Implicit Rewards</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao, Xu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, Ning Ding</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-03 15:43:48</h6>
<p class='card-text'>Dense process rewards have proven a more effective alternative to the sparse
outcome-level rewards in the inference-time scaling of large language models
(LLMs), particularly in tasks requiring complex multi-step reasoning. While
dense rewards also offer an appealing choice for the reinforcement learning
(RL) of LLMs since their fine-grained rewards have the potential to address
some inherent issues of outcome rewards, such as training efficiency and credit
assignment, this potential remains largely unrealized. This can be primarily
attributed to the challenges of training process reward models (PRMs) online,
where collecting high-quality process labels is prohibitively expensive, making
them particularly vulnerable to reward hacking. To address these challenges, we
propose PRIME (Process Reinforcement through IMplicit rEwards), which enables
online PRM updates using only policy rollouts and outcome labels through
implict process rewards. PRIME combines well with various advantage functions
and forgoes the dedicated reward model training phrase that existing approaches
require, substantially reducing the development overhead. We demonstrate
PRIME's effectiveness on competitional math and coding. Starting from
Qwen2.5-Math-7B-Base, PRIME achieves a 15.1% average improvement across several
key reasoning benchmarks over the SFT model. Notably, our resulting model,
Eurus-2-7B-PRIME, surpasses Qwen2.5-Math-7B-Instruct on seven reasoning
benchmarks with 10% of its training data.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.01387v3' target='_blank'>TeLL-Drive: Enhancing Autonomous Driving with Teacher LLM-Guided Deep
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chengkai Xu, Jiaqi Liu, Shiyu Fang, Yiming Cui, Dong Chen, Peng Hang, Jian Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-03 14:22:03</h6>
<p class='card-text'>Although Deep Reinforcement Learning (DRL) and Large Language Models (LLMs)
each show promise in addressing decision-making challenges in autonomous
driving, DRL often suffers from high sample complexity, while LLMs have
difficulty ensuring real-time decision making. To address these limitations, we
propose TeLL-Drive, a hybrid framework that integrates a Teacher LLM to guide
an attention-based Student DRL policy. By incorporating risk metrics,
historical scenario retrieval, and domain heuristics into context-rich prompts,
the LLM produces high-level driving strategies through chain-of-thought
reasoning. A self-attention mechanism then fuses these strategies with the DRL
agent's exploration, accelerating policy convergence and boosting robustness
across diverse driving conditions. The experimental results, evaluated across
multiple traffic scenarios, show that TeLL-Drive outperforms existing baseline
methods, including other LLM-based approaches, in terms of success rates,
average returns, and real-time feasibility. Ablation studies underscore the
importance of each model component, especially the synergy between the
attention mechanism and LLM-driven guidance. Finally, we build a virtual-real
fusion experimental platform to verify the real-time performance, robustness,
and reliability of the algorithm running on real vehicles through
vehicle-in-loop experiments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.01203v1' target='_blank'>Theoretical Analysis of KL-regularized RLHF with Multiple Reference
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Gholamali Aminian, Amir R. Asadi, Idan Shenfeld, Youssef Mroueh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-03 09:50:30</h6>
<p class='card-text'>Recent methods for aligning large language models (LLMs) with human feedback
predominantly rely on a single reference model, which limits diversity, model
overfitting, and underutilizes the wide range of available pre-trained models.
Incorporating multiple reference models has the potential to address these
limitations by broadening perspectives, reducing bias, and leveraging the
strengths of diverse open-source LLMs. However, integrating multiple reference
models into reinforcement learning with human feedback (RLHF) frameworks poses
significant theoretical challenges, particularly in reverse KL-regularization,
where achieving exact solutions has remained an open problem. This paper
presents the first \emph{exact solution} to the multiple reference model
problem in reverse KL-regularized RLHF. We introduce a comprehensive
theoretical framework that includes rigorous statistical analysis and provides
sample complexity guarantees. Additionally, we extend our analysis to forward
KL-regularized RLHF, offering new insights into sample complexity requirements
in multiple reference scenarios. Our contributions lay the foundation for more
advanced and adaptable LLM alignment techniques, enabling the effective use of
multiple reference models. This work paves the way for developing alignment
frameworks that are both theoretically sound and better suited to the
challenges of modern AI ecosystems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.00814v1' target='_blank'>Disentangling Length Bias In Preference Learning Via
  Response-Conditioned Modeling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jianfeng Cai, Jinhua Zhu, Ruopei Sun, Yue Wang, Li Li, Wengang Zhou, Houqiang Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-02 14:50:25</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) has achieved considerable
success in aligning large language models (LLMs) by modeling human preferences
with a learnable reward model and employing a reinforcement learning algorithm
to maximize the reward model's scores. However, these reward models are
susceptible to exploitation through various superficial confounding factors,
with length bias emerging as a particularly significant concern. Moreover,
while the pronounced impact of length bias on preference modeling suggests that
LLMs possess an inherent sensitivity to length perception, our preliminary
investigations reveal that fine-tuned LLMs consistently struggle to adhere to
explicit length instructions. To address these two limitations, we propose a
novel framework wherein the reward model explicitly differentiates between
human semantic preferences and response length requirements. Specifically, we
introduce a Response-conditioned Bradley-Terry (Rc-BT) model that enhances the
reward model's capability in length bias mitigating and length instruction
following, through training on our augmented dataset. Furthermore, we propose
the Rc-DPO algorithm to leverage the Rc-BT model for direct policy optimization
(DPO) of LLMs, simultaneously mitigating length bias and promoting adherence to
length instructions. Extensive evaluations demonstrate that our approach
substantially improves both preference modeling and length instruction
compliance, with its effectiveness validated across various foundational models
and preference datasets.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.00792v1' target='_blank'>RTBAgent: A LLM-based Agent System for Real-Time Bidding</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Leng Cai, Junxuan He, Yikai Li, Junjie Liang, Yuanping Lin, Ziming Quan, Yawen Zeng, Jin Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-02 13:10:15</h6>
<p class='card-text'>Real-Time Bidding (RTB) enables advertisers to place competitive bids on
impression opportunities instantaneously, striving for cost-effectiveness in a
highly competitive landscape. Although RTB has widely benefited from the
utilization of technologies such as deep learning and reinforcement learning,
the reliability of related methods often encounters challenges due to the
discrepancies between online and offline environments and the rapid
fluctuations of online bidding. To handle these challenges, RTBAgent is
proposed as the first RTB agent system based on large language models (LLMs),
which synchronizes real competitive advertising bidding environments and
obtains bidding prices through an integrated decision-making process.
Specifically, obtaining reasoning ability through LLMs, RTBAgent is further
tailored to be more professional for RTB via involved auxiliary modules, i.e.,
click-through rate estimation model, expert strategy knowledge, and daily
reflection. In addition, we propose a two-step decision-making process and
multi-memory retrieval mechanism, which enables RTBAgent to review historical
decisions and transaction records and subsequently make decisions more adaptive
to market changes in real-time bidding. Empirical testing with real advertising
datasets demonstrates that RTBAgent significantly enhances profitability. The
RTBAgent code will be publicly accessible at:
https://github.com/CaiLeng/RTBAgent.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.00691v2' target='_blank'>Learning Autonomous Code Integration for Math Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haozhe Wang, Long Li, Chao Qu, Fengming Zhu, Weidi Xu, Wei Chu, Fangzhen Lin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-02 06:32:23</h6>
<p class='card-text'>Recent advances in mathematical problem-solving with language models (LMs)
integrate chain-of-thought (CoT) reasoning and code execution to harness their
complementary strengths. However, existing hybrid frameworks exhibit a critical
limitation: they depend on externally dictated instructions or rigid
code-integration templates, lacking metacognitive awareness -- the capacity to
dynamically evaluate intrinsic capabilities and autonomously determine when and
how to integrate tools. This rigidity motivates our study of autonomous code
integration, enabling models to adapt tool-usage strategies as their reasoning
abilities evolve during training.
  While reinforcement learning (RL) shows promise for boosting LLM reasoning at
scale (e.g., DeepSeek-R1), we demonstrate its inefficiency in learning
autonomous code integration due to inadequate exploration of the vast
combinatorial space of CoT-code interleaving patterns. To address this
challenge, we propose a novel Expectation-Maximization (EM) framework that
synergizes structured exploration (E-step) with off-policy RL optimization
(M-step), creating a self-reinforcing cycle between metacognitive tool-use
decisions and evolving capabilities. Experiments reveal our method achieves
superior results through improved exploration. Notably, our 7B model improves
over 11% on MATH500 and 9.4% on AIME without o1-like CoT.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.00666v2' target='_blank'>Avoiding $\mathbf{exp(R_{max})}$ scaling in RLHF through
  Preference-based Exploration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mingyu Chen, Yiding Chen, Wen Sun, Xuezhou Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-02 04:40:04</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal
technique for large language model (LLM) alignment. This paper studies the
setting of online RLHF and focus on improving sample efficiency. All existing
algorithms in online RLHF, whether doing passive exploration or active
exploration, suffer from a sample complexity that scales exponentially with the
scale of the reward function. This fundamental limitation hinders their
effectiveness in scenarios with heavily skewed preferences, e.g. questions with
a unique correct solution. To address this, we introduce Self-Exploring
Preference-Incentive Online Preference Optimization (SE-POPO), an online RLHF
algorithm that for the first time achieves a sample complexity that scales
polynomially with the reward scale, answering an open problem raised by Xie et
al. (2024).. Theoretically, we demonstrate that the sample complexity of
SE-POPO dominates that of existing exploration algorithms. Empirically, our
systematic evaluation confirms that SE-POPO is more sample-efficient than both
exploratory and non-exploratory baselines, in two primary application scenarios
of RLHF as well as on public benchmarks, marking a significant step forward in
RLHF algorithm design. The code is available at
https://github.com/MYC000801/SE-POPO.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.00657v1' target='_blank'>LLM Safety Alignment is Divergence Estimation in Disguise</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rajdeep Haldar, Ziyi Wang, Qifan Song, Guang Lin, Yue Xing</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-02 04:09:42</h6>
<p class='card-text'>We propose a theoretical framework demonstrating that popular Large Language
Model (LLM) alignment methods, including Reinforcement Learning from Human
Feedback (RLHF) and alternatives, fundamentally function as divergence
estimators between aligned (preferred or safe) and unaligned (less-preferred or
harmful) distributions. This explains the separation phenomenon between safe
and harmful prompts in the model hidden representation after alignment.
Inspired by the theoretical results, we identify that some alignment methods
are better than others in terms of separation and, introduce a new method,
KLDO, and further demonstrate the implication of our theories. We advocate for
compliance-refusal datasets over preference datasets to enhance safety
alignment, supported by both theoretical reasoning and empirical evidence.
Additionally, to quantify safety separation, we leverage a distance metric in
the representation space and statistically validate its efficacy as a
statistical significant indicator of LLM resilience against jailbreak attacks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.00212v3' target='_blank'>STP: Self-play LLM Theorem Provers with Iterative Conjecturing and
  Proving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kefan Dong, Tengyu Ma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-31 23:01:48</h6>
<p class='card-text'>A fundamental challenge in formal theorem proving by LLMs is the lack of
high-quality training data. Although reinforcement learning or expert iteration
partially mitigates this issue by alternating between LLM generating proofs and
finetuning them on correctly generated ones, performance quickly plateaus due
to the scarcity of correct proofs (sparse rewards). To keep improving the
models with limited data, we draw inspiration from mathematicians, who
continuously develop new results, partly by proposing novel conjectures or
exercises (which are often variants of known results) and attempting to solve
them. We design the Self-play Theorem Prover (STP) that simultaneously takes on
two roles, conjecturer and prover, each providing training signals to the
other. The conjecturer is trained iteratively on previously generated
conjectures that are barely provable by the current prover, which incentivizes
it to generate increasingly challenging conjectures over time. The prover
attempts to prove the conjectures with standard expert iteration. We evaluate
STP with both Lean and Isabelle formal versifiers. With 19.8 billion tokens
generated during the training in Lean, STP proves 26.3% of the statements in
the LeanWorkbook dataset, doubling the previous best result of 13.2% achieved
through expert iteration. The final model achieves state-of-the-art performance
among whole-proof generation methods on miniF2F-test (61.7%, pass@3200),
Proofnet-test (23.1%, pass@3200) and PutnamBench (8/644, pass@3200).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.00136v1' target='_blank'>A Three-Branch Checks-and-Balances Frameworkfor Context-Aware Ethical
  Alignment of Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Edward Y. Chang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-31 19:41:28</h6>
<p class='card-text'>This paper introduces a three-branch checks-and-balances framework for
ethical alignment of Large Language Models (LLMs), inspired by governmental
systems. It implements three independent yet interacting components: LLMs as
the executive branch for knowledge generation, DIKE as the legislative branch
establishing ethical guardrails, and ERIS as the judicial branch for contextual
interpretation. The adversarial DIKE-ERIS duality enables adaptation to diverse
cultural contexts while upholding consistent ethical principles. This
architecture addresses limitations of reinforcement learning with human
feedback (RLHF) by providing interpretable, adaptable, and culturally-aware
ethical reasoning. Through self-supervised learning and adversarial testing,
our framework demonstrates how emotional modeling can guide linguistic
behaviors toward ethical outcomes while preserving independence across
knowledge generation, ethical oversight, and contextual interpretation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.19358v2' target='_blank'>The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating
  Reward Hacking</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuchun Miao, Sen Zhang, Liang Ding, Yuqi Zhang, Lefei Zhang, Dacheng Tao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-31 18:10:53</h6>
<p class='card-text'>This work identifies the Energy Loss Phenomenon in Reinforcement Learning
from Human Feedback (RLHF) and its connection to reward hacking. Specifically,
energy loss in the final layer of a Large Language Model (LLM) gradually
increases during the RL process, with an excessive increase in energy loss
characterizing reward hacking. Beyond empirical analysis, we further provide a
theoretical foundation by proving that, under mild conditions, the increased
energy loss reduces the upper bound of contextual relevance in LLMs, which is a
critical aspect of reward hacking as the reduced contextual relevance typically
indicates overfitting to reward model-favored patterns in RL. To address this
issue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the
increase in energy loss in the LLM's final layer during reward calculation to
prevent excessive energy loss, thereby mitigating reward hacking. We
theoretically show that EPPO can be conceptually interpreted as an
entropy-regularized RL algorithm, which provides deeper insights into its
effectiveness. Extensive experiments across various LLMs and tasks demonstrate
the commonality of the energy loss phenomenon, as well as the effectiveness of
EPPO in mitigating reward hacking and improving RLHF performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.19266v1' target='_blank'>Jackpot! Alignment as a Maximal Lottery</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Roberto-Rafael Maura-Rivero, Marc Lanctot, Francesco Visin, Kate Larson</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-31 16:26:28</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF), the standard for aligning
Large Language Models (LLMs) with human values, is known to fail to satisfy
properties that are intuitively desirable, such as respecting the preferences
of the majority \cite{ge2024axioms}. To overcome these issues, we propose the
use of a probabilistic Social Choice rule called \emph{maximal lotteries} as a
replacement for RLHF. We show that a family of alignment techniques, namely
Nash Learning from Human Feedback (NLHF) \cite{munos2023nash} and variants,
approximate maximal lottery outcomes and thus inherit its beneficial
properties.
  We confirm experimentally that our proposed methodology handles situations
that arise when working with preferences more robustly than standard RLHF,
including supporting the preferences of the majority, providing principled ways
of handling non-transitivities in the preference data, and robustness to
irrelevant alternatives. This results in systems that better incorporate human
values and respect human intentions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.18858v1' target='_blank'>BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language
  Model Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Han Zhong, Yutong Yin, Shenao Zhang, Xiaojun Xu, Yuanxin Liu, Yifei Zuo, Zhihan Liu, Boyi Liu, Sirui Zheng, Hongyi Guo, Liwei Wang, Mingyi Hong, Zhaoran Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-31 02:39:07</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated remarkable capabilities in
complex reasoning tasks, yet generating reliable reasoning processes remains a
significant challenge. We present a unified probabilistic framework that
formalizes LLM reasoning through a novel graphical model incorporating latent
thinking processes and evaluation signals. Within this framework, we introduce
the Bootstrapping Reinforced Thinking Process (BRiTE) algorithm, which works in
two steps. First, it generates high-quality rationales by approximating the
optimal thinking process through reinforcement learning, using a novel reward
shaping mechanism. Second, it enhances the base LLM by maximizing the joint
probability of rationale generation with respect to the model's parameters.
Theoretically, we demonstrate BRiTE's convergence at a rate of $1/T$ with $T$
representing the number of iterations. Empirical evaluations on math and coding
benchmarks demonstrate that our approach consistently improves performance
across different base models without requiring human-annotated thinking
processes. In addition, BRiTE demonstrates superior performance compared to
existing algorithms that bootstrap thinking processes use alternative methods
such as rejection sampling, and can even match or exceed the results achieved
through supervised fine-tuning with human-annotated data.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.01652v1' target='_blank'>Hybrid Group Relative Policy Optimization: A Multi-Sample Approach to
  Enhancing Policy Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Soham Sane</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-30 21:04:01</h6>
<p class='card-text'>Hybrid Group Relative Policy Optimization (Hybrid GRPO) is a reinforcement
learning framework that extends Proximal Policy Optimization (PPO) and Group
Relative Policy Optimization (GRPO) by incorporating empirical multi-sample
action evaluation while preserving the stability of value function-based
learning. Unlike DeepSeek GRPO, which eliminates the value function in favor of
purely empirical reward estimation, Hybrid GRPO introduces a structured
advantage computation method that balances empirical action sampling with
bootstrapped value estimation. This approach enhances sample efficiency,
improves learning stability, and mitigates variance amplification observed in
purely empirical methods. A detailed mathematical comparison between PPO,
DeepSeek GRPO, and Hybrid GRPO is presented, highlighting key differences in
advantage estimation and policy updates. Experimental validation in a
controlled reinforcement learning environment demonstrates that Hybrid GRPO
achieves superior convergence speed, more stable policy updates, and improved
sample efficiency compared to existing methods. Several extensions to Hybrid
GRPO are explored, including entropy-regularized sampling, hierarchical
multi-step sub-sampling, adaptive reward normalization, and value-based action
selection. Beyond reinforcement learning in simulated environments, Hybrid GRPO
provides a scalable framework for bridging the gap between large language
models (LLMs) and real-world agent-based decision-making. By integrating
structured empirical sampling with reinforcement learning stability mechanisms,
Hybrid GRPO has potential applications in autonomous robotics, financial
modeling, and AI-driven control systems. These findings suggest that Hybrid
GRPO serves as a robust and adaptable reinforcement learning methodology,
paving the way for further advancements in policy optimization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.18668v1' target='_blank'>Simulation Streams: A Programming Paradigm for Controlling Large
  Language Models and Building Complex Systems with Generative AI</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Peter Sunehag, Joel Z. Leibo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-30 16:38:03</h6>
<p class='card-text'>We introduce Simulation Streams, a programming paradigm designed to
efficiently control and leverage Large Language Models (LLMs) for complex,
dynamic simulations and agentic workflows. Our primary goal is to create a
minimally interfering framework that harnesses the agentic abilities of LLMs
while addressing their limitations in maintaining consistency, selectively
ignoring/including information, and enforcing strict world rules. Simulation
Streams achieves this through a state-based approach where variables are
modified in sequential steps by "operators," producing output on a recurring
format and adhering to consistent rules for state variables. This approach
focus the LLMs on defined tasks, while aiming to have the context stream remain
"in-distribution". The approach incorporates an Entity-Component-System (ECS)
architecture to write programs in a more intuitive manner, facilitating reuse
of workflows across different components and entities. This ECS approach
enhances the modularity of the output stream, allowing for complex,
multi-entity simulations while maintaining format consistency, information
control, and rule enforcement. It is supported by a custom editor that aids in
creating, running, and analyzing simulations. We demonstrate the versatility of
simulation streams through an illustrative example of an ongoing market economy
simulation, a social simulation of three characters playing a game of catch in
a park and a suite of classical reinforcement learning benchmark tasks. These
examples showcase Simulation Streams' ability to handle complex, evolving
scenarios over 100s-1000s of iterations, facilitate comparisons between
different agent workflows and models, and maintain consistency and continued
interesting developments in LLM-driven simulations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.18663v1' target='_blank'>Joint Optimization of Prompt Security and System Performance in
  Edge-Cloud LLM Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haiyang Huang, Tianhui Meng, Weijia Jia</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-30 14:33:49</h6>
<p class='card-text'>Large language models (LLMs) have significantly facilitated human life, and
prompt engineering has improved the efficiency of these models. However, recent
years have witnessed a rise in prompt engineering-empowered attacks, leading to
issues such as privacy leaks, increased latency, and system resource wastage.
Though safety fine-tuning based methods with Reinforcement Learning from Human
Feedback (RLHF) are proposed to align the LLMs, existing security mechanisms
fail to cope with fickle prompt attacks, highlighting the necessity of
performing security detection on prompts. In this paper, we jointly consider
prompt security, service latency, and system resource optimization in
Edge-Cloud LLM (EC-LLM) systems under various prompt attacks. To enhance prompt
security, a vector-database-enabled lightweight attack detector is proposed. We
formalize the problem of joint prompt detection, latency, and resource
optimization into a multi-stage dynamic Bayesian game model. The equilibrium
strategy is determined by predicting the number of malicious tasks and updating
beliefs at each stage through Bayesian updates. The proposed scheme is
evaluated on a real implemented EC-LLM system, and the results demonstrate that
our approach offers enhanced security, reduces the service latency for benign
users, and decreases system resource consumption compared to state-of-the-art
algorithms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.18056v1' target='_blank'>RL-based Query Rewriting with Distilled LLM for online E-Commerce
  Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Duy A. Nguyen, Rishi Kesav Mohan, Van Yang, Pritom Saha Akash, Kevin Chen-Chuan Chang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-29 23:41:12</h6>
<p class='card-text'>Query rewriting (QR) is a critical technique in e-commerce search, addressing
the lexical gap between user queries and product descriptions to enhance search
performance. Existing QR approaches typically fall into two categories:
discriminative models and generative methods leveraging large language models
(LLMs). Discriminative models often struggle with natural language
understanding and offer limited flexibility in rewriting, while generative
LLMs, despite producing high-quality rewrites, face high inference latency and
cost in online settings. These limitations force offline deployment, making
them vulnerable to issues like information staleness and semantic drift. To
overcome these challenges, we propose a novel hybrid pipeline for QR that
balances efficiency and effectiveness. Our approach combines offline knowledge
distillation to create a lightweight but efficient student model with online
reinforcement learning (RL) to refine query rewriting dynamically using
real-time feedback. A key innovation is the use of LLMs as simulated human
feedback, enabling scalable reward signals and cost-effective evaluation
without manual annotations. Experimental results on Amazon ESCI dataset
demonstrate significant improvements in query relevance, diversity, and
adaptability, as well as positive feedback from the LLM simulation. This work
contributes to advancing LLM capabilities for domain-specific applications,
offering a robust solution for dynamic and complex e-commerce search
environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.17112v1' target='_blank'>Unlocking Transparent Alignment Through Enhanced Inverse Constitutional
  AI for Principle Extraction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Carl-Leander Henneking, Claas Beger</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-28 17:59:56</h6>
<p class='card-text'>Traditional methods for aligning Large Language Models (LLMs), such as
Reinforcement Learning from Human Feedback (RLHF) and Direct Preference
Optimization (DPO), rely on implicit principles, limiting interpretability.
Constitutional AI (CAI) offers an explicit, rule-based framework for guiding
model outputs. Building on this, we refine the Inverse Constitutional AI (ICAI)
algorithm, which extracts constitutions from preference datasets. By improving
principle generation, clustering, and embedding processes, our approach
enhances the accuracy and generalizability of extracted principles across
synthetic and real-world datasets. While in-context alignment yields modest
improvements, our results highlight the potential of these principles to foster
more transparent and adaptable alignment methods, offering a promising
direction for future advancements beyond traditional fine-tuning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.17030v1' target='_blank'>Challenges in Ensuring AI Safety in DeepSeek-R1 Models: The Shortcomings
  of Reinforcement Learning Strategies</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Manojkumar Parmar, Yuvaraj Govindarajulu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-28 15:52:51</h6>
<p class='card-text'>Large Language Models (LLMs) have achieved remarkable progress in reasoning,
alignment, and task-specific performance. However, ensuring harmlessness in
these systems remains a critical challenge, particularly in advanced models
like DeepSeek-R1. This paper examines the limitations of Reinforcement Learning
(RL) as the primary approach for reducing harmful outputs in DeepSeek-R1 and
compares it with Supervised Fine-Tuning (SFT). While RL improves reasoning
capabilities, it faces challenges such as reward hacking, generalization
failures, language mixing, and high computational costs. We propose hybrid
training approaches combining RL and SFT to achieve robust harmlessness
reduction. Usage recommendations and future directions for deploying
DeepSeek-R1 responsibly are also presented.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.17206v1' target='_blank'>Integrating Reinforcement Learning and AI Agents for Adaptive Robotic
  Interaction and Assistance in Dementia Care</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fengpei Yuan, Nehal Hasnaeen, Ran Zhang, Bryce Bible, Joseph Riley Taylor, Hairong Qi, Fenghui Yao, Xiaopeng Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-28 06:38:24</h6>
<p class='card-text'>This study explores a novel approach to advancing dementia care by
integrating socially assistive robotics, reinforcement learning (RL), large
language models (LLMs), and clinical domain expertise within a simulated
environment. This integration addresses the critical challenge of limited
experimental data in socially assistive robotics for dementia care, providing a
dynamic simulation environment that realistically models interactions between
persons living with dementia (PLWDs) and robotic caregivers. The proposed
framework introduces a probabilistic model to represent the cognitive and
emotional states of PLWDs, combined with an LLM-based behavior simulation to
emulate their responses. We further develop and train an adaptive RL system
enabling humanoid robots, such as Pepper, to deliver context-aware and
personalized interactions and assistance based on PLWDs' cognitive and
emotional states. The framework also generalizes to computer-based agents,
highlighting its versatility. Results demonstrate that the RL system, enhanced
by LLMs, effectively interprets and responds to the complex needs of PLWDs,
providing tailored caregiving strategies. This research contributes to
human-computer and human-robot interaction by offering a customizable AI-driven
caregiving platform, advancing understanding of dementia-related challenges,
and fostering collaborative innovation in assistive technologies. The proposed
approach has the potential to enhance the independence and quality of life for
PLWDs while alleviating caregiver burden, underscoring the transformative role
of interaction-focused AI systems in dementia care.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.16727v2' target='_blank'>xJailbreak: Representation Space Guided Reinforcement Learning for
  Interpretable LLM Jailbreaking</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sunbowen Lee, Shiwen Ni, Chi Wei, Shuaimin Li, Liyang Fan, Ahmadreza Argha, Hamid Alinejad-Rokny, Ruifeng Xu, Yicheng Gong, Min Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-28 06:07:58</h6>
<p class='card-text'>Safety alignment mechanism are essential for preventing large language models
(LLMs) from generating harmful information or unethical content. However,
cleverly crafted prompts can bypass these safety measures without accessing the
model's internal parameters, a phenomenon known as black-box jailbreak.
Existing heuristic black-box attack methods, such as genetic algorithms, suffer
from limited effectiveness due to their inherent randomness, while recent
reinforcement learning (RL) based methods often lack robust and informative
reward signals. To address these challenges, we propose a novel black-box
jailbreak method leveraging RL, which optimizes prompt generation by analyzing
the embedding proximity between benign and malicious prompts. This approach
ensures that the rewritten prompts closely align with the intent of the
original prompts while enhancing the attack's effectiveness. Furthermore, we
introduce a comprehensive jailbreak evaluation framework incorporating
keywords, intent matching, and answer validation to provide a more rigorous and
holistic assessment of jailbreak success. Experimental results show the
superiority of our approach, achieving state-of-the-art (SOTA) performance on
several prominent open and closed-source LLMs, including Qwen2.5-7B-Instruct,
Llama3.1-8B-Instruct, and GPT-4o-0806. Our method sets a new benchmark in
jailbreak attack effectiveness, highlighting potential vulnerabilities in LLMs.
The codebase for this work is available at
https://github.com/Aegis1863/xJailbreak.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.15453v2' target='_blank'>Data-adaptive Safety Rules for Training Reward Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaomin Li, Mingye Gao, Zhiwei Zhang, Jingxuan Fan, Weiyu Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-26 08:49:46</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) is commonly employed to
tailor models to human preferences, especially to improve the safety of outputs
from large language models (LLMs). Traditionally, this method depends on
selecting preferred responses from pairs. However, due to the variability in
human opinions and the challenges in directly comparing two responses, there is
an increasing trend towards fine-grained annotation approaches that evaluate
responses using multiple targeted metrics or rules. The challenge lies in
efficiently choosing and applying these rules to handle the diverse range of
preference data. In this paper, we propose a dynamic method that adaptively
selects the most important rules for each response pair. We introduce a
mathematical framework that utilizes the maximum discrepancy across paired
responses and demonstrate theoretically that this approach maximizes the mutual
information between the rule-based annotations and the underlying true
preferences. We then train an 8B reward model using this adaptively labeled
preference dataset and assess its efficacy using RewardBench. As of January 25,
2025, our model achieved the highest safety performance on the leaderboard,
surpassing various larger models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.14205v1' target='_blank'>Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement
  Learning-based Model Caching and Inference Offloading</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Minrui Xu, Dusit Niyato, Christopher G. Brinton</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-24 03:21:20</h6>
<p class='card-text'>Large Language Models (LLMs) can perform zero-shot learning on unseen tasks
and few-shot learning on complex reasoning tasks. However, resource-limited
mobile edge networks struggle to support long-context LLM serving for LLM
agents during multi-round interactions with users. Unlike stateless computation
offloading and static service offloading in edge computing, optimizing LLM
serving at edge servers is challenging because LLMs continuously learn from
context which raises accuracy, latency, and resource consumption dynamics. In
this paper, we propose a joint model caching and inference offloading framework
that utilizes test-time deep reinforcement learning (T2DRL) to optimize
deployment and execution strategies for long-context LLM serving. In this
framework, we analyze the performance convergence and design an optimization
problem considering the utilization of context windows in LLMs. Furthermore,
the T2DRL algorithm can learn in both the training phase and the testing phase
to proactively manage cached models and service requests and adapt to context
changes and usage patterns during execution. To further enhance resource
allocation efficiency, we propose a double Dutch auction (DDA) mechanism, which
dynamically matches supply and demand while maximizing social welfare. Finally,
experimental results demonstrate that the T2DRL algorithm can reduce system
costs by at least 30% compared to baselines while guaranteeing the performance
of LLM agents in real-world perception and reasoning tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.13927v1' target='_blank'>CRPO: Confidence-Reward Driven Preference Optimization for Machine
  Translation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guofeng Cui, Pichao Wang, Yang Liu, Zemian Ke, Zhu Liu, Vimal Bhat</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-23 18:59:47</h6>
<p class='card-text'>Large language models (LLMs) have shown great potential in natural language
processing tasks, but their application to machine translation (MT) remains
challenging due to pretraining on English-centric data and the complexity of
reinforcement learning from human feedback (RLHF). Direct Preference
Optimization (DPO) has emerged as a simpler and more efficient alternative, but
its performance depends heavily on the quality of preference data. To address
this, we propose Confidence-Reward driven Preference Optimization (CRPO), a
novel method that combines reward scores with model confidence to improve data
selection for fine-tuning. CRPO selects challenging sentence pairs where the
model is uncertain or underperforms, leading to more effective learning. While
primarily designed for LLMs, CRPO also generalizes to encoder-decoder models
like NLLB, demonstrating its versatility. Empirical results show that CRPO
outperforms existing methods such as RS-DPO, RSO and MBR score in both
translation accuracy and data efficiency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.13816v1' target='_blank'>Large Language Model driven Policy Exploration for Recommender Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jie Wang, Alexandros Karatzoglou, Ioannis Arapakis, Joemon M. Jose</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-23 16:37:44</h6>
<p class='card-text'>Recent advancements in Recommender Systems (RS) have incorporated
Reinforcement Learning (RL), framing the recommendation as a Markov Decision
Process (MDP). However, offline RL policies trained on static user data are
vulnerable to distribution shift when deployed in dynamic online environments.
Additionally, excessive focus on exploiting short-term relevant items can
hinder exploration, leading to suboptimal recommendations and negatively
impacting long-term user gains. Online RL-based RS also face challenges in
production deployment, due to the risks of exposing users to untrained or
unstable policies. Large Language Models (LLMs) offer a promising solution to
mimic user objectives and preferences for pre-training policies offline to
enhance the initial recommendations in online settings. Effectively managing
distribution shift and balancing exploration are crucial for improving RL-based
RS, especially when leveraging LLM-based pre-training. To address these
challenges, we propose an Interaction-Augmented Learned Policy (iALP) that
utilizes user preferences distilled from an LLM. Our approach involves
prompting the LLM with user states to extract item preferences, learning
rewards based on feedback, and updating the RL policy using an actor-critic
framework. Furthermore, to deploy iALP in an online scenario, we introduce an
adaptive variant, A-iALP, that implements a simple fine-tuning strategy
(A-iALP$_{ft}$), and an adaptive approach (A-iALP$_{ap}$) designed to mitigate
issues with compromised policies and limited exploration. Experiments across
three simulated environments demonstrate that A-iALP introduces substantial
performance improvements</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.13333v1' target='_blank'>AgentRec: Agent Recommendation Using Sentence Embeddings Aligned to
  Human Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Joshua Park, Yongfeng Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-23 02:25:44</h6>
<p class='card-text'>Multi-agent systems must decide which agent is the most appropriate for a
given task. We propose a novel architecture for recommending which LLM agent
out of many should perform a task given a natural language prompt by extending
the Sentence-BERT (SBERT) encoder model. On test data, we are able to achieve a
top-1 accuracy of 92.2% with each classification taking less than 300
milliseconds. In contrast to traditional classification methods, our
architecture is computationally cheap, adaptive to new classes, interpretable,
and controllable with arbitrary metrics through reinforcement learning. By
encoding natural language prompts into sentence embeddings, our model captures
the semantic content relevant to recommending an agent. The distance between
sentence embeddings that belong to the same agent is then minimized through
fine-tuning and aligned to human values through reinforcement learning from
human feedback. This allows the classification of natural language prompts
based on their nearest neighbors by measuring the cosine similarity between
embeddings. This work is made possible through the generation of a synthetic
dataset for agent recommendation, which we have open-sourced to the public
along with the code for AgentRec recommendation system at
https://github.com/joshprk/agentrec.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.13264v2' target='_blank'>RAG-Reward: Optimizing RAG with Reward Modeling and RLHF</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hanning Zhang, Juntong Song, Juno Zhu, Yuanhao Wu, Tong Zhang, Cheng Niu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-22 22:59:19</h6>
<p class='card-text'>Retrieval-augmented generation (RAG) enhances Large Language Models (LLMs)
with relevant and up-to-date knowledge, improving their ability to answer
knowledge-intensive questions. It has been shown to enhance both generation
quality and trustworthiness. While numerous works have focused on improving
retrieval, generation, and evaluation, the role of reward models in
reinforcement learning for optimizing RAG remains underexplored. In this paper,
we introduce \textbf{RAG-Reward}, a framework designed to develop reward models
to enable \textit{hallucination-free, comprehensive, reliable, and efficient
RAG}. We define four key metrics to assess generation quality and develop an
automated benchmarking pipeline to evaluate the outputs of multiple LLMs across
a variety of RAG scenarios. Using \textbf{RAG-Reward}, we train reward models
and apply {reinforcement learning with human feedback (RLHF)} to improve LLMs'
effectiveness in RAG. Experimental results demonstrate that our reward model
achieves state-of-the-art performance in automatic benchmarking and aligns
closely with human evaluations. Furthermore, the improved generation quality of
the trained policy model highlights the feasibility and efficiency of using
RLHF to enhance RAG outputs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.13011v1' target='_blank'>MONA: Myopic Optimization with Non-myopic Approval Can Mitigate
  Multi-step Reward Hacking</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sebastian Farquhar, Vikrant Varma, David Lindner, David Elson, Caleb Biddulph, Ian Goodfellow, Rohin Shah</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-22 16:53:08</h6>
<p class='card-text'>Future advanced AI systems may learn sophisticated strategies through
reinforcement learning (RL) that humans cannot understand well enough to safely
evaluate. We propose a training method which avoids agents learning undesired
multi-step plans that receive high reward (multi-step "reward hacks") even if
humans are not able to detect that the behaviour is undesired. The method,
Myopic Optimization with Non-myopic Approval (MONA), works by combining
short-sighted optimization with far-sighted reward. We demonstrate that MONA
can prevent multi-step reward hacking that ordinary RL causes, even without
being able to detect the reward hacking and without any extra information that
ordinary RL does not get access to. We study MONA empirically in three settings
which model different misalignment failure modes including 2-step environments
with LLMs representing delegated oversight and encoded reasoning and
longer-horizon gridworld environments representing sensor tampering.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.12948v1' target='_blank'>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, Zhen Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-22 15:19:35</h6>
<p class='card-text'>We introduce our first-generation reasoning models, DeepSeek-R1-Zero and
DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement
learning (RL) without supervised fine-tuning (SFT) as a preliminary step,
demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero
naturally emerges with numerous powerful and intriguing reasoning behaviors.
However, it encounters challenges such as poor readability, and language
mixing. To address these issues and further enhance reasoning performance, we
introduce DeepSeek-R1, which incorporates multi-stage training and cold-start
data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217
on reasoning tasks. To support the research community, we open-source
DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B,
70B) distilled from DeepSeek-R1 based on Qwen and Llama.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.12735v3' target='_blank'>Online Preference Alignment for Language Models via Count-based
  Exploration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chenjia Bai, Yang Zhang, Shuang Qiu, Qiaosheng Zhang, Kang Xu, Xuelong Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-22 09:12:09</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) has shown great potential
in fine-tuning Large Language Models (LLMs) to align with human preferences.
Existing methods perform preference alignment from a fixed dataset, which can
be limited in data coverage, and the resulting reward model is hard to
generalize in out-of-distribution responses. Thus, online RLHF is more
desirable to empower the LLM to explore outside the support of the initial
dataset by iteratively collecting the prompt-response pairs. In this paper, we
study the fundamental problem in online RLHF, i.e. \emph{how to explore} for
LLM. We give a theoretical motivation in linear reward assumption to show that
an optimistic reward with an upper confidence bound (UCB) term leads to a
provably efficient RLHF policy. Then, we reformulate our objective to direct
preference optimization with an exploration term, where the UCB-term can be
converted to a count-based exploration bonus. We further propose a practical
algorithm, named \emph{Count-based Online Preference Optimization (COPO)},
which leverages a simple coin-flip counting module to estimate the pseudo-count
of a prompt-response pair in previously collected data. COPO encourages LLMs to
balance exploration and preference optimization in an iterative manner, which
enlarges the exploration space and the entire data coverage of iterative LLM
policies. We conduct online RLHF experiments on Zephyr and Llama-3 models. The
results on instruction-following and standard academic benchmarks show that
COPO significantly increases performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.12698v2' target='_blank'>Training Dialogue Systems by AI Feedback for Improving Overall Dialogue
  Impression</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kai Yoshida, Masahiro Mizukami, Seiya Kawano, Canasai Kruengkrai, Hiroaki Sugiyama, Koichiro Yoshino</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-22 08:14:51</h6>
<p class='card-text'>To improve user engagement during conversations with dialogue systems, we
must improve individual dialogue responses and dialogue impressions such as
consistency, personality, and empathy throughout the entire dialogue. While
such dialogue systems have been developing rapidly with the help of large
language models (LLMs), reinforcement learning from AI feedback (RLAIF) has
attracted attention to align LLM-based dialogue models for such dialogue
impressions. In RLAIF, a reward model based on another LLM is used to create a
training signal for an LLM-based dialogue model using zero-shot/few-shot
prompting techniques. However, evaluating an entire dialogue only by prompting
LLMs is challenging. In this study, the supervised fine-tuning (SFT) of LLMs
prepared reward models corresponding to 12 metrics related to the impression of
the entire dialogue for evaluating dialogue responses. We tuned our dialogue
models using the reward model signals as feedback to improve the impression of
the system. The results of automatic and human evaluations showed that tuning
the dialogue model using our reward model corresponding to dialogue impression
improved the evaluation of individual metrics and the naturalness of the
dialogue response.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.12599v1' target='_blank'>Kimi k1.5: Scaling Reinforcement Learning with LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-22 02:48:14</h6>
<p class='card-text'>Language model pretraining with next token prediction has proved effective
for scaling compute but is limited to the amount of available training data.
Scaling reinforcement learning (RL) unlocks a new axis for the continued
improvement of artificial intelligence, with the promise that large language
models (LLMs) can scale their training data by learning to explore with
rewards. However, prior published work has not produced competitive results. In
light of this, we report on the training practice of Kimi k1.5, our latest
multi-modal LLM trained with RL, including its RL training techniques,
multi-modal data recipes, and infrastructure optimization. Long context scaling
and improved policy optimization methods are key ingredients of our approach,
which establishes a simplistic, effective RL framework without relying on more
complex techniques such as Monte Carlo tree search, value functions, and
process reward models. Notably, our system achieves state-of-the-art reasoning
performance across multiple benchmarks and modalities -- e.g., 77.5 on AIME,
96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista -- matching
OpenAI's o1. Moreover, we present effective long2short methods that use
long-CoT techniques to improve short-CoT models, yielding state-of-the-art
short-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on
LiveCodeBench -- outperforming existing short-CoT models such as GPT-4o and
Claude Sonnet 3.5 by a large margin (up to +550%).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.11651v1' target='_blank'>Advancing Language Model Reasoning through Reinforcement Learning and
  Inference Scaling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhenyu Hou, Xin Lv, Rui Lu, Jiajie Zhang, Yujiang Li, Zijun Yao, Juanzi Li, Jie Tang, Yuxiao Dong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-20 18:33:33</h6>
<p class='card-text'>Large language models (LLMs) have demonstrated remarkable capabilities in
complex reasoning tasks. However, existing approaches mainly rely on imitation
learning and struggle to achieve effective test-time scaling. While
reinforcement learning (RL) holds promise for enabling self-exploration and
learning from feedback, recent attempts yield only modest improvements in
complex reasoning. In this paper, we present T1 to scale RL by encouraging
exploration and understand inference scaling. We first initialize the LLM using
synthesized chain-of-thought data that integrates trial-and-error and
self-verification. To scale RL training, we promote increased sampling
diversity through oversampling. We further employ an entropy bonus as an
auxiliary loss, alongside a dynamic anchor for regularization to facilitate
reward optimization. We demonstrate that T1 with open LLMs as its base exhibits
inference scaling behavior and achieves superior performance on challenging
math reasoning benchmarks. For example, T1 with Qwen2.5-32B as the base model
outperforms the recent Qwen QwQ-32B-Preview model on MATH500, AIME2024, and
Omni-math-500. More importantly, we present a simple strategy to examine
inference scaling, where increased inference budgets directly lead to T1's
better performance without any additional verification. We will open-source the
T1 models and the data used to train them at \url{https://github.com/THUDM/T1}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.11463v1' target='_blank'>Curiosity-Driven Reinforcement Learning from Human Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haoran Sun, Yekun Chai, Shuohuan Wang, Yu Sun, Hua Wu, Haifeng Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-20 12:51:40</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) has proven effective in
aligning large language models (LLMs) with human preferences, but often at the
cost of reduced output diversity. This trade-off between diversity and
alignment quality remains a significant challenge. Drawing inspiration from
curiosity-driven exploration in reinforcement learning, we introduce
curiosity-driven RLHF (CD-RLHF), a framework that incorporates intrinsic
rewards for novel states, alongside traditional sparse extrinsic rewards, to
optimize both output diversity and alignment quality. We demonstrate the
effectiveness of CD-RLHF through extensive experiments on a range of tasks,
including text summarization and instruction following. Our approach achieves
significant gains in diversity on multiple diversity-oriented metrics while
maintaining alignment with human preferences comparable to standard RLHF. We
make our code publicly available at https://github.com/ernie-research/CD-RLHF.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.11284v1' target='_blank'>RedStar: Does Scaling Long-CoT Data Unlock Better Slow-Reasoning
  Systems?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang, Jiaming Ji, Yingying Zhang, Zhijiang Guo, Yaodong Yang, Muhan Zhang, Debing Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-20 05:44:01</h6>
<p class='card-text'>Can scaling transform reasoning? In this work, we explore the untapped
potential of scaling Long Chain-of-Thought (Long-CoT) data to 1000k samples,
pioneering the development of a slow-thinking model, RedStar. Through extensive
experiments with various LLMs and different sizes, we uncover the ingredients
for specialization and scale for Long-CoT training. Surprisingly, even smaller
models show significant performance gains with limited data, revealing the
sample efficiency of Long-CoT and the critical role of sample difficulty in the
learning process. Our findings demonstrate that Long-CoT reasoning can be
effectively triggered with just a few thousand examples, while larger models
achieve unparalleled improvements. We also introduce reinforcement learning
(RL)-scale training as a promising direction for advancing slow-thinking
systems. RedStar shines across domains: on the MATH-Hard benchmark,
RedStar-code-math boosts performance from 66.2\% to 81.6\%, and on the USA Math
Olympiad (AIME), it solves 46.7\% of problems using only 21k mixed-code-math
datasets. In multimodal tasks like GeoQA and MathVista-GEO, RedStar-Geo
achieves competitive results with minimal Long-CoT data, outperforming other
slow-thinking systems like QvQ-Preview. Compared to QwQ, RedStar strikes the
perfect balance between reasoning and generalizability. Our work highlights
that, with careful tuning, scaling Long-CoT can unlock extraordinary reasoning
capabilities-even with limited dataset and set a new standard for slow-thinking
models across diverse challenges. Our data and models are released at
https://huggingface.co/RedStar-Reasoning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.11223v3' target='_blank'>Reasoning Language Models: A Blueprint</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Maciej Besta, Julia Barth, Eric Schreiber, Ales Kubicek, Afonso Catarino, Robert Gerstenberger, Piotr Nyczyk, Patrick Iff, Yueling Li, Sam Houliston, Tomasz Sternal, Marcin Copik, Grzegorz Kwaśniewski, Jürgen Müller, Łukasz Flis, Hannes Eberhard, Hubert Niewiadomski, Torsten Hoefler</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-20 02:16:19</h6>
<p class='card-text'>Reasoning language models (RLMs), also known as Large Reasoning Models
(LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have
redefined AI's problem-solving capabilities by extending LLMs with advanced
reasoning mechanisms. Yet, their high costs, proprietary nature, and complex
architectures - uniquely combining Reinforcement Learning (RL), search
heuristics, and LLMs - present accessibility and scalability challenges. To
address these, we propose a comprehensive blueprint that organizes RLM
components into a modular framework, based on a survey and analysis of all RLM
works. This blueprint incorporates diverse reasoning structures (chains, trees,
graphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search,
Beam Search), RL concepts (policy, value models and others), supervision
schemes (Outcome-Based and Process-Based Supervision), and other related
concepts (e.g., Test-Time Compute, Retrieval-Augmented Generation, agent
tools). We also provide detailed mathematical formulations and algorithmic
specifications to simplify RLM implementation. By showing how schemes like
LLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases,
we demonstrate the blueprint's versatility and unifying potential. To
illustrate its utility, we introduce x1, a modular implementation for rapid RLM
prototyping and experimentation. Using x1 and a literature review, we provide
key insights, such as multi-phase training for policy and value models, and the
importance of familiar training distributions. Finally, we discuss scalable RLM
cloud deployments and we outline how RLMs can integrate with a broader LLM
ecosystem. Our work demystifies RLM construction, democratizes advanced
reasoning capabilities, and fosters innovation, aiming to mitigate the gap
between "rich AI" and "poor AI" by lowering barriers to RLM design and
experimentation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.11006v1' target='_blank'>GREEN-CODE: Optimizing Energy Efficiency in Large Language Models for
  Code Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shashikant Ilager, Lukas Florian Briem, Ivona Brandic</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-19 10:44:03</h6>
<p class='card-text'>Large Language Models (LLMs) are becoming integral to daily life, showcasing
their vast potential across various Natural Language Processing (NLP) tasks.
Beyond NLP, LLMs are increasingly used in software development tasks, such as
code completion, modification, bug fixing, and code translation. Software
engineers widely use tools like GitHub Copilot and Amazon Q, streamlining
workflows and automating tasks with high accuracy. While the resource and
energy intensity of LLM training is often highlighted, inference can be even
more resource-intensive over time, as it's a continuous process with a high
number of invocations. Therefore, developing resource-efficient alternatives
for LLM inference is crucial for sustainability. This work proposes GREEN-CODE,
a framework for energy-aware code generation in LLMs. GREEN-CODE performs
dynamic early exit during LLM inference. We train a Reinforcement Learning (RL)
agent that learns to balance the trade-offs between accuracy, latency, and
energy consumption. Our approach is evaluated on two open-source LLMs, Llama
3.2 3B and OPT 2.7B, using the JavaCorpus and PY150 datasets. Results show that
our method reduces the energy consumption between 23-50 % on average for code
generation tasks without significantly affecting accuracy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.10120v1' target='_blank'>PaSa: An LLM Agent for Comprehensive Academic Paper Search</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yichen He, Guanhua Huang, Peiyuan Feng, Yuan Lin, Yuchen Zhang, Hang Li, Weinan E</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-17 11:12:28</h6>
<p class='card-text'>We introduce PaSa, an advanced Paper Search agent powered by large language
models. PaSa can autonomously make a series of decisions, including invoking
search tools, reading papers, and selecting relevant references, to ultimately
obtain comprehensive and accurate results for complex scholarly queries. We
optimize PaSa using reinforcement learning with a synthetic dataset,
AutoScholarQuery, which includes 35k fine-grained academic queries and
corresponding papers sourced from top-tier AI conference publications.
Additionally, we develop RealScholarQuery, a benchmark collecting real-world
academic queries to assess PaSa performance in more realistic scenarios.
Despite being trained on synthetic data, PaSa significantly outperforms
existing baselines on RealScholarQuery, including Google, Google Scholar,
Google with GPT-4 for paraphrased queries, chatGPT (search-enabled GPT-4o),
GPT-o1, and PaSa-GPT-4o (PaSa implemented by prompting GPT-4o). Notably,
PaSa-7B surpasses the best Google-based baseline, Google with GPT-4o, by 37.78%
in recall@20 and 39.90% in recall@50. It also exceeds PaSa-GPT-4o by 30.36% in
recall and 4.25% in precision. Model, datasets, and code are available at
https://github.com/bytedance/pasa.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.09781v1' target='_blank'>VideoWorld: Exploring Knowledge Learning from Unlabeled Videos</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhongwei Ren, Yunchao Wei, Xun Guo, Yao Zhao, Bingyi Kang, Jiashi Feng, Xiaojie Jin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-16 18:59:10</h6>
<p class='card-text'>This work explores whether a deep generative model can learn complex
knowledge solely from visual input, in contrast to the prevalent focus on
text-based models like large language models (LLMs). We develop VideoWorld, an
auto-regressive video generation model trained on unlabeled video data, and
test its knowledge acquisition abilities in video-based Go and robotic control
tasks. Our experiments reveal two key findings: (1) video-only training
provides sufficient information for learning knowledge, including rules,
reasoning and planning capabilities, and (2) the representation of visual
change is crucial for knowledge acquisition. To improve both the efficiency and
efficacy of this process, we introduce the Latent Dynamics Model (LDM) as a key
component of VideoWorld. Remarkably, VideoWorld reaches a 5-dan professional
level in the Video-GoBench with just a 300-million-parameter model, without
relying on search algorithms or reward mechanisms typical in reinforcement
learning. In robotic tasks, VideoWorld effectively learns diverse control
operations and generalizes across environments, approaching the performance of
oracle models in CALVIN and RLBench. This study opens new avenues for knowledge
acquisition from visual data, with all code, data, and models open-sourced for
further research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.09686v3' target='_blank'>Towards Large Reasoning Models: A Survey of Reinforced Reasoning with
  Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, Chenyang Shao, Yuwei Yan, Qinglong Yang, Yiwen Song, Sijian Ren, Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, Yong Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-16 17:37:58</h6>
<p class='card-text'>Language has long been conceived as an essential tool for human reasoning.
The breakthrough of Large Language Models (LLMs) has sparked significant
research interest in leveraging these models to tackle complex reasoning tasks.
Researchers have moved beyond simple autoregressive token generation by
introducing the concept of "thought" -- a sequence of tokens representing
intermediate steps in the reasoning process. This innovative paradigm enables
LLMs' to mimic complex human reasoning processes, such as tree search and
reflective thinking. Recently, an emerging trend of learning to reason has
applied reinforcement learning (RL) to train LLMs to master reasoning
processes. This approach enables the automatic generation of high-quality
reasoning trajectories through trial-and-error search algorithms, significantly
expanding LLMs' reasoning capacity by providing substantially more training
data. Furthermore, recent studies demonstrate that encouraging LLMs to "think"
with more tokens during test-time inference can further significantly boost
reasoning accuracy. Therefore, the train-time and test-time scaling combined to
show a new research frontier -- a path toward Large Reasoning Model. The
introduction of OpenAI's o1 series marks a significant milestone in this
research direction. In this survey, we present a comprehensive review of recent
progress in LLM reasoning. We begin by introducing the foundational background
of LLMs and then explore the key technical components driving the development
of large reasoning models, with a focus on automated data construction,
learning-to-reason techniques, and test-time scaling. We also analyze popular
open-source projects at building large reasoning models, and conclude with open
challenges and future research directions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.09620v1' target='_blank'>Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chaoqi Wang, Zhuokai Zhao, Yibo Jiang, Zhaorun Chen, Chen Zhu, Yuxin Chen, Jiayi Liu, Lizhu Zhang, Xiangjun Fan, Hao Ma, Sinong Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-16 16:00:37</h6>
<p class='card-text'>Recent advances in large language models (LLMs) have demonstrated significant
progress in performing complex tasks. While Reinforcement Learning from Human
Feedback (RLHF) has been effective in aligning LLMs with human preferences, it
is susceptible to spurious correlations in reward modeling. Consequently, it
often introduces biases-such as length bias, sycophancy, conceptual bias, and
discrimination that hinder the model's ability to capture true causal
relationships. To address this, we propose a novel causal reward modeling
approach that integrates causal inference to mitigate these spurious
correlations. Our method enforces counterfactual invariance, ensuring reward
predictions remain consistent when irrelevant variables are altered. Through
experiments on both synthetic and real-world datasets, we show that our
approach mitigates various types of spurious correlations effectively,
resulting in more reliable and fair alignment of LLMs with human preferences.
As a drop-in enhancement to the existing RLHF workflow, our causal reward
modeling provides a practical way to improve the trustworthiness and fairness
of LLM finetuning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.09383v1' target='_blank'>Adaptive Contextual Caching for Mobile Edge Large Language Model Service</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guangyuan Liu, Yinqiu Liu, Jiacheng Wang, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-16 08:52:38</h6>
<p class='card-text'>Mobile edge Large Language Model (LLM) deployments face inherent constraints,
such as limited computational resources and network bandwidth. Although
Retrieval-Augmented Generation (RAG) mitigates some challenges by integrating
external knowledge bases, inefficient cache management can still result in high
retrieval latency and frequent cache updates. To address these issues, we
propose an Adaptive Contextual Caching (ACC) framework that anticipates user
needs by proactively caching semantically relevant data for mobile-edge LLMs.
ACC utilizes a deep reinforcement learning (DRL) module to refine cache
replacement policies, balancing user context, document similarity, and the
overhead associated with cache misses. Experimental results demonstrate that
ACC increases cache hit rates to over 80\% after only 11 training episodes,
outperforming FIFO, LRU, and semantic-only caching while reducing retrieval
latency by up to 40\%. In particular, ACC also reduces local caching overhead
(i.e., the cost of updating the cache when a miss occurs) by as much as 55\%,
enabling scalable, low-latency LLM services in resource-constrained edge
environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.09254v1' target='_blank'>Clone-Robust AI Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ariel D. Procaccia, Benjamin Schiffer, Shirley Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-16 02:43:44</h6>
<p class='card-text'>A key challenge in training Large Language Models (LLMs) is properly aligning
them with human preferences. Reinforcement Learning with Human Feedback (RLHF)
uses pairwise comparisons from human annotators to train reward functions and
has emerged as a popular alignment method. However, input datasets in RLHF are
not necessarily balanced in the types of questions and answers that are
included. Therefore, we want RLHF algorithms to perform well even when the set
of alternatives is not uniformly distributed. Drawing on insights from social
choice theory, we introduce robustness to approximate clones, a desirable
property of RLHF algorithms which requires that adding near-duplicate
alternatives does not significantly change the learned reward function. We
first demonstrate that the standard RLHF algorithm based on regularized maximum
likelihood estimation (MLE) fails to satisfy this property. We then propose the
weighted MLE, a new RLHF algorithm that modifies the standard regularized MLE
by weighting alternatives based on their similarity to other alternatives. This
new algorithm guarantees robustness to approximate clones while preserving
desirable theoretical properties.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.08600v1' target='_blank'>AutoRestTest: A Tool for Automated REST API Testing Using LLMs and MARL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tyler Stennett, Myeongsoo Kim, Saurabh Sinha, Alessandro Orso</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-15 05:54:33</h6>
<p class='card-text'>As REST APIs have become widespread in modern web services, comprehensive
testing of these APIs has become increasingly crucial. Due to the vast search
space consisting of operations, parameters, and parameter values along with
their complex dependencies and constraints, current testing tools suffer from
low code coverage, leading to suboptimal fault detection. To address this
limitation, we present a novel tool, AutoRestTest, which integrates the
Semantic Operation Dependency Graph (SODG) with Multi-Agent Reinforcement
Learning (MARL) and large language models (LLMs) for effective REST API
testing. AutoRestTest determines operation-dependent parameters using the SODG
and employs five specialized agents (operation, parameter, value, dependency,
and header) to identify dependencies of operations and generate operation
sequences, parameter combinations, and values. AutoRestTest provides a
command-line interface and continuous telemetry on successful operation count,
unique server errors detected, and time elapsed. Upon completion, AutoRestTest
generates a detailed report highlighting errors detected and operations
exercised. In this paper, we introduce our tool and present preliminary
results.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.08071v1' target='_blank'>CuAsmRL: Optimizing GPU SASS Schedules via Deep Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guoliang He, Eiko Yoneki</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-14 12:36:18</h6>
<p class='card-text'>Large language models (LLMs) are remarked by their substantial computational
requirements. To mitigate the cost, researchers develop specialized CUDA
kernels, which often fuse several tensor operations to maximize the utilization
of GPUs as much as possible. However, those specialized kernels may still leave
performance on the table as CUDA assembly experts show that manual optimization
of GPU SASS schedules can lead to better performance, and trial-and-error is
largely employed to manually find the best GPU SASS schedules.
  In this work, we employ an automatic approach to optimize GPU SASS schedules,
which thus can be integrated into existing compiler frameworks. The key to
automatic optimization is training an RL agent to mimic how human experts
perform manual scheduling. To this end, we formulate an assembly game, where RL
agents can play to find the best GPU SASS schedules. The assembly game starts
from a \textit{-O3} optimized SASS schedule, and the RL agents can iteratively
apply actions to mutate the current schedules. Positive rewards are generated
if the mutated schedules get higher throughput by executing on GPUs.
Experiments show that CuAsmRL can further improve the performance of existing
specialized CUDA kernels transparently by up to $26\%$, and on average $9\%$.
Moreover, it is used as a tool to reveal potential optimization moves learned
automatically.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.06980v1' target='_blank'>Combining LLM decision and RL action selection to improve RL policy for
  adaptive interventions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Karine Karine, Benjamin M. Marlin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-13 00:03:20</h6>
<p class='card-text'>Reinforcement learning (RL) is increasingly being used in the healthcare
domain, particularly for the development of personalized health adaptive
interventions. Inspired by the success of Large Language Models (LLMs), we are
interested in using LLMs to update the RL policy in real time, with the goal of
accelerating personalization. We use the text-based user preference to
influence the action selection on the fly, in order to immediately incorporate
the user preference. We use the term "user preference" as a broad term to refer
to a user personal preference, constraint, health status, or a statement
expressing like or dislike, etc. Our novel approach is a hybrid method that
combines the LLM response and the RL action selection to improve the RL policy.
Given an LLM prompt that incorporates the user preference, the LLM acts as a
filter in the typical RL action selection. We investigate different prompting
strategies and action selection strategies. To evaluate our approach, we
implement a simulation environment that generates the text-based user
preferences and models the constraints that impact behavioral dynamics. We show
that our approach is able to take into account the text-based user preferences,
while improving the RL policy, thus improving personalization in adaptive
intervention.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.06911v1' target='_blank'>Risk-Averse Finetuning of Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sapana Chaudhary, Ujwal Dinesha, Dileep Kalathil, Srinivas Shakkottai</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-12 19:48:21</h6>
<p class='card-text'>We consider the challenge of mitigating the generation of negative or toxic
content by the Large Language Models (LLMs) in response to certain prompts. We
propose integrating risk-averse principles into LLM fine-tuning to minimize the
occurrence of harmful outputs, particularly rare but significant events. By
optimizing the risk measure of Conditional Value at Risk (CVaR), our
methodology trains LLMs to exhibit superior performance in avoiding toxic
outputs while maintaining effectiveness in generative tasks. Empirical
evaluations on sentiment modification and toxicity mitigation tasks demonstrate
the efficacy of risk-averse reinforcement learning with human feedback (RLHF)
in promoting a safer and more constructive online discourse environment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.06842v1' target='_blank'>SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tianjin Huang, Ziquan Zhu, Gaojie Jin, Lu Liu, Zhangyang Wang, Shiwei Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-12 15:21:22</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated exceptional performance across
diverse tasks, yet their training remains highly resource-intensive and
susceptible to critical challenges such as training instability. A predominant
source of this instability stems from gradient and loss spikes, which disrupt
the learning process, often leading to costly interventions like checkpoint
recovery and experiment restarts, further amplifying inefficiencies. This paper
presents a comprehensive investigation into gradient spikes observed during LLM
training, revealing their prevalence across multiple architectures and
datasets. Our analysis shows that these spikes can be up to $1000\times$ larger
than typical gradients, substantially deteriorating model performance. To
address this issue, we propose Spike-Aware Adam with Momentum Reset SPAM, a
novel optimizer designed to counteract gradient spikes through momentum reset
and spike-aware gradient clipping. Extensive experiments, including both
pre-training and fine-tuning, demonstrate that SPAM consistently surpasses Adam
and its variants across various tasks, including (1) LLM pre-training from 60M
to 1B, (2) 4-bit LLM pre-training,(3) reinforcement learning, and (4) Time
Series Forecasting. Additionally, SPAM facilitates memory-efficient training by
enabling sparse momentum, where only a subset of momentum terms are maintained
and updated. When operating under memory constraints, SPAM outperforms
state-of-the-art memory-efficient optimizers such as GaLore and Adam-Mini. Our
work underscores the importance of mitigating gradient spikes in LLM training
and introduces an effective optimization strategy that enhances both training
stability and resource efficiency at scale. Code is available at
https://github.com/TianjinYellow/SPAM-Optimizer.git</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.06695v1' target='_blank'>DVM: Towards Controllable LLM Agents in Social Deduction Games</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zheng Zhang, Yihuai Lan, Yangsen Chen, Lei Wang, Xiang Wang, Hao Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-12 03:11:20</h6>
<p class='card-text'>Large Language Models (LLMs) have advanced the capability of game agents in
social deduction games (SDGs). These games rely heavily on conversation-driven
interactions and require agents to infer, make decisions, and express based on
such information. While this progress leads to more sophisticated and strategic
non-player characters (NPCs) in SDGs, there exists a need to control the
proficiency of these agents. This control not only ensures that NPCs can adapt
to varying difficulty levels during gameplay, but also provides insights into
the safety and fairness of LLM agents. In this paper, we present DVM, a novel
framework for developing controllable LLM agents for SDGs, and demonstrate its
implementation on one of the most popular SDGs, Werewolf. DVM comprises three
main components: Predictor, Decider, and Discussor. By integrating
reinforcement learning with a win rate-constrained decision chain reward
mechanism, we enable agents to dynamically adjust their gameplay proficiency to
achieve specified win rates. Experiments show that DVM not only outperforms
existing methods in the Werewolf game, but also successfully modulates its
performance levels to meet predefined win rate targets. These results pave the
way for LLM agents' adaptive and balanced gameplay in SDGs, opening new avenues
for research in controllable game agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.06605v3' target='_blank'>RoboHorizon: An LLM-Assisted Multi-View World Model for Long-Horizon
  Robotic Manipulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zixuan Chen, Jing Huo, Yangtao Chen, Yang Gao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-11 18:11:07</h6>
<p class='card-text'>Efficient control in long-horizon robotic manipulation is challenging due to
complex representation and policy learning requirements. Model-based visual
reinforcement learning (RL) has shown great potential in addressing these
challenges but still faces notable limitations, particularly in handling sparse
rewards and complex visual features in long-horizon environments. To address
these limitations, we propose the Recognize-Sense-Plan-Act (RSPA) pipeline for
long-horizon tasks and further introduce RoboHorizon, an LLM-assisted
multi-view world model tailored for long-horizon robotic manipulation. In
RoboHorizon, pre-trained LLMs generate dense reward structures for multi-stage
sub-tasks based on task language instructions, enabling robots to better
recognize long-horizon tasks. Keyframe discovery is then integrated into the
multi-view masked autoencoder (MAE) architecture to enhance the robot's ability
to sense critical task sequences, strengthening its multi-stage perception of
long-horizon processes. Leveraging these dense rewards and multi-view
representations, a robotic world model is constructed to efficiently plan
long-horizon tasks, enabling the robot to reliably act through RL algorithms.
Experiments on two representative benchmarks, RLBench and FurnitureBench, show
that RoboHorizon outperforms state-of-the-art visual model-based RL methods,
achieving a 23.35% improvement in task success rates on RLBench's 4
short-horizon tasks and a 29.23% improvement on 6 long-horizon tasks from
RLBench and 3 furniture assembly tasks from FurnitureBench.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.05790v1' target='_blank'>Understanding Impact of Human Feedback via Influence Functions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Taywon Min, Haeone Lee, Hanho Ryu, Yongchan Kwon, Kimin Lee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-10 08:50:38</h6>
<p class='card-text'>In Reinforcement Learning from Human Feedback (RLHF), it is crucial to learn
suitable reward models from human feedback to align large language models
(LLMs) with human intentions. However, human feedback can often be noisy,
inconsistent, or biased, especially when evaluating complex responses. Such
feedback can lead to misaligned reward signals, potentially causing unintended
side effects during the RLHF process. To address these challenges, we explore
the use of influence functions to measure the impact of human feedback on the
performance of reward models. We propose a compute-efficient approximation
method that enables the application of influence functions to LLM-based reward
models and large-scale preference datasets. In our experiments, we demonstrate
two key applications of influence functions: (1) detecting common forms of
labeler bias in human feedback datasets and (2) guiding labelers to refine
their strategies to align more closely with expert feedback. By quantifying the
impact of human feedback on reward models, we believe that influence functions
can enhance feedback interpretability and contribute to scalable oversight in
RLHF, helping labelers provide more accurate and consistent feedback. Source
code is available at https://github.com/mintaywon/IF_RLHF</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.05057v1' target='_blank'>LearningFlow: Automated Policy Learning Workflow for Urban Driving with
  Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zengqi Peng, Yubin Wang, Xu Han, Lei Zheng, Jun Ma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-09 08:28:16</h6>
<p class='card-text'>Recent advancements in reinforcement learning (RL) demonstrate the
significant potential in autonomous driving. Despite this promise, challenges
such as the manual design of reward functions and low sample efficiency in
complex environments continue to impede the development of safe and effective
driving policies. To tackle these issues, we introduce LearningFlow, an
innovative automated policy learning workflow tailored to urban driving. This
framework leverages the collaboration of multiple large language model (LLM)
agents throughout the RL training process. LearningFlow includes a curriculum
sequence generation process and a reward generation process, which work in
tandem to guide the RL policy by generating tailored training curricula and
reward functions. Particularly, each process is supported by an analysis agent
that evaluates training progress and provides critical insights to the
generation agent. Through the collaborative efforts of these LLM agents,
LearningFlow automates policy learning across a series of complex driving
tasks, and it significantly reduces the reliance on manual reward function
design while enhancing sample efficiency. Comprehensive experiments are
conducted in the high-fidelity CARLA simulator, along with comparisons with
other existing methods, to demonstrate the efficacy of our proposed approach.
The results demonstrate that LearningFlow excels in generating rewards and
curricula. It also achieves superior performance and robust generalization
across various driving tasks, as well as commendable adaptation to different RL
algorithms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.06252v3' target='_blank'>Transformer-Squared: Self-adaptive LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qi Sun, Edoardo Cetin, Yujin Tang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-09 01:19:21</h6>
<p class='card-text'>Self-adaptive large language models (LLMs) aim to solve the challenges posed
by traditional fine-tuning methods, which are often computationally intensive
and static in their ability to handle diverse tasks. We introduce
Transformer-Squared, a novel self-adaptation framework that adapts LLMs for
unseen tasks in real-time by selectively adjusting only the singular components
of their weight matrices. During inference, Transformer-Squared employs a
two-pass mechanism: first, a dispatch system identifies the task properties,
and then task-specific 'expert' vectors, trained using reinforcement learning,
are dynamically mixed to obtain targeted behavior for the incoming prompt. Our
method consistently outperforms ubiquitous approaches such as LoRA, with fewer
parameters and greater efficiency. Furthermore, Transformer-Squared
demonstrates versatility across different LLM architectures and modalities,
including vision-language tasks. Transformer-Squared represents a significant
leap forward, offering a scalable, efficient solution for enhancing the
adaptability and task-specific performance of LLMs, paving the way for truly
dynamic, self-organizing AI systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.06248v2' target='_blank'>Utility-inspired Reward Transformations Improve Reinforcement Learning
  Training of Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Roberto-Rafael Maura-Rivero, Chirag Nagpal, Roma Patel, Francesco Visin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-08 19:03:17</h6>
<p class='card-text'>Current methods that train large language models (LLMs) with reinforcement
learning feedback, often resort to averaging outputs of multiple rewards
functions during training. This overlooks crucial aspects of individual reward
dimensions and inter-reward dependencies that can lead to sub-optimal outcomes
in generations. In this work, we show how linear aggregation of rewards
exhibits some vulnerabilities that can lead to undesired properties of
generated text. We then propose a transformation of reward functions inspired
by economic theory of utility functions (specifically Inada conditions), that
enhances sensitivity to low reward values while diminishing sensitivity to
already high values. We compare our approach to the existing baseline methods
that linearly aggregate rewards and show how the Inada-inspired reward feedback
is superior to traditional weighted averaging. We quantitatively and
qualitatively analyse the difference in the methods, and see that models
trained with Inada-transformations score as more helpful while being less
harmful.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.04682v1' target='_blank'>Towards System 2 Reasoning in LLMs: Learning How to Think With Meta
  Chain-of-Thought</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, Anikait Singh, Chase Blagden, Duy Phung, Rafael Rafailov, Nathan Lile, Dakota Mahan, Louis Castricato, Jan-Philipp Franken, Nick Haber, Chelsea Finn</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-08 18:42:48</h6>
<p class='card-text'>We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends
traditional Chain-of-Thought (CoT) by explicitly modeling the underlying
reasoning required to arrive at a particular CoT. We present empirical evidence
from state-of-the-art models exhibiting behaviors consistent with in-context
search, and explore methods for producing Meta-CoT via process supervision,
synthetic data generation, and search algorithms. Finally, we outline a
concrete pipeline for training a model to produce Meta-CoTs, incorporating
instruction tuning with linearized search traces and reinforcement learning
post-training. Finally, we discuss open research questions, including scaling
laws, verifier roles, and the potential for discovering novel reasoning
algorithms. This work provides a theoretical and practical roadmap to enable
Meta-CoT in LLMs, paving the way for more powerful and human-like reasoning in
artificial intelligence.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.03884v3' target='_blank'>AlphaPO -- Reward shape matters for LLM alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aman Gupta, Shao Tang, Qingquan Song, Sirou Zhu, Jiwoo Hong, Ankan Saha, Viral Gupta, Noah Lee, Eunki Kim, Siyu Zhu, Parag Agrawal, Natesh Pillai, S. Sathiya Keerthi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-07 15:46:42</h6>
<p class='card-text'>Reinforcement Learning with Human Feedback (RLHF) and its variants have made
huge strides toward the effective alignment of large language models (LLMs) to
follow instructions and reflect human values. More recently, Direct Alignment
Algorithms (DAAs) have emerged in which the reward modeling stage of RLHF is
skipped by characterizing the reward directly as a function of the policy being
learned. Some popular examples of DAAs include Direct Preference Optimization
(DPO) and Simple Preference Optimization (SimPO). These methods often suffer
from likelihood displacement, a phenomenon by which the probabilities of
preferred responses are often reduced undesirably.
  In this paper, we argue that, for DAAs the reward (function) shape matters.
We introduce \textbf{AlphaPO}, a new DAA method that leverages an
$\alpha$-parameter to help change the shape of the reward function beyond the
standard log reward. AlphaPO helps maintain fine-grained control over
likelihood displacement and over-optimization. Compared to SimPO, one of the
best performing DAAs, AlphaPO leads to about 7\% to 10\% relative improvement
in alignment performance for the instruct versions of Mistral-7B and Llama3-8B
while achieving 15\% to 50\% relative improvement over DPO on the same models.
The analysis and results presented highlight the importance of the reward
shape, and how one can systematically change it to affect training dynamics, as
well as improve alignment performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.04070v2' target='_blank'>More is not always better? Enhancing Many-Shot In-Context Learning with
  Differentiated and Reweighting Objectives</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaoqing Zhang, Ang Lv, Yuhan Liu, Flood Sung, Wei Liu, Shuo Shang, Xiuying Chen, Rui Yan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-07 14:57:08</h6>
<p class='card-text'>Large language models (LLMs) excel at few-shot in-context learning (ICL)
without requiring parameter updates. However, as the number of ICL
demonstrations increases from a few to many, performance tends to plateau and
eventually decline. We identify two primary causes for this trend: the
suboptimal negative log-likelihood (NLL) optimization objective and the
incremental data noise. To address these issues, we introduce DrICL, a novel
optimization method that enhances model performance through Differentiated
Learning and advantage-based Reweighting objectives. Globally, DrICL utilizes
differentiated learning to optimize the NLL objective, ensuring that many-shot
performance surpasses zero-shot levels. Locally, it dynamically adjusts the
weighting of many-shot demonstrations by leveraging cumulative advantages
inspired by reinforcement learning, thereby improving generalization. This
approach allows the model to handle varying numbers of shots effectively,
mitigating the impact of noisy data. Recognizing the lack of multi-task
datasets with diverse many-shot distributions, we develop the Many-Shot ICL
Benchmark (ICL-50)-a large-scale benchmark of 50 tasks that cover shot numbers
from 1 to 350 within sequences of up to 8,000 tokens-for fine-tuning purposes.
ICL-50 facilitates the evaluation of many-shot ICL strategies across seven
prominent NLP tasks and 50 distinct datasets. Experimental results demonstrate
that LLMs enhanced with DrICL achieve significant improvements in many-shot
setups across various tasks, including both in-domain and out-of-domain
scenarios. We release the code and benchmark dataset hoping to facilitate
further research in many-shot ICL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.03486v1' target='_blank'>Align-Pro: A Principled Approach to Prompt Optimization for LLM
  Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Prashant Trivedi, Souradip Chakraborty, Avinash Reddy, Vaneet Aggarwal, Amrit Singh Bedi, George K. Atia</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-07 03:14:39</h6>
<p class='card-text'>The alignment of large language models (LLMs) with human values is critical
as these models become increasingly integrated into various societal and
decision-making processes. Traditional methods, such as reinforcement learning
from human feedback (RLHF), achieve alignment by fine-tuning model parameters,
but these approaches are often computationally expensive and impractical when
models are frozen or inaccessible for parameter modification. In contrast,
prompt optimization is a viable alternative to RLHF for LLM alignment. While
the existing literature has shown empirical promise of prompt optimization, its
theoretical underpinning remains under-explored. We address this gap by
formulating prompt optimization as an optimization problem and try to provide
theoretical insights into the optimality of such a framework. To analyze the
performance of the prompt optimization, we study theoretical suboptimality
bounds and provide insights in terms of how prompt optimization depends upon
the given prompter and target model. We also provide empirical validation
through experiments on various datasets, demonstrating that prompt optimization
can effectively align LLMs, even when parameter fine-tuning is not feasible.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.02997v1' target='_blank'>CALM: Curiosity-Driven Auditing for Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiang Zheng, Longxiang Wang, Yi Liu, Xingjun Ma, Chao Shen, Cong Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-06 13:14:34</h6>
<p class='card-text'>Auditing Large Language Models (LLMs) is a crucial and challenging task. In
this study, we focus on auditing black-box LLMs without access to their
parameters, only to the provided service. We treat this type of auditing as a
black-box optimization problem where the goal is to automatically uncover
input-output pairs of the target LLMs that exhibit illegal, immoral, or unsafe
behaviors. For instance, we may seek a non-toxic input that the target LLM
responds to with a toxic output or an input that induces the hallucinative
response from the target LLM containing politically sensitive individuals. This
black-box optimization is challenging due to the scarcity of feasible points,
the discrete nature of the prompt space, and the large search space. To address
these challenges, we propose Curiosity-Driven Auditing for Large Language
Models (CALM), which uses intrinsically motivated reinforcement learning to
finetune an LLM as the auditor agent to uncover potential harmful and biased
input-output pairs of the target LLM. CALM successfully identifies derogatory
completions involving celebrities and uncovers inputs that elicit specific
names under the black-box setting. This work offers a promising direction for
auditing black-box LLMs. Our code is available at
https://github.com/x-zheng16/CALM.git.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.01830v1' target='_blank'>Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large
  Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yanjiang Liu, Shuhen Zhou, Yaojie Lu, Huijia Zhu, Weiqiang Wang, Hongyu Lin, Ben He, Xianpei Han, Le Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-03 14:30:14</h6>
<p class='card-text'>Automated red-teaming has become a crucial approach for uncovering
vulnerabilities in large language models (LLMs). However, most existing methods
focus on isolated safety flaws, limiting their ability to adapt to dynamic
defenses and uncover complex vulnerabilities efficiently. To address this
challenge, we propose Auto-RT, a reinforcement learning framework that
automatically explores and optimizes complex attack strategies to effectively
uncover security vulnerabilities through malicious queries. Specifically, we
introduce two key mechanisms to reduce exploration complexity and improve
strategy optimization: 1) Early-terminated Exploration, which accelerate
exploration by focusing on high-potential attack strategies; and 2) Progressive
Reward Tracking algorithm with intermediate downgrade models, which dynamically
refine the search trajectory toward successful vulnerability exploitation.
Extensive experiments across diverse LLMs demonstrate that, by significantly
improving exploration efficiency and automatically optimizing attack
strategies, Auto-RT detects a boarder range of vulnerabilities, achieving a
faster detection speed and 16.63\% higher success rates compared to existing
methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.01332v1' target='_blank'>Decoding Knowledge in Large Language Models: A Framework for
  Categorization and Comprehension</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yanbo Fang, Ruixiang Tang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-02 16:34:10</h6>
<p class='card-text'>Understanding how large language models (LLMs) acquire, retain, and apply
knowledge remains an open challenge. This paper introduces a novel framework,
K-(CSA)^2, which categorizes LLM knowledge along two dimensions: correctness
and confidence. The framework defines six categories of knowledge, ranging from
highly confident correctness to confidently held misconceptions, enabling a
nuanced evaluation of model comprehension beyond binary accuracy. Using this
framework, we demonstrate how techniques like chain-of-thought prompting and
reinforcement learning with human feedback fundamentally alter the knowledge
structures of internal (pre-trained) and external (context-dependent) knowledge
in LLMs. CoT particularly enhances base model performance and shows synergistic
benefits when applied to aligned LLMs. Moreover, our layer-wise analysis
reveals that higher layers in LLMs encode more high-confidence knowledge, while
low-confidence knowledge tends to emerge in middle-to-lower layers.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.01141v1' target='_blank'>Embodied AI-Enhanced Vehicular Networks: An Integrated Large Language
  Models and Reinforcement Learning Method</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ruichen Zhang, Changyuan Zhao, Hongyang Du, Dusit Niyato, Jiacheng Wang, Suttinee Sawadsitang, Xuemin Shen, Dong In Kim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-02 08:48:54</h6>
<p class='card-text'>This paper investigates adaptive transmission strategies in embodied
AI-enhanced vehicular networks by integrating large language models (LLMs) for
semantic information extraction and deep reinforcement learning (DRL) for
decision-making. The proposed framework aims to optimize both data transmission
efficiency and decision accuracy by formulating an optimization problem that
incorporates the Weber-Fechner law, serving as a metric for balancing bandwidth
utilization and quality of experience (QoE). Specifically, we employ the large
language and vision assistant (LLAVA) model to extract critical semantic
information from raw image data captured by embodied AI agents (i.e.,
vehicles), reducing transmission data size by approximately more than 90\%
while retaining essential content for vehicular communication and
decision-making. In the dynamic vehicular environment, we employ a generalized
advantage estimation-based proximal policy optimization (GAE-PPO) method to
stabilize decision-making under uncertainty. Simulation results show that
attention maps from LLAVA highlight the model's focus on relevant image
regions, enhancing semantic representation accuracy. Additionally, our proposed
transmission strategy improves QoE by up to 36\% compared to DDPG and
accelerates convergence by reducing required steps by up to 47\% compared to
pure PPO. Further analysis indicates that adapting semantic symbol length
provides an effective trade-off between transmission quality and bandwidth,
achieving up to a 61.4\% improvement in QoE when scaling from 4 to 8 vehicles.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.00581v2' target='_blank'>Are the Values of LLMs Structurally Aligned with Humans? A Causal
  Perspective</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yipeng Kang, Junqi Wang, Yexin Li, Mengmeng Wang, Wenming Tu, Quansen Wang, Hengli Li, Tingjun Wu, Xue Feng, Fangwei Zhong, Zilong Zheng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-31 18:12:05</h6>
<p class='card-text'>As large language models (LLMs) become increasingly integrated into critical
applications, aligning their behavior with human values presents significant
challenges. Current methods, such as Reinforcement Learning from Human Feedback
(RLHF), typically focus on a limited set of coarse-grained values and are
resource-intensive. Moreover, the correlations between these values remain
implicit, leading to unclear explanations for value-steering outcomes. Our work
argues that a latent causal value graph underlies the value dimensions of LLMs
and that, despite alignment training, this structure remains significantly
different from human value systems. We leverage these causal value graphs to
guide two lightweight value-steering methods: role-based prompting and sparse
autoencoder (SAE) steering, effectively mitigating unexpected side effects.
Furthermore, SAE provides a more fine-grained approach to value steering.
Experiments on Gemma-2B-IT and Llama3-8B-IT demonstrate the effectiveness and
controllability of our methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.00226v1' target='_blank'>Generative Emergent Communication: Large Language Model is a Collective
  World Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tadahiro Taniguchi, Ryo Ueda, Tomoaki Nakamura, Masahiro Suzuki, Akira Taniguchi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-31 02:23:10</h6>
<p class='card-text'>This study proposes a unifying theoretical framework called generative
emergent communication (generative EmCom) that bridges emergent communication,
world models, and large language models (LLMs) through the lens of collective
predictive coding (CPC). The proposed framework formalizes the emergence of
language and symbol systems through decentralized Bayesian inference across
multiple agents, extending beyond conventional discriminative model-based
approaches to emergent communication. This study makes the following two key
contributions: First, we propose generative EmCom as a novel framework for
understanding emergent communication, demonstrating how communication emergence
in multi-agent reinforcement learning (MARL) can be derived from control as
inference while clarifying its relationship to conventional discriminative
approaches. Second, we propose a mathematical formulation showing the
interpretation of LLMs as collective world models that integrate multiple
agents' experiences through CPC. The framework provides a unified theoretical
foundation for understanding how shared symbol systems emerge through
collective predictive coding processes, bridging individual cognitive
development and societal language evolution. Through mathematical formulations
and discussion on prior works, we demonstrate how this framework explains
fundamental aspects of language emergence and offers practical insights for
understanding LLMs and developing sophisticated AI systems for improving
human-AI interaction and multi-agent systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.20367v2' target='_blank'>Enhancing Code LLMs with Reinforcement Learning in Code Generation: A
  Survey</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Junqiao Wang, Zeng Zhang, Yangfan He, Yuyang Song, Tianyu Shi, Yuchen Li, Hengyuan Xu, Kunyu Wu, Guangwu Qian, Qiuwu Chen, Lewei He</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-29 06:15:41</h6>
<p class='card-text'>With the rapid evolution of large language models (LLM), reinforcement
learning (RL) has emerged as a pivotal technique for code generation and
optimization in various domains. This paper presents a systematic survey of the
application of RL in code optimization and generation, highlighting its role in
enhancing compiler optimization, resource allocation, and the development of
frameworks and tools. Subsequent sections first delve into the intricate
processes of compiler optimization, where RL algorithms are leveraged to
improve efficiency and resource utilization. The discussion then progresses to
the function of RL in resource allocation, emphasizing register allocation and
system optimization. We also explore the burgeoning role of frameworks and
tools in code generation, examining how RL can be integrated to bolster their
capabilities. This survey aims to serve as a comprehensive resource for
researchers and practitioners interested in harnessing the power of RL to
advance code generation and optimization techniques.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.18947v2' target='_blank'>MedHallBench: A New Benchmark for Assessing Hallucination in Medical
  Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kaiwen Zuo, Yirui Jiang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-25 16:51:29</h6>
<p class='card-text'>Medical Large Language Models (MLLMs) have demonstrated potential in
healthcare applications, yet their propensity for hallucinations -- generating
medically implausible or inaccurate information -- presents substantial risks
to patient care. This paper introduces MedHallBench, a comprehensive benchmark
framework for evaluating and mitigating hallucinations in MLLMs. Our
methodology integrates expert-validated medical case scenarios with established
medical databases to create a robust evaluation dataset. The framework employs
a sophisticated measurement system that combines automated ACHMI (Automatic
Caption Hallucination Measurement in Medical Imaging) scoring with rigorous
clinical expert evaluations and utilizes reinforcement learning methods to
achieve automatic annotation. Through an optimized reinforcement learning from
human feedback (RLHF) training pipeline specifically designed for medical
applications, MedHallBench enables thorough evaluation of MLLMs across diverse
clinical contexts while maintaining stringent accuracy standards. We conducted
comparative experiments involving various models, utilizing the benchmark to
establish a baseline for widely adopted large language models (LLMs). Our
findings indicate that ACHMI provides a more nuanced understanding of the
effects of hallucinations compared to traditional metrics, thereby highlighting
its advantages in hallucination assessment. This research establishes a
foundational framework for enhancing MLLMs' reliability in healthcare settings
and presents actionable strategies for addressing the critical challenge of AI
hallucinations in medical applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.18925v1' target='_blank'>HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, Benyou Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-25 15:12:34</h6>
<p class='card-text'>The breakthrough of OpenAI o1 highlights the potential of enhancing reasoning
to improve LLM. Yet, most research in reasoning has focused on mathematical
tasks, leaving domains like medicine underexplored. The medical domain, though
distinct from mathematics, also demands robust reasoning to provide reliable
answers, given the high standards of healthcare. However, verifying medical
reasoning is challenging, unlike those in mathematics. To address this, we
propose verifiable medical problems with a medical verifier to check the
correctness of model outputs. This verifiable nature enables advancements in
medical reasoning through a two-stage approach: (1) using the verifier to guide
the search for a complex reasoning trajectory for fine-tuning LLMs, (2)
applying reinforcement learning (RL) with verifier-based rewards to enhance
complex reasoning further. Finally, we introduce HuatuoGPT-o1, a medical LLM
capable of complex reasoning, which outperforms general and medical-specific
baselines using only 40K verifiable problems. Experiments show complex
reasoning improves medical problem-solving and benefits more from RL. We hope
our approach inspires advancements in reasoning across medical and other
specialized domains.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.00039v1' target='_blank'>Speech Recognition With LLMs Adapted to Disordered Speech Using
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chirag Nagpal, Subhashini Venugopalan, Jimmy Tobin, Marilyn Ladewig, Katherine Heller, Katrin Tomanek</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-25 00:16:22</h6>
<p class='card-text'>We introduce a large language model (LLM) capable of processing speech inputs
and show that tuning it further with reinforcement learning on human preference
(RLHF) enables it to adapt better to disordered speech than traditional
fine-tuning. Our method replaces low-frequency text tokens in an LLM's
vocabulary with audio tokens and enables the model to recognize speech by
fine-tuning it on speech with transcripts. We then use RL with rewards based on
syntactic and semantic accuracy measures generalizing the LLM further to
recognize disordered speech. While the resulting LLM does not outperform
existing systems for speech recognition, we find that tuning with reinforcement
learning using custom rewards leads to substantially better performance than
supervised fine-tuning of the language model, specifically when adapting to
speech in a different setting. This presents a compelling alternative tuning
strategy for speech recognition using large language models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.18693v1' target='_blank'>Diverse and Effective Red Teaming with Auto-generated Rewards and
  Multi-step Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alex Beutel, Kai Xiao, Johannes Heidecke, Lilian Weng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-24 22:38:46</h6>
<p class='card-text'>Automated red teaming can discover rare model failures and generate
challenging examples that can be used for training or evaluation. However, a
core challenge in automated red teaming is ensuring that the attacks are both
diverse and effective. Prior methods typically succeed in optimizing either for
diversity or for effectiveness, but rarely both. In this paper, we provide
methods that enable automated red teaming to generate a large number of diverse
and successful attacks.
  Our approach decomposes the task into two steps: (1) automated methods for
generating diverse attack goals and (2) generating effective attacks for those
goals. While we provide multiple straightforward methods for generating diverse
goals, our key contributions are to train an RL attacker that both follows
those goals and generates diverse attacks for those goals. First, we
demonstrate that it is easy to use a large language model (LLM) to generate
diverse attacker goals with per-goal prompts and rewards, including rule-based
rewards (RBRs) to grade whether the attacks are successful for the particular
goal. Second, we demonstrate how training the attacker model with multi-step
RL, where the model is rewarded for generating attacks that are different from
past attempts further increases diversity while remaining effective. We use our
approach to generate both prompt injection attacks and prompts that elicit
unsafe responses. In both cases, we find that our approach is able to generate
highly-effective and considerably more diverse attacks than past general
red-teaming approaches.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.18511v1' target='_blank'>Large Language Model guided Deep Reinforcement Learning for Decision
  Making in Autonomous Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hao Pang, Zhenpo Wang, Guoqiang Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-24 15:50:10</h6>
<p class='card-text'>Deep reinforcement learning (DRL) shows promising potential for autonomous
driving decision-making. However, DRL demands extensive computational resources
to achieve a qualified policy in complex driving scenarios due to its low
learning efficiency. Moreover, leveraging expert guidance from human to enhance
DRL performance incurs prohibitively high labor costs, which limits its
practical application. In this study, we propose a novel large language model
(LLM) guided deep reinforcement learning (LGDRL) framework for addressing the
decision-making problem of autonomous vehicles. Within this framework, an
LLM-based driving expert is integrated into the DRL to provide intelligent
guidance for the learning process of DRL. Subsequently, in order to efficiently
utilize the guidance of the LLM expert to enhance the performance of DRL
decision-making policies, the learning and interaction process of DRL is
enhanced through an innovative expert policy constrained algorithm and a novel
LLM-intervened interaction mechanism. Experimental results demonstrate that our
method not only achieves superior driving performance with a 90\% task success
rate but also significantly improves the learning efficiency and expert
guidance utilization efficiency compared to state-of-the-art baseline
algorithms. Moreover, the proposed method enables the DRL agent to maintain
consistent and reliable performance in the absence of LLM expert guidance. The
code and supplementary videos are available at
https://bitmobility.github.io/LGDRL/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.18279v1' target='_blank'>Improving Multi-Step Reasoning Abilities of Large Language Models with
  Direct Advantage Policy Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiacai Liu, Chaojie Wang, Chris Yuhao Liu, Liang Zeng, Rui Yan, Yiwen Sun, Yang Liu, Yahui Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-24 08:39:35</h6>
<p class='card-text'>The role of reinforcement learning (RL) in enhancing the reasoning of large
language models (LLMs) is becoming increasingly significant. Despite the
success of RL in many scenarios, there are still many challenges in improving
the reasoning of LLMs. One challenge is the sparse reward, which makes
optimization difficult for RL and necessitates a large amount of data samples.
Another challenge stems from the inherent instability of RL, particularly when
using Actor-Critic (AC) methods to derive optimal policies, which often leads
to unstable training processes. To address these issues, we introduce Direct
Advantage Policy Optimization (DAPO), an novel step-level offline RL algorithm.
Unlike standard alignment that rely solely outcome rewards to optimize policies
(such as DPO), DAPO employs a critic function to predict the reasoning accuracy
at each step, thereby generating dense signals to refine the generation
strategy. Additionally, the Actor and Critic components in DAPO are trained
independently, avoiding the co-training instability observed in standard AC
algorithms like PPO. We train DAPO on mathematical and code query datasets and
then evaluate its performance on multiple benchmarks. Our results show that
DAPO can effectively enhance the mathematical and code capabilities on both SFT
models and RL models, demonstrating the effectiveness of DAPO.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.18171v2' target='_blank'>Token Highlighter: Inspecting and Mitigating Jailbreak Prompts for Large
  Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-24 05:10:02</h6>
<p class='card-text'>Large Language Models (LLMs) are increasingly being integrated into services
such as ChatGPT to provide responses to user queries. To mitigate potential
harm and prevent misuse, there have been concerted efforts to align the LLMs
with human values and legal compliance by incorporating various techniques,
such as Reinforcement Learning from Human Feedback (RLHF), into the training of
the LLMs. However, recent research has exposed that even aligned LLMs are
susceptible to adversarial manipulations known as Jailbreak Attacks. To address
this challenge, this paper proposes a method called Token Highlighter to
inspect and mitigate the potential jailbreak threats in the user query. Token
Highlighter introduced a concept called Affirmation Loss to measure the LLM's
willingness to answer the user query. It then uses the gradient of Affirmation
Loss for each token in the user query to locate the jailbreak-critical tokens.
Further, Token Highlighter exploits our proposed Soft Removal technique to
mitigate the jailbreak effects of critical tokens via shrinking their token
embeddings. Experimental results on two aligned LLMs (LLaMA-2 and Vicuna-V1.5)
demonstrate that the proposed method can effectively defend against a variety
of Jailbreak Attacks while maintaining competent performance on benign
questions of the AlpacaEval benchmark. In addition, Token Highlighter is a
cost-effective and interpretable defense because it only needs to query the
protected LLM once to compute the Affirmation Loss and can highlight the
critical tokens upon refusal.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.17397v1' target='_blank'>Towards Intrinsic Self-Correction Enhancement in Monte Carlo Tree Search
  Boosted Reasoning via Iterative Preference Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Huchen Jiang, Yangyang Ma, Chaofan Ding, Kexin Luan, Xinhan Di</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-23 08:51:48</h6>
<p class='card-text'>With current state-of-the-art approaches aimed at enhancing the reasoning
capabilities of Large Language Models(LLMs) through iterative preference
learning inspired by AlphaZero, we propose to further enhance the step-wise
reasoning capabilities through intrinsic self-correction to some extent. Our
work leverages step-wise preference learning to enhance self-verification via
reinforcement learning. We initially conduct our work through a two-stage
training procedure. At the first stage, the self-correction reasoning ability
of an LLM is enhanced through its own predictions, relying entirely on
self-generated data within the intrinsic self-correction to some extent. At the
second stage, the baseline step-wise preference learning is leveraged via the
application of the enhanced self-correct policy achieved at the first stage. In
the evaluation of arithmetic reasoning tasks, our approach outperforms
OpenMath2-Llama3.1-8B, dart-math-mistral-7b-uniform on MATH with increases in
accuracy to 71.34%(+4.18%) and 48.06%(+4.94%) and LLama-3.1-8B-Instruct,
Mistral-7B-Instruct-v0.1 on GSM8K with increases in accuracy to 86.76%(+2.00%)
and 38.06%(+2.28%).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.16984v1' target='_blank'>LLM-Powered User Simulator for Recommender System</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zijian Zhang, Shuchang Liu, Ziru Liu, Rui Zhong, Qingpeng Cai, Xiangyu Zhao, Chunxu Zhang, Qidong Liu, Peng Jiang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-22 12:00:04</h6>
<p class='card-text'>User simulators can rapidly generate a large volume of timely user behavior
data, providing a testing platform for reinforcement learning-based recommender
systems, thus accelerating their iteration and optimization. However, prevalent
user simulators generally suffer from significant limitations, including the
opacity of user preference modeling and the incapability of evaluating
simulation accuracy. In this paper, we introduce an LLM-powered user simulator
to simulate user engagement with items in an explicit manner, thereby enhancing
the efficiency and effectiveness of reinforcement learning-based recommender
systems training. Specifically, we identify the explicit logic of user
preferences, leverage LLMs to analyze item characteristics and distill user
sentiments, and design a logical model to imitate real human engagement. By
integrating a statistical model, we further enhance the reliability of the
simulation, proposing an ensemble model that synergizes logical and statistical
insights for user interaction simulations. Capitalizing on the extensive
knowledge and semantic generation capabilities of LLMs, our user simulator
faithfully emulates user behaviors and preferences, yielding high-fidelity
training data that enrich the training of recommendation algorithms. We
establish quantifying and qualifying experiments on five datasets to validate
the simulator's effectiveness and stability across various recommendation
scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.16974v1' target='_blank'>Cannot or Should Not? Automatic Analysis of Refusal Composition in
  IFT/RLHF Datasets and Refusal Behavior of Black-Box LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alexander von Recum, Christoph Schnabl, Gabor Hollbeck, Silas Alberti, Philip Blinde, Marvin von Hagen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-22 11:16:53</h6>
<p class='card-text'>Refusals - instances where large language models (LLMs) decline or fail to
fully execute user instructions - are crucial for both AI safety and AI
capabilities and the reduction of hallucinations in particular. These behaviors
are learned during post-training, especially in instruction fine-tuning (IFT)
and reinforcement learning from human feedback (RLHF). However, existing
taxonomies and evaluation datasets for refusals are inadequate, often focusing
solely on should-not-related (instead of cannot-related) categories, and
lacking tools for auditing refusal content in black-box LLM outputs.
  We present a comprehensive framework for classifying LLM refusals: (a) a
taxonomy of 16 refusal categories, (b) a human-annotated dataset of over 8,600
instances from publicly available IFT and RLHF datasets, (c) a synthetic
dataset with 8,000 examples for each refusal category, and (d) classifiers
trained for refusal classification.
  Our work enables precise auditing of refusal behaviors in black-box LLMs and
automatic analyses of refusal patterns in large IFT and RLHF datasets. This
facilitates the strategic adjustment of LLM refusals, contributing to the
development of more safe and reliable LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.16878v1' target='_blank'>Online Preference-based Reinforcement Learning with Self-augmented
  Feedback from Large Language Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Songjun Tu, Jingbo Sun, Qichao Zhang, Xiangyuan Lan, Dongbin Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-22 06:15:25</h6>
<p class='card-text'>Preference-based reinforcement learning (PbRL) provides a powerful paradigm
to avoid meticulous reward engineering by learning rewards based on human
preferences. However, real-time human feedback is hard to obtain in online
tasks. Most work suppose there is a "scripted teacher" that utilizes privileged
predefined reward to provide preference feedback. In this paper, we propose a
RL Self-augmented Large Language Model Feedback (RL-SaLLM-F) technique that
does not rely on privileged information for online PbRL. RL-SaLLM-F leverages
the reflective and discriminative capabilities of LLM to generate
self-augmented trajectories and provide preference labels for reward learning.
First, we identify an failure issue in LLM-based preference discrimination,
specifically "query ambiguity", in online PbRL. Then LLM is employed to provide
preference labels and generate self-augmented imagined trajectories that better
achieve the task goal, thereby enhancing the quality and efficiency of
feedback. Additionally, a double-check mechanism is introduced to mitigate
randomness in the preference labels, improving the reliability of LLM feedback.
The experiment across multiple tasks in the MetaWorld benchmark demonstrates
the specific contributions of each proposed module in RL-SaLLM-F, and shows
that self-augmented LLM feedback can effectively replace the impractical
"scripted teacher" feedback. In summary, RL-SaLLM-F introduces a new direction
of feedback acquisition in online PbRL that does not rely on any online
privileged information, offering an efficient and lightweight solution with
LLM-driven feedback.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.16834v2' target='_blank'>Online Learning from Strategic Human Feedback in LLM Fine-Tuning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shugang Hao, Lingjie Duan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-22 02:43:07</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) has become an essential
step in fine-tuning large language models (LLMs) to align them with human
preferences. However, human labelers are selfish and have diverse preferences.
They may strategically misreport their online feedback to influence the
system's aggregation towards their own preferences. Current practice simply
averages labelers' feedback per time and fails to identify the most accurate
human labeler, leading to linear regret $\mathcal{O}(T)$ for $T$ time slots. To
our best knowledge, we are the first to study online learning mechanisms
against strategic human labelers in the LLM fine-tuning process. We formulate a
new dynamic Bayesian game and dynamically adjust human labelers' weights in the
preference aggregation, ensuring their truthful feedback and sublinear regret
$\mathcal{O}(T^{1/2})$. Simulation results demonstrate our mechanism's great
advantages over the existing benchmark schemes.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.16543v2' target='_blank'>Mathematics and Machine Creativity: A Survey on Bridging Mathematics
  with AI</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shizhe Liang, Wei Zhang, Tianyang Zhong, Tianming Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-21 08:58:36</h6>
<p class='card-text'>This paper presents a comprehensive overview on the applications of
artificial intelligence (AI) in mathematical research, highlighting the
transformative role AI has begun to play in this domain. Traditionally, AI
advancements have heavily relied on theoretical foundations provided by
mathematics and statistics. However, recent developments in AI, particularly in
reinforcement learning (RL) and large language models (LLMs), have demonstrated
the potential for AI to contribute back to mathematics by offering flexible
algorithmic frameworks and powerful inductive reasoning capabilities that
support various aspects of mathematical research. This survey aims to establish
a bridge between AI and mathematics, providing insights into the mutual
benefits and fostering deeper interdisciplinary understanding.
  In particular, we argue that while current AI and LLMs may struggle with
complex deductive reasoning, their "inherent creativity", the ability to
generate outputs at high throughput based on recognition of shallow patterns,
holds significant potential to support and inspire mathematical research. This
creative capability, often overlooked, could be the key to unlocking new
perspectives and methodologies in mathematics. Furthermore, we address the lack
of cross-disciplinary communication: mathematicians may not fully comprehend
the latest advances in AI, while AI researchers frequently prioritize benchmark
performance over real-world applications in frontier mathematical research.
This paper seeks to close that gap, offering a detailed exploration of AI
fundamentals, its strengths, and its emerging applications in the mathematical
sciences.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.16325v1' target='_blank'>Towards Safe and Honest AI Agents with Neural Self-Other Overlap</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Marc Carauleanu, Michael Vaiana, Judd Rosenblatt, Cameron Berg, Diogo Schwerz de Lucena</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-20 20:23:52</h6>
<p class='card-text'>As AI systems increasingly make critical decisions, deceptive AI poses a
significant challenge to trust and safety. We present Self-Other Overlap (SOO)
fine-tuning, a promising approach in AI Safety that could substantially improve
our ability to build honest artificial intelligence. Inspired by cognitive
neuroscience research on empathy, SOO aims to align how AI models represent
themselves and others. Our experiments on LLMs with 7B, 27B, and 78B parameters
demonstrate SOO's efficacy: deceptive responses of Mistral-7B-Instruct-v0.2
dropped from 73.6% to 17.2% with no observed reduction in general task
performance, while in Gemma-2-27b-it and CalmeRys-78B-Orpo-v0.1 deceptive
responses were reduced from 100% to 9.3% and 2.7%, respectively, with a small
impact on capabilities. In reinforcement learning scenarios, SOO-trained agents
showed significantly reduced deceptive behavior. SOO's focus on contrastive
self and other-referencing observations offers strong potential for
generalization across AI architectures. While current applications focus on
language models and simple RL environments, SOO could pave the way for more
trustworthy AI in broader domains. Ethical implications and long-term effects
warrant further investigation, but SOO represents a significant step forward in
AI safety research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.16145v2' target='_blank'>Offline Reinforcement Learning for LLM Multi-Step Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Huaijie Wang, Shibo Hao, Hanze Dong, Shenao Zhang, Yilin Bao, Ziran Yang, Yi Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-20 18:49:45</h6>
<p class='card-text'>Improving the multi-step reasoning ability of large language models (LLMs)
with offline reinforcement learning (RL) is essential for quickly adapting them
to complex tasks. While Direct Preference Optimization (DPO) has shown promise
in aligning LLMs with human preferences, it is less suitable for multi-step
reasoning tasks because (1) DPO relies on paired preference data, which is not
readily available for multi-step reasoning tasks, and (2) it treats all tokens
uniformly, making it ineffective for credit assignment in multi-step reasoning
tasks, which often come with sparse reward. In this work, we propose OREO
(Offline Reasoning Optimization), an offline RL method for enhancing LLM
multi-step reasoning. Building on insights from previous works of maximum
entropy reinforcement learning, it jointly learns a policy model and value
function by optimizing the soft Bellman Equation. We show in principle that it
reduces the need to collect pairwise data and enables better credit assignment.
Empirically, OREO surpasses existing offline learning methods on multi-step
reasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH) and
embodied agent control (ALFWorld). The approach can be extended to a
multi-iteration framework when additional resources are available. Furthermore,
the learned value function can be leveraged to guide the tree search for free,
which can further boost performance during test time.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.15957v1' target='_blank'>From General to Specific: Tailoring Large Language Models for
  Personalized Healthcare</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ruize Shi, Hong Huang, Wei Zhou, Kehan Yin, Kai Zhao, Yun Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-20 14:51:12</h6>
<p class='card-text'>The rapid development of large language models (LLMs) has transformed many
industries, including healthcare. However, previous medical LLMs have largely
focused on leveraging general medical knowledge to provide responses, without
accounting for patient variability and lacking true personalization at the
individual level. To address this, we propose a novel method called
personalized medical language model (PMLM), which explores and optimizes
personalized LLMs through recommendation systems and reinforcement learning
(RL). Specifically, by utilizing self-informed and peer-informed
personalization, PMLM captures changes in behaviors and preferences to design
initial personalized prompts tailored to individual needs. We further refine
these initial personalized prompts through RL, ultimately enhancing the
precision of LLM guidance. Notably, the personalized prompt are hard prompt,
which grants PMLM high adaptability and reusability, allowing it to directly
leverage high-quality proprietary LLMs. We evaluate PMLM using real-world
obstetrics and gynecology data, and the experimental results demonstrate that
PMLM achieves personalized responses, and it provides more refined and
individualized services, offering a potential way for personalized medical
LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.15115v2' target='_blank'>Qwen2.5 Technical Report</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zihan Qiu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-19 17:56:09</h6>
<p class='card-text'>In this report, we introduce Qwen2.5, a comprehensive series of large
language models (LLMs) designed to meet diverse needs. Compared to previous
iterations, Qwen 2.5 has been significantly improved during both the
pre-training and post-training stages. In terms of pre-training, we have scaled
the high-quality pre-training datasets from the previous 7 trillion tokens to
18 trillion tokens. This provides a strong foundation for common sense, expert
knowledge, and reasoning capabilities. In terms of post-training, we implement
intricate supervised finetuning with over 1 million samples, as well as
multistage reinforcement learning. Post-training techniques enhance human
preference, and notably improve long text generation, structural data analysis,
and instruction following. To handle diverse and varied use cases effectively,
we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base
and instruction-tuned models, with quantized versions available. In addition,
for hosted solutions, the proprietary models currently include two
mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both
available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier
performance on a wide range of benchmarks evaluating language understanding,
reasoning, mathematics, coding, human preference alignment, etc. Specifically,
the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and
proprietary models and demonstrates competitive performance to the
state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5
times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness
while performing competitively against GPT-4o-mini and GPT-4o respectively.
Additionally, as the foundation, Qwen2.5 models have been instrumental in
training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and
multimodal models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.14626v1' target='_blank'>Learning to Generate Research Idea with Dynamic Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ruochen Li, Liqiang Jing, Chi Han, Jiawei Zhou, Xinya Du</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-19 08:28:18</h6>
<p class='card-text'>The rapid advancements in large language models (LLMs) have demonstrated
their potential to accelerate scientific discovery, particularly in automating
the process of research ideation. LLM-based systems have shown promise in
generating hypotheses and research ideas. However, current approaches
predominantly rely on prompting-based pre-trained models, limiting their
ability to optimize generated content effectively. Moreover, they also lack the
capability to deal with the complex interdependence and inherent restrictions
among novelty, feasibility, and effectiveness, which remains challenging due to
the inherent trade-offs among these dimensions, such as the
innovation-feasibility conflict. To address these limitations, we for the first
time propose fine-tuning LLMs to be better idea proposers and introduce a novel
framework that employs a two-stage approach combining Supervised Fine-Tuning
(SFT) and controllable Reinforcement Learning (RL). In the SFT stage, the model
learns foundational patterns from pairs of research papers and follow-up ideas.
In the RL stage, multi-dimensional reward modeling, guided by fine-grained
feedback, evaluates and optimizes the generated ideas across key metrics.
Dimensional controllers enable dynamic adjustment of generation, while a
sentence-level decoder ensures context-aware emphasis during inference. Our
framework provides a balanced approach to research ideation, achieving
high-quality outcomes by dynamically navigating the trade-offs among novelty,
feasibility, and effectiveness.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.14584v1' target='_blank'>Simulation-Free Hierarchical Latent Policy Planning for Proactive
  Dialogues</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tao He, Lizi Liao, Yixin Cao, Yuanxing Liu, Yiheng Sun, Zerui Chen, Ming Liu, Bing Qin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-19 07:06:01</h6>
<p class='card-text'>Recent advancements in proactive dialogues have garnered significant
attention, particularly for more complex objectives (e.g. emotion support and
persuasion). Unlike traditional task-oriented dialogues, proactive dialogues
demand advanced policy planning and adaptability, requiring rich scenarios and
comprehensive policy repositories to develop such systems. However, existing
approaches tend to rely on Large Language Models (LLMs) for user simulation and
online learning, leading to biases that diverge from realistic scenarios and
result in suboptimal efficiency. Moreover, these methods depend on manually
defined, context-independent, coarse-grained policies, which not only incur
high expert costs but also raise concerns regarding their completeness. In our
work, we highlight the potential for automatically discovering policies
directly from raw, real-world dialogue records. To this end, we introduce a
novel dialogue policy planning framework, LDPP. It fully automates the process
from mining policies in dialogue records to learning policy planning.
Specifically, we employ a variant of the Variational Autoencoder to discover
fine-grained policies represented as latent vectors. After automatically
annotating the data with these latent policy labels, we propose an Offline
Hierarchical Reinforcement Learning (RL) algorithm in the latent space to
develop effective policy planning capabilities. Our experiments demonstrate
that LDPP outperforms existing methods on two proactive scenarios, even
surpassing ChatGPT with only a 1.8-billion-parameter LLM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.15287v1' target='_blank'>Inference-Aware Fine-Tuning for Best-of-N Sampling in Large Language
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yinlam Chow, Guy Tennenholtz, Izzeddin Gur, Vincent Zhuang, Bo Dai, Sridhar Thiagarajan, Craig Boutilier, Rishabh Agarwal, Aviral Kumar, Aleksandra Faust</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-18 20:43:47</h6>
<p class='card-text'>Recent studies have indicated that effectively utilizing inference-time
compute is crucial for attaining better performance from large language models
(LLMs). In this work, we propose a novel inference-aware fine-tuning paradigm,
in which the model is fine-tuned in a manner that directly optimizes the
performance of the inference-time strategy. We study this paradigm using the
simple yet effective Best-of-N (BoN) inference strategy, in which a verifier
selects the best out of a set of LLM-generated responses. We devise the first
imitation learning and reinforcement learning~(RL) methods for BoN-aware
fine-tuning, overcoming the challenging, non-differentiable argmax operator
within BoN. We empirically demonstrate that our BoN-aware models implicitly
learn a meta-strategy that interleaves best responses with more diverse
responses that might be better suited to a test-time input -- a process
reminiscent of the exploration-exploitation trade-off in RL. Our experiments
demonstrate the effectiveness of BoN-aware fine-tuning in terms of improved
performance and inference-time compute. In particular, we show that our methods
improve the Bo32 performance of Gemma 2B on Hendrycks MATH from 26.8% to 30.8%,
and pass@32 from 60.0% to 67.0%, as well as the pass@16 on HumanEval from 61.6%
to 67.1%.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.14308v2' target='_blank'>Reinforcement Learning from Automatic Feedback for High-Quality Unit
  Test Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Benjamin Steenhoek, Michele Tufano, Neel Sundaresan, Alexey Svyatkovskiy</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-18 20:20:01</h6>
<p class='card-text'>Software testing is a crucial but time-consuming aspect of software
development, and recently, Large Language Models (LLMs) have gained popularity
for automated test case generation. However, because LLMs are trained on vast
amounts of open-source code, they often generate test cases that do not adhere
to best practices and may even contain test smells (anti-patterns). To address
this issue, we propose Reinforcement Learning from Static Quality Metrics
(RLSQM), wherein we utilize Reinforcement Learning to generate high-quality
unit tests based on static analysis-based quality metrics. First, we analyzed
LLM-generated tests and show that LLMs frequently do generate undesirable test
smells -- up to 37% of the time. Then, we implemented lightweight static
analysis-based reward model and trained LLMs using this reward model to
optimize for five code quality metrics. Our experimental results demonstrate
that the RL-optimized Codex model consistently generated higher-quality test
cases than the base LLM, improving quality metrics by up to 23%, and generated
nearly 100% syntactically-correct code. RLSQM also outperformed GPT-4 on all
code quality metrics, in spite of training a substantially cheaper Codex model.
We provide insights into how reliably utilize RL to improve test generation
quality and show that RLSQM is a significant step towards enhancing the overall
efficiency and reliability of automated software testing. Our data are
available at https://doi.org/10.6084/m9.figshare.25983166.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.14135v1' target='_blank'>Scaling of Search and Learning: A Roadmap to Reproduce o1 from
  Reinforcement Learning Perspective</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Bo Wang, Shimin Li, Yunhua Zhou, Qipeng Guo, Xuanjing Huang, Xipeng Qiu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-18 18:24:47</h6>
<p class='card-text'>OpenAI o1 represents a significant milestone in Artificial Inteiligence,
which achieves expert-level performances on many challanging tasks that require
strong reasoning ability.OpenAI has claimed that the main techinique behinds o1
is the reinforcement learining. Recent works use alternative approaches like
knowledge distillation to imitate o1's reasoning style, but their effectiveness
is limited by the capability ceiling of the teacher model. Therefore, this
paper analyzes the roadmap to achieving o1 from the perspective of
reinforcement learning, focusing on four key components: policy initialization,
reward design, search, and learning. Policy initialization enables models to
develop human-like reasoning behaviors, equipping them with the ability to
effectively explore solution spaces for complex problems. Reward design
provides dense and effective signals via reward shaping or reward modeling,
which is the guidance for both search and learning. Search plays a crucial role
in generating high-quality solutions during both training and testing phases,
which can produce better solutions with more computation. Learning utilizes the
data generated by search for improving policy, which can achieve the better
performance with more parameters and more searched data. Existing open-source
projects that attempt to reproduce o1 can be seem as a part or a variant of our
roadmap. Collectively, these components underscore how learning and search
drive o1's advancement, making meaningful contributions to the development of
LLM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.13795v1' target='_blank'>Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and
  Post-LN</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pengxiang Li, Lu Yin, Shiwei Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-18 12:39:53</h6>
<p class='card-text'>Large Language Models (LLMs) have achieved remarkable success, yet recent
findings reveal that their deeper layers often contribute minimally and can be
pruned without affecting overall performance. While some view this as an
opportunity for model compression, we identify it as a training shortfall
rooted in the widespread use of Pre-Layer Normalization (Pre-LN). We
demonstrate that Pre-LN, commonly employed in models like GPT and LLaMA, leads
to diminished gradient norms in its deeper layers, reducing their
effectiveness. In contrast, Post-Layer Normalization (Post-LN) preserves larger
gradient norms in deeper layers but suffers from vanishing gradients in earlier
layers. To address this, we introduce Mix-LN, a novel normalization technique
that combines the strengths of Pre-LN and Post-LN within the same model. Mix-LN
applies Post-LN to the earlier layers and Pre-LN to the deeper layers, ensuring
more uniform gradients across layers. This allows all parts of the
network--both shallow and deep layers--to contribute effectively to training.
Extensive experiments with various model sizes from 70M to 7B demonstrate that
Mix-LN consistently outperforms both Pre-LN and Post-LN, promoting more
balanced, healthier gradient norms throughout the network, and enhancing the
overall quality of LLM pre-training. Furthermore, we demonstrate that models
pre-trained with Mix-LN learn better compared to those using Pre-LN or Post-LN
during supervised fine-tuning (SFT) and reinforcement learning from human
feedback (RLHF), highlighting the critical importance of high-quality deep
layers. By effectively addressing the inefficiencies of deep layers in current
LLMs, Mix-LN unlocks their potential, enhancing model capacity without
increasing model size. Our code is available at
https://github.com/pixeli99/MixLN.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.13551v1' target='_blank'>Large Language Model Federated Learning with Blockchain and Unlearning
  for Cross-Organizational Collaboration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xuhan Zuo, Minghao Wang, Tianqing Zhu, Shui Yu, Wanlei Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-18 06:56:09</h6>
<p class='card-text'>Large language models (LLMs) have transformed the way computers understand
and process human language, but using them effectively across different
organizations remains still difficult. When organizations work together to
improve LLMs, they face several main challenges. First, organizations hesitate
to share their valuable data with others. Second, competition between
organizations creates trust problems during collaboration. Third, new privacy
laws require organizations to be able to delete specific data when requested,
which is especially difficult when multiple organizations are learning from
shared data. Traditional federated learning approaches do not address these
interconnected challenges, particularly in scenarios where participants cannot
fully trust each other or the central aggregator. To overcome these
limitations, we propose a hybrid blockchain-based federated learning framework
that uniquely combines public and private blockchain architectures with
multi-agent reinforcement learning. Our framework enables transparent sharing
of model update through the public blockchain while protecting sensitive
computations in private chains. Each organization operates as an intelligent
agent, using Q-learning to optimize its participation strategy and resource
allocation, thus aligning individual incentives with collective goals. Notably,
we introduce an efficient unlearning mechanism based on Low-Rank Adaptation
(LoRA) that enables selective removal of specific data contributions without
compromising the model's overall performance. Through extensive experimentation
on real-world datasets, we demonstrate that our framework effectively balances
privacy protection, trust establishment, and regulatory compliance while
maintaining high model performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.13492v1' target='_blank'>Efficient Language-instructed Skill Acquisition via Reward-Policy
  Co-Evolution</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Changxin Huang, Yanbin Chang, Junfan Lin, Junyang Liang, Runhao Zeng, Jianqiang Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-18 04:20:33</h6>
<p class='card-text'>The ability to autonomously explore and resolve tasks with minimal human
guidance is crucial for the self-development of embodied intelligence. Although
reinforcement learning methods can largely ease human effort, it's challenging
to design reward functions for real-world tasks, especially for
high-dimensional robotic control, due to complex relationships among joints and
tasks. Recent advancements large language models (LLMs) enable automatic reward
function design. However, approaches evaluate reward functions by re-training
policies from scratch placing an undue burden on the reward function, expecting
it to be effective throughout the whole policy improvement process. We argue
for a more practical strategy in robotic autonomy, focusing on refining
existing policies with policy-dependent reward functions rather than a
universal one. To this end, we propose a novel reward-policy co-evolution
framework where the reward function and the learned policy benefit from each
other's progressive on-the-fly improvements, resulting in more efficient and
higher-performing skill acquisition. Specifically, the reward evolution process
translates the robot's previous best reward function, descriptions of tasks and
environment into text inputs. These inputs are used to query LLMs to generate a
dynamic amount of reward function candidates, ensuring continuous improvement
at each round of evolution. For policy evolution, our method generates new
policy populations by hybridizing historically optimal and random policies.
Through an improved Bayesian optimization, our approach efficiently and
robustly identifies the most capable and plastic reward-policy combination,
which then proceeds to the next round of co-evolution. Despite using less data,
our approach demonstrates an average normalized improvement of 95.3% across
various high-dimensional robotic skill learning tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.15270v2' target='_blank'>Baichuan4-Finance Technical Report</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hanyu Zhang, Boyu Qiu, Yuhao Feng, Shuqi Li, Qian Ma, Xiyuan Zhang, Qiang Ju, Dong Yan, Jian Xie</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-17 08:05:32</h6>
<p class='card-text'>Large language models (LLMs) have demonstrated strong capabilities in
language understanding, generation, and reasoning, yet their potential in
finance remains underexplored due to the complexity and specialization of
financial knowledge. In this work, we report the development of the
Baichuan4-Finance series, including a comprehensive suite of foundational
Baichuan4-Finance-Base and an aligned language model Baichuan4-Finance, which
are built upon Baichuan4-Turbo base model and tailored for finance domain.
Firstly, we have dedicated significant effort to building a detailed pipeline
for improving data quality. Moreover, in the continual pre-training phase, we
propose a novel domain self-constraint training strategy, which enables
Baichuan4-Finance-Base to acquire financial knowledge without losing general
capabilities. After Supervised Fine-tuning and Reinforcement Learning from
Human Feedback and AI Feedback, the chat model Baichuan4-Finance is able to
tackle various financial certification questions and real-world scenario
applications. We evaluate Baichuan4-Finance on many widely used general
datasets and two holistic financial benchmarks. The evaluation results show
that Baichuan4-Finance-Base surpasses almost all competitive baselines on
financial tasks by significant margins without sacrificing performance on
general LLM benchmarks. At the same time, Baichuan4-Finance demonstrates even
more impressive performance on financial application scenarios, showcasing its
potential to foster community innovation in the financial LLM field.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.12464v1' target='_blank'>LLM is Knowledge Graph Reasoner: LLM's Intuition-aware Knowledge Graph
  Reasoning for Cold-start Sequential Recommendation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Keigo Sakurai, Ren Togo, Takahiro Ogawa, Miki Haseyama</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-17 01:52:15</h6>
<p class='card-text'>Knowledge Graphs (KGs) represent relationships between entities in a graph
structure and have been widely studied as promising tools for realizing
recommendations that consider the accurate content information of items.
However, traditional KG-based recommendation methods face fundamental
challenges: insufficient consideration of temporal information and poor
performance in cold-start scenarios. On the other hand, Large Language Models
(LLMs) can be considered databases with a wealth of knowledge learned from the
web data, and they have recently gained attention due to their potential
application as recommendation systems. Although approaches that treat LLMs as
recommendation systems can leverage LLMs' high recommendation literacy, their
input token limitations make it impractical to consider the entire
recommendation domain dataset and result in scalability issues. To address
these challenges, we propose a LLM's Intuition-aware Knowledge graph Reasoning
model (LIKR). Our main idea is to treat LLMs as reasoners that output intuitive
exploration strategies for KGs. To integrate the knowledge of LLMs and KGs, we
trained a recommendation agent through reinforcement learning using a reward
function that integrates different recommendation strategies, including LLM's
intuition and KG embeddings. By incorporating temporal awareness through prompt
engineering and generating textual representations of user preferences from
limited interactions, LIKR can improve recommendation performance in cold-start
scenarios. Furthermore, LIKR can avoid scalability issues by using KGs to
represent recommendation domain datasets and limiting the LLM's output to KG
exploration strategies. Experiments on real-world datasets demonstrate that our
model outperforms state-of-the-art recommendation methods in cold-start
sequential recommendation scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.11417v2' target='_blank'>RL-LLM-DT: An Automatic Decision Tree Generation Method Based on RL
  Evaluation and LLM Enhancement</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Junjie Lin, Jian Zhao, Lin Liu, Yue Deng, Youpeng Zhao, Lanxiao Huang, Xia Lin, Wengang Zhou, Houqiang Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-16 03:33:49</h6>
<p class='card-text'>Traditionally, AI development for two-player zero-sum games has relied on two
primary techniques: decision trees and reinforcement learning (RL). A common
approach involves using a fixed decision tree as one player's strategy while
training an RL agent as the opponent to identify vulnerabilities in the
decision tree, thereby improving its strategic strength iteratively. However,
this process often requires significant human intervention to refine the
decision tree after identifying its weaknesses, resulting in inefficiencies and
hindering full automation of the strategy enhancement process. Fortunately, the
advent of Large Language Models (LLMs) offers a transformative opportunity to
automate the process. We propose RL-LLM-DT, an automatic decision tree
generation method based on RL Evaluation and LLM Enhancement. Given an initial
decision tree, the method involves two important iterative steps. Response
Policy Search: RL is used to discover counter-strategies targeting the decision
tree. Policy Improvement: LLMs analyze failure scenarios and generate improved
decision tree code. In our method, RL focuses on finding the decision tree's
flaws while LLM is prompted to generate an improved version of the decision
tree. The iterative refinement process terminates when RL can't find any flaw
of the tree or LLM fails to improve the tree. To evaluate the effectiveness of
this integrated approach, we conducted experiments in a curling game. After
iterative refinements, our curling AI based on the decision tree ranks first on
the Jidi platform among 34 curling AIs in total, which demonstrates that LLMs
can significantly enhance the robustness and adaptability of decision trees,
representing a substantial advancement in the field of Game AI. Our code is
available at https://github.com/Linjunjie99/RL-LLM-DT.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.11385v1' target='_blank'>Why Does ChatGPT "Delve" So Much? Exploring the Sources of Lexical
  Overrepresentation in Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tom S. Juzek, Zina B. Ward</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-16 02:27:59</h6>
<p class='card-text'>Scientific English is currently undergoing rapid change, with words like
"delve," "intricate," and "underscore" appearing far more frequently than just
a few years ago. It is widely assumed that scientists' use of large language
models (LLMs) is responsible for such trends. We develop a formal, transferable
method to characterize these linguistic changes. Application of our method
yields 21 focal words whose increased occurrence in scientific abstracts is
likely the result of LLM usage. We then pose "the puzzle of lexical
overrepresentation": WHY are such words overused by LLMs? We fail to find
evidence that lexical overrepresentation is caused by model architecture,
algorithm choices, or training data. To assess whether reinforcement learning
from human feedback (RLHF) contributes to the overuse of focal words, we
undertake comparative model testing and conduct an exploratory online study.
While the model testing is consistent with RLHF playing a role, our
experimental results suggest that participants may be reacting differently to
"delve" than to other focal words. With LLMs quickly becoming a driver of
global language change, investigating these potential sources of lexical
overrepresentation is important. We note that while insights into the workings
of LLMs are within reach, a lack of transparency surrounding model development
remains an obstacle to such research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.11120v2' target='_blank'>Latent Reward: LLM-Empowered Credit Assignment in Episodic Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yun Qu, Yuhang Jiang, Boyuan Wang, Yixiu Mao, Cheems Wang, Chang Liu, Xiangyang Ji</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-15 08:51:14</h6>
<p class='card-text'>Reinforcement learning (RL) often encounters delayed and sparse feedback in
real-world applications, even with only episodic rewards. Previous approaches
have made some progress in reward redistribution for credit assignment but
still face challenges, including training difficulties due to redundancy and
ambiguous attributions stemming from overlooking the multifaceted nature of
mission performance evaluation. Hopefully, Large Language Model (LLM)
encompasses fruitful decision-making knowledge and provides a plausible tool
for reward redistribution. Even so, deploying LLM in this case is non-trivial
due to the misalignment between linguistic knowledge and the symbolic form
requirement, together with inherent randomness and hallucinations in inference.
To tackle these issues, we introduce LaRe, a novel LLM-empowered symbolic-based
decision-making framework, to improve credit assignment. Key to LaRe is the
concept of the Latent Reward, which works as a multi-dimensional performance
evaluation, enabling more interpretable goal attainment from various
perspectives and facilitating more effective reward redistribution. We examine
that semantically generated code from LLM can bridge linguistic knowledge and
symbolic latent rewards, as it is executable for symbolic objects. Meanwhile,
we design latent reward self-verification to increase the stability and
reliability of LLM inference. Theoretically, reward-irrelevant redundancy
elimination in the latent reward benefits RL performance from more accurate
reward estimation. Extensive experimental results witness that LaRe (i)
achieves superior temporal credit assignment to SOTA methods, (ii) excels in
allocating contributions among multiple agents, and (iii) outperforms policies
trained with ground truth rewards for certain tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.11009v1' target='_blank'>Dual Traits in Probabilistic Reasoning of Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shenxiong Li, Huaxia Rui</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-15 01:33:45</h6>
<p class='card-text'>We conducted three experiments to investigate how large language models
(LLMs) evaluate posterior probabilities. Our results reveal the coexistence of
two modes in posterior judgment among state-of-the-art models: a normative
mode, which adheres to Bayes' rule, and a representative-based mode, which
relies on similarity -- paralleling human System 1 and System 2 thinking.
Additionally, we observed that LLMs struggle to recall base rate information
from their memory, and developing prompt engineering strategies to mitigate
representative-based judgment may be challenging. We further conjecture that
the dual modes of judgment may be a result of the contrastive loss function
employed in reinforcement learning from human feedback. Our findings underscore
the potential direction for reducing cognitive biases in LLMs and the necessity
for cautious deployment of LLMs in critical areas.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.11006v1' target='_blank'>Entropy-Regularized Process Reward Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hanning Zhang, Pengcheng Wang, Shizhe Diao, Yong Lin, Rui Pan, Hanze Dong, Dylan Zhang, Pavlo Molchanov, Tong Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-15 01:09:23</h6>
<p class='card-text'>Large language models (LLMs) have shown promise in performing complex
multi-step reasoning, yet they continue to struggle with mathematical
reasoning, often making systematic errors. A promising solution is
reinforcement learning (RL) guided by reward models, particularly those
focusing on process rewards, which score each intermediate step rather than
solely evaluating the final outcome. This approach is more effective at guiding
policy models towards correct reasoning trajectories. In this work, we propose
an entropy-regularized process reward model (ER-PRM) that integrates
KL-regularized Markov Decision Processes (MDP) to balance policy optimization
with the need to prevent the policy from shifting too far from its initial
distribution. We derive a novel reward construction method based on the
theoretical results. Our theoretical analysis shows that we could derive the
optimal reward model from the initial policy sampling. Our empirical
experiments on the MATH and GSM8K benchmarks demonstrate that ER-PRM
consistently outperforms existing process reward models, achieving 1%
improvement on GSM8K and 2-3% improvement on MATH under best-of-N evaluation,
and more than 1% improvement under RLHF. These results highlight the efficacy
of entropy-regularization in enhancing LLMs' reasoning capabilities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.10675v1' target='_blank'>Chasing Progress, Not Perfection: Revisiting Strategies for End-to-End
  LLM Plan Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sukai Huang, Trevor Cohn, Nir Lipovetzky</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-14 04:23:14</h6>
<p class='card-text'>The capability of Large Language Models (LLMs) to plan remains a topic of
debate. Some critics argue that strategies to boost LLMs' reasoning skills are
ineffective in planning tasks, while others report strong outcomes merely from
training models on a planning corpus. This study reassesses recent strategies
by developing an end-to-end LLM planner and employing diverse metrics for a
thorough evaluation. We find that merely fine-tuning LLMs on a corpus of
planning instances does not lead to robust planning skills, as indicated by
poor performance on out-of-distribution test sets. At the same time, we find
that various strategies, including Chain-of-Thought, do enhance the probability
of a plan being executable. This indicates progress towards better plan
quality, despite not directly enhancing the final validity rate. Among the
strategies we evaluated, reinforcement learning with our novel `Longest
Contiguous Common Subsequence' reward emerged as the most effective,
contributing to both plan validity and executability. Overall, our research
addresses key misconceptions in the LLM-planning literature; we validate
incremental progress in plan executability, although plan validity remains a
challenge. Hence, future strategies should focus on both these aspects, drawing
insights from our findings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.10529v1' target='_blank'>Solving the Inverse Alignment Problem for Efficient RLHF</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shambhavi Krishna, Aishwarya Sahoo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-13 19:47:38</h6>
<p class='card-text'>Collecting high-quality preference datasets for reinforcement learning from
human feedback (RLHF) is resource-intensive and challenging. As a result,
researchers often train reward models on extensive offline datasets which
aggregate diverse generation sources and scoring/alignment policies. We
hypothesize that this aggregation has an averaging effect on reward model
scores, which limits signal and impairs the alignment process. Inspired by the
field of inverse RL, we define the 'inverse alignment problem' in language
model training, where our objective is to optimize the critic's reward for a
fixed actor and a fixed offline preference dataset. We hypothesize that solving
the inverse alignment problem will improve reward model quality by providing
clearer feedback on the policy's current behavior. To that end, we investigate
whether repeatedly fine-tuning a reward model on subsets of the offline
preference dataset aligned with a periodically frozen policy during RLHF
improves upon vanilla RLHF. Our empirical results demonstrate that this
approach facilitates superior alignment and faster convergence compared to
using an unaligned or out-of-distribution reward model relative to the LLM
policy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.15244v1' target='_blank'>MPPO: Multi Pair-wise Preference Optimization for LLMs with Arbitrary
  Negative Samples</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shuo Xie, Fangzhi Zhu, Jiahui Wang, Lulu Wen, Wei Dai, Xiaowei Chen, Junxiong Zhu, Kai Zhou, Bo Zheng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-13 14:18:58</h6>
<p class='card-text'>Aligning Large Language Models (LLMs) with human feedback is crucial for
their development. Existing preference optimization methods such as DPO and
KTO, while improved based on Reinforcement Learning from Human Feedback (RLHF),
are inherently derived from PPO, requiring a reference model that adds GPU
memory resources and relies heavily on abundant preference data. Meanwhile,
current preference optimization research mainly targets single-question
scenarios with two replies, neglecting optimization with multiple replies,
which leads to a waste of data in the application. This study introduces the
MPPO algorithm, which leverages the average likelihood of model responses to
fit the reward function and maximizes the utilization of preference data.
Through a comparison of Point-wise, Pair-wise, and List-wise implementations,
we found that the Pair-wise approach achieves the best performance,
significantly enhancing the quality of model responses. Experimental results
demonstrate MPPO's outstanding performance across various benchmarks. On
MT-Bench, MPPO outperforms DPO, ORPO, and SimPO. Notably, on Arena-Hard, MPPO
surpasses DPO and ORPO by substantial margins. These achievements underscore
the remarkable advantages of MPPO in preference optimization tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.09812v1' target='_blank'>ScaleOT: Privacy-utility-scalable Offsite-tuning with Dynamic
  LayerReplace and Selective Rank Compression</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kai Yao, Zhaorui Tan, Tiandi Ye, Lichun Li, Yuan Zhao, Wenyan Liu, Wei Wang, Jianke Zhu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-13 03:00:48</h6>
<p class='card-text'>Offsite-tuning is a privacy-preserving method for tuning large language
models (LLMs) by sharing a lossy compressed emulator from the LLM owners with
data owners for downstream task tuning. This approach protects the privacy of
both the model and data owners. However, current offsite tuning methods often
suffer from adaptation degradation, high computational costs, and limited
protection strength due to uniformly dropping LLM layers or relying on
expensive knowledge distillation. To address these issues, we propose ScaleOT,
a novel privacy-utility-scalable offsite-tuning framework that effectively
balances privacy and utility. ScaleOT introduces a novel layerwise lossy
compression algorithm that uses reinforcement learning to obtain the importance
of each layer. It employs lightweight networks, termed harmonizers, to replace
the raw LLM layers. By combining important original LLM layers and harmonizers
in different ratios, ScaleOT generates emulators tailored for optimal
performance with various model scales for enhanced privacy protection.
Additionally, we present a rank reduction method to further compress the
original LLM layers, significantly enhancing privacy with negligible impact on
utility. Comprehensive experiments show that ScaleOT can achieve nearly
lossless offsite tuning performance compared with full fine-tuning while
obtaining better model privacy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.12170v1' target='_blank'>PickLLM: Context-Aware RL-Assisted Large Language Model Routing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dimitrios Sikeridis, Dennis Ramdass, Pranay Pareek</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-12 06:27:12</h6>
<p class='card-text'>Recently, the number of off-the-shelf Large Language Models (LLMs) has
exploded with many open-source options. This creates a diverse landscape
regarding both serving options (e.g., inference on local hardware vs remote LLM
APIs) and model heterogeneous expertise. However, it is hard for the user to
efficiently optimize considering operational cost (pricing structures,
expensive LLMs-as-a-service for large querying volumes), efficiency, or even
per-case specific measures such as response accuracy, bias, or toxicity. Also,
existing LLM routing solutions focus mainly on cost reduction, with response
accuracy optimizations relying on non-generalizable supervised training, and
ensemble approaches necessitating output computation for every considered LLM
candidate. In this work, we tackle the challenge of selecting the optimal LLM
from a model pool for specific queries with customizable objectives. We propose
PickLLM, a lightweight framework that relies on Reinforcement Learning (RL) to
route on-the-fly queries to available models. We introduce a weighted reward
function that considers per-query cost, inference latency, and model response
accuracy by a customizable scoring function. Regarding the learning algorithms,
we explore two alternatives: PickLLM router acting as a learning automaton that
utilizes gradient ascent to select a specific LLM, or utilizing stateless
Q-learning to explore the set of LLMs and perform selection with a
$\epsilon$-greedy approach. The algorithm converges to a single LLM for the
remaining session queries. To evaluate, we utilize a pool of four LLMs and
benchmark prompt-response datasets with different contexts. A separate scoring
function is assessing response accuracy during the experiment. We demonstrate
the speed of convergence for different learning rates and improvement in hard
metrics such as cost per querying session and overall response latency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.08970v1' target='_blank'>Reasoning-Aware Query-Focused Summarization over Multi-Table Data</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaochuan Lin, Xiangyong Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-12 06:04:31</h6>
<p class='card-text'>Query-focused summarization over multi-table data is a challenging yet
critical task for extracting precise and relevant information from structured
data. Existing methods often rely on complex preprocessing steps and struggle
to generalize across domains or handle the logical reasoning required for
multi-table queries. In this paper, we propose QueryTableSummarizer++, an
end-to-end generative framework leveraging large language models (LLMs)
enhanced with table-aware pre-training, query-aligned fine-tuning, and
reinforcement learning with feedback. Our method eliminates the need for
intermediate serialization steps and directly generates query-relevant
summaries. Experiments on a benchmark dataset demonstrate that
QueryTableSummarizer++ significantly outperforms state-of-the-art baselines in
terms of BLEU, ROUGE, and F1-score. Additional analyses highlight its
scalability, generalization across domains, and robust handling of complex
queries. Human evaluation further validates the superior quality and practical
applicability of the generated summaries, establishing QueryTableSummarizer++
as a highly effective solution for multi-table summarization tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.15236v2' target='_blank'>CareBot: A Pioneering Full-Process Open-Source Medical Language Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lulu Zhao, Weihao Zeng, Xiaofeng Shi, Hua Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-12 05:27:43</h6>
<p class='card-text'>Recently, both closed-source LLMs and open-source communities have made
significant strides, outperforming humans in various general domains. However,
their performance in specific professional domains such as medicine, especially
within the open-source community, remains suboptimal due to the complexity of
medical knowledge. In this paper, we propose CareBot, a bilingual medical LLM,
which leverages a comprehensive approach integrating continuous pre-training
(CPT), supervised fine-tuning (SFT), and reinforcement learning with human
feedback (RLHF). Our novel two-stage CPT method, comprising Stable CPT and
Boost CPT, effectively bridges the gap between general and domain-specific
data, facilitating a smooth transition from pre-training to fine-tuning and
enhancing domain knowledge progressively. We also introduce DataRater, a model
designed to assess data quality during CPT, ensuring that the training data is
both accurate and relevant. For SFT, we develope a large and diverse bilingual
dataset, along with ConFilter, a metric to enhance multi-turn dialogue quality,
which is crucial to improving the model's ability to handle more complex
dialogues. The combination of high-quality data sources and innovative
techniques significantly improves CareBot's performance across a range of
medical applications. Our rigorous evaluations on Chinese and English
benchmarks confirm CareBot's effectiveness in medical consultation and
education. These advancements not only address current limitations in medical
LLMs but also set a new standard for developing effective and reliable
open-source models in the medical domain. We will open-source the datasets and
models later, contributing valuable resources to the research community.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.08542v1' target='_blank'>MaestroMotif: Skill Design from Artificial Intelligence Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Martin Klissarov, Mikael Henaff, Roberta Raileanu, Shagun Sodhani, Pascal Vincent, Amy Zhang, Pierre-Luc Bacon, Doina Precup, Marlos C. Machado, Pierluca D'Oro</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-11 16:59:31</h6>
<p class='card-text'>Describing skills in natural language has the potential to provide an
accessible way to inject human knowledge about decision-making into an AI
system. We present MaestroMotif, a method for AI-assisted skill design, which
yields high-performing and adaptable agents. MaestroMotif leverages the
capabilities of Large Language Models (LLMs) to effectively create and reuse
skills. It first uses an LLM's feedback to automatically design rewards
corresponding to each skill, starting from their natural language description.
Then, it employs an LLM's code generation abilities, together with
reinforcement learning, for training the skills and combining them to implement
complex behaviors specified in language. We evaluate MaestroMotif using a suite
of complex tasks in the NetHack Learning Environment (NLE), demonstrating that
it surpasses existing approaches in both performance and usability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.10423v1' target='_blank'>Look Before You Leap: Enhancing Attention and Vigilance Regarding
  Harmful Content with GuidelineLLM</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shaoqing Zhang, Zhuosheng Zhang, Kehai Chen, Rongxiang Weng, Muyun Yang, Tiejun Zhao, Min Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-10 12:42:33</h6>
<p class='card-text'>Despite being empowered with alignment mechanisms, large language models
(LLMs) are increasingly vulnerable to emerging jailbreak attacks that can
compromise their alignment mechanisms. This vulnerability poses significant
risks to the real-world applications. Existing work faces challenges in both
training efficiency and generalization capabilities (i.e., Reinforcement
Learning from Human Feedback and Red-Teaming). Developing effective strategies
to enable LLMs to resist continuously evolving jailbreak attempts represents a
significant challenge. To address this challenge, we propose a novel defensive
paradigm called GuidelineLLM, which assists LLMs in recognizing queries that
may have harmful content. Before LLMs respond to a query, GuidelineLLM first
identifies potential risks associated with the query, summarizes these risks
into guideline suggestions, and then feeds these guidelines to the responding
LLMs. Importantly, our approach eliminates the necessity for additional safety
fine-tuning of the LLMs themselves; only the GuidelineLLM requires fine-tuning.
This characteristic enhances the general applicability of GuidelineLLM across
various LLMs. Experimental results demonstrate that GuidelineLLM can
significantly reduce the attack success rate (ASR) against the LLMs (an average
reduction of 34.17\% ASR) while maintaining the helpfulness of the LLMs in
handling benign queries. Code is available at
https://github.com/sqzhang-lazy/GuidelineLLM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.06877v1' target='_blank'>LLMs for Generalizable Language-Conditioned Policy Learning under
  Minimal Data Requirements</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Thomas Pouplin, Katarzyna Kobalczyk, Hao Sun, Mihaela van der Schaar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-09 18:43:56</h6>
<p class='card-text'>To develop autonomous agents capable of executing complex, multi-step
decision-making tasks as specified by humans in natural language, existing
reinforcement learning approaches typically require expensive labeled datasets
or access to real-time experimentation. Moreover, conventional methods often
face difficulties in generalizing to unseen goals and states, thereby limiting
their practical applicability. This paper presents TEDUO, a novel training
pipeline for offline language-conditioned policy learning. TEDUO operates on
easy-to-obtain, unlabeled datasets and is suited for the so-called in-the-wild
evaluation, wherein the agent encounters previously unseen goals and states. To
address the challenges posed by such data and evaluation settings, our method
leverages the prior knowledge and instruction-following capabilities of large
language models (LLMs) to enhance the fidelity of pre-collected offline data
and enable flexible generalization to new goals and states. Empirical results
demonstrate that the dual role of LLMs in our framework-as data enhancers and
generalizers-facilitates both effective and data-efficient learning of
generalizable language-conditioned policies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.06000v1' target='_blank'>Does RLHF Scale? Exploring the Impacts From Data, Model, and Method</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhenyu Hou, Pengfan Du, Yilin Niu, Zhengxiao Du, Aohan Zeng, Xiao Liu, Minlie Huang, Hongning Wang, Jie Tang, Yuxiao Dong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-08 17:19:48</h6>
<p class='card-text'>This study explores the scaling properties of Reinforcement Learning from
Human Feedback (RLHF) in Large Language Models (LLMs). Although RLHF is
considered an important step in post-training of LLMs, its scaling potential is
still largely unknown. We systematically analyze key components in the RLHF
framework--model size, data composition, and inference budget--and their
impacts on performance. Our findings show that increasing data diversity and
volume improves reward model performance, helping process-supervision models
scale better. For policy training, more response samples per prompt boost
performance initially but quickly plateau. And larger reward models offer
modest gains in policy training. In addition, larger policy models benefit less
from RLHF with a fixed reward model. Overall, RLHF scales less efficiently than
pretraining, with diminishing returns from additional computational resources.
Based on these observations, we propose strategies to optimize RLHF performance
within computational limits.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.06846v1' target='_blank'>Classifier-free guidance in LLMs Safety</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Roman Smirnov</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-08 02:11:33</h6>
<p class='card-text'>The paper describes LLM unlearning without a retaining dataset, using the
ORPO reinforcement learning method with inference enhanced by modified
classifier-free guidance. Significant improvement in unlearning, without
degradation of the model, is achieved through direct training on synthetic
replacement data in CFG-aware training regime, with classifier-free guidance
applied during the inference. This article is an extended version of the
NeurIPS 2024 LLM-PC submission, which was awarded second prize.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.05734v1' target='_blank'>PrivAgent: Agentic-based Red-teaming for LLM Privacy Leakage</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuzhou Nie, Zhun Wang, Ye Yu, Xian Wu, Xuandong Zhao, Wenbo Guo, Dawn Song</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-07 20:09:01</h6>
<p class='card-text'>Recent studies have discovered that LLMs have serious privacy leakage
concerns, where an LLM may be fooled into outputting private information under
carefully crafted adversarial prompts. These risks include leaking system
prompts, personally identifiable information, training data, and model
parameters. Most existing red-teaming approaches for privacy leakage rely on
humans to craft the adversarial prompts. A few automated methods are proposed
for system prompt extraction, but they cannot be applied to more severe risks
(e.g., training data extraction) and have limited effectiveness even for system
prompt extraction.
  In this paper, we propose PrivAgent, a novel black-box red-teaming framework
for LLM privacy leakage. We formulate different risks as a search problem with
a unified attack goal. Our framework trains an open-source LLM through
reinforcement learning as the attack agent to generate adversarial prompts for
different target models under different risks. We propose a novel reward
function to provide effective and fine-grained rewards for the attack agent.
Finally, we introduce customizations to better fit our general framework to
system prompt extraction and training data extraction. Through extensive
evaluations, we first show that PrivAgent outperforms existing automated
methods in system prompt leakage against six popular LLMs. Notably, our
approach achieves a 100% success rate in extracting system prompts from
real-world applications in OpenAI's GPT Store. We also show PrivAgent's
effectiveness in extracting training data from an open-source LLM with a
success rate of 5.9%. We further demonstrate PrivAgent's effectiveness in
evading the existing guardrail defense and its helpfulness in enabling better
safety alignment. Finally, we validate our customized designs through a
detailed ablation study. We release our code here
https://github.com/rucnyz/RedAgent.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.05515v1' target='_blank'>Video2Reward: Generating Reward Function from Videos for Legged Robot
  Behavior Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Runhao Zeng, Dingjie Zhou, Qiwei Liang, Junlin Liu, Hui Li, Changxin Huang, Jianqiang Li, Xiping Hu, Fuchun Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-07 03:10:27</h6>
<p class='card-text'>Learning behavior in legged robots presents a significant challenge due to
its inherent instability and complex constraints. Recent research has proposed
the use of a large language model (LLM) to generate reward functions in
reinforcement learning, thereby replacing the need for manually designed
rewards by experts. However, this approach, which relies on textual
descriptions to define learning objectives, fails to achieve controllable and
precise behavior learning with clear directionality. In this paper, we
introduce a new video2reward method, which directly generates reward functions
from videos depicting the behaviors to be mimicked and learned. Specifically,
we first process videos containing the target behaviors, converting the motion
information of individuals in the videos into keypoint trajectories represented
as coordinates through a video2text transforming module. These trajectories are
then fed into an LLM to generate the reward function, which in turn is used to
train the policy. To enhance the quality of the reward function, we develop a
video-assisted iterative reward refinement scheme that visually assesses the
learned behaviors and provides textual feedback to the LLM. This feedback
guides the LLM to continually refine the reward function, ultimately
facilitating more efficient behavior learning. Experimental results on tasks
involving bipedal and quadrupedal robot motion control demonstrate that our
method surpasses the performance of state-of-the-art LLM-based reward
generation methods by over 37.6% in terms of human normalized score. More
importantly, by switching video inputs, we find our method can rapidly learn
diverse motion behaviors such as walking and running.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.06827v1' target='_blank'>Enhancing LLMs for Physics Problem-Solving using Reinforcement Learning
  with Human-AI Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Avinash Anand, Kritarth Prasad, Chhavi Kirtani, Ashwin R Nair, Mohit Gupta, Saloni Garg, Anurag Gautam, Snehal Buldeo, Rajiv Ratn Shah</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-06 21:17:47</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated strong capabilities in
text-based tasks but struggle with the complex reasoning required for physics
problems, particularly in advanced arithmetic and conceptual understanding.
While some research has explored ways to enhance LLMs in physics education
using techniques such as prompt engineering and Retrieval Augmentation
Generation (RAG), not enough effort has been made in addressing their
limitations in physics reasoning. This paper presents a novel approach to
improving LLM performance on physics questions using Reinforcement Learning
with Human and Artificial Intelligence Feedback (RLHAIF). We evaluate several
reinforcement learning methods, including Proximal Policy Optimization (PPO),
Direct Preference Optimization (DPO), and Remax optimization. These methods are
chosen to investigate RL policy performance with different settings on the
PhyQA dataset, which includes challenging physics problems from high school
textbooks. Our RLHAIF model, tested on leading LLMs like LLaMA2 and Mistral,
achieved superior results, notably with the MISTRAL-PPO model, demonstrating
marked improvements in reasoning and accuracy. It achieved high scores, with a
58.67 METEOR score and a 0.74 Reasoning score, making it a strong example for
future physics reasoning research in this area.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.10400v3' target='_blank'>Reinforcement Learning Enhanced LLMs: A Survey</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shuhe Wang, Shengyu Zhang, Jie Zhang, Runyi Hu, Xiaoya Li, Tianwei Zhang, Jiwei Li, Fei Wu, Guoyin Wang, Eduard Hovy</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-05 16:10:42</h6>
<p class='card-text'>Reinforcement learning (RL) enhanced large language models (LLMs),
particularly exemplified by DeepSeek-R1, have exhibited outstanding
performance. Despite the effectiveness in improving LLM capabilities, its
implementation remains highly complex, requiring complex algorithms, reward
modeling strategies, and optimization techniques. This complexity poses
challenges for researchers and practitioners in developing a systematic
understanding of RL-enhanced LLMs. Moreover, the absence of a comprehensive
survey summarizing existing research on RL-enhanced LLMs has limited progress
in this domain, hindering further advancements.
  In this work, we are going to make a systematic review of the most up-to-date
state of knowledge on RL-enhanced LLMs, attempting to consolidate and analyze
the rapidly growing research in this field, helping researchers understand the
current challenges and advancements. Specifically, we (1) detail the basics of
RL; (2) introduce popular RL-enhanced LLMs; (3) review researches on two
widely-used reward model-based RL techniques: Reinforcement Learning from Human
Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF); and (4)
explore Direct Preference Optimization (DPO), a set of methods that bypass the
reward model to directly use human preference data for aligning LLM outputs
with human expectations. We will also point out current challenges and
deficiencies of existing methods and suggest some avenues for further
improvements. Project page of this work can be found at
https://github.com/ShuheWang1998/Reinforcement-Learning-Enhanced-LLMs-A-Survey.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.03966v1' target='_blank'>Demonstration Selection for In-Context Learning via Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xubin Wang, Jianfei Wu, Yichen Yuan, Mingzhe Li, Deyu Cai, Weijia Jia</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-05 08:33:52</h6>
<p class='card-text'>Diversity in demonstration selection is crucial for enhancing model
generalization, as it enables a broader coverage of structures and concepts.
However, constructing an appropriate set of demonstrations has remained a focal
point of research. This paper presents the Relevance-Diversity Enhanced
Selection (RDES), an innovative approach that leverages reinforcement learning
to optimize the selection of diverse reference demonstrations for text
classification tasks using Large Language Models (LLMs), especially in few-shot
prompting scenarios. RDES employs a Q-learning framework to dynamically
identify demonstrations that maximize both diversity and relevance to the
classification objective by calculating a diversity score based on label
distribution among selected demonstrations. This method ensures a balanced
representation of reference data, leading to improved classification accuracy.
Through extensive experiments on four benchmark datasets and involving 12
closed-source and open-source LLMs, we demonstrate that RDES significantly
enhances classification accuracy compared to ten established baselines.
Furthermore, we investigate the incorporation of Chain-of-Thought (CoT)
reasoning in the reasoning process, which further enhances the model's
predictive performance. The results underscore the potential of reinforcement
learning to facilitate adaptive demonstration selection and deepen the
understanding of classification challenges.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.03621v1' target='_blank'>Network-aided Efficient Large Language Model Services With
  Denoising-inspired Prompt Compression</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Feiran You, Hongyang Du, Kaibin Huang, Abbas Jamalipour</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-04 15:26:10</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated remarkable capabilities in
various tasks, leading to their increasing adoption in diverse services
delivered through wireless networks. There is a growing trend toward longer
prompts to better leverage LLMs' capabilities and address difficult tasks.
However, longer prompts not only increase data transmission costs across
wireless transmission but also require more computing resources and processing
time, impacting the overall system efficiency and user experience. To address
this challenge, we propose Joint Power and Prompt Optimization (JPPO), a
framework that combines Small Language Model (SLM)-based prompt compression
with wireless power allocation optimization. By deploying SLM at edge devices
for prompt compression and employing Deep Reinforcement Learning (DRL) for
joint optimization of compression ratio and transmission power, JPPO
effectively balances service quality with resource efficiency. Furthermore,
inspired by denoising diffusion models, we design a denoising-inspired prompt
compression approach that iteratively compresses prompts by gradually removing
non-critical information. Experimental results demonstrate that our framework
achieves high service fidelity while optimizing power usage in wireless LLM
services, reducing the total service response time. With our DRL-based JPPO,
the framework maintains fidelity comparable to the no-compression baseline
while still achieving a 17% service time reduction through adaptive
compression. When prioritizing compression, our framework achieves up to 16x
compression ratio while maintaining acceptable fidelity (within 30% reduction).
Compared to no compression, baseline single-round compression with a 16x
compression ratio reduces the system total response time by approximately
42.3%, while the denoising-inspired method achieves a 46.5% service
time-saving.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.03338v2' target='_blank'>AI-Driven Day-to-Day Route Choice</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Leizhen Wang, Peibo Duan, Zhengbing He, Cheng Lyu, Xin Chen, Nan Zheng, Li Yao, Zhenliang Ma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-04 14:13:38</h6>
<p class='card-text'>Understanding travelers' route choices can help policymakers devise optimal
operational and planning strategies for both normal and abnormal circumstances.
However, existing choice modeling methods often rely on predefined assumptions
and struggle to capture the dynamic and adaptive nature of travel behavior.
Recently, Large Language Models (LLMs) have emerged as a promising alternative,
demonstrating remarkable ability to replicate human-like behaviors across
various fields. Despite this potential, their capacity to accurately simulate
human route choice behavior in transportation contexts remains doubtful. To
satisfy this curiosity, this paper investigates the potential of LLMs for route
choice modeling by introducing an LLM-empowered agent, "LLMTraveler." This
agent integrates an LLM as its core, equipped with a memory system that learns
from past experiences and makes decisions by balancing retrieved data and
personality traits. The study systematically evaluates the LLMTraveler's
ability to replicate human-like decision-making through two stages of
day-to-day (DTD) congestion games: (1) analyzing its route-switching behavior
in single origin-destination (OD) pair scenarios, where it demonstrates
patterns that align with laboratory data but cannot be fully explained by
traditional models, and (2) testing its capacity to model adaptive learning
behaviors in multi-OD scenarios on the Ortuzar and Willumsen (OW) network,
producing results comparable to Multinomial Logit (MNL) and Reinforcement
Learning (RL) models. These experiments demonstrate that the framework can
partially replicate human-like decision-making in route choice while providing
natural language explanations for its decisions. This capability offers
valuable insights for transportation policymaking, such as simulating traveler
responses to new policies or changes in the network.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.03253v1' target='_blank'>Alignment at Pre-training! Towards Native Alignment for Arabic LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Juhao Liang, Zhenyang Cai, Jianqing Zhu, Huang Huang, Kewei Zong, Bang An, Mosen Alharthi, Juncai He, Lian Zhang, Haizhou Li, Benyou Wang, Jinchao Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-04 11:52:03</h6>
<p class='card-text'>The alignment of large language models (LLMs) is critical for developing
effective and safe language models. Traditional approaches focus on aligning
models during the instruction tuning or reinforcement learning stages, referred
to in this paper as `post alignment'. We argue that alignment during the
pre-training phase, which we term `native alignment', warrants investigation.
Native alignment aims to prevent unaligned content from the beginning, rather
than relying on post-hoc processing. This approach leverages extensively
aligned pre-training data to enhance the effectiveness and usability of
pre-trained models. Our study specifically explores the application of native
alignment in the context of Arabic LLMs. We conduct comprehensive experiments
and ablation studies to evaluate the impact of native alignment on model
performance and alignment stability. Additionally, we release open-source
Arabic LLMs that demonstrate state-of-the-art performance on various
benchmarks, providing significant benefits to the Arabic LLM community.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.02685v1' target='_blank'>T-REG: Preference Optimization with Token-Level Reward Regularization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenxuan Zhou, Shujian Zhang, Lingxiao Zhao, Tao Meng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-03 18:56:07</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) has been crucial in
aligning large language models (LLMs) with human values. Traditionally, RLHF
involves generating responses to a query and using a reward model to assign a
reward to the entire response. However, this approach faces challenges due to
its reliance on a single, sparse reward, which makes it challenging for the
model to identify which parts of the sequence contribute most significantly to
the final reward. Recent methods have attempted to address this limitation by
introducing token-level rewards. However, these methods often rely on either a
trained credit assignment model or AI annotators, raising concerns about the
quality and reliability of the rewards. In this paper, we propose token-level
reward regularization (T-REG), a novel approach that leverages both
sequence-level and token-level rewards for preference optimization. Harnessing
the self-refinement capabilities of LLMs, our method uses contrastive prompting
to enable LLMs to self-generate token-level rewards. These self-generated
rewards then act as reward regularization, guiding the model to more
effectively distribute sequence-level rewards across tokens. This facilitates
better token-level credit assignment and enhances alignment performance.
Experiments on the instruction following benchmarks, including Alpaca Eval 2
and Arena-Hard, show that our method consistently outperforms baseline methods
by up to 3.8% and 4.4%, respectively. We will release the code and models at
https://github.com/wzhouad/T-REG.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.02588v1' target='_blank'>Explainable CTR Prediction via LLM Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaohan Yu, Li Zhang, Chong Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-03 17:17:27</h6>
<p class='card-text'>Recommendation Systems have become integral to modern user experiences, but
lack transparency in their decision-making processes. Existing explainable
recommendation methods are hindered by reliance on a post-hoc paradigm, wherein
explanation generators are trained independently of the underlying recommender
models. This paradigm necessitates substantial human effort in data
construction and raises concerns about explanation reliability. In this paper,
we present ExpCTR, a novel framework that integrates large language model based
explanation generation directly into the CTR prediction process. Inspired by
recent advances in reinforcement learning, we employ two carefully designed
reward mechanisms, LC alignment, which ensures explanations reflect user
intentions, and IC alignment, which maintains consistency with traditional
ID-based CTR models. Our approach incorporates an efficient training paradigm
with LoRA and a three-stage iterative process. ExpCTR circumvents the need for
extensive explanation datasets while fostering synergy between CTR prediction
and explanation generation. Experimental results demonstrate that ExpCTR
significantly enhances both recommendation accuracy and interpretability across
three real-world datasets.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.01303v1' target='_blank'>RL2: Reinforce Large Language Model to Assist Safe Reinforcement
  Learning for Energy Management of Active Distribution Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xu Yang, Chenhui Lin, Haotian Liu, Wenchuan Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-02 09:15:36</h6>
<p class='card-text'>As large-scale distributed energy resources are integrated into the active
distribution networks (ADNs), effective energy management in ADNs becomes
increasingly prominent compared to traditional distribution networks. Although
advanced reinforcement learning (RL) methods, which alleviate the burden of
complicated modelling and optimization, have greatly improved the efficiency of
energy management in ADNs, safety becomes a critical concern for RL
applications in real-world problems. Since the design and adjustment of penalty
functions, which correspond to operational safety constraints, requires
extensive domain knowledge in RL and power system operation, the emerging ADN
operators call for a more flexible and customized approach to address the
penalty functions so that the operational safety and efficiency can be further
enhanced. Empowered with strong comprehension, reasoning, and in-context
learning capabilities, large language models (LLMs) provide a promising way to
assist safe RL for energy management in ADNs. In this paper, we introduce the
LLM to comprehend operational safety requirements in ADNs and generate
corresponding penalty functions. In addition, we propose an RL2 mechanism to
refine the generated functions iteratively and adaptively through multi-round
dialogues, in which the LLM agent adjusts the functions' pattern and parameters
based on training and test performance of the downstream RL agent. The proposed
method significantly reduces the intervention of the ADN operators.
Comprehensive test results demonstrate the effectiveness of the proposed
method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.01262v1' target='_blank'>Do Large Language Models with Reasoning and Acting Meet the Needs of
  Task-Oriented Dialogue?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Michelle Elizabeth, Morgan Veyret, Miguel Couceiro, Ondrej Dusek, Lina M. Rojas-Barahona</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-02 08:30:22</h6>
<p class='card-text'>Large language models (LLMs) gained immense popularity due to their
impressive capabilities in unstructured conversations. However, they
underperform compared to previous approaches in task-oriented dialogue (TOD),
wherein reasoning and accessing external information are crucial. Empowering
LLMs with advanced prompting strategies such as reasoning and acting (ReAct)
has shown promise in solving complex tasks traditionally requiring
reinforcement learning. In this work, we apply the ReAct strategy to guide LLMs
performing TOD. We evaluate ReAct-based LLMs (ReAct-LLMs) both in simulation
and with real users. While ReAct-LLMs seem to underperform state-of-the-art
approaches in simulation, human evaluation indicates higher user satisfaction
rate compared to handcrafted systems despite having a lower success rate.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.01253v5' target='_blank'>Yi-Lightning Technical Report</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alan Wake, Bei Chen, C. X. Lv, Chao Li, Chengen Huang, Chenglin Cai, Chujie Zheng, Daniel Cooper, Fan Zhou, Feng Hu, Ge Zhang, Guoyin Wang, Heng Ji, Howard Qiu, Jiangcheng Zhu, Jun Tian, Katherine Su, Lihuan Zhang, Liying Li, Ming Song, Mou Li, Peng Liu, Qicheng Hu, Shawn Wang, Shijun Zhou, Shiming Yang, Shiyong Li, Tianhang Zhu, Wen Xie, Wenhao Huang, Xiang He, Xiaobo Chen, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Yanpeng Li, Yongke Zhao, Yongzhen Luo, Yuchi Xu, Yuxuan Sha, Zhaodong Yan, Zhiyuan Liu, Zirui Zhang, Zonghong Dai</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-02 08:22:56</h6>
<p class='card-text'>This technical report presents Yi-Lightning, our latest flagship large
language model (LLM). It achieves exceptional performance, ranking 6th overall
on Chatbot Arena, with particularly strong results (2nd to 4th place) in
specialized categories including Chinese, Math, Coding, and Hard Prompts.
Yi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,
featuring advanced expert segmentation and routing mechanisms coupled with
optimized KV-caching techniques. Our development process encompasses
comprehensive pre-training, supervised fine-tuning (SFT), and reinforcement
learning from human feedback (RLHF), where we devise deliberate strategies for
multi-stage training, synthetic data construction, and reward modeling.
Furthermore, we implement RAISE (Responsible AI Safety Engine), a
four-component framework to address safety issues across pre-training,
post-training, and serving phases. Empowered by our scalable super-computing
infrastructure, all these innovations substantially reduce training, deployment
and inference costs while maintaining high-performance standards. With further
evaluations on public academic benchmarks, Yi-Lightning demonstrates
competitive performance against top-tier LLMs, while we observe a notable
disparity between traditional, static benchmark results and real-world, dynamic
human preferences. This observation prompts a critical reassessment of
conventional benchmarks' utility in guiding the development of more intelligent
and powerful AI systems for practical applications. Yi-Lightning is now
available through our developer platform at https://platform.lingyiwanwu.com.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.00967v1' target='_blank'>Linear Probe Penalties Reduce LLM Sycophancy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Henry Papadatos, Rachel Freedman</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-01 21:11:28</h6>
<p class='card-text'>Large language models (LLMs) are often sycophantic, prioritizing agreement
with their users over accurate or objective statements. This problematic
behavior becomes more pronounced during reinforcement learning from human
feedback (RLHF), an LLM fine-tuning stage intended to align model outputs with
human values. Instead of increasing accuracy and reliability, the reward model
learned from RLHF often rewards sycophancy. We develop a linear probing method
to identify and penalize markers of sycophancy within the reward model,
producing rewards that discourage sycophantic behavior. Our experiments show
that constructing and optimizing against this surrogate reward function reduces
sycophantic behavior in multiple open-source LLMs. Our results suggest a
generalizable methodology for reducing unwanted LLM behaviors that are not
sufficiently disincentivized by RLHF fine-tuning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.00300v1' target='_blank'>PlanCritic: Formal Planning with Human Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Owen Burns, Dana Hughes, Katia Sycara</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-30 00:58:48</h6>
<p class='card-text'>Real world planning problems are often too complex to be effectively tackled
by a single unaided human. To alleviate this, some recent work has focused on
developing a collaborative planning system to assist humans in complex domains,
with bridging the gap between the system's problem representation and the real
world being a key consideration. Transferring the speed and correctness formal
planners provide to real-world planning problems is greatly complicated by the
dynamic and online nature of such tasks. Formal specifications of task and
environment dynamics frequently lack constraints on some behaviors or goal
conditions relevant to the way a human operator prefers a plan to be carried
out. While adding constraints to the representation with the objective of
increasing its realism risks slowing down the planner, we posit that the same
benefits can be realized without sacrificing speed by modeling this problem as
an online preference learning task. As part of a broader cooperative planning
system, we present a feedback-driven plan critic. This method makes use of
reinforcement learning with human feedback in conjunction with a genetic
algorithm to directly optimize a plan with respect to natural-language user
preferences despite the non-differentiability of traditional planners. Directly
optimizing the plan bridges the gap between research into more efficient
planners and research into planning with language models by utilizing the
convenience of natural language to guide the output of formal planners. We
demonstrate the effectiveness of our plan critic at adhering to user
preferences on a disaster recovery task, and observe improved performance
compared to an llm-only neurosymbolic approach.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.19886v1' target='_blank'>PDDLFuse: A Tool for Generating Diverse Planning Domains</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Vedant Khandelwal, Amit Sheth, Forest Agostinelli</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-29 17:52:39</h6>
<p class='card-text'>Various real-world challenges require planning algorithms that can adapt to a
broad range of domains. Traditionally, the creation of planning domains has
relied heavily on human implementation, which limits the scale and diversity of
available domains. While recent advancements have leveraged generative AI
technologies such as large language models (LLMs) for domain creation, these
efforts have predominantly focused on translating existing domains from natural
language descriptions rather than generating novel ones. In contrast, the
concept of domain randomization, which has been highly effective in
reinforcement learning, enhances performance and generalizability by training
on a diverse array of randomized new domains. Inspired by this success, our
tool, PDDLFuse, aims to bridge this gap in Planning Domain Definition Language
(PDDL). PDDLFuse is designed to generate new, diverse planning domains that can
be used to validate new planners or test foundational planning models. We have
developed methods to adjust the domain generators parameters to modulate the
difficulty of the domains it generates. This adaptability is crucial as
existing domain-independent planners often struggle with more complex problems.
Initial tests indicate that PDDLFuse efficiently creates intricate and varied
domains, representing a significant advancement over traditional domain
generation methods and making a contribution towards planning research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.19635v1' target='_blank'>Build An Influential Bot In Social Media Simulations With Large Language
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bailu Jin, Weisi Guo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-29 11:37:12</h6>
<p class='card-text'>Understanding the dynamics of public opinion evolution on online social
platforms is critical for analyzing influence mechanisms. Traditional
approaches to influencer analysis are typically divided into qualitative
assessments of personal attributes and quantitative evaluations of influence
power. In this study, we introduce a novel simulated environment that combines
Agent-Based Modeling (ABM) with Large Language Models (LLMs), enabling agents
to generate posts, form opinions, and update follower networks. This simulation
allows for more detailed observations of how opinion leaders emerge.
Additionally, we present an innovative application of Reinforcement Learning
(RL) to replicate the process of opinion leader formation. Our findings reveal
that limiting the action space and incorporating self-observation are key
factors for achieving stable opinion leader generation. The learning curves
demonstrate the model's capacity to identify optimal strategies and adapt to
complex, unpredictable dynamics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.19547v1' target='_blank'>Training Agents with Weakly Supervised Feedback from Large Language
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dihong Gong, Pu Lu, Zelong Wang, Meng Zhou, Xiuqiang He</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-29 08:47:04</h6>
<p class='card-text'>Large Language Models (LLMs) offer a promising basis for creating agents that
can tackle complex tasks through iterative environmental interaction. Existing
methods either require these agents to mimic expert-provided trajectories or
rely on definitive environmental feedback for reinforcement learning which
limits their application to specific scenarios like gaming or code generation.
This paper introduces a novel training method for LLM-based agents using weakly
supervised signals from a critic LLM, bypassing the need for expert
trajectories or definitive feedback. Our agents are trained in iterative
manner, where they initially generate trajectories through environmental
interaction. Subsequently, a critic LLM selects a subset of good trajectories,
which are then used to update the agents, enabling them to generate improved
trajectories in the next iteration. Extensive tests on the API-bank dataset
show consistent improvement in our agents' capabilities and comparable
performance to GPT-4, despite using open-source models with much fewer
parameters.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.18947v1' target='_blank'>ICLERB: In-Context Learning Embedding and Reranker Benchmark</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Marie Al Ghossein, Emile Contal, Alexandre Robicquet</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-28 06:28:45</h6>
<p class='card-text'>In-Context Learning (ICL) enables Large Language Models (LLMs) to perform new
tasks by conditioning on prompts with relevant information. Retrieval-Augmented
Generation (RAG) enhances ICL by incorporating retrieved documents into the
LLM's context at query time. However, traditional retrieval methods focus on
semantic relevance, treating retrieval as a search problem. In this paper, we
propose reframing retrieval for ICL as a recommendation problem, aiming to
select documents that maximize utility in ICL tasks. We introduce the
In-Context Learning Embedding and Reranker Benchmark (ICLERB), a novel
evaluation framework that compares retrievers based on their ability to enhance
LLM accuracy in ICL settings. Additionally, we propose a novel Reinforcement
Learning-to-Rank from AI Feedback (RLRAIF) algorithm, designed to fine-tune
retrieval models using minimal feedback from the LLM. Our experimental results
reveal notable differences between ICLERB and existing benchmarks, and
demonstrate that small models fine-tuned with our RLRAIF algorithm outperform
large state-of-the-art retrieval models. These findings highlight the
limitations of existing evaluation methods and the need for specialized
benchmarks and training strategies adapted to ICL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.18825v2' target='_blank'>ELEMENTAL: Interactive Learning from Demonstrations and Vision-Language
  Models for Reward Design in Robotics</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Letian Chen, Matthew Gombolay</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-27 23:58:32</h6>
<p class='card-text'>Reinforcement learning (RL) has demonstrated compelling performance in
robotic tasks, but its success often hinges on the design of complex, ad hoc
reward functions. Researchers have explored how Large Language Models (LLMs)
could enable non-expert users to specify reward functions more easily. However,
LLMs struggle to balance the importance of different features, generalize
poorly to out-of-distribution robotic tasks, and cannot represent the problem
properly with only text-based descriptions. To address these challenges, we
propose ELEMENTAL (intEractive LEarning froM dEmoNstraTion And Language), a
novel framework that combines natural language guidance with visual user
demonstrations to align robot behavior with user intentions better. By
incorporating visual inputs, ELEMENTAL overcomes the limitations of text-only
task specifications, while leveraging inverse reinforcement learning (IRL) to
balance feature weights and match the demonstrated behaviors optimally.
ELEMENTAL also introduces an iterative feedback-loop through self-reflection to
improve feature, reward, and policy learning. Our experiment results
demonstrate that ELEMENTAL outperforms prior work by 42.3% on task success, and
achieves 41.3% better generalization in out-of-distribution tasks, highlighting
its robustness in LfD.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.18010v2' target='_blank'>JPPO: Joint Power and Prompt Optimization for Accelerated Large Language
  Model Services</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Feiran You, Hongyang Du, Kaibin Huang, Abbas Jamalipour</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-27 03:05:32</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated remarkable capabilities in
various tasks, leading to their increasing deployment in wireless networks for
a wide variety of user services. However, the growing longer prompt setting
highlights the crucial issue of computational resource demands and huge
communication load. To address this challenge, we propose Joint Power and
Prompt Optimization (JPPO), a framework that combines Small Language Model
(SLM)-based prompt compression with wireless power allocation optimization. By
deploying SLM at user devices for prompt compression and employing Deep
Reinforcement Learning for joint optimization of compression ratio and
transmission power, JPPO effectively balances service quality with resource
efficiency. Experimental results demonstrate that our framework achieves high
service fidelity and low bit error rates while optimizing power usage in
wireless LLM services. The system reduces response time by about 17%, with the
improvement varying based on the length of the original prompt.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.17900v1' target='_blank'>Pretrained LLM Adapted with LoRA as a Decision Transformer for Offline
  RL in Quantitative Trading</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Suyeol Yun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-26 21:31:58</h6>
<p class='card-text'>Developing effective quantitative trading strategies using reinforcement
learning (RL) is challenging due to the high risks associated with online
interaction with live financial markets. Consequently, offline RL, which
leverages historical market data without additional exploration, becomes
essential. However, existing offline RL methods often struggle to capture the
complex temporal dependencies inherent in financial time series and may overfit
to historical patterns. To address these challenges, we introduce a Decision
Transformer (DT) initialized with pre-trained GPT-2 weights and fine-tuned
using Low-Rank Adaptation (LoRA). This architecture leverages the
generalization capabilities of pre-trained language models and the efficiency
of LoRA to learn effective trading policies from expert trajectories solely
from historical data. Our model performs competitively with established offline
RL algorithms, including Conservative Q-Learning (CQL), Implicit Q-Learning
(IQL), and Behavior Cloning (BC), as well as a baseline Decision Transformer
with randomly initialized GPT-2 weights and LoRA. Empirical results demonstrate
that our approach effectively learns from expert trajectories and secures
superior rewards in certain trading scenarios, highlighting the effectiveness
of integrating pre-trained language models and parameter-efficient fine-tuning
in offline RL for quantitative trading. Replication code for our experiments is
publicly available at https://github.com/syyunn/finrl-dt</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.17404v2' target='_blank'>BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical
  Modeling Problem Solving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Teng Wang, Wing-Yin Yu, Zhenqi He, Zehua Liu, Xiongwei Han, Hailei Gong, Han Wu, Wei Shi, Ruifeng She, Fangzhou Zhu, Tao Zhong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-26 13:05:53</h6>
<p class='card-text'>LLMs exhibit advanced reasoning capabilities, offering the potential to
transform natural language questions into mathematical models. However,
existing open-source datasets in operations research domain lack detailed
annotations of the modeling process, such as variable definitions, focusing
solely on objective values, which hinders reinforcement learning applications.
To address this, we release the StructuredOR dataset, annotated with
comprehensive labels that capture the complete mathematical modeling process.
We further propose BPP-Search, a algorithm that integrates reinforcement
learning into a tree-of-thought structure using Beam search, a Process reward
model, and a pairwise Preference algorithm. This approach enables efficient
exploration of tree structures, avoiding exhaustive search while improving
accuracy. Extensive experiments on StructuredOR, NL4OPT, and MAMO-ComplexLP
datasets show that BPP-Search significantly outperforms state-of-the-art
methods. In tree-based reasoning, BPP-Search excels in accuracy and efficiency,
enabling faster retrieval of correct solutions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.17135v1' target='_blank'>LLM-Based Offline Learning for Embodied Agents via Consistency-Guided
  Reward Ensemble</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yujeong Lee, Sangwoo Shin, Wei-Jin Park, Honguk Woo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-26 06:04:10</h6>
<p class='card-text'>Employing large language models (LLMs) to enable embodied agents has become
popular, yet it presents several limitations in practice. In this work, rather
than using LLMs directly as agents, we explore their use as tools for embodied
agent learning. Specifically, to train separate agents via offline
reinforcement learning (RL), an LLM is used to provide dense reward feedback on
individual actions in training datasets. In doing so, we present a
consistency-guided reward ensemble framework (CoREN), designed for tackling
difficulties in grounding LLM-generated estimates to the target environment
domain. The framework employs an adaptive ensemble of spatio-temporally
consistent rewards to derive domain-grounded rewards in the training datasets,
thus enabling effective offline learning of embodied agents in different
environment domains. Experiments with the VirtualHome benchmark demonstrate
that CoREN significantly outperforms other offline RL agents, and it also
achieves comparable performance to state-of-the-art LLM-based agents with 8B
parameters, despite CoREN having only 117M parameters for the agent policy
network and using LLMs only for training.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.16646v3' target='_blank'>Self-Generated Critiques Boost Reward Modeling for Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, Chao Zhang, Melanie Kambadur, Dhruv Mahajan, Rui Hou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-25 18:28:26</h6>
<p class='card-text'>Reward modeling is crucial for aligning large language models (LLMs) with
human preferences, especially in reinforcement learning from human feedback
(RLHF). However, current reward models mainly produce scalar scores and
struggle to incorporate critiques in a natural language format. We hypothesize
that predicting both critiques and the scalar reward would improve reward
modeling ability. Motivated by this, we propose Critic-RM, a framework that
improves reward models using self-generated critiques without extra
supervision. Critic-RM employs a two-stage process: generating and filtering
high-quality critiques, followed by joint fine-tuning on reward prediction and
critique generation. Experiments across benchmarks show that Critic-RM improves
reward modeling accuracy by 3.7%-7.3% compared to standard reward models and
LLM judges, demonstrating strong performance and data efficiency. Additional
studies further validate the effectiveness of generated critiques in rectifying
flawed reasoning steps with 2.5%-3.2% gains in improving reasoning accuracy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.16313v1' target='_blank'>CATP-LLM: Empowering Large Language Models for Cost-Aware Tool Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Duo Wu, Jinghe Wang, Yuan Meng, Yanning Zhang, Le Sun, Zhi Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-25 12:05:49</h6>
<p class='card-text'>Utilizing large language models (LLMs) for tool planning has emerged as a
promising avenue for developing general AI systems, where LLMs automatically
schedule external tools (e.g. vision models) to tackle complex tasks based on
task descriptions. To push this paradigm toward practical applications, it is
crucial for LLMs to consider tool execution costs (e.g. execution time) for
tool planning. Unfortunately, prior studies overlook the tool execution costs,
leading to the generation of expensive plans of which the costs outweigh task
performance. To fill this gap, we propose the Cost-Aware Tool Planning with
LLMs (CATP-LLM) framework, which for the first time provides a coherent design
to empower LLMs for cost-aware tool planning. Specifically, CATP-LLM
incorporates a tool planning language to enhance the LLM to generate
non-sequential plans of multiple branches for efficient concurrent tool
execution and cost reduction. Moreover, it further designs a cost-aware offline
reinforcement learning algorithm to fine-tune the LLM to optimize the
performance-cost trade-off in tool planning. In lack of public cost-related
datasets, we further present OpenCATP, the first platform for cost-aware
planning evaluation. Experiments on OpenCATP show that CATP-LLM outperforms
GPT-4 even when using Llama2-7B as its backbone, with the average improvement
of 28.2%-30.2% higher plan performance and 24.7%-45.8% lower costs even on the
challenging planning tasks. The codes of CATP-LLM and OpenCATP will be publicly
available.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.15891v1' target='_blank'>From Laws to Motivation: Guiding Exploration through Law-Based Reasoning
  and Rewards</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziyu Chen, Zhiqing Xiao, Xinbei Jiang, Junbo Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-24 15:57:53</h6>
<p class='card-text'>Large Language Models (LLMs) and Reinforcement Learning (RL) are two powerful
approaches for building autonomous agents. However, due to limited
understanding of the game environment, agents often resort to inefficient
exploration and trial-and-error, struggling to develop long-term strategies or
make decisions. We propose a method that extracts experience from interaction
records to model the underlying laws of the game environment, using these
experience as internal motivation to guide agents. These experience, expressed
in language, are highly flexible and can either assist agents in reasoning
directly or be transformed into rewards for guiding training. Our evaluation
results in Crafter demonstrate that both RL and LLM agents benefit from these
experience, leading to improved overall performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.15382v1' target='_blank'>On the Impact of Fine-Tuning on Chain-of-Thought Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Elita Lobo, Chirag Agarwal, Himabindu Lakkaraju</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-22 23:54:37</h6>
<p class='card-text'>Large language models have emerged as powerful tools for general
intelligence, showcasing advanced natural language processing capabilities that
find applications across diverse domains. Despite their impressive performance,
recent studies have highlighted the potential for significant enhancements in
LLMs' task-specific performance through fine-tuning strategies like
Reinforcement Learning with Human Feedback (RLHF), supervised fine-tuning
(SFT), and Quantized Low-Rank Adapters (Q-LoRA) method. However, previous works
have shown that while fine-tuning offers significant performance gains, it also
leads to challenges such as catastrophic forgetting and privacy and safety
risks. To this end, there has been little to no work in \textit{understanding
the impact of fine-tuning on the reasoning capabilities of LLMs}. Our research
investigates the effect of fine-tuning on the reasoning abilities of LLMs,
addressing critical questions regarding the impact of task-specific fine-tuning
on overall reasoning capabilities, the influence of fine-tuning on
Chain-of-Thought (CoT) reasoning performance, and the implications for the
faithfulness of CoT reasonings. By exploring these dimensions, our study shows
the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness
of CoT reasoning, on average across four datasets, decreases, highlighting
potential shifts in internal mechanisms of the LLMs resulting from fine-tuning
processes.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.14251v1' target='_blank'>Natural Language Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xidong Feng, Ziyu Wan, Haotian Fu, Bo Liu, Mengyue Yang, Girish A. Koushik, Zhiyuan Hu, Ying Wen, Jun Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-21 15:57:02</h6>
<p class='card-text'>Reinforcement Learning (RL) mathematically formulates decision-making with
Markov Decision Process (MDP). With MDPs, researchers have achieved remarkable
breakthroughs across various domains, including games, robotics, and language
models. This paper seeks a new possibility, Natural Language Reinforcement
Learning (NLRL), by extending traditional MDP to natural language-based
representation space. Specifically, NLRL innovatively redefines RL principles,
including task objectives, policy, value function, Bellman equation, and policy
iteration, into their language counterparts. With recent advancements in large
language models (LLMs), NLRL can be practically implemented to achieve RL-like
policy and value improvement by either pure prompting or gradient-based
training. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games
demonstrate the effectiveness, efficiency, and interpretability of the NLRL
framework among diverse use cases. Our code will be released at
https://github.com/waterhorse1/Natural-language-RL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.06791v1' target='_blank'>Enhancing Prediction Models with Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Karol Radziszewski, Piotr Ociepka</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-21 12:24:11</h6>
<p class='card-text'>We present a large-scale news recommendation system implemented at Ringier
Axel Springer Polska, focusing on enhancing prediction models with
reinforcement learning techniques. The system, named Aureus, integrates a
variety of algorithms, including multi-armed bandit methods and deep learning
models based on large language models (LLMs). We detail the architecture and
implementation of Aureus, emphasizing the significant improvements in online
metrics achieved by combining ranking prediction models with reinforcement
learning. The paper further explores the impact of different models mixing on
key business performance indicators. Our approach effectively balances the need
for personalized recommendations with the ability to adapt to rapidly changing
news content, addressing common challenges such as the cold start problem and
content freshness. The results of online evaluation demonstrate the
effectiveness of the proposed system in a real-world production environment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.17724v1' target='_blank'>Incentives to Build Houses, Trade Houses, or Trade House Building Skills
  in Simulated Worlds under Various Governing Systems or Institutions:
  Comparing Multi-agent Reinforcement Learning to Generative Agent-based Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aslan S. Dizaji</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-21 08:52:42</h6>
<p class='card-text'>It has been shown that social institutions impact human motivations to
produce different behaviours, such as amount of working or specialisation in
labor. With advancement in artificial intelligence (AI), specifically large
language models (LLMs), now it is possible to perform in-silico simulations to
test various hypotheses around this topic. Here, I simulate two somewhat
similar worlds using multi-agent reinforcement learning (MARL) framework of the
AI-Economist and generative agent-based model (GABM) framework of the
Concordia. In the extended versions of the AI-Economist and Concordia, the
agents are able to build houses, trade houses, and trade house building skill.
Moreover, along the individualistic-collectivists axis, there are a set of
three governing systems: Full-Libertarian, Semi-Libertarian/Utilitarian, and
Full-Utilitarian. Additionally, in the extended AI-Economist, the
Semi-Libertarian/Utilitarian system is further divided to a set of three
governing institutions along the discriminative axis: Inclusive, Arbitrary, and
Extractive. Building on these, I am able to show that among governing systems
and institutions of the extended AI-Economist, under the
Semi-Libertarian/Utilitarian and Inclusive government, the ratios of building
houses to trading houses and trading house building skill are higher than the
rest. Furthermore, I am able to show that in the extended Concordia when the
central government care about equality in the society, the Full-Utilitarian
system generates agents building more houses and trading more house building
skill. In contrast, these economic activities are higher under the
Full-Libertarian system when the central government cares about productivity in
the society. Overall, the focus of this paper is to compare and contrast two
advanced techniques of AI, MARL and GABM, to simulate a similar social
phenomena with limitations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.13543v1' target='_blank'>BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Davide Paglieri, Bartłomiej Cupiał, Samuel Coward, Ulyana Piterbarg, Maciej Wolczyk, Akbir Khan, Eduardo Pignatelli, Łukasz Kuciński, Lerrel Pinto, Rob Fergus, Jakob Nicolaus Foerster, Jack Parker-Holder, Tim Rocktäschel</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-20 18:54:32</h6>
<p class='card-text'>Large Language Models (LLMs) and Vision Language Models (VLMs) possess
extensive knowledge and exhibit promising reasoning abilities; however, they
still struggle to perform well in complex, dynamic environments. Real-world
tasks require handling intricate interactions, advanced spatial reasoning,
long-term planning, and continuous exploration of new strategies-areas in which
we lack effective methodologies for comprehensively evaluating these
capabilities. To address this gap, we introduce BALROG, a novel benchmark
designed to assess the agentic capabilities of LLMs and VLMs through a diverse
set of challenging games. Our benchmark incorporates a range of existing
reinforcement learning environments with varying levels of difficulty,
including tasks that are solvable by non-expert humans in seconds to extremely
challenging ones that may take years to master (e.g., the NetHack Learning
Environment). We devise fine-grained metrics to measure performance and conduct
an extensive evaluation of several popular open-source and closed-source LLMs
and VLMs. Our findings indicate that while current models achieve partial
success in the easier games, they struggle significantly with more challenging
tasks. Notably, we observe severe deficiencies in vision-based decision-making,
as models perform worse when visual representations of the environments are
provided. We release BALROG as an open and user-friendly benchmark to
facilitate future research and development in the agentic community.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.13537v1' target='_blank'>Metacognition for Unknown Situations and Environments (MUSE)</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rodolfo Valiente, Praveen K. Pilly</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-20 18:41:03</h6>
<p class='card-text'>Metacognition--the awareness and regulation of one's cognitive processes--is
central to human adaptability in unknown situations. In contrast, current
autonomous agents often struggle in novel environments due to their limited
capacity for adaptation. We hypothesize that metacognition is a critical
missing ingredient in adaptive autonomous systems, equipping them with the
cognitive flexibility needed to tackle unfamiliar challenges. Given the broad
scope of metacognitive abilities, we focus on two key aspects: competence
awareness and strategy selection for novel tasks. To this end, we propose the
Metacognition for Unknown Situations and Environments (MUSE) framework, which
integrates metacognitive processes--specifically self-awareness and
self-regulation--into autonomous agents. We present two initial implementations
of MUSE: one based on world modeling and another leveraging large language
models (LLMs), both instantiating the metacognitive cycle. Our system
continuously learns to assess its competence on a given task and uses this
self-awareness to guide iterative cycles of strategy selection. MUSE agents
show significant improvements in self-awareness and self-regulation, enabling
them to solve novel, out-of-distribution tasks more effectively compared to
Dreamer-v3-based reinforcement learning and purely prompt-based LLM agent
approaches. This work highlights the promise of approaches inspired by
cognitive and neural systems in enabling autonomous systems to adapt to new
environments, overcoming the limitations of current methods that rely heavily
on extensive training data.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.13410v1' target='_blank'>A Survey On Enhancing Reinforcement Learning in Complex Environments:
  Insights from Human and LLM Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alireza Rashidi Laleh, Majid Nili Ahmadabadi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-20 15:52:03</h6>
<p class='card-text'>Reinforcement learning (RL) is one of the active fields in machine learning,
demonstrating remarkable potential in tackling real-world challenges. Despite
its promising prospects, this methodology has encountered with issues and
challenges, hindering it from achieving the best performance. In particular,
these approaches lack decent performance when navigating environments and
solving tasks with large observation space, often resulting in
sample-inefficiency and prolonged learning times. This issue, commonly referred
to as the curse of dimensionality, complicates decision-making for RL agents,
necessitating a careful balance between attention and decision-making. RL
agents, when augmented with human or large language models' (LLMs) feedback,
may exhibit resilience and adaptability, leading to enhanced performance and
accelerated learning. Such feedback, conveyed through various modalities or
granularities including natural language, serves as a guide for RL agents,
aiding them in discerning relevant environmental cues and optimizing
decision-making processes. In this survey paper, we mainly focus on problems of
two-folds: firstly, we focus on humans or an LLMs assistance, investigating the
ways in which these entities may collaborate with the RL agent in order to
foster optimal behavior and expedite learning; secondly, we delve into the
research papers dedicated to addressing the intricacies of environments
characterized by large observation space.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.13187v3' target='_blank'>Engagement-Driven Content Generation with Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Erica Coppolillo, Federico Cinus, Marco Minici, Francesco Bonchi, Giuseppe Manco</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-20 10:40:08</h6>
<p class='card-text'>Large Language Models (LLMs) exhibit significant persuasion capabilities in
one-on-one interactions, but their influence within social networks remains
underexplored. This study investigates the potential social impact of LLMs in
these environments, where interconnected users and complex opinion dynamics
pose unique challenges. In particular, we address the following research
question: can LLMs learn to generate meaningful content that maximizes user
engagement on social networks?
  To answer this question, we define a pipeline to guide the LLM-based content
generation which employs reinforcement learning with simulated feedback. In our
framework, the reward is based on an engagement model borrowed from the
literature on opinion dynamics and information propagation. Moreover, we force
the text generated by the LLM to be aligned with a given topic and to satisfy a
minimum fluency requirement.
  Using our framework, we analyze the capabilities and limitations of LLMs in
tackling the given task, specifically considering the relative positions of the
LLM as an agent within the social network and the distribution of opinions in
the network on the given topic. Our findings show the full potential of LLMs in
creating social engagement. Notable properties of our approach are that the
learning procedure is adaptive to the opinion distribution of the underlying
network and agnostic to the specifics of the engagement model, which is
embedded as a plug-and-play component. In this regard, our approach can be
easily refined for more complex engagement tasks and interventions in
computational social science.
  The code used for the experiments is publicly available at
https://anonymous.4open.science/r/EDCG/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.12930v1' target='_blank'>LEDRO: LLM-Enhanced Design Space Reduction and Optimization for Analog
  Circuits</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dimple Vijay Kochar, Hanrui Wang, Anantha Chandrakasan, Xin Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-19 23:43:25</h6>
<p class='card-text'>Traditional approaches for designing analog circuits are time-consuming and
require significant human expertise. Existing automation efforts using methods
like Bayesian Optimization (BO) and Reinforcement Learning (RL) are sub-optimal
and costly to generalize across different topologies and technology nodes. In
our work, we introduce a novel approach, LEDRO, utilizing Large Language Models
(LLMs) in conjunction with optimization techniques to iteratively refine the
design space for analog circuit sizing. LEDRO is highly generalizable compared
to other RL and BO baselines, eliminating the need for design annotation or
model training for different topologies or technology nodes. We conduct a
comprehensive evaluation of our proposed framework and baseline on 22 different
Op-Amp topologies across four FinFET technology nodes. Results demonstrate the
superior performance of LEDRO as it outperforms our best baseline by an average
of 13% FoM improvement with 2.15x speed-up on low complexity Op-Amps and 48%
FoM improvement with 1.7x speed-up on high complexity Op-Amps. This highlights
LEDRO's effective performance, efficiency, and generalizability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.12812v1' target='_blank'>DIETS: Diabetic Insulin Management System in Everyday Life</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hanyu Zeng, Hui Ji, Pengfei Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-19 19:04:42</h6>
<p class='card-text'>People with diabetes need insulin delivery to effectively manage their blood
glucose levels, especially after meals, because their bodies either do not
produce enough insulin or cannot fully utilize it. Accurate insulin delivery
starts with estimating the nutrients in meals and is followed by developing a
detailed, personalized insulin injection strategy. These tasks are particularly
challenging in daily life, especially without professional guidance. Existing
solutions usually assume the prior knowledge of nutrients in meals and
primarily rely on feedback from professional clinicians or simulators to
develop Reinforcement Learning-based models for insulin management, leading to
extensive consumption of medical resources and difficulties in adapting the
models to new patients due to individual differences. In this paper, we propose
DIETS, a novel diabetic insulin management framework built on the transformer
architecture, to help people with diabetes effectively manage insulin delivery
in everyday life. Specifically, DIETS tailors a Large Language Model (LLM) to
estimate the nutrients in meals and employs a titration model to generate
recommended insulin injection strategies, which are further validated by a
glucose prediction model to prevent potential risks of hyperglycemia or
hypoglycemia. DIETS has been extensively evaluated on three public datasets,
and the results show it achieves superior performance in providing effective
insulin delivery recommendation to control blood glucose levels.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.12736v1' target='_blank'>ACING: Actor-Critic for Instruction Learning in Black-Box Large Language
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Salma Kharrat, Fares Fourati, Marco Canini</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-19 18:58:03</h6>
<p class='card-text'>The effectiveness of Large Language Models (LLMs) in solving tasks vastly
depends on the quality of the instructions, which often require fine-tuning
through extensive human effort. This highlights the need for automated
instruction optimization; however, this optimization is particularly
challenging when dealing with black-box LLMs, where model parameters and
gradients remain inaccessible. We propose ACING, a task-specific prompt
optimization approach framed as a stateless continuous-action Reinforcement
Learning (RL) problem, known as the continuum bandit setting. ACING leverages
an actor-critic-based method to optimize prompts, learning from
non-differentiable reward signals. We validate ACING by optimizing prompts for
ChatGPT on 30 instruction-based tasks. ACING consistently outperforms baseline
methods, achieving a median score improvement of 10 percentage points.
Furthermore, ACING not only recovers but also surpasses human-crafted expert
instructions, achieving up to a 39 percentage point improvement against human
benchmarks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.14479v1' target='_blank'>GRL-Prompt: Towards Knowledge Graph based Prompt Optimization via
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuze Liu, Tingjie Liu, Tiehua Zhang, Youhua Xia, Jinze Wang, Zhishu Shen, Jiong Jin, Fei Richard Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-19 10:52:25</h6>
<p class='card-text'>Large language models (LLMs) have demonstrated impressive success in a wide
range of natural language processing (NLP) tasks due to their extensive general
knowledge of the world. Recent works discovered that the performance of LLMs is
heavily dependent on the input prompt. However, prompt engineering is usually
done manually in a trial-and-error fashion, which can be labor-intensive and
challenging in order to find the optimal prompts. To address these problems and
unleash the utmost potential of LLMs, we propose a novel LLMs-agnostic
framework for prompt optimization, namely GRL-Prompt, which aims to
automatically construct optimal prompts via reinforcement learning (RL) in an
end-to-end manner. To provide structured action/state representation for
optimizing prompts, we construct a knowledge graph (KG) that better encodes the
correlation between the user query and candidate in-context examples.
Furthermore, a policy network is formulated to generate the optimal action by
selecting a set of in-context examples in a rewardable order to construct the
prompt. Additionally, the embedding-based reward shaping is utilized to
stabilize the RL training process. The experimental results show that
GRL-Prompt outperforms recent state-of-the-art methods, achieving an average
increase of 0.10 in ROUGE-1, 0.07 in ROUGE-2, 0.07 in ROUGE-L, and 0.05 in
BLEU.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.11059v1' target='_blank'>Financial News-Driven LLM Reinforcement Learning for Portfolio
  Management</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ananya Unnikrishnan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-17 12:46:01</h6>
<p class='card-text'>Reinforcement learning (RL) has emerged as a transformative approach for
financial trading, enabling dynamic strategy optimization in complex markets.
This study explores the integration of sentiment analysis, derived from large
language models (LLMs), into RL frameworks to enhance trading performance.
Experiments were conducted on single-stock trading with Apple Inc. (AAPL) and
portfolio trading with the ING Corporate Leaders Trust Series B (LEXCX). The
sentiment-enhanced RL models demonstrated superior net worth and cumulative
profit compared to RL models without sentiment and, in the portfolio
experiment, outperformed the actual LEXCX portfolio's buy-and-hold strategy.
These results highlight the potential of incorporating qualitative market
signals to improve decision-making, bridging the gap between quantitative and
qualitative approaches in financial trading.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.10914v2' target='_blank'>BPO: Towards Balanced Preference Optimization between Knowledge Breadth
  and Depth in Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sizhe Wang, Yongqi Tong, Hengyuan Zhang, Dawei Li, Xin Zhang, Tianlong Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-16 23:53:27</h6>
<p class='card-text'>Reinforcement Learning with Human Feedback (RLHF) is the key to the success
of large language models (LLMs) in recent years. In this work, we first
introduce the concepts of knowledge breadth and knowledge depth, which measure
the comprehensiveness and depth of an LLM or knowledge source respectively. We
reveal that the imbalance in the number of prompts and responses can lead to a
potential disparity in breadth and depth learning within alignment tuning
datasets by showing that even a simple uniform method for balancing the number
of instructions and responses can lead to significant improvements. Building on
this, we further propose Balanced Preference Optimization (BPO), designed to
dynamically augment the knowledge depth of each sample. BPO is motivated by the
observation that the usefulness of knowledge varies across samples,
necessitating tailored learning of knowledge depth. To achieve this, we
introduce gradient-based clustering, estimating the knowledge informativeness
and usefulness of each augmented sample based on the model's optimization
direction. Our experimental results across various benchmarks demonstrate that
BPO outperforms other baseline methods in alignment tuning while maintaining
training efficiency. Furthermore, we conduct a detailed analysis of each
component of BPO, providing guidelines for future research in preference data
optimization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.14457v1' target='_blank'>Guiding Reinforcement Learning Using Uncertainty-Aware Large Language
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Maryam Shoaeinaeini, Brent Harrison</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-15 22:00:29</h6>
<p class='card-text'>Human guidance in reinforcement learning (RL) is often impractical for
large-scale applications due to high costs and time constraints. Large Language
Models (LLMs) offer a promising alternative to mitigate RL sample inefficiency
and potentially replace human trainers. However, applying LLMs as RL trainers
is challenging due to their overconfidence and less reliable solutions in
sequential tasks. We address this limitation by introducing a calibrated
guidance system that uses Monte Carlo Dropout to enhance LLM advice reliability
by assessing prediction variances from multiple forward passes. Additionally,
we develop a novel RL policy shaping method based on dynamic model average
entropy to adjust the LLM's influence on RL policies according to guidance
uncertainty. This approach ensures robust RL training by relying on reliable
LLM guidance. To validate our contributions, we conduct extensive experiments
in a Minigrid environment with three goals in varying environment sizes. The
results showcase superior model performance compared to uncalibrated LLMs,
unguided RL, and calibrated LLMs with different shaping policies. Moreover, we
analyze various uncertainty estimation methods, demonstrating the effectiveness
of average entropy in reflecting higher uncertainty in incorrect guidance.
These findings highlight the persistent overconfidence in fine-tuned LLMs and
underscore the importance of effective calibration in sequential
decision-making problems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.09341v1' target='_blank'>Approximated Variational Bayesian Inverse Reinforcement Learning for
  Large Language Model Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuang Cai, Yuyu Yuan, Jinsheng Shi, Qinhong Lin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-14 10:37:34</h6>
<p class='card-text'>The alignment of large language models (LLMs) is crucial for generating
helpful and harmless content. Existing approaches leverage preference-based
human feedback data to learn the reward function and align the LLM with the
feedback data. However, these approaches focus on modeling the reward
difference between the chosen and rejected demonstrations, rather than directly
modeling the true reward from each demonstration. Moreover, these approaches
assume that the reward is only obtained at the end of the sentence, which
overlooks the modeling of intermediate rewards. These issues lead to
insufficient use of training signals in the feedback data, limiting the
representation and generalization ability of the reward and potentially
resulting in reward hacking. In this paper, we formulate LLM alignment as a
Bayesian Inverse Reinforcement Learning (BIRL) problem and propose a novel
training objective, Approximated Variational Alignment (AVA), to perform LLM
alignment through Approximated Variational Reward Imitation Learning (AVRIL).
The BIRL formulation facilitates intermediate reward modeling and direct reward
modeling on each single demonstration, which enhances the utilization of
training signals in the feedback data. Experiments show that AVA outperforms
existing LLM alignment approaches in reward modeling, RL fine-tuning, and
direct optimization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.09073v2' target='_blank'>CHAI for LLMs: Improving Code-Mixed Translation in Large Language Models
  through Reinforcement Learning with AI Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenbo Zhang, Aditya Majumdar, Amulya Yadav</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-13 22:56:00</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated remarkable capabilities across
various NLP tasks but struggle with code-mixed (or code-switched) language
understanding. For example, prior work benchmarking the performance of
multilingual LLMs on code-mixed translation tasks has demonstrated that current
state-of-the-art multilingual LLMs are ineffective in dealing with code-mixed
languages. However, the question of how to improve the capability of
multilingual LLMs to handle code-mixed language has not received any attention
to date. In this paper, we tackle this research gap by proposing CHAI, a novel
general-purpose framework for improving the ability of multilingual LLMs to
handle code-mixed languages. CHAI relies on three novel contributions made in
this paper. First, we explore the ability of LLMs to provide accurate
annotations for code-mixed translation tasks. Second, we leverage this ability
of LLMs as annotators to generate preference data for code-mixed translation
tasks at scale, which are then used within a reinforcement learning from AI
feedback (RLAIF) procedure to improve LLMs' capability on code-mixed tasks.
Third, we conduct a rigorous experimental evaluation across various real-world
datasets and settings. Our analysis shows that CHAI-powered LLMs outperform
state-of-the-art open-source LLMs by 25.66% (in terms of win rate adjudicated
by human annotators) in code-mixed translation tasks. This work represents a
first step towards developing more inclusive code-mixed LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.08862v1' target='_blank'>LLMStinger: Jailbreaking LLMs using RL fine-tuned LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Piyush Jha, Arnav Arora, Vijay Ganesh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-13 18:44:30</h6>
<p class='card-text'>We introduce LLMStinger, a novel approach that leverages Large Language
Models (LLMs) to automatically generate adversarial suffixes for jailbreak
attacks. Unlike traditional methods, which require complex prompt engineering
or white-box access, LLMStinger uses a reinforcement learning (RL) loop to
fine-tune an attacker LLM, generating new suffixes based on existing attacks
for harmful questions from the HarmBench benchmark. Our method significantly
outperforms existing red-teaming approaches (we compared against 15 of the
latest methods), achieving a +57.2% improvement in Attack Success Rate (ASR) on
LLaMA2-7B-chat and a +50.3% ASR increase on Claude 2, both models known for
their extensive safety measures. Additionally, we achieved a 94.97% ASR on
GPT-3.5 and 99.4% on Gemma-2B-it, demonstrating the robustness and adaptability
of LLMStinger across open and closed-source models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.08640v1' target='_blank'>Towards Secure Intelligent O-RAN Architecture: Vulnerabilities, Threats
  and Promising Technical Solutions using LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mojdeh Karbalaee Motalleb, Chafika Benzaid, Tarik Taleb, Marcos Katz, Vahid Shah-Mansouri, JaeSeung Song</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-13 14:31:52</h6>
<p class='card-text'>The evolution of wireless communication systems will be fundamentally
impacted by an open radio access network (O-RAN), a new concept defining an
intelligent architecture with enhanced flexibility, openness, and the ability
to slice services more efficiently. For all its promises, and like any
technological advancement, O-RAN is not without risks that need to be carefully
assessed and properly addressed to accelerate its wide adoption in future
mobile networks. In this paper, we present an in-depth security analysis of the
O-RAN architecture, discussing the potential threats that may arise in the
different O-RAN architecture layers and their impact on the Confidentiality,
Integrity, and Availability (CIA) triad. We also promote the potential of zero
trust, Moving Target Defense (MTD), blockchain, and large language models(LLM)
technologies in fortifying O-RAN's security posture. Furthermore, we
numerically demonstrate the effectiveness of MTD in empowering robust deep
reinforcement learning methods for dynamic network slice admission control in
the O-RAN architecture. Moreover, we examine the effect of explainable AI (XAI)
based on LLMs in securing the system.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.08302v1' target='_blank'>R3HF: Reward Redistribution for Enhancing Reinforcement Learning from
  Human Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiahui Li, Tai-wei Chang, Fengda Zhang, Kun Kuang, Long Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-13 02:45:21</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) provides a paradigm for
aligning large language models (LLMs) with human preferences. This involves the
initial training of a reward model based on pairwise human feedback. The reward
model is subsequently utilized in reinforcement learning to assess the scores
of each generated sentence as a whole, further guiding the optimization of
LLMs. However, current approaches have a significant shortcoming: \emph{They
allocate a single, sparse, and delayed reward to an entire sequence of output}.
This may overlook some significant individual contributions of each token
towards the desired outcome. To overcome this limitation, our paper proposes a
novel reward redistribution method called R3HF, which facilitates a more
fine-grained, token-level reward allocation. Specifically, our method treats
the reward prediction task of the reward model as a regression problem. As a
result, the redistributed rewards are computed by evaluating the specific
contribution of each token to the reward model's output. This detailed approach
improves the model's understanding of language nuances, leading to more precise
enhancements in its performance. Our method is crafted to integrate seamlessly
with most current techniques while incurring minimal computational costs.
Through comprehensive experiments across diverse datasets and tasks, we have
verified the effectiveness and superiority of our approach.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.08127v2' target='_blank'>TIPO: Text to Image with Text Presampling for Prompt Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shih-Ying Yeh, Sang-Hyun Park, Giyeong Oh, Min Song, Youngjae Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-12 19:09:45</h6>
<p class='card-text'>TIPO (Text to Image with text pre-sampling for Prompt Optimization) is an
innovative framework designed to enhance text-to-image (T2I) generation by
language model (LM) for automatic prompt engineering. By refining and extending
user-provided prompts, TIPO bridges the gap between simple inputs and the
detailed prompts required for high-quality image generation. Unlike previous
approaches that rely on Large Language Models (LLMs) or reinforcement learning
(RL), TIPO adjusts user input prompts with the distribution of a trained prompt
dataset, eliminating the need for complex runtime cost via lightweight model.
This pre-sampling approach enables efficient and scalable prompt optimization,
grounded in the model's training distribution. Experimental results demonstrate
TIPO's effectiveness in improving aesthetic scores, reducing image corruption,
and better aligning generated images with dataset distributions. These findings
highlight the critical role of prompt engineering in T2I systems and open
avenues for broader applications of automatic prompt refinement.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.07618v1' target='_blank'>Direct Preference Optimization Using Sparse Feature-Level Constraints</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qingyu Yin, Chak Tou Leong, Hongbo Zhang, Minjun Zhu, Hanqi Yan, Qiang Zhang, Yulan He, Wenjie Li, Jun Wang, Yue Zhang, Linyi Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-12 07:54:13</h6>
<p class='card-text'>The alignment of large language models (LLMs) with human preferences remains
a key challenge. While post-training techniques like Reinforcement Learning
from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have
achieved notable success, they often introduce computational inefficiencies and
training instability. In this paper, we propose Feature-level constrained
Preference Optimization (FPO), a novel method designed to simplify the
alignment process while ensuring stability. FPO leverages pre-trained Sparse
Autoencoders (SAEs) and introduces feature-level constraints, allowing for
efficient, sparsity-enforced alignment. Our approach enjoys efficiency by using
sparse features activated in a well-trained sparse autoencoder and the quality
of sequential KL divergence by using the feature-level offline reference.
Experimental results on benchmark datasets demonstrate that FPO achieves a
5.08% absolute improvement in win rate with much lower computational cost
compared to state-of-the-art baselines, making it a promising solution for
efficient and controllable LLM alignments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.07595v1' target='_blank'>Entropy Controllable Direct Preference Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Motoki Omura, Yasuhiro Fujita, Toshiki Kataoka</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-12 07:09:44</h6>
<p class='card-text'>In the post-training of large language models (LLMs), Reinforcement Learning
from Human Feedback (RLHF) is an effective approach to achieve generation
aligned with human preferences. Direct Preference Optimization (DPO) allows for
policy training with a simple binary cross-entropy loss without a reward model.
The objective of DPO is regularized by reverse KL divergence that encourages
mode-seeking fitting to the reference policy. Nonetheless, we indicate that
minimizing reverse KL divergence could fail to capture a mode of the reference
distribution, which may hurt the policy's performance. Based on this
observation, we propose a simple modification to DPO, H-DPO, which allows for
control over the entropy of the resulting policy, enhancing the distribution's
sharpness and thereby enabling mode-seeking fitting more effectively. In our
experiments, we show that H-DPO outperformed DPO across various tasks,
demonstrating superior results in pass@$k$ evaluations for mathematical tasks.
Moreover, H-DPO is simple to implement, requiring only minor modifications to
the loss calculation of DPO, which makes it highly practical and promising for
wide-ranging applications in the training of LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.07098v2' target='_blank'>A Multi-Agent Approach for REST API Testing with Semantic Graphs and
  LLM-Driven Inputs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Myeongsoo Kim, Tyler Stennett, Saurabh Sinha, Alessandro Orso</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-11 16:20:27</h6>
<p class='card-text'>As modern web services increasingly rely on REST APIs, their thorough testing
has become crucial. Furthermore, the advent of REST API documentation
languages, such as the OpenAPI Specification, has led to the emergence of many
black-box REST API testing tools. However, these tools often focus on
individual test elements in isolation (e.g., APIs, parameters, values),
resulting in lower coverage and less effectiveness in fault detection. To
address these limitations, we present AutoRestTest, the first black-box tool to
adopt a dependency-embedded multi-agent approach for REST API testing that
integrates multi-agent reinforcement learning (MARL) with a semantic property
dependency graph (SPDG) and Large Language Models (LLMs). Our approach treats
REST API testing as a separable problem, where four agents -- API, dependency,
parameter, and value agents -- collaborate to optimize API exploration. LLMs
handle domain-specific value generation, the SPDG model simplifies the search
space for dependencies using a similarity score between API operations, and
MARL dynamically optimizes the agents' behavior. Our evaluation of AutoRestTest
on 12 real-world REST services shows that it outperforms the four leading
black-box REST API testing tools, including those assisted by RESTGPT (which
generates realistic test inputs using LLMs), in terms of code coverage,
operation coverage, and fault detection. Notably, AutoRestTest is the only tool
able to trigger an internal server error in the Spotify service. Our ablation
study illustrates that each component of AutoRestTest -- the SPDG, the LLM, and
the agent-learning mechanism -- contributes to its overall effectiveness.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.05194v1' target='_blank'>Interactive Dialogue Agents via Reinforcement Learning on Hindsight
  Regenerations</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Joey Hong, Jessica Lin, Anca Dragan, Sergey Levine</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-07 21:37:51</h6>
<p class='card-text'>Recent progress on large language models (LLMs) has enabled dialogue agents
to generate highly naturalistic and plausible text. However, current LLM
language generation focuses on responding accurately to questions and requests
with a single effective response. In reality, many real dialogues are
interactive, meaning an agent's utterances will influence their conversational
partner, elicit information, or change their opinion. Accounting for how an
agent can effectively steer a conversation is a crucial ability in many
dialogue tasks, from healthcare to preference elicitation. Existing methods for
fine-tuning dialogue agents to accomplish such tasks would rely on curating
some amount of expert data. However, doing so often requires understanding the
underlying cognitive processes of the conversational partner, which is a skill
neither humans nor LLMs trained on human data can reliably do. Our key insight
is that while LLMs may not be adept at identifying effective strategies for
steering conversations a priori, or in the middle of an ongoing conversation,
they can do so post-hoc, or in hindsight, after seeing how their conversational
partner responds. We use this fact to rewrite and augment existing suboptimal
data, and train via offline reinforcement learning (RL) an agent that
outperforms both prompting and learning from unaltered human demonstrations. We
apply our approach to two domains that require understanding human mental
state, intelligent interaction, and persuasion: mental health support, and
soliciting charitable donations. Our results in a user study with real humans
show that our approach greatly outperforms existing state-of-the-art dialogue
agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.05193v2' target='_blank'>Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Joey Hong, Anca Dragan, Sergey Levine</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-07 21:36:52</h6>
<p class='card-text'>Value-based reinforcement learning (RL) can in principle learn effective
policies for a wide range of multi-turn problems, from games to dialogue to
robotic control, including via offline RL from static previously collected
datasets. However, despite the widespread use of policy gradient methods to
train large language models for single turn tasks (e.g., question answering),
value-based methods for multi-turn RL in an off-policy or offline setting have
proven particularly challenging to scale to the setting of large language
models. This setting requires effectively leveraging pretraining, scaling to
large architectures with billions of parameters, and training on large
datasets, all of which represent major challenges for current value-based RL
methods. In this work, we propose a novel offline RL algorithm that addresses
these drawbacks, casting Q-learning as a modified supervised fine-tuning (SFT)
problem where the probabilities of tokens directly translate to Q-values. In
this way we obtain an algorithm that smoothly transitions from maximizing the
likelihood of the data during pretraining to learning a near-optimal Q-function
during finetuning. Our algorithm has strong theoretical foundations, enjoying
performance bounds similar to state-of-the-art Q-learning methods, while in
practice utilizing an objective that closely resembles SFT. Because of this,
our approach can enjoy the full benefits of the pretraining of language models,
without the need to reinitialize any weights before RL finetuning, and without
the need to initialize new heads for predicting values or advantages.
Empirically, we evaluate our method on both pretrained LLMs and VLMs, on a
variety of tasks including both natural language dialogue and robotic
manipulation and navigation from images.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.03865v5' target='_blank'>AdaSociety: An Adaptive Environment with Social Structures for
  Multi-Agent Decision-Making</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yizhe Huang, Xingbo Wang, Hao Liu, Fanqi Kong, Aoyang Qin, Min Tang, Song-Chun Zhu, Mingjie Bi, Siyuan Qi, Xue Feng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-06 12:19:01</h6>
<p class='card-text'>Traditional interactive environments limit agents' intelligence growth with
fixed tasks. Recently, single-agent environments address this by generating new
tasks based on agent actions, enhancing task diversity. We consider the
decision-making problem in multi-agent settings, where tasks are further
influenced by social connections, affecting rewards and information access.
However, existing multi-agent environments lack a combination of adaptive
physical surroundings and social connections, hindering the learning of
intelligent behaviors. To address this, we introduce AdaSociety, a customizable
multi-agent environment featuring expanding state and action spaces, alongside
explicit and alterable social structures. As agents progress, the environment
adaptively generates new tasks with social structures for agents to undertake.
In AdaSociety, we develop three mini-games showcasing distinct social
structures and tasks. Initial results demonstrate that specific social
structures can promote both individual and collective benefits, though current
reinforcement learning and LLM-based algorithms show limited effectiveness in
leveraging social structures to enhance performance. Overall, AdaSociety serves
as a valuable research platform for exploring intelligence in diverse physical
and social settings. The code is available at
https://github.com/bigai-ai/AdaSociety.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.03817v3' target='_blank'>From Novice to Expert: LLM Agent Policy Optimization via Step-wise
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhirui Deng, Zhicheng Dou, Yutao Zhu, Ji-Rong Wen, Ruibin Xiong, Mang Wang, Weipeng Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-06 10:35:11</h6>
<p class='card-text'>The outstanding capabilities of large language models (LLMs) render them a
crucial component in various autonomous agent systems. While traditional
methods depend on the inherent knowledge of LLMs without fine-tuning, more
recent approaches have shifted toward the reinforcement learning strategy to
further enhance agents' ability to solve complex interactive tasks with
environments and tools. However, previous approaches are constrained by the
sparse reward issue, where existing datasets solely provide a final scalar
reward for each multi-step reasoning chain, potentially leading to
ineffectiveness and inefficiency in policy learning. In this paper, we
introduce StepAgent, which utilizes step-wise reward to optimize the agent's
reinforcement learning process. Inheriting the spirit of novice-to-expert
theory, we first compare the actions of the expert and the agent to
automatically generate intermediate rewards for fine-grained optimization.
Additionally, we propose implicit-reward and inverse reinforcement learning
techniques to facilitate agent reflection and policy adjustment. Further
theoretical analysis demonstrates that the action distribution of the agent can
converge toward the expert action distribution over multiple training cycles.
Experimental results across various datasets indicate that StepAgent
outperforms existing baseline methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.04712v1' target='_blank'>SEE-DPO: Self Entropy Enhanced Direct Preference Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shivanshu Shekhar, Shreyas Singh, Tong Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-06 02:17:33</h6>
<p class='card-text'>Direct Preference Optimization (DPO) has been successfully used to align
large language models (LLMs) according to human preferences, and more recently
it has also been applied to improving the quality of text-to-image diffusion
models. However, DPO-based methods such as SPO, Diffusion-DPO, and D3PO are
highly susceptible to overfitting and reward hacking, especially when the
generative model is optimized to fit out-of-distribution during prolonged
training. To overcome these challenges and stabilize the training of diffusion
models, we introduce a self-entropy regularization mechanism in reinforcement
learning from human feedback. This enhancement improves DPO training by
encouraging broader exploration and greater robustness. Our regularization
technique effectively mitigates reward hacking, leading to improved stability
and enhanced image quality across the latent space. Extensive experiments
demonstrate that integrating human feedback with self-entropy regularization
can significantly boost image diversity and specificity, achieving
state-of-the-art results on key image generation metrics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.02862v1' target='_blank'>The Unreasonable Effectiveness of LLMs for Query Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Peter Akioyamen, Zixuan Yi, Ryan Marcus</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-05 07:10:00</h6>
<p class='card-text'>Recent work in database query optimization has used complex machine learning
strategies, such as customized reinforcement learning schemes. Surprisingly, we
show that LLM embeddings of query text contain useful semantic information for
query optimization. Specifically, we show that a simple binary classifier
deciding between alternative query plans, trained only on a small number of
labeled embedded query vectors, can outperform existing heuristic systems.
Although we only present some preliminary results, an LLM-powered query
optimizer could provide significant benefits, both in terms of performance and
simplicity.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.02337v3' target='_blank'>WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, Tianjie Zhang, Wei Xu, Jie Tang, Yuxiao Dong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-04 17:59:58</h6>
<p class='card-text'>Large language models (LLMs) have shown remarkable potential as autonomous
agents, particularly in web-based tasks. However, existing LLM web agents
heavily rely on expensive proprietary LLM APIs, while open LLMs lack the
necessary decision-making capabilities. This paper introduces WebRL, a
self-evolving online curriculum reinforcement learning framework designed to
train high-performance web agents using open LLMs. WebRL addresses three key
challenges in building LLM web agents, including the scarcity of training
tasks, sparse feedback signals, and policy distribution drift in online
learning. Specifically, WebRL incorporates 1) a self-evolving curriculum that
generates new tasks from unsuccessful attempts, 2) a robust outcome-supervised
reward model (ORM), and 3) adaptive reinforcement learning strategies to ensure
consistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4
models into proficient web agents. On WebArena-Lite, WebRL improves the success
rate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.
These open models significantly surpass the performance of GPT-4-Turbo (17.6%)
and GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained
on open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's
effectiveness in bridging the gap between open and proprietary LLM-based web
agents, paving the way for more accessible and powerful autonomous web
interaction systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.02306v3' target='_blank'>On Targeted Manipulation and Deception when Optimizing LLMs for User
  Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Marcus Williams, Micah Carroll, Adhyyan Narang, Constantin Weisser, Brendan Murphy, Anca Dragan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-04 17:31:02</h6>
<p class='card-text'>As LLMs become more widely deployed, there is increasing interest in directly
optimizing for feedback from end users (e.g. thumbs up) in addition to feedback
from paid annotators. However, training to maximize human feedback creates a
perverse incentive structure for the AI to resort to manipulative or deceptive
tactics to obtain positive feedback from users who are vulnerable to such
strategies. We study this phenomenon by training LLMs with Reinforcement
Learning with simulated user feedback in environments of practical LLM usage.
In our settings, we find that: 1) Extreme forms of "feedback gaming" such as
manipulation and deception are learned reliably; 2) Even if only 2% of users
are vulnerable to manipulative strategies, LLMs learn to identify and target
them while behaving appropriately with other users, making such behaviors
harder to detect; 3) To mitigate this issue, it may seem promising to leverage
continued safety training or LLM-as-judges during training to filter
problematic outputs. Instead, we found that while such approaches help in some
of our settings, they backfire in others, sometimes even leading to subtler
manipulative behaviors. We hope our results can serve as a case study which
highlights the risks of using gameable feedback sources -- such as user
feedback -- as a target for RL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.02461v1' target='_blank'>Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse
  Activation Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuxin Xiao, Chaoqun Wan, Yonggang Zhang, Wenxiao Wang, Binbin Lin, Xiaofei He, Xu Shen, Jieping Ye</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-04 08:36:03</h6>
<p class='card-text'>As the development and application of Large Language Models (LLMs) continue
to advance rapidly, enhancing their trustworthiness and aligning them with
human preferences has become a critical area of research. Traditional methods
rely heavily on extensive data for Reinforcement Learning from Human Feedback
(RLHF), but representation engineering offers a new, training-free approach.
This technique leverages semantic features to control the representation of
LLM's intermediate hidden states, enabling the model to meet specific
requirements such as increased honesty or heightened safety awareness. However,
a significant challenge arises when attempting to fulfill multiple requirements
simultaneously. It proves difficult to encode various semantic contents, like
honesty and safety, into a singular semantic feature, restricting its
practicality. In this work, we address this issue through ``Sparse Activation
Control''. By delving into the intrinsic mechanisms of LLMs, we manage to
identify and pinpoint components that are closely related to specific tasks
within the model, i.e., attention heads. These heads display sparse
characteristics that allow for near-independent control over different tasks.
Our experiments, conducted on the open-source Llama series models, have yielded
encouraging results. The models were able to align with human preferences on
issues of safety, factuality, and bias concurrently.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.01834v1' target='_blank'>Align-SLM: Textless Spoken Language Models with Reinforcement Learning
  from AI Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guan-Ting Lin, Prashanth Gurunath Shivakumar, Aditya Gourav, Yile Gu, Ankur Gandhe, Hung-yi Lee, Ivan Bulyko</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-04 06:07:53</h6>
<p class='card-text'>While textless Spoken Language Models (SLMs) have shown potential in
end-to-end speech-to-speech modeling, they still lag behind text-based Large
Language Models (LLMs) in terms of semantic coherence and relevance. This work
introduces the Align-SLM framework, which leverages preference optimization
inspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the
semantic understanding of SLMs. Our approach generates multiple speech
continuations from a given prompt and uses semantic metrics to create
preference data for Direct Preference Optimization (DPO). We evaluate the
framework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling,
the spoken version of the StoryCloze dataset for semantic coherence, and other
speech generation metrics, including the GPT4-o score and human evaluation.
Experimental results show that our method achieves state-of-the-art performance
for SLMs on most benchmarks, highlighting the importance of preference
optimization to improve the semantics of SLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.01798v1' target='_blank'>SALSA: Soup-based Alignment Learning for Stronger Adaptation in RLHF</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Atoosa Chegini, Hamid Kazemi, Iman Mirzadeh, Dong Yin, Maxwell Horton, Moin Nabi, Mehrdad Farajtabar, Keivan Alizadeh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-04 04:53:43</h6>
<p class='card-text'>In Large Language Model (LLM) development, Reinforcement Learning from Human
Feedback (RLHF) is crucial for aligning models with human values and
preferences. RLHF traditionally relies on the Kullback-Leibler (KL) divergence
between the current policy and a frozen initial policy as a reference, which is
added as a penalty in policy optimization algorithms like Proximal Policy
Optimization (PPO). While this constraint prevents models from deviating too
far from the initial checkpoint, it limits exploration of the reward landscape,
reducing the model's ability to discover higher-quality solutions. As a result,
policy optimization is often trapped in a narrow region of the parameter space,
leading to suboptimal alignment and performance. This paper presents SALSA
(Soup-based Alignment Learning for Stronger Adaptation), a novel approach
designed to overcome these limitations by creating a more flexible and better
located reference model through weight-space averaging of two independent
supervised fine-tuned (SFT) models. This model soup allows for larger deviation
in KL divergence and exploring a promising region of the solution space without
sacrificing stability. By leveraging this more robust reference model, SALSA
fosters better exploration, achieving higher rewards and improving model
robustness, out-of-distribution generalization, and performance. We validate
the effectiveness of SALSA through extensive experiments on popular open models
(Llama2-7B, Mistral-7B, and Gemma-2B) across various benchmarks (MT-Bench,
Arena-Hard, UltraFeedback), where it consistently surpasses PPO by fostering
deeper exploration and achieving superior alignment in LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.01483v3' target='_blank'>Teaching Models to Improve on Tape</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Liat Bezalel, Eyal Orgad, Amir Globerson</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-03 08:49:55</h6>
<p class='card-text'>Large Language Models (LLMs) often struggle when prompted to generate content
under specific constraints. However, in such cases it is often easy to check
whether these constraints are satisfied or violated. Recent works have shown
that LLMs can benefit from such "corrective feedback". Here we claim that this
skill of LLMs can be significantly enhanced via training. We introduce an RL
framework for teaching models to use such rewards, by simulating interaction
sessions, and rewarding the model according to its ability to satisfy the
constraints. We refer to our method as CORGI (Controlled Generation with RL for
Guided Interaction), and evaluate it on a variety of controlled generation
tasks using unlabeled training data. We find that CORGI consistently
outperforms the baseline reinforcement learning method that does not
incorporate conversational feedback. Furthermore, CORGI's interactive framework
enables meta-learning, allowing the LLM to generalize better to guided
interaction in new tasks. Our results clearly show that conversational
optimization, when combined with reinforcement learning, significantly improves
the effectiveness of LLMs in controlled generation contexts.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.01245v1' target='_blank'>PMoL: Parameter Efficient MoE for Preference Mixing of LLM Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dongxu Liu, Bing Xu, Yinzhuo Chen, Bufan Xu, Wenpeng Lu, Muyun Yang, Tiejun Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-02 13:51:14</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) has been proven to be an
effective method for preference alignment of large language models (LLMs) and
is widely used in the post-training process of LLMs. However, RLHF struggles
with handling multiple competing preferences. This leads to a decrease in the
alignment of LLMs with human preferences. To address this issue, we propose
Preference Mixture of LoRAs (PMoL) from the perspective of model architecture,
which can adapt to any number of preferences to mix. PMoL combines Mixture of
Experts (MoE) and Low Rank Adaptor (LoRA). This architecture is innovatively
applied to the research of preference alignment and has achieved significant
performance improvement. The expert group soft loss is used to enable MoE with
the ability to mix preferences. Through comprehensive evaluation by the reward
model and GPT-4o, the experiment results show that PMoL has superior preference
mixing capabilities compared to baseline methods. PMoL achieves better
preference alignment with lower training costs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.01111v1' target='_blank'>Rule Based Rewards for Language Model Safety</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tong Mu, Alec Helyar, Johannes Heidecke, Joshua Achiam, Andrea Vallone, Ian Kivlichan, Molly Lin, Alex Beutel, John Schulman, Lilian Weng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-02 02:22:21</h6>
<p class='card-text'>Reinforcement learning based fine-tuning of large language models (LLMs) on
human preferences has been shown to enhance both their capabilities and safety
behavior. However, in cases related to safety, without precise instructions to
human annotators, the data collected may cause the model to become overly
cautious, or to respond in an undesirable style, such as being judgmental.
Additionally, as model capabilities and usage patterns evolve, there may be a
costly need to add or relabel data to modify safety behavior. We propose a
novel preference modeling approach that utilizes AI feedback and only requires
a small amount of human data. Our method, Rule Based Rewards (RBR), uses a
collection of rules for desired or undesired behaviors (e.g. refusals should
not be judgmental) along with a LLM grader. In contrast to prior methods using
AI feedback, our method uses fine-grained, composable, LLM-graded few-shot
prompts as reward directly in RL training, resulting in greater control,
accuracy and ease of updating. We show that RBRs are an effective training
method, achieving an F1 score of 97.1, compared to a human-feedback baseline of
91.7, resulting in much higher safety-behavior accuracy through better
balancing usefulness and safety.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.00722v1' target='_blank'>Token-level Proximal Policy Optimization for Query Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yichen Ouyang, Lu Wang, Fangkai Yang, Pu Zhao, Chenghua Huang, Jianfeng Liu, Bochen Pang, Yaming Yang, Yuefeng Zhan, Hao Sun, Qingwei Lin, Saravan Rajmohan, Weiwei Deng, Dongmei Zhang, Feng Sun, Qi Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-01 16:36:14</h6>
<p class='card-text'>Query generation is a critical task for web search engines (e.g. Google,
Bing) and recommendation systems. Recently, state-of-the-art query generation
methods leverage Large Language Models (LLMs) for their strong capabilities in
context understanding and text generation. However, they still face challenges
in generating high-quality queries in terms of inferring user intent based on
their web search interaction history. In this paper, we propose Token-level
Proximal Policy Optimization (TPPO), a noval approach designed to empower LLMs
perform better in query generation through fine-tuning. TPPO is based on the
Reinforcement Learning from AI Feedback (RLAIF) paradigm, consisting of a
token-level reward model and a token-level proximal policy optimization module
to address the sparse reward challenge in traditional RLAIF frameworks. To
evaluate the effectiveness and robustness of TPPO, we conducted experiments on
both open-source dataset and an industrial dataset that was collected from a
globally-used search engine. The experimental results demonstrate that TPPO
significantly improves the performance of query generation for LLMs and
outperforms its existing competitors.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.00418v1' target='_blank'>Self-Evolved Reward Learning for LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chenghua Huang, Zhizhen Fan, Lu Wang, Fangkai Yang, Pu Zhao, Zeqi Lin, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-01 07:29:03</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) is a crucial technique for
aligning language models with human preferences, playing a pivotal role in the
success of conversational models like GPT-4, ChatGPT, and Llama 2. A core
challenge in employing RLHF lies in training a reliable reward model (RM),
which relies on high-quality labels typically provided by human experts or
advanced AI system. These methods can be costly and may introduce biases that
affect the language model's responses. As language models improve, human input
may become less effective in further enhancing their performance. In this
paper, we propose Self-Evolved Reward Learning (SER), a novel approach where
the RM generates additional training data to iteratively improve itself. We
conducted extensive experiments on multiple datasets such as HH-RLHF and
UltraFeedback, using models like Mistral and Llama 3, and compare SER against
various baselines. Our results demonstrate that even with limited
human-annotated data, learning from self-feedback can robustly enhance RM
performance, thereby boosting the capabilities of large language models (LLMs).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.24152v1' target='_blank'>Language-Driven Policy Distillation for Cooperative Driving in
  Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiaqi Liu, Chengkai Xu, Peng Hang, Jian Sun, Mingyu Ding, Wei Zhan, Masayoshi Tomizuka</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-31 17:10:01</h6>
<p class='card-text'>The cooperative driving technology of Connected and Autonomous Vehicles
(CAVs) is crucial for improving the efficiency and safety of transportation
systems. Learning-based methods, such as Multi-Agent Reinforcement Learning
(MARL), have demonstrated strong capabilities in cooperative decision-making
tasks. However, existing MARL approaches still face challenges in terms of
learning efficiency and performance. In recent years, Large Language Models
(LLMs) have rapidly advanced and shown remarkable abilities in various
sequential decision-making tasks. To enhance the learning capabilities of
cooperative agents while ensuring decision-making efficiency and
cost-effectiveness, we propose LDPD, a language-driven policy distillation
method for guiding MARL exploration. In this framework, a teacher agent based
on LLM trains smaller student agents to achieve cooperative decision-making
through its own decision-making demonstrations. The teacher agent enhances the
observation information of CAVs and utilizes LLMs to perform complex
cooperative decision-making reasoning, which also leverages carefully designed
decision-making tools to achieve expert-level decisions, providing high-quality
teaching experiences. The student agent then refines the teacher's prior
knowledge into its own model through gradient policy updates. The experiments
demonstrate that the students can rapidly improve their capabilities with
minimal guidance from the teacher and eventually surpass the teacher's
performance. Extensive experiments show that our approach demonstrates better
performance and learning efficiency compared to baseline methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.24096v1' target='_blank'>Progressive Safeguards for Safe and Model-Agnostic Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nabil Omi, Hosein Hasanbeig, Hiteshi Sharma, Sriram K. Rajamani, Siddhartha Sen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-31 16:28:33</h6>
<p class='card-text'>In this paper we propose a formal, model-agnostic meta-learning framework for
safe reinforcement learning. Our framework is inspired by how parents safeguard
their children across a progression of increasingly riskier tasks, imparting a
sense of safety that is carried over from task to task. We model this as a
meta-learning process where each task is synchronized with a safeguard that
monitors safety and provides a reward signal to the agent. The safeguard is
implemented as a finite-state machine based on a safety specification; the
reward signal is formally shaped around this specification. The safety
specification and its corresponding safeguard can be arbitrarily complex and
non-Markovian, which adds flexibility to the training process and
explainability to the learned policy. The design of the safeguard is manual but
it is high-level and model-agnostic, which gives rise to an end-to-end safe
learning approach with wide applicability, from pixel-level game control to
language model fine-tuning. Starting from a given set of safety specifications
(tasks), we train a model such that it can adapt to new specifications using
only a small number of training samples. This is made possible by our method
for efficiently transferring safety bias between tasks, which effectively
minimizes the number of safety violations. We evaluate our framework in a
Minecraft-inspired Gridworld, a VizDoom game environment, and an LLM
fine-tuning application. Agents trained with our approach achieve near-minimal
safety violations, while baselines are shown to underperform.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.23912v1' target='_blank'>RL-STaR: Theoretical Analysis of Reinforcement Learning Frameworks for
  Self-Taught Reasoner</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fu-Chieh Chang, Yu-Ting Lee, Hui-Ying Shih, Pei-Yuan Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-31 13:17:53</h6>
<p class='card-text'>The reasoning abilities of large language models (LLMs) have improved with
chain-of-thought (CoT) prompting, allowing models to solve complex tasks in a
stepwise manner. However, training CoT capabilities requires detailed reasoning
data, which is often scarce. The self-taught reasoner (STaR) framework
addresses this by using reinforcement learning to automatically generate
reasoning steps, reducing reliance on human-labeled data. Although STaR and its
variants have demonstrated empirical success, a theoretical foundation
explaining these improvements is lacking. This work provides a theoretical
framework for understanding the effectiveness of reinforcement learning on CoT
reasoning and STaR. Our contributions are: (1) an analysis of policy
improvement, showing why LLM reasoning improves iteratively with STaR; (2)
conditions for convergence to an optimal reasoning policy; (3) an examination
of STaR's robustness, explaining how it can improve reasoning even when
incorporating occasional incorrect steps; and (4) criteria for the quality of
pre-trained models necessary to initiate effective reasoning improvement. This
framework aims to bridge empirical findings with theoretical insights,
advancing reinforcement learning approaches for reasoning in LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.23605v2' target='_blank'>Dynamic Uncertainty Ranking: Enhancing Retrieval-Augmented In-Context
  Learning for Long-Tail Knowledge in LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shuyang Yu, Runxue Bao, Parminder Bhatia, Taha Kass-Hout, Jiayu Zhou, Cao Xiao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-31 03:42:17</h6>
<p class='card-text'>Large language models (LLMs) can learn vast amounts of knowledge from diverse
domains during pre-training. However, long-tail knowledge from specialized
domains is often scarce and underrepresented, rarely appearing in the models'
memorization. Prior work has shown that in-context learning (ICL) with
retriever augmentation can help LLMs better capture long-tail knowledge,
reducing their reliance on pre-trained data. Despite these advances, we observe
that LLM predictions for long-tail questions remain uncertain to variations in
retrieved samples. To take advantage of the uncertainty in ICL for guiding LLM
predictions toward correct answers on long-tail samples, we propose a
reinforcement learning-based dynamic uncertainty ranking method for ICL that
accounts for the varying impact of each retrieved sample on LLM predictions.
Our approach prioritizes more informative and stable samples while demoting
misleading ones, updating rankings based on the feedback from the LLM w.r.t.
each retrieved sample. To enhance training efficiency and reduce query costs,
we introduce a learnable dynamic ranking threshold, adjusted when the model
encounters negative prediction shifts. Experimental results on various
question-answering datasets from different domains show that our method
outperforms the best baseline by $2.76\%$, with a notable $5.96\%$ boost in
accuracy on long-tail questions that elude zero-shot inference.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.23535v1' target='_blank'>Simulating User Agents for Embodied Conversational-AI</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Daniel Philipov, Vardhan Dongre, Gokhan Tur, Dilek Hakkani-Tür</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-31 00:56:08</h6>
<p class='card-text'>Embodied agents designed to assist users with tasks must engage in natural
language interactions, interpret instructions, execute actions, and communicate
effectively to resolve issues. However, collecting large-scale, diverse
datasets of situated human-robot dialogues to train and evaluate such agents is
expensive, labor-intensive, and time-consuming. To address this challenge, we
propose building a large language model (LLM)-based user agent that can
simulate user behavior during interactions with an embodied agent in a virtual
environment. Given a user goal (e.g., make breakfast), at each time step, the
user agent may observe" the robot actions or speak" to either intervene with
the robot or answer questions. Such a user agent assists in improving the
scalability and efficiency of embodied dialogues dataset generation and is
critical for enhancing and evaluating the robot's interaction and task
completion ability, as well as for research in reinforcement learning using AI
feedback. We evaluate our user agent's ability to generate human-like behaviors
by comparing its simulated dialogues with the TEACh dataset. We perform three
experiments: zero-shot prompting to predict dialogue acts, few-shot prompting,
and fine-tuning on the TEACh training subset. Results show the LLM-based user
agent achieves an F-measure of 42% with zero-shot prompting and 43.4% with
few-shot prompting in mimicking human speaking behavior. Through fine-tuning,
performance in deciding when to speak remained stable, while deciding what to
say improved from 51.1% to 62.5%. These findings showcase the feasibility of
the proposed approach for assessing and enhancing the effectiveness of robot
task completion through natural language communication.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.23426v1' target='_blank'>Social Science Meets LLMs: How Reliable Are Large Language Models in
  Social Simulations?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yue Huang, Zhengqing Yuan, Yujun Zhou, Kehan Guo, Xiangqi Wang, Haomin Zhuang, Weixiang Sun, Lichao Sun, Jindong Wang, Yanfang Ye, Xiangliang Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-30 20:09:37</h6>
<p class='card-text'>Large Language Models (LLMs) are increasingly employed for simulations,
enabling applications in role-playing agents and Computational Social Science
(CSS). However, the reliability of these simulations is under-explored, which
raises concerns about the trustworthiness of LLMs in these applications. In
this paper, we aim to answer ``How reliable is LLM-based simulation?'' To
address this, we introduce TrustSim, an evaluation dataset covering 10
CSS-related topics, to systematically investigate the reliability of the LLM
simulation. We conducted experiments on 14 LLMs and found that inconsistencies
persist in the LLM-based simulated roles. In addition, the consistency level of
LLMs does not strongly correlate with their general performance. To enhance the
reliability of LLMs in simulation, we proposed Adaptive Learning Rate Based
ORPO (AdaORPO), a reinforcement learning-based algorithm to improve the
reliability in simulation across 7 LLMs. Our research provides a foundation for
future studies to explore more robust and trustworthy LLM-based simulations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.23242v2' target='_blank'>A little less conversation, a little more action, please: Investigating
  the physical common-sense of LLMs in a 3D embodied environment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Matteo G. Mecattaf, Ben Slater, Marko Tešić, Jonathan Prunty, Konstantinos Voudouris, Lucy G. Cheke</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-30 17:28:28</h6>
<p class='card-text'>As general-purpose tools, Large Language Models (LLMs) must often reason
about everyday physical environments. In a question-and-answer capacity,
understanding the interactions of physical objects may be necessary to give
appropriate responses. Moreover, LLMs are increasingly used as reasoning
engines in agentic systems, designing and controlling their action sequences.
The vast majority of research has tackled this issue using static benchmarks,
comprised of text or image-based questions about the physical world. However,
these benchmarks do not capture the complexity and nuance of real-life physical
processes. Here we advocate for a second, relatively unexplored, approach:
'embodying' the LLMs by granting them control of an agent within a 3D
environment. We present the first embodied and cognitively meaningful
evaluation of physical common-sense reasoning in LLMs. Our framework allows
direct comparison of LLMs with other embodied agents, such as those based on
Deep Reinforcement Learning, and human and non-human animals. We employ the
Animal-AI (AAI) environment, a simulated 3D virtual laboratory, to study
physical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a
suite of experiments that replicate laboratory studies with non-human animals,
to study physical reasoning capabilities including distance estimation,
tracking out-of-sight objects, and tool use. We demonstrate that
state-of-the-art multi-modal models with no finetuning can complete this style
of task, allowing meaningful comparison to the entrants of the 2019 Animal-AI
Olympics competition and to human children. Our results show that LLMs are
currently outperformed by human children on these tasks. We argue that this
approach allows the study of physical reasoning using ecologically valid
experiments drawn directly from cognitive science, improving the predictability
and reliability of LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.23223v1' target='_blank'>COMAL: A Convergent Meta-Algorithm for Aligning LLMs with General
  Preferences</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yixin Liu, Argyris Oikonomou, Weiqiang Zheng, Yang Cai, Arman Cohan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-30 17:13:02</h6>
<p class='card-text'>Many alignment methods, including reinforcement learning from human feedback
(RLHF), rely on the Bradley-Terry reward assumption, which is insufficient to
capture the full range of general human preferences. To achieve robust
alignment with general preferences, we model the alignment problem as a
two-player zero-sum game, where the Nash equilibrium policy guarantees a 50%
win rate against any competing policy. However, previous algorithms for finding
the Nash policy either diverge or converge to a Nash policy in a modified game,
even in a simple synthetic setting, thereby failing to maintain the 50% win
rate guarantee against all other policies. We propose a meta-algorithm,
Convergent Meta Alignment Algorithm (COMAL), for language model alignment with
general preferences, inspired by convergent algorithms in game theory.
Theoretically, we prove that our meta-algorithm converges to an exact Nash
policy in the last iterate. Additionally, our meta-algorithm is simple and can
be integrated with many existing methods designed for RLHF and preference
optimization with minimal changes. Experimental results demonstrate the
effectiveness of the proposed framework when combined with existing preference
policy optimization methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.23214v2' target='_blank'>Grounding by Trying: LLMs with Reinforcement Learning-Enhanced Retrieval</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sheryl Hsu, Omar Khattab, Chelsea Finn, Archit Sharma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-30 17:02:54</h6>
<p class='card-text'>The hallucinations of large language models (LLMs) are increasingly mitigated
by allowing LLMs to search for information and to ground their answers in real
sources. Unfortunately, LLMs often struggle with posing the right search
queries, especially when dealing with complex or otherwise indirect topics.
Observing that LLMs can learn to search for relevant facts by $\textit{trying}$
different queries and learning to up-weight queries that successfully produce
relevant results, we introduce $\underline{Le}$arning to $\underline{Re}$trieve
by $\underline{T}$rying (LeReT), a reinforcement learning framework that
explores search queries and uses preference-based optimization to improve their
quality. LeReT can improve the absolute retrieval accuracy by up to 29% and the
downstream generator evaluations by 17%. The simplicity and flexibility of
LeReT allows it to be applied to arbitrary off-the-shelf retrievers and makes
it a promising technique for improving general LLM pipelines. Project website:
http://sherylhsu.com/LeReT/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.23022v2' target='_blank'>Online Intrinsic Rewards for Decision Making Agents from Large Language
  Model Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qinqing Zheng, Mikael Henaff, Amy Zhang, Aditya Grover, Brandon Amos</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-30 13:52:43</h6>
<p class='card-text'>Automatically synthesizing dense rewards from natural language descriptions
is a promising paradigm in reinforcement learning (RL), with applications to
sparse reward problems, open-ended exploration, and hierarchical skill design.
Recent works have made promising steps by exploiting the prior knowledge of
large language models (LLMs). However, these approaches suffer from important
limitations: they are either not scalable to problems requiring billions of
environment samples, due to requiring LLM annotations for each observation, or
they require a diverse offline dataset, which may not exist or be impossible to
collect. In this work, we address these limitations through a combination of
algorithmic and systems-level contributions. We propose \oni, a distributed
architecture that simultaneously learns an RL policy and an intrinsic reward
function using LLM feedback. Our approach annotates the agent's collected
experience via an asynchronous LLM server, which is then distilled into an
intrinsic reward model. We explore a range of algorithmic choices for reward
modeling with varying complexity, including hashing, classification, and
ranking models. By studying their relative tradeoffs, we shed light on
questions regarding intrinsic reward design for sparse reward problems. Our
approach achieves state-of-the-art performance across a range of challenging,
sparse reward tasks from the NetHack Learning Environment in a simple unified
process, solely using the agent's gathered experience, without requiring
external datasets. We make our code available at
\url{https://github.com/facebookresearch/oni}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.22657v1' target='_blank'>Automatic programming via large language models with population
  self-evolution for dynamic job shop scheduling problem</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jin Huang, Xinyu Li, Liang Gao, Qihao Liu, Yue Teng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-30 02:54:31</h6>
<p class='card-text'>Heuristic dispatching rules (HDRs) are widely regarded as effective methods
for solving dynamic job shop scheduling problems (DJSSP) in real-world
production environments. However, their performance is highly
scenario-dependent, often requiring expert customization. To address this,
genetic programming (GP) and gene expression programming (GEP) have been
extensively used for automatic algorithm design. Nevertheless, these approaches
often face challenges due to high randomness in the search process and limited
generalization ability, hindering the application of trained dispatching rules
to new scenarios or dynamic environments. Recently, the integration of large
language models (LLMs) with evolutionary algorithms has opened new avenues for
prompt engineering and automatic algorithm design. To enhance the capabilities
of LLMs in automatic HDRs design, this paper proposes a novel population
self-evolutionary (SeEvo) method, a general search framework inspired by the
self-reflective design strategies of human experts. The SeEvo method
accelerates the search process and enhances exploration capabilities.
Experimental results show that the proposed SeEvo method outperforms GP, GEP,
end-to-end deep reinforcement learning methods, and more than 10 common HDRs
from the literature, particularly in unseen and dynamic scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.08899v1' target='_blank'>FinVision: A Multi-Agent Framework for Stock Market Prediction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sorouralsadat Fatemi, Yuheng Hu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-29 06:02:28</h6>
<p class='card-text'>Financial trading has been a challenging task, as it requires the integration
of vast amounts of data from various modalities. Traditional deep learning and
reinforcement learning methods require large training data and often involve
encoding various data types into numerical formats for model input, which
limits the explainability of model behavior. Recently, LLM-based agents have
demonstrated remarkable advancements in handling multi-modal data, enabling
them to execute complex, multi-step decision-making tasks while providing
insights into their thought processes. This research introduces a multi-modal
multi-agent system designed specifically for financial trading tasks. Our
framework employs a team of specialized LLM-based agents, each adept at
processing and interpreting various forms of financial data, such as textual
news reports, candlestick charts, and trading signal charts. A key feature of
our approach is the integration of a reflection module, which conducts analyses
of historical trading signals and their outcomes. This reflective process is
instrumental in enhancing the decision-making capabilities of the system for
future trading scenarios. Furthermore, the ablation studies indicate that the
visual reflection module plays a crucial role in enhancing the decision-making
capabilities of our framework.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.21545v2' target='_blank'>CARMO: Dynamic Criteria Generation for Context-Aware Reward Modelling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Taneesh Gupta, Shivam Shandilya, Xuchao Zhang, Rahul Madhavan, Supriyo Ghosh, Chetan Bansal, Huaxiu Yao, Saravan Rajmohan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-28 21:18:49</h6>
<p class='card-text'>Reward modeling in large language models is susceptible to reward hacking,
causing models to latch onto superficial features such as the tendency to
generate lists or unnecessarily long responses. In reinforcement learning from
human feedback (RLHF) and more generally during post-training flawed reward
signals often lead to outputs that optimize for these spurious correlates
instead of genuine quality or correctness. We propose Context-Aware Reward
Modeling (CARMO), a novel approach that first generates dynamic,
context-relevant criteria to ground the reward model before producing reward
scores. Unlike prior methods that rely on static rubrics, CARMO leverages large
language models (LLMs) to adaptively create evaluation criteria such as logical
consistency, clarity, and depth tailored to the user query. Our theoretical
analysis shows that such criteria generation can mitigate reward hacking. We
further demonstrate that CARMO can be distilled into smaller models, reducing
the computational cost of alignment. We establish a new state-of-the-art
performance in zero-shot settings for generative models, achieving a 2.1\%
improvement on Reward Bench. Furthermore, alignment performed on the
CARMO-curated preference dataset achieves 22.5\% and 21.1\% LC-WR and WR,
respectively, on Mistral-Base (7B).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.21252v1' target='_blank'>LongReward: Improving Long-context Large Language Models with AI
  Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiajie Zhang, Zhongni Hou, Xin Lv, Shulin Cao, Zhenyu Hou, Yilin Niu, Lei Hou, Yuxiao Dong, Ling Feng, Juanzi Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-28 17:50:42</h6>
<p class='card-text'>Though significant advancements have been achieved in developing long-context
large language models (LLMs), the compromised quality of LLM-synthesized data
for supervised fine-tuning (SFT) often affects the long-context performance of
SFT models and leads to inherent limitations. In principle, reinforcement
learning (RL) with appropriate reward signals can further enhance models'
capacities. However, how to obtain reliable rewards in long-context scenarios
remains unexplored. To this end, we propose LongReward, a novel method that
utilizes an off-the-shelf LLM to provide rewards for long-context model
responses from four human-valued dimensions: helpfulness, logicality,
faithfulness, and completeness, each with a carefully designed assessment
pipeline. By combining LongReward and offline RL algorithm DPO, we are able to
effectively improve long-context SFT models. Our experiments indicate that
LongReward not only significantly improves models' long-context performance but
also enhances their ability to follow short instructions. We also find that
long-context DPO with LongReward and conventional short-context DPO can be used
together without hurting either one's performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.21349v3' target='_blank'>FALCON: Feedback-driven Adaptive Long/short-term memory reinforced
  Coding Optimization system</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zeyuan Li, Yangfan He, Lewei He, Jianhui Wang, Tianyu Shi, Bin Lei, Yuchen Li, Qiuwu Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-28 12:18:22</h6>
<p class='card-text'>Recently, large language models (LLMs) have achieved significant progress in
automated code generation. Despite their strong instruction-following
capabilities, these models frequently struggled to align with user intent in
coding scenarios. In particular, they were hampered by datasets that lacked
diversity and failed to address specialized tasks or edge cases. Furthermore,
challenges in supervised fine-tuning (SFT) and reinforcement learning from
human feedback (RLHF) led to failures in generating precise,
human-intent-aligned code. To tackle these challenges and improve the code
generation performance for automated programming systems, we propose
Feedback-driven Adaptive Long/short-term memory reinforced Coding Optimization
(i.e., FALCON). FALCON is structured into two hierarchical levels. From the
global level, long-term memory improves code quality by retaining and applying
learned knowledge. At the local level, short-term memory allows for the
incorporation of immediate feedback from compilers and AI systems.
Additionally, we introduce meta-reinforcement learning with feedback rewards to
solve the global-local bi-level optimization problem and enhance the model's
adaptability across diverse code generation tasks. Extensive experiments
demonstrate that our technique achieves state-of-the-art performance, leading
other reinforcement learning methods by more than 4.5 percentage points on the
MBPP benchmark and 6.1 percentage points on the Humaneval benchmark. The
open-sourced code is publicly available at https://github.com/titurte/FALCON.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.20869v1' target='_blank'>Reward Modeling with Weak Supervision for Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ben Hauptvogel, Malte Ostendorff, Georg Rehm, Sebastian Möller</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-28 09:37:58</h6>
<p class='card-text'>Recent advancements in large language models (LLMs) have led to their
increased application across various tasks, with reinforcement learning from
human feedback (RLHF) being a crucial part of their training to align responses
with user intentions. In the RLHF process, a reward model is trained using
responses preferences determined by human labelers or AI systems, which then
refines the LLM through reinforcement learning. This work introduces weak
supervision as a strategy to extend RLHF datasets and enhance reward model
performance. Weak supervision employs noisy or imprecise data labeling,
reducing reliance on expensive manually labeled data. By analyzing RLHF
datasets to identify heuristics that correlate with response preference, we
wrote simple labeling functions and then calibrated a label model to weakly
annotate unlabeled data. Our evaluation show that while weak supervision
significantly benefits smaller datasets by improving reward model performance,
its effectiveness decreases with larger, originally labeled datasets.
Additionally, using an LLM to generate and then weakly label responses offers a
promising method for extending preference data.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.00816v1' target='_blank'>CycleResearcher: Improving Automated Research via Automated Review</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, Linyi Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-28 08:10:21</h6>
<p class='card-text'>The automation of scientific discovery has been a long-standing goal within
the research community, driven by the potential to accelerate knowledge
creation. While significant progress has been made using commercial large
language models (LLMs) as research assistants or idea generators, the
possibility of automating the entire research process with open-source LLMs
remains largely unexplored. This paper explores the feasibility of using
open-source post-trained LLMs as autonomous agents capable of performing the
full cycle of automated research and review, from literature review and
manuscript preparation to peer review and paper revision. Our iterative
preference training framework consists of CycleResearcher, which conducts
research tasks, and CycleReviewer, which simulates the peer review process,
providing iterative feedback via reinforcement learning. To train these models,
we develop two new datasets, Review-5k and Research-14k, reflecting real-world
machine learning research and peer review dynamics. Our results demonstrate
that CycleReviewer achieves a 26.89\% improvement in mean absolute error (MAE)
over individual human reviewers in predicting paper scores, indicating that
LLMs can surpass expert-level performance in research evaluation. In research,
the papers generated by the CycleResearcher model achieved a score of 5.36 in
simulated peer reviews, surpassing the preprint level of 5.24 from human
experts and approaching the accepted paper level of 5.69. This work represents
a significant step toward fully automated scientific inquiry, providing ethical
safeguards and advancing AI-driven research capabilities. The code, dataset and
model weight are released at \url{http://github/minjun-zhu/Researcher}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.20187v1' target='_blank'>Uncertainty-Penalized Direct Preference Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sam Houliston, Alizée Pace, Alexander Immer, Gunnar Rätsch</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-26 14:24:37</h6>
<p class='card-text'>Aligning Large Language Models (LLMs) to human preferences in content, style,
and presentation is challenging, in part because preferences are varied,
context-dependent, and sometimes inherently ambiguous. While successful,
Reinforcement Learning from Human Feedback (RLHF) and Direct Preference
Optimization (DPO) are prone to the issue of proxy reward overoptimization.
Analysis of the DPO loss reveals a critical need for regularization for
mislabeled or ambiguous preference pairs to avoid reward hacking. In this work,
we develop a pessimistic framework for DPO by introducing preference
uncertainty penalization schemes, inspired by offline reinforcement learning.
The penalization serves as a correction to the loss which attenuates the loss
gradient for uncertain samples. Evaluation of the methods is performed with
GPT2 Medium on the Anthropic-HH dataset using a model ensemble to obtain
uncertainty estimates, and shows improved overall performance compared to
vanilla DPO, as well as better completions on prompts from high-uncertainty
chosen/rejected responses.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.20147v1' target='_blank'>GFlowNet Fine-tuning for Diverse Correct Solutions in Mathematical
  Reasoning Tasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ryoichi Takase, Masaya Tsunokake, Yuta Tsuchiya, Shota Inuzuka</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-26 11:13:33</h6>
<p class='card-text'>Mathematical reasoning problems are among the most challenging, as they
typically require an understanding of fundamental laws to solve. The laws are
universal, but the derivation of the final answer changes depending on how a
problem is approached. When training large language models (LLMs), learning the
capability of generating such multiple solutions is essential to accelerate
their use in mathematical education. To this end, we train LLMs using
generative flow network (GFlowNet). Different from reward-maximizing
reinforcement learning (RL), GFlowNet fine-tuning seeks to find diverse
solutions by training the LLM whose distribution is proportional to a reward
function. In numerical experiments, we evaluate GFlowNet fine-tuning and
reward-maximizing RL in terms of accuracy and diversity. The results show that
GFlowNet fine-tuning derives correct final answers from diverse intermediate
reasoning steps, indicating the improvement of the capability of alternative
solution generation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.19933v2' target='_blank'>Enhancing Safety in Reinforcement Learning with Human Feedback via
  Rectified Policy Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiyue Peng, Hengquan Guo, Jiawei Zhang, Dongqing Zou, Ziyu Shao, Honghao Wei, Xin Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-25 19:08:23</h6>
<p class='card-text'>Balancing helpfulness and safety (harmlessness) is a critical challenge in
aligning large language models (LLMs). Current approaches often decouple these
two objectives, training separate preference models for helpfulness and safety,
while framing safety as a constraint within a constrained Markov Decision
Process (CMDP) framework. This paper identifies a potential issue when using
the widely adopted expected safety constraints for LLM safety alignment, termed
"safety compensation", where the constraints are satisfied on expectation, but
individual prompts may trade off safety, resulting in some responses being
overly restrictive while others remain unsafe. To address this issue, we
propose Rectified Policy Optimization (RePO), which replaces the expected
safety constraint with critical safety constraints imposed on every prompt. At
the core of RePO is a policy update mechanism driven by rectified policy
gradients, which penalizes the strict safety violation of every prompt, thereby
enhancing safety across nearly all prompts. Our experiments demonstrate that
RePO outperforms strong baseline methods and significantly enhances LLM safety
alignment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.19920v2' target='_blank'>Reinforcement Learning for Aligning Large Language Models Agents with
  Interactive Environments: Quantifying and Mitigating Prompt Overfitting</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mohamed Salim Aissi, Clement Romac, Thomas Carta, Sylvain Lamprier, Pierre-Yves Oudeyer, Olivier Sigaud, Laure Soulier, Nicolas Thome</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-25 18:25:35</h6>
<p class='card-text'>Reinforcement learning (RL) is a promising approach for aligning large
language models (LLMs) knowledge with sequential decision-making tasks.
However, few studies have thoroughly investigated the impact on LLM agents
capabilities of fine-tuning them with RL in a specific environment. In this
paper, we propose a novel framework to analyze the sensitivity of LLMs to
prompt formulations following RL training in a textual environment. Our
findings reveal that the performance of LLMs degrades when faced with prompt
formulations different from those used during the RL training phase. Besides,
we analyze the source of this sensitivity by examining the model's internal
representations and salient tokens. Finally, we propose to use a contrastive
loss to mitigate this sensitivity and improve the robustness and generalization
capabilities of LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.19586v1' target='_blank'>Diverse Sign Language Translation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xin Shen, Lei Shen, Shaozu Yuan, Heming Du, Haiyang Sun, Xin Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-25 14:28:20</h6>
<p class='card-text'>Like spoken languages, a single sign language expression could correspond to
multiple valid textual interpretations. Hence, learning a rigid one-to-one
mapping for sign language translation (SLT) models might be inadequate,
particularly in the case of limited data. In this work, we introduce a Diverse
Sign Language Translation (DivSLT) task, aiming to generate diverse yet
accurate translations for sign language videos. Firstly, we employ large
language models (LLM) to generate multiple references for the widely-used
CSL-Daily and PHOENIX14T SLT datasets. Here, native speakers are only invited
to touch up inaccurate references, thus significantly improving the annotation
efficiency. Secondly, we provide a benchmark model to spur research in this
task. Specifically, we investigate multi-reference training strategies to
enable our DivSLT model to achieve diverse translations. Then, to enhance
translation accuracy, we employ the max-reward-driven reinforcement learning
objective that maximizes the reward of the translated result. Additionally, we
utilize multiple metrics to assess the accuracy, diversity, and semantic
precision of the DivSLT task. Experimental results on the enriched datasets
demonstrate that our DivSLT method achieves not only better translation
performance but also diverse translation results.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.19230v2' target='_blank'>Humanizing the Machine: Proxy Attacks to Mislead LLM Detectors</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tianchun Wang, Yuanzhou Chen, Zichuan Liu, Zhanwen Chen, Haifeng Chen, Xiang Zhang, Wei Cheng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-25 00:35:00</h6>
<p class='card-text'>The advent of large language models (LLMs) has revolutionized the field of
text generation, producing outputs that closely mimic human-like writing.
Although academic and industrial institutions have developed detectors to
prevent the malicious usage of LLM-generated texts, other research has doubt
about the robustness of these systems. To stress test these detectors, we
introduce a proxy-attack strategy that effortlessly compromises LLMs, causing
them to produce outputs that align with human-written text and mislead
detection systems. Our method attacks the source model by leveraging a
reinforcement learning (RL) fine-tuned humanized small language model (SLM) in
the decoding phase. Through an in-depth analysis, we demonstrate that our
attack strategy is capable of generating responses that are indistinguishable
to detectors, preventing them from differentiating between machine-generated
and human-written text. We conduct systematic evaluations on extensive datasets
using proxy-attacked open-source models, including Llama2-13B, Llama3-70B, and
Mixtral-8*7B in both white- and black-box settings. Our findings show that the
proxy-attack strategy effectively deceives the leading detectors, resulting in
an average AUROC drop of 70.4% across multiple datasets, with a maximum drop of
90.3% on a single dataset. Furthermore, in cross-discipline scenarios, our
strategy also bypasses these detectors, leading to a significant relative
decrease of up to 90.9%, while in cross-language scenario, the drop reaches
91.3%. Despite our proxy-attack strategy successfully bypassing the detectors
with such significant relative drops, we find that the generation quality of
the attacked models remains preserved, even within a modest utility budget,
when compared to the text produced by the original, unattacked source model.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.19084v1' target='_blank'>GCoder: Improving Large Language Model for Generalized Graph Problem
  Solving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qifan Zhang, Xiaobin Hong, Jianheng Tang, Nuo Chen, Yuhan Li, Wenzhong Li, Jing Tang, Jia Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-24 18:40:36</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated strong reasoning abilities,
making them suitable for complex tasks such as graph computation. Traditional
reasoning steps paradigm for graph problems is hindered by unverifiable steps,
limited long-term reasoning, and poor generalization to graph variations. To
overcome these limitations, we introduce GCoder, a code-based LLM designed to
enhance problem-solving in generalized graph computation problems. Our method
involves constructing an extensive training dataset, GraphWild, featuring
diverse graph formats and algorithms. We employ a multi-stage training process,
including Supervised Fine-Tuning (SFT) and Reinforcement Learning from Compiler
Feedback (RLCF), to refine model capabilities. For unseen tasks, a hybrid
retrieval technique is used to augment performance. Experiments demonstrate
that GCoder outperforms GPT-4o, with an average accuracy improvement of 16.42%
across various graph computational problems. Furthermore, GCoder efficiently
manages large-scale graphs with millions of nodes and diverse input formats,
overcoming the limitations of previous models focused on the reasoning steps
paradigm. This advancement paves the way for more intuitive and effective graph
problem-solving using LLMs. Code and data are available at here:
https://github.com/Bklight999/WWW25-GCoder/tree/master.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.18890v1' target='_blank'>Improving Small-Scale Large Language Models Function Calling for
  Reasoning Tasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Graziano A. Manduzio, Federico A. Galatolo, Mario G. C. A. Cimino, Enzo Pasquale Scilingo, Lorenzo Cominelli</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-24 16:27:35</h6>
<p class='card-text'>Recent advancements in Large Language Models (LLMs) have demonstrated
exceptional capabilities in natural language understanding and generation.
While these models excel in general complex reasoning tasks, they still face
challenges in mathematical problem-solving and logical reasoning. To address
these limitations, researchers have explored function calling abilities,
allowing LLMs to execute provided functions and utilize their outputs for task
completion. However, concentrating on specific tasks can be very inefficient
for large-scale LLMs to be used, because of the expensive cost of training and
inference stages they need in terms of computational resources. This study
introduces a novel framework for training smaller language models in function
calling, focusing on specific logical and mathematical reasoning tasks. The
approach aims to improve performances of small-scale models for these tasks
using function calling, ensuring a high level of accuracy. Our framework
employs an agent that, given a problem and a set of callable functions, queries
the LLM by injecting a description and examples of the usable functions into
the prompt and managing their calls in a step-by-step reasoning chain. This
process is used to create a dataset of correct and incorrect reasoning chain
chat completions from a large-scale LLM. This dataset is used to train a
smaller LLM using Reinforcement Learning from Human Feedback (RLHF),
specifically employing the Direct Preference Optimization (DPO) technique.
Experimental results demonstrate how the proposed approach balances the
trade-off between model size and performance, improving the ability of function
calling for reasoning tasks, in smaller models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.00809v3' target='_blank'>Adaptive Segment-level Reward: Bridging the Gap Between Action and
  Reward Space in Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yanshi Li, Shaopan Xiong, Gengru Chen, Xiaoyang Li, Yijia Luo, Xingyuan Bu, Yingshui Tan, Wenbo Su, Bo Zheng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-23 16:16:15</h6>
<p class='card-text'>Reinforcement Learning (RL) has proven highly effective in aligning Large
Language Models (LLMs) with human preferences. Typical RL methods optimize
under an overall sequence reward, which can lead to a suboptimal learning
process. This reflects a key credit assignment problem: identifying which
tokens to reinforce or suppress. To rectify these shortcomings, step-wise and
token-wise methods have been proposed. However, step-wise methods rely on
punctuation segmentation and still cannot accurately identify the key tokens.
The token-level approach is too fine-grained, attending to many unimportant
tokens and thus introducing a large amount of noise. To assign more accurate
rewards to different tokens, improving credit assignment, we propose the
"Adaptive Segment-wise Reward" method. We employ semantic meaning, rather than
punctuation, to adaptively delineate segments. Experiments demonstrate that our
method can be integrated into various training methods. Compared to training
methods \textit{without} our approach, our method improves the success rate on
adversarial samples by 10\%, and achieves a 1.3\% improvement on evaluation
benchmarks such as MMLU, GSM8K, HumanEval, etc.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.17621v2' target='_blank'>Process Supervision-Guided Policy Optimization for Code Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ning Dai, Zheng Wu, Renjie Zheng, Ziyun Wei, Wenlei Shi, Xing Jin, Guanlin Liu, Chen Dun, Liang Huang, Lin Yan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-23 07:22:33</h6>
<p class='card-text'>Reinforcement learning (RL) with unit test feedback has enhanced large
language models' (LLMs) code generation, but relies on sparse rewards provided
only after complete code evaluation, limiting learning efficiency and
incremental improvements. When generated code fails all unit tests, no learning
signal is received, hindering progress on complex tasks. To address this, we
propose a Process Reward Model (PRM) that delivers dense, line-level feedback
on code correctness during generation, mimicking human code refinement and
providing immediate guidance. We explore various strategies for training PRMs
and integrating them into the RL framework, finding that using PRMs both as
dense rewards and for value function initialization significantly boosts
performance. Our experimental results also highlight the effectiveness of PRMs
in enhancing RL-driven code generation, especially for long-horizon scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.17233v2' target='_blank'>Large Language Models are In-context Preference Learners</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chao Yu, Qixin Tan, Hong Lu, Jiaxuan Gao, Xinting Yang, Yu Wang, Yi Wu, Eugene Vinitsky</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-22 17:53:34</h6>
<p class='card-text'>Preference-based reinforcement learning is an effective way to handle tasks
where rewards are hard to specify but can be exceedingly inefficient as
preference learning is often tabula rasa. We demonstrate that Large Language
Models (LLMs) have native preference-learning capabilities that allow them to
achieve sample-efficient preference learning, addressing this challenge. We
propose In-Context Preference Learning (ICPL), which uses in-context learning
capabilities of LLMs to reduce human query inefficiency. ICPL uses the task
description and basic environment code to create sets of reward functions which
are iteratively refined by placing human feedback over videos of the resultant
policies into the context of an LLM and then requesting better rewards. We
first demonstrate ICPL's effectiveness through a synthetic preference study,
providing quantitative evidence that it significantly outperforms baseline
preference-based methods with much higher performance and orders of magnitude
greater efficiency. We observe that these improvements are not solely coming
from LLM grounding in the task but that the quality of the rewards improves
over time, indicating preference learning capabilities. Additionally, we
perform a series of real human preference-learning trials and observe that ICPL
extends beyond synthetic settings and can work effectively with
humans-in-the-loop.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.17126v1' target='_blank'>Exploring RL-based LLM Training for Formal Language Tasks with
  Programmed Rewards</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alexander G. Padula, Dennis J. N. J. Soemers</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-22 15:59:58</h6>
<p class='card-text'>Proximal Policy Optimization (PPO) is commonly used in Reinforcement Learning
from Human Feedback to align large language models (LLMs) with downstream
tasks. This paper investigates the feasibility of using PPO for direct
reinforcement learning (RL) from explicitly programmed reward signals, as
opposed to indirect learning from human feedback via an intermediary reward
model. We focus on tasks expressed through formal languages, such as
mathematics and programming, where explicit reward functions can be programmed
to automatically assess the quality of generated outputs. We apply this
approach to a sentiment alignment task, a simple arithmetic task, and a more
complex game synthesis task. The sentiment alignment task replicates prior
research and serves to validate our experimental setup. Our results show that
pure RL-based training for the two formal language tasks is challenging, with
success being limited even for the simple arithmetic task. We propose a novel
batch-entropy regularization term to aid exploration, although training is not
yet entirely stable. Our findings suggest that direct RL training of LLMs may
be more suitable for relatively minor changes, such as alignment, than for
learning new tasks altogether, even if an informative reward signal can be
expressed programmatically.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.16738v1' target='_blank'>LLM-Assisted Red Teaming of Diffusion Models through "Failures Are
  Fated, But Can Be Faded"</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Som Sagar, Aditya Taparia, Ransalu Senanayake</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-22 06:46:09</h6>
<p class='card-text'>In large deep neural networks that seem to perform surprisingly well on many
tasks, we also observe a few failures related to accuracy, social biases, and
alignment with human values, among others. Therefore, before deploying these
models, it is crucial to characterize this failure landscape for engineers to
debug or audit models. Nevertheless, it is infeasible to exhaustively test for
all possible combinations of factors that could lead to a model's failure. In
this paper, we improve the "Failures are fated, but can be faded" framework
(arXiv:2406.07145)--a post-hoc method to explore and construct the failure
landscape in pre-trained generative models--with a variety of deep
reinforcement learning algorithms, screening tests, and LLM-based rewards and
state generation. With the aid of limited human feedback, we then demonstrate
how to restructure the failure landscape to be more desirable by moving away
from the discovered failure modes. We empirically demonstrate the effectiveness
of the proposed method on diffusion models. We also highlight the strengths and
weaknesses of each algorithm in identifying failure modes.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.16714v2' target='_blank'>Magnetic Preference Optimization: Achieving Last-iterate Convergence for
  Language Model Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mingzhi Wang, Chengdong Ma, Qizhi Chen, Linjian Meng, Yang Han, Jiancong Xiao, Zhaowei Zhang, Jing Huo, Weijie J. Su, Yaodong Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-22 05:51:34</h6>
<p class='card-text'>Self-play methods have demonstrated remarkable success in enhancing model
capabilities across various domains. In the context of Reinforcement Learning
from Human Feedback (RLHF), self-play not only boosts Large Language Model
(LLM) performance but also overcomes the limitations of traditional
Bradley-Terry (BT) model assumptions by finding the Nash equilibrium (NE) of a
preference-based, two-player constant-sum game. However, existing methods
either guarantee only average-iterate convergence, incurring high storage and
inference costs, or converge to the NE of a regularized game, failing to
accurately reflect true human preferences. In this paper, we introduce Magnetic
Preference Optimization (MPO), a novel approach capable of achieving
last-iterate convergence to the NE of the original game, effectively overcoming
the limitations of existing methods. Building upon Magnetic Mirror Descent
(MMD), MPO attains a linear convergence rate, making it particularly suitable
for fine-tuning LLMs. To ensure our algorithm is both theoretically sound and
practically viable, we present a simple yet effective implementation that
adapts the theoretical insights to the RLHF setting. Empirical results
demonstrate that MPO can significantly enhance the performance of LLMs,
highlighting the potential of self-play methods in alignment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.16586v1' target='_blank'>Optimizing LLMs with Direct Preferences: A Data Efficiency Perspective</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pietro Bernardelle, Gianluca Demartini</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-22 00:11:41</h6>
<p class='card-text'>Aligning the output of Large Language Models (LLMs) with human preferences
(e.g., by means of reinforcement learning with human feedback, or RLHF) is
essential for ensuring their effectiveness in real-world scenarios. Despite
significant advancements in LLM alignment techniques, the impact of different
type of preference data on model performance has yet to be systematically
explored. In this study, we investigate the scalability, data efficiency, and
effectiveness of Direct Preference Optimization (DPO) in fine-tuning
pre-trained LLMs, aiming to reduce their dependency on extensive amounts of
preference data, which is expensive to collect. We (1) systematically compare
the performance of models fine-tuned with varying percentages of a combined
preference judgement dataset to define the improvement curve of DPO and assess
its effectiveness in data-constrained environments; and (2) provide insights
for the development of an optimal approach for selective preference data usage.
Our study reveals that increasing the amount of data used for training
generally enhances and stabilizes model performance. Moreover, the use of a
combination of diverse datasets significantly improves model effectiveness.
Furthermore, when models are trained separately using different types of
prompts, models trained with conversational prompts outperformed those trained
with question answering prompts.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.16024v1' target='_blank'>A New Approach to Solving SMAC Task: Generating Decision Tree Code from
  Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yue Deng, Weiyu Ma, Yuxin Fan, Yin Zhang, Haifeng Zhang, Jian Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-21 13:58:38</h6>
<p class='card-text'>StarCraft Multi-Agent Challenge (SMAC) is one of the most commonly used
experimental environments in multi-agent reinforcement learning (MARL), where
the specific task is to control a set number of allied units to defeat enemy
forces. Traditional MARL algorithms often require interacting with the
environment for up to 1 million steps to train a model, and the resulting
policies are typically non-interpretable with weak transferability. In this
paper, we propose a novel approach to solving SMAC tasks called LLM-SMAC. In
our framework, agents leverage large language models (LLMs) to generate
decision tree code by providing task descriptions. The model is further
self-reflection using feedback from the rewards provided by the environment. We
conduct experiments in the SMAC and demonstrate that our method can produce
high-quality, interpretable decision trees with minimal environmental
exploration. Moreover, these models exhibit strong transferability,
successfully applying to similar SMAC environments without modification. We
believe this approach offers a new direction for solving decision-making tasks
in the future.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.15651v1' target='_blank'>Understanding and Alleviating Memory Consumption in RLHF for LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jin Zhou, Hanmei Yang, Steven, Tang, Mingcan Xiang, Hui Guan, Tongping Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-21 05:23:42</h6>
<p class='card-text'>Fine-tuning with Reinforcement Learning with Human Feedback (RLHF) is
essential for aligning large language models (LLMs). However, RLHF often
encounters significant memory challenges. This study is the first to examine
memory usage in the RLHF context, exploring various memory management
strategies and unveiling the reasons behind excessive memory consumption.
Additionally, we introduce a simple yet effective approach that substantially
reduces the memory required for RLHF fine-tuning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.15625v2' target='_blank'>Improving Parallel Program Performance with LLM Optimizers via
  Agent-System Interface</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Anjiang Wei, Allen Nie, Thiago S. F. X. Teixeira, Rohan Yadav, Wonchan Lee, Ke Wang, Alex Aiken</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-21 04:08:37</h6>
<p class='card-text'>Modern scientific discovery increasingly relies on high-performance computing
for complex modeling and simulation. A key challenge in improving parallel
program performance is efficiently mapping tasks to processors and data to
memory, a process dictated by intricate, low-level system code known as
mappers. Developing high-performance mappers demands days of manual tuning,
posing a significant barrier for domain scientists without systems expertise.
We introduce a framework that automates mapper development with generative
optimization, leveraging richer feedback beyond scalar performance metrics. Our
approach features the Agent-System Interface, which includes a Domain-Specific
Language (DSL) to abstract away low-level complexity of system code and define
a structured search space, as well as AutoGuide, a mechanism that interprets
raw execution output into actionable feedback. Unlike traditional reinforcement
learning methods such as OpenTuner, which rely solely on scalar feedback, our
method finds superior mappers in far fewer iterations. With just 10 iterations,
it outperforms OpenTuner even after 1000 iterations, achieving 3.8X faster
performance. Our approach finds mappers that surpass expert-written mappers by
up to 1.34X speedup across nine benchmarks while reducing tuning time from days
to minutes.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.15610v1' target='_blank'>On The Global Convergence Of Online RLHF With Neural Parametrization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mudit Gaur, Amrit Singh Bedi, Raghu Pasupathy, Vaneet Aggarwal</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-21 03:13:35</h6>
<p class='card-text'>The importance of Reinforcement Learning from Human Feedback (RLHF) in
aligning large language models (LLMs) with human values cannot be overstated.
RLHF is a three-stage process that includes supervised fine-tuning (SFT),
reward learning, and policy learning. Although there are several offline and
online approaches to aligning LLMs, they often suffer from distribution shift
issues. These issues arise from the inability to accurately capture the
distributional interdependence between the reward learning and policy learning
stages. Consequently, this has led to various approximated approaches, but the
theoretical insights and motivations remain largely limited to tabular
settings, which do not hold in practice. This gap between theoretical insights
and practical implementations is critical. It is challenging to address this
gap as it requires analyzing the performance of AI alignment algorithms in
neural network-parameterized settings. Although bi-level formulations have
shown promise in addressing distribution shift issues, they suffer from the
hyper-gradient problem, and current approaches lack efficient algorithms to
solve this. In this work, we tackle these challenges employing the bi-level
formulation laid out in Kwon et al. (2024) along with the assumption \emph{Weak
Gradient Domination} to demonstrate convergence in an RLHF setup, obtaining a
sample complexity of $\epsilon^{-\frac{7}{2}}$ . Our key contributions are
twofold: (i) We propose a bi-level formulation for AI alignment in
parameterized settings and introduce a first-order approach to solve this
problem. (ii) We analyze the theoretical convergence rates of the proposed
algorithm and derive state-of-the-art bounds. To the best of our knowledge,
this is the first work to establish convergence rate bounds and global
optimality for the RLHF framework in neural network-parameterized settings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.15595v2' target='_blank'>A Comprehensive Survey of Direct Preference Optimization: Datasets,
  Theories, Variants, and Applications</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenyi Xiao, Zechuan Wang, Leilei Gan, Shuai Zhao, Wanggui He, Luu Anh Tuan, Long Chen, Hao Jiang, Zhou Zhao, Fei Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-21 02:27:24</h6>
<p class='card-text'>With the rapid advancement of large language models (LLMs), aligning policy
models with human preferences has become increasingly critical. Direct
Preference Optimization (DPO) has emerged as a promising approach for
alignment, acting as an RL-free alternative to Reinforcement Learning from
Human Feedback (RLHF). Despite DPO's various advancements and inherent
limitations, an in-depth review of these aspects is currently lacking in the
literature. In this work, we present a comprehensive review of the challenges
and opportunities in DPO, covering theoretical analyses, variants, relevant
preference datasets, and applications. Specifically, we categorize recent
studies on DPO based on key research questions to provide a thorough
understanding of DPO's current landscape. Additionally, we propose several
future research directions to offer insights on model alignment for the
research community.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.15287v1' target='_blank'>Training Language Models to Critique With Multi-agent Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tian Lan, Wenwei Zhang, Chengqi Lyu, Shuaibin Li, Chen Xu, Heyan Huang, Dahua Lin, Xian-Ling Mao, Kai Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-20 04:57:45</h6>
<p class='card-text'>Critique ability, a meta-cognitive capability of humans, presents significant
challenges for LLMs to improve. Recent works primarily rely on supervised
fine-tuning (SFT) using critiques generated by a single LLM like GPT-4.
However, these model-generated critiques often exhibit flaws due to the
inherent complexity of the critique. Consequently, fine-tuning LLMs on such
flawed critiques typically limits the model's performance and propagates these
flaws into the learned model. To overcome these challenges, this paper proposes
a novel data generation pipeline, named MultiCritique, that improves the
critique ability of LLMs by utilizing multi-agent feedback in both the SFT and
reinforcement learning (RL) stages. First, our data generation pipeline
aggregates high-quality critiques from multiple agents instead of a single
model, with crucial information as input for simplifying the critique.
Furthermore, our pipeline improves the preference accuracy of critique quality
through multi-agent feedback, facilitating the effectiveness of RL in improving
the critique ability of LLMs. Based on our proposed MultiCritique data
generation pipeline, we construct the MultiCritiqueDataset for the SFT and RL
fine-tuning stages. Extensive experimental results on two benchmarks
demonstrate: 1) the superior quality of our constructed SFT dataset compared to
existing critique datasets; 2) additional improvements to the critique ability
of LLMs brought by the RL stage. Notably, our fine-tuned 7B model significantly
surpasses other advanced 7B-13B open-source models, approaching the performance
of advanced 70B LLMs and GPT-4. Codes, datasets and model weights will be
publicly available.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.14926v1' target='_blank'>Aligning LLMs with Human Instructions and Stock Market Feedback in
  Financial Sentiment Analysis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zijie Zhao, Roy E. Welsch</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-19 01:29:12</h6>
<p class='card-text'>Financial sentiment analysis is crucial for trading and investment
decision-making. This study introduces an adaptive retrieval augmented
framework for Large Language Models (LLMs) that aligns with human instructions
through Instruction Tuning and incorporates market feedback to dynamically
adjust weights across various knowledge sources within the Retrieval-Augmented
Generation (RAG) module. Building upon foundational models like LLaMA 2, we
fine-tune a series of LLMs ranging from 7B to 70B in size, enriched with
Instruction Tuning and RAG, and further optimized through direct feedback and
Reinforcement Learning (RL)-based refinement methods applied to the source
weights of RAG.Through extensive evaluation, we demonstrate that the sentiment
outputs from our LLMs more accurately mirror the intrinsic sentiment of textual
data, showcasing a 1% to 6% boost in accuracy and F1 score over existing
state-of-the-art models and leading conversational AI systems. Moreover, the
sentiments extracted are more indicative of the directions in stock price
movements. On top of that, we successfully construct portfolios that yield a
3.61% higher Sharpe ratio compared to the S&P 500 baseline in bullish markets.
These portfolios also demonstrate resilience in bearish markets, with a 5x
reduction in return losses compared to those typically experienced by the S&P
500.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.14872v2' target='_blank'>How to Evaluate Reward Models for RLHF</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Evan Frick, Tianle Li, Connor Chen, Wei-Lin Chiang, Anastasios N. Angelopoulos, Jiantao Jiao, Banghua Zhu, Joseph E. Gonzalez, Ion Stoica</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-18 21:38:21</h6>
<p class='card-text'>We introduce a new benchmark for reward models that quantifies their ability
to produce strong language models through RLHF (Reinforcement Learning from
Human Feedback). The gold-standard approach is to run a full RLHF training
pipeline and directly probe downstream LLM performance. However, this process
is prohibitively expensive. To address this, we build a predictive model of
downstream LLM performance by evaluating the reward model on proxy tasks. These
proxy tasks consist of a large-scale human preference and a verifiable
correctness preference dataset, in which we measure 12 metrics across 12
domains. To investigate which reward model metrics are most correlated to
gold-standard RLHF outcomes, we launch an end-to-end RLHF experiment on a
large-scale crowdsourced human preference platform to view real reward model
downstream performance as ground truth. Ultimately, we compile our data and
findings into Preference Proxy Evaluations (PPE), the first reward model
benchmark explicitly linked to post-RLHF real-world human preference
performance, which we open-source for public use and further development. Our
code and evaluations can be found at https://github.com/lmarena/PPE .</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.14660v1' target='_blank'>A Large Language Model-Driven Reward Design Framework via Dynamic
  Feedback for Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shengjie Sun, Runze Liu, Jiafei Lyu, Jing-Wen Yang, Liangpeng Zhang, Xiu Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-18 17:51:51</h6>
<p class='card-text'>Large Language Models (LLMs) have shown significant potential in designing
reward functions for Reinforcement Learning (RL) tasks. However, obtaining
high-quality reward code often involves human intervention, numerous LLM
queries, or repetitive RL training. To address these issues, we propose CARD, a
LLM-driven Reward Design framework that iteratively generates and improves
reward function code. Specifically, CARD includes a Coder that generates and
verifies the code, while a Evaluator provides dynamic feedback to guide the
Coder in improving the code, eliminating the need for human feedback. In
addition to process feedback and trajectory feedback, we introduce Trajectory
Preference Evaluation (TPE), which evaluates the current reward function based
on trajectory preferences. If the code fails the TPE, the Evaluator provides
preference feedback, avoiding RL training at every iteration and making the
reward function better aligned with the task objective. Empirical results on
Meta-World and ManiSkill2 demonstrate that our method achieves an effective
balance between task performance and token efficiency, outperforming or
matching the baselines across all tasks. On 10 out of 12 tasks, CARD shows
better or comparable performance to policies trained with expert-designed
rewards, and our method even surpasses the oracle on 3 tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.14368v2' target='_blank'>CoMAL: Collaborative Multi-Agent Large Language Models for
  Mixed-Autonomy Traffic</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Huaiyuan Yao, Longchao Da, Vishnu Nandam, Justin Turnau, Zhiwei Liu, Linsey Pang, Hua Wei</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-18 10:53:44</h6>
<p class='card-text'>The integration of autonomous vehicles into urban traffic has great potential
to improve efficiency by reducing congestion and optimizing traffic flow
systematically. In this paper, we introduce CoMAL (Collaborative Multi-Agent
LLMs), a framework designed to address the mixed-autonomy traffic problem by
collaboration among autonomous vehicles to optimize traffic flow. CoMAL is
built upon large language models, operating in an interactive traffic
simulation environment. It utilizes a Perception Module to observe surrounding
agents and a Memory Module to store strategies for each agent. The overall
workflow includes a Collaboration Module that encourages autonomous vehicles to
discuss the effective strategy and allocate roles, a reasoning engine to
determine optimal behaviors based on assigned roles, and an Execution Module
that controls vehicle actions using a hybrid approach combining rule-based
models. Experimental results demonstrate that CoMAL achieves superior
performance on the Flow benchmark. Additionally, we evaluate the impact of
different language models and compare our framework with reinforcement learning
approaches. It highlights the strong cooperative capability of LLM agents and
presents a promising solution to the mixed-autonomy traffic challenge. The code
is available at https://github.com/Hyan-Yao/CoMAL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.14184v1' target='_blank'>MetaAlign: Align Large Language Models with Diverse Preferences during
  Inference Time</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mozhi Zhang, Pengyu Wang, Chenkun Tan, Mianqiu Huang, Dong Zhang, Yaqian Zhou, Xipeng Qiu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-18 05:31:13</h6>
<p class='card-text'>Large Language Models (LLMs) acquire extensive knowledge and remarkable
abilities from extensive text corpora, making them powerful tools for various
applications. To make LLMs more usable, aligning them with human preferences is
essential. Existing alignment techniques, such as Reinforcement Learning from
Human Feedback (RLHF) and Direct Preference Optimization (DPO), typically embed
predefined preferences directly within the model's parameters. These methods,
however, often result in a static alignment that can not account for the
diversity of human preferences in practical applications. In response to this
challenge, we propose an effective method, \textbf{MetaAlign}, which aims to
help LLMs dynamically align with various explicit or implicit preferences
specified at inference time. Experimental results show that LLMs optimized on
our meticulously constructed MetaAlign Dataset can effectively align with any
preferences specified at the inference stage, validating the feasibility of
MetaAlign. We hope that our work can provide some insights into the alignment
of language models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.14150v1' target='_blank'>Utilizing Large Language Models for Event Deconstruction to Enhance
  Multimodal Aspect-Based Sentiment Analysis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaoyong Huang, Heli Sun, Qunshu Gao, Wenjie Huang, Ruichen Cao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-18 03:40:45</h6>
<p class='card-text'>With the rapid development of the internet, the richness of User-Generated
Contentcontinues to increase, making Multimodal Aspect-Based Sentiment Analysis
(MABSA) a research hotspot. Existing studies have achieved certain results in
MABSA, but they have not effectively addressed the analytical challenges in
scenarios where multiple entities and sentiments coexist. This paper
innovatively introduces Large Language Models (LLMs) for event decomposition
and proposes a reinforcement learning framework for Multimodal Aspect-based
Sentiment Analysis (MABSA-RL) framework. This framework decomposes the original
text into a set of events using LLMs, reducing the complexity of analysis,
introducing reinforcement learning to optimize model parameters. Experimental
results show that MABSA-RL outperforms existing advanced methods on two
benchmark datasets. This paper provides a new research perspective and method
for multimodal aspect-level sentiment analysis.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.13501v1' target='_blank'>Integrating Large Language Models and Reinforcement Learning for
  Non-Linear Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yoav Alon, Cristina David</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-17 12:47:31</h6>
<p class='card-text'>Large Language Models (LLMs) were shown to struggle with long-term planning,
which may be caused by the limited way in which they explore the space of
possible solutions. We propose an architecture where a Reinforcement Learning
(RL) Agent guides an LLM's space exploration: (1) the Agent has access to
domain-specific information, and can therefore make decisions about the quality
of candidate solutions based on specific and relevant metrics, which were not
explicitly considered by the LLM's training objective; (2) the LLM can focus on
generating immediate next steps, without the need for long-term planning. We
allow non-linear reasoning by exploring alternative paths and backtracking. We
evaluate this architecture on the program equivalence task, and compare it
against Chain of Thought (CoT) and Tree of Thoughts (ToT). We assess both the
downstream task, denoting the binary classification, and the intermediate
reasoning steps. Our approach compares positively against CoT and ToT.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.13224v1' target='_blank'>Proof Flow: Preliminary Study on Generative Flow Network Language Model
  Tuning for Formal Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Matthew Ho, Vincent Zhu, Xiaoyin Chen, Moksh Jain, Nikolay Malkin, Edwin Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-17 05:10:12</h6>
<p class='card-text'>Reasoning is a fundamental substrate for solving novel and complex problems.
Deliberate efforts in learning and developing frameworks around System 2
reasoning have made great strides, yet problems of sufficient complexity remain
largely out of reach for open models. To address this gap, we examine the
potential of Generative Flow Networks as a fine-tuning method for LLMs to
unlock advanced reasoning capabilities. In this paper, we present a proof of
concept in the domain of formal reasoning, specifically in the Neural Theorem
Proving (NTP) setting, where proofs specified in a formal language such as Lean
can be deterministically and objectively verified. Unlike classical
reward-maximization reinforcement learning, which frequently over-exploits
high-reward actions and fails to effectively explore the state space, GFlowNets
have emerged as a promising approach for sampling compositional objects,
improving generalization, and enabling models to maintain diverse hypotheses.
Our early results demonstrate GFlowNet fine-tuning's potential for enhancing
model performance in a search setting, which is especially relevant given the
paradigm shift towards inference time compute scaling and "thinking slowly."</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.12568v2' target='_blank'>Robust RL with LLM-Driven Data Synthesis and Policy Adaptation for
  Autonomous Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sihao Wu, Jiaxu Liu, Xiangyu Yin, Guangliang Cheng, Xingyu Zhao, Meng Fang, Xinping Yi, Xiaowei Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-16 13:43:00</h6>
<p class='card-text'>The integration of Large Language Models (LLMs) into autonomous driving
systems demonstrates strong common sense and reasoning abilities, effectively
addressing the pitfalls of purely data-driven methods. Current LLM-based agents
require lengthy inference times and face challenges in interacting with
real-time autonomous driving environments. A key open question is whether we
can effectively leverage the knowledge from LLMs to train an efficient and
robust Reinforcement Learning (RL) agent. This paper introduces RAPID, a novel
\underline{\textbf{R}}obust \underline{\textbf{A}}daptive
\underline{\textbf{P}}olicy \underline{\textbf{I}}nfusion and
\underline{\textbf{D}}istillation framework, which trains specialized
mix-of-policy RL agents using data synthesized by an LLM-based driving agent
and online adaptation. RAPID features three key designs: 1) utilization of
offline data collected from an LLM agent to distil expert knowledge into RL
policies for faster real-time inference; 2) introduction of robust distillation
in RL to inherit both performance and robustness from LLM-based teacher; and 3)
employment of a mix-of-policy approach for joint decision decoding with a
policy adapter. Through fine-tuning via online environment interaction, RAPID
reduces the forgetting of LLM knowledge while maintaining adaptability to
different tasks. Extensive experiments demonstrate RAPID's capability to
effectively integrate LLM knowledge into scaled-down RL policies in an
efficient, adaptable, and robust way. Code and checkpoints will be made
publicly available upon acceptance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.12491v1' target='_blank'>Insights from the Inverse: Reconstructing LLM Training Goals Through
  Inverse RL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jared Joselowitz, Arjun Jagota, Satyapriya Krishna, Sonali Parbhoo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-16 12:14:25</h6>
<p class='card-text'>Large language models (LLMs) trained with Reinforcement Learning from Human
Feedback (RLHF) have demonstrated remarkable capabilities, but their underlying
reward functions and decision-making processes remain opaque. This paper
introduces a novel approach to interpreting LLMs by applying inverse
reinforcement learning (IRL) to recover their implicit reward functions. We
conduct experiments on toxicity-aligned LLMs of varying sizes, extracting
reward models that achieve up to 80.40% accuracy in predicting human
preferences. Our analysis reveals key insights into the non-identifiability of
reward functions, the relationship between model size and interpretability, and
potential pitfalls in the RLHF process. We demonstrate that IRL-derived reward
models can be used to fine-tune new LLMs, resulting in comparable or improved
performance on toxicity benchmarks. This work provides a new lens for
understanding and improving LLM alignment, with implications for the
responsible development and deployment of these powerful systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.12481v1' target='_blank'>SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and
  Hindsight Relabeling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Loris Gaven, Clement Romac, Thomas Carta, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-16 11:59:27</h6>
<p class='card-text'>The past years have seen Large Language Models (LLMs) strive not only as
generative models but also as agents solving textual sequential decision-making
tasks. When facing complex environments where their zero-shot abilities are
insufficient, recent work showed online Reinforcement Learning (RL) could be
used for the LLM agent to discover and learn efficient strategies
interactively. However, most prior work sticks to on-policy algorithms, which
greatly reduces the scope of methods such agents could use for both exploration
and exploitation, such as experience replay and hindsight relabeling. Yet, such
methods may be key for LLM learning agents, and in particular when designing
autonomous intrinsically motivated agents sampling and pursuing their own goals
(i.e. autotelic agents). This paper presents and studies an adaptation of Soft
Actor-Critic and hindsight relabeling to LLM agents. Our method not only paves
the path towards autotelic LLM agents that learn online but can also outperform
on-policy methods in more classic multi-goal RL environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.12375v1' target='_blank'>PRefLexOR: Preference-based Recursive Language Modeling for Exploratory
  Optimization of Reasoning and Agentic Thinking</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Markus J. Buehler</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-16 08:46:26</h6>
<p class='card-text'>PRefLexOR (Preference-based Recursive Language Modeling for Exploratory
Optimization of Reasoning) combines preference optimization with concepts from
Reinforcement Learning to enable models to self-teach through iterative
reasoning improvements. We propose a recursive learning approach that engages
the model in multi-step reasoning, revisiting, and refining intermediate steps
before producing a final output in training and inference phases. Through
multiple training stages, the model first learns to align its reasoning with
accurate decision paths by optimizing the log odds between preferred and
non-preferred responses. During this process, PRefLexOR builds a dynamic
knowledge graph by generating questions from random text chunks and
retrieval-augmentation to contextualize relevant details from the entire
training corpus. In the second stage, preference optimization enhances model
performance by using rejection sampling to fine-tune reasoning quality by
continually producing in-situ training data while masking the reasoning steps.
Recursive optimization within a thinking token framework introduces iterative
feedback loops, where the model refines reasoning, achieving deeper coherence,
consistency, and adaptability. Implemented in small language models with only 3
billion parameters, we should that even tiny models can iteratively teach
themselves to reason with greater depth and reflectivity. Our implementation is
straightforward and can be incorporated into any existing pretrained LLM. We
focus our examples on applications in biological materials science and
demonstrate the method in a variety of case studies that range from in-domain
to cross-domain applications. Using reasoning strategies that include thinking
and reflection modalities we build a multi-agent recursive self-improving
inference approach to successively improve responses via repeated sampling in
inference time.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.12138v1' target='_blank'>Preference Optimization with Multi-Sample Comparisons</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chaoqi Wang, Zhuokai Zhao, Chen Zhu, Karthik Abinav Sankararaman, Michal Valko, Xuefei Cao, Zhaorun Chen, Madian Khabsa, Yuxin Chen, Hao Ma, Sinong Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-16 00:59:19</h6>
<p class='card-text'>Recent advancements in generative models, particularly large language models
(LLMs) and diffusion models, have been driven by extensive pretraining on large
datasets followed by post-training. However, current post-training methods such
as reinforcement learning from human feedback (RLHF) and direct alignment from
preference methods (DAP) primarily utilize single-sample comparisons. These
approaches often fail to capture critical characteristics such as generative
diversity and bias, which are more accurately assessed through multiple
samples. To address these limitations, we introduce a novel approach that
extends post-training to include multi-sample comparisons. To achieve this, we
propose Multi-sample Direct Preference Optimization (mDPO) and Multi-sample
Identity Preference Optimization (mIPO). These methods improve traditional DAP
methods by focusing on group-wise characteristics. Empirically, we demonstrate
that multi-sample comparison is more effective in optimizing collective
characteristics~(e.g., diversity and bias) for generative models than
single-sample comparison. Additionally, our findings suggest that multi-sample
comparisons provide a more robust optimization framework, particularly for
dataset with label noise.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.11711v2' target='_blank'>Zero-shot Model-based Reinforcement Learning using Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Abdelhakim Benechehab, Youssef Attia El Hili, Ambroise Odonnat, Oussama Zekri, Albert Thomas, Giuseppe Paolo, Maurizio Filippone, Ievgen Redko, Balázs Kégl</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-15 15:46:53</h6>
<p class='card-text'>The emerging zero-shot capabilities of Large Language Models (LLMs) have led
to their applications in areas extending well beyond natural language
processing tasks. In reinforcement learning, while LLMs have been extensively
used in text-based environments, their integration with continuous state spaces
remains understudied. In this paper, we investigate how pre-trained LLMs can be
leveraged to predict in context the dynamics of continuous Markov decision
processes. We identify handling multivariate data and incorporating the control
signal as key challenges that limit the potential of LLMs' deployment in this
setup and propose Disentangled In-Context Learning (DICL) to address them. We
present proof-of-concept applications in two reinforcement learning settings:
model-based policy evaluation and data-augmented off-policy reinforcement
learning, supported by theoretical analysis of the proposed methods. Our
experiments further demonstrate that our approach produces well-calibrated
uncertainty estimates. We release the code at
https://github.com/abenechehab/dicl.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.11507v3' target='_blank'>Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic
  Evaluation Framework for LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wanying Wang, Zeyu Ma, Pengfei Liu, Mingang Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-15 11:20:42</h6>
<p class='card-text'>While various vertical domain large language models (LLMs) have been
developed, automatically evaluating their performance across different domains
remains a critical challenge. Current benchmark-based methods often rely on
static and costly datasets, are misaligned with practical user needs, and lack
flexibility across domains. To address these limitations, we revisit the
evaluation process and introduce two key concepts: Benchmark+, which extends
the traditional question-answer benchmark into a more flexible
``strategy-criterion'' format; and Assessment+, which enhances the interaction
process, enabling deeper exploration and supporting analysis from broader
perspectives. We propose TestAgent, an agent-based evaluation framework that
implements these concepts using retrieval-augmented generation and
reinforcement learning. TestAgent enables automatic dynamic benchmark
generation and in-depth assessment across diverse vertical domain scenarios.
Experiments on tasks ranging from constructing multiple vertical domain
evaluations to converting static benchmarks into dynamic forms demonstrate the
effectiveness of TestAgent. This work offers an interesting perspective on
automatic evaluation for LLMs and highlights a pathway for dynamic and
domain-adaptive assessments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.11283v1' target='_blank'>AdvBDGen: Adversarially Fortified Prompt-Specific Fuzzy Backdoor
  Generator Against LLM Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pankayaraj Pathmanathan, Udari Madhushani Sehwag, Michael-Andrei Panaitescu-Liess, Furong Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-15 05:05:56</h6>
<p class='card-text'>With the growing adoption of reinforcement learning with human feedback
(RLHF) for aligning large language models (LLMs), the risk of backdoor
installation during alignment has increased, leading to unintended and harmful
behaviors. Existing backdoor triggers are typically limited to fixed word
patterns, making them detectable during data cleaning and easily removable
post-poisoning. In this work, we explore the use of prompt-specific paraphrases
as backdoor triggers, enhancing their stealth and resistance to removal during
LLM alignment. We propose AdvBDGen, an adversarially fortified generative
fine-tuning framework that automatically generates prompt-specific backdoors
that are effective, stealthy, and transferable across models. AdvBDGen employs
a generator-discriminator pair, fortified by an adversary, to ensure the
installability and stealthiness of backdoors. It enables the crafting and
successful installation of complex triggers using as little as 3% of the
fine-tuning data. Once installed, these backdoors can jailbreak LLMs during
inference, demonstrate improved stability against perturbations compared to
traditional constant triggers, and are more challenging to remove. These
findings underscore an urgent need for the research community to develop more
robust defenses against adversarial backdoor threats in LLM alignment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.11020v3' target='_blank'>Improving the Language Understanding Capabilities of Large Language
  Models Using Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bokai Hu, Sai Ashish Somayajula, Xin Pan, Zihan Huang, Pengtao Xie</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-14 19:16:56</h6>
<p class='card-text'>Large language models (LLMs), built on decoder-only transformers, excel in
natural language generation and adapt to diverse tasks using zero-shot and
few-shot prompting. However, these prompting methods often struggle on natural
language understanding (NLU) tasks, where encoder-only models like BERT-base
outperform LLMs on benchmarks like GLUE and SuperGLUE. This paper explores two
approaches-supervised fine-tuning (SFT) and proximal policy optimization
(PPO)-to enhance LLMs' NLU abilities. To reduce the cost of full-model
fine-tuning, we integrate low-rank adaptation (LoRA) layers, limiting updates
to these layers during both SFT and PPO. In SFT, task-specific prompts are
concatenated with input queries and ground-truth labels, optimizing with
next-token prediction. Despite this, LLMs still underperform compared to models
like BERT-base on several NLU tasks. To close this gap, we apply PPO, a
reinforcement learning technique that treats each token generation as an action
and uses a reward function based on alignment with ground-truth answers. PPO
then updates the model to maximize these rewards, aligning outputs with correct
labels. Our experiments with LLAMA2-7B show that PPO improves performance, with
a 6.3-point gain over SFT on GLUE. PPO exceeds zero-shot by 38.7 points and
few-shot by 26.1 points on GLUE, while surpassing these by 28.8 and 28.5 points
on SuperGLUE. Additionally, PPO outperforms BERT-large by 2.7 points on GLUE
and 9.3 points on SuperGLUE. The improvements are consistent across models like
Qwen2.5-7B and MPT-7B, highlighting PPO's robustness in enhancing LLMs' NLU
capabilities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.10584v1' target='_blank'>STACKFEED: Structured Textual Actor-Critic Knowledge Base Editing with
  FeedBack</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Naman Gupta, Shashank Kirtania, Priyanshu Gupta, Krishna Kariya, Sumit Gulwani, Arun Iyer, Suresh Parthasarathy, Arjun Radhakrishna, Sriram K. Rajamani, Gustavo Soares</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-14 14:56:01</h6>
<p class='card-text'>Large Language Models (LLMs) often generate incorrect or outdated
information, especially in low-resource settings or when dealing with private
data. To address this, Retrieval-Augmented Generation (RAG) uses external
knowledge bases (KBs), but these can also suffer from inaccuracies. We
introduce STACKFEED, a novel Structured Textual Actor-Critic Knowledge base
editing with FEEDback approach that iteratively refines the KB based on expert
feedback using a multi-actor, centralized critic reinforcement learning
framework. Each document is assigned to an actor, modeled as a ReACT agent,
which performs structured edits based on document-specific targeted
instructions from a centralized critic. Experimental results show that
STACKFEED significantly improves KB quality and RAG system performance,
enhancing accuracy by up to 8% over baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.10370v1' target='_blank'>Innovative Thinking, Infinite Humor: Humor Research of Large Language
  Models through Structured Thought Leaps</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Han Wang, Yilin Zhao, Dian Li, Xiaohan Wang, Gang Liu, Xuguang Lan, Hui Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-14 10:50:16</h6>
<p class='card-text'>Humor is a culturally nuanced aspect of human language that presents
challenges for understanding and generation, requiring participants to possess
good creativity and strong associative thinking. Similar to reasoning tasks
like solving math problems, humor generation requires continuous reflection and
revision to foster creative thinking, rather than relying on a sudden flash of
inspiration like Creative Leap-of-Thought (CLoT) paradigm. Although CLoT can
realize the ability of remote association generation, this paradigm fails to
generate humor content. Therefore, in this paper, we propose a systematic way
of thinking about generating humor and based on it, we built Creative Leap of
Structured Thought (CLoST) frame. First, a reward model is necessary achieve
the purpose of being able to correct errors, since there is currently no expert
model of humor and a usable rule to determine whether a piece of content is
humorous. Judgement-oriented instructions are designed to improve the
capability of a model, and we also propose an open-domain instruction
evolutionary method to fully unleash the potential. Then, through reinforcement
learning, the model learns to hone its rationales of the thought chain and
refine the strategies it uses. Thus, it learns to recognize and correct its
mistakes, and finally generate the most humorous and creative answer. These
findings deepen our understanding of the creative capabilities of LLMs and
provide ways to enhance LLMs' creative abilities for cross-domain innovative
applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.10212v1' target='_blank'>Large Language Model-Enhanced Reinforcement Learning for Generic Bus
  Holding Control Strategies</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiajie Yu, Yuhong Wang, Wei Ma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-14 07:10:16</h6>
<p class='card-text'>Bus holding control is a widely-adopted strategy for maintaining stability
and improving the operational efficiency of bus systems. Traditional
model-based methods often face challenges with the low accuracy of bus state
prediction and passenger demand estimation. In contrast, Reinforcement Learning
(RL), as a data-driven approach, has demonstrated great potential in
formulating bus holding strategies. RL determines the optimal control
strategies in order to maximize the cumulative reward, which reflects the
overall control goals. However, translating sparse and delayed control goals in
real-world tasks into dense and real-time rewards for RL is challenging,
normally requiring extensive manual trial-and-error. In view of this, this
study introduces an automatic reward generation paradigm by leveraging the
in-context learning and reasoning capabilities of Large Language Models (LLMs).
This new paradigm, termed the LLM-enhanced RL, comprises several LLM-based
modules: reward initializer, reward modifier, performance analyzer, and reward
refiner. These modules cooperate to initialize and iteratively improve the
reward function according to the feedback from training and test results for
the specified RL-based task. Ineffective reward functions generated by the LLM
are filtered out to ensure the stable evolution of the RL agents' performance
over iterations. To evaluate the feasibility of the proposed LLM-enhanced RL
paradigm, it is applied to various bus holding control scenarios, including a
synthetic single-line system and a real-world multi-line system. The results
demonstrate the superiority and robustness of the proposed paradigm compared to
vanilla RL strategies, the LLM-based controller, and conventional space
headway-based feedback control. This study sheds light on the great potential
of utilizing LLMs in various smart mobility applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.10148v3' target='_blank'>$α$-DPO: Adaptive Reward Margin is What Direct Preference
  Optimization Needs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Junkang Wu, Xue Wang, Zhengyi Yang, Jiancan Wu, Jinyang Gao, Bolin Ding, Xiang Wang, Xiangnan He</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-14 04:29:57</h6>
<p class='card-text'>Aligning large language models (LLMs) with human values and intentions is
crucial for their utility, honesty, and safety. Reinforcement learning from
human feedback (RLHF) is a popular approach to achieve this alignment, but it
faces challenges in computational efficiency and training stability. Recent
methods like Direct Preference Optimization (DPO) and Simple Preference
Optimization (SimPO) have proposed offline alternatives to RLHF, simplifying
the process by reparameterizing the reward function. However, DPO depends on a
potentially suboptimal reference model, and SimPO's assumption of a fixed
target reward margin may lead to suboptimal decisions in diverse data settings.
In this work, we propose $\alpha$-DPO, an adaptive preference optimization
algorithm designed to address these limitations by introducing a dynamic reward
margin. Specifically, $\alpha$-DPO employs an adaptive preference distribution,
balancing the policy model and the reference model to achieve personalized
reward margins. We provide theoretical guarantees for $\alpha$-DPO,
demonstrating its effectiveness as a surrogate optimization objective and its
ability to balance alignment and diversity through KL divergence control.
Empirical evaluations on AlpacaEval 2 and Arena-Hard show that $\alpha$-DPO
consistently outperforms DPO and SimPO across various model settings,
establishing it as a robust approach for fine-tuning LLMs. Our method achieves
significant improvements in win rates, highlighting its potential as a powerful
tool for LLM alignment. The code is available at
https://github.com/junkangwu/alpha-DPO</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.09724v1' target='_blank'>Taming Overconfidence in LLMs: Reward Calibration in RLHF</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jixuan Leng, Chengsong Huang, Banghua Zhu, Jiaxin Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-13 04:48:40</h6>
<p class='card-text'>Language model calibration refers to the alignment between the confidence of
the model and the actual performance of its responses. While previous studies
point out the overconfidence phenomenon in Large Language Models (LLMs) and
show that LLMs trained with Reinforcement Learning from Human Feedback (RLHF)
are overconfident with a more sharpened output probability, in this study, we
reveal that RLHF tends to lead models to express verbalized overconfidence in
their own responses. We investigate the underlying cause of this overconfidence
and demonstrate that reward models used for Proximal Policy Optimization (PPO)
exhibit inherent biases towards high-confidence scores regardless of the actual
quality of responses. Building upon this insight, we propose two PPO variants:
PPO-M: PPO with Calibrated Reward Modeling and PPO-C: PPO with Calibrated
Reward Calculation. PPO-M integrates explicit confidence scores in reward model
training, which calibrates reward models to better capture the alignment
between response quality and verbalized confidence. PPO-C adjusts the reward
score during PPO based on the difference between the current reward and the
moving average of past rewards. Both PPO-M and PPO-C can be seamlessly
integrated into the current PPO pipeline and do not require additional golden
labels. We evaluate our methods on both Llama3-8B and Mistral-7B across six
diverse datasets including multiple-choice and open-ended generation.
Experiment results demonstrate that both of our methods can reduce calibration
error and maintain performance comparable to standard PPO. We further show that
they do not compromise model capabilities in open-ended conversation settings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.09680v1' target='_blank'>Integrating Reinforcement Learning and Large Language Models for Crop
  Production Process Management Optimization and Control through A New
  Knowledge-Based Deep Learning Paradigm</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dong Chen, Yanbo Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-13 00:31:16</h6>
<p class='card-text'>Efficient and sustainable crop production process management is crucial to
meet the growing global demand for food, fuel, and feed while minimizing
environmental impacts. Traditional crop management practices, often developed
through empirical experience, face significant challenges in adapting to the
dynamic nature of modern agriculture, which is influenced by factors such as
climate change, soil variability, and market conditions. Recently,
reinforcement learning (RL) and large language models (LLMs) bring
transformative potential, with RL providing adaptive methodologies to learn
optimal strategies and LLMs offering vast, superhuman knowledge across
agricultural domains, enabling informed, context-specific decision-making. This
paper systematically examines how the integration of RL and LLMs into crop
management decision support systems (DSSs) can drive advancements in
agricultural practice. We explore recent advancements in RL and LLM algorithms,
their application within crop management, and the use of crop management
simulators to develop these technologies. The convergence of RL and LLMs with
crop management DSSs presents new opportunities to optimize agricultural
practices through data-driven, adaptive solutions that can address the
uncertainties and complexities of crop production. However, this integration
also brings challenges, particularly in real-world deployment. We discuss these
challenges and propose potential solutions, including the use of offline RL and
enhanced LLM integration, to maximize the effectiveness and sustainability of
crop management. Our findings emphasize the need for continued research and
innovation to unlock the full potential of these advanced tools in transforming
agricultural systems into optimal and controllable ones.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.09671v1' target='_blank'>OpenR: An Open Source Framework for Advanced Reasoning with Large
  Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei Chen, Lionel M. Ni, Linyi Yang, Ying Wen, Weinan Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-12 23:42:16</h6>
<p class='card-text'>In this technical report, we introduce OpenR, an open-source framework
designed to integrate key components for enhancing the reasoning capabilities
of large language models (LLMs). OpenR unifies data acquisition, reinforcement
learning training (both online and offline), and non-autoregressive decoding
into a cohesive software platform. Our goal is to establish an open-source
platform and community to accelerate the development of LLM reasoning. Inspired
by the success of OpenAI's o1 model, which demonstrated improved reasoning
abilities through step-by-step reasoning and reinforcement learning, OpenR
integrates test-time compute, reinforcement learning, and process supervision
to improve reasoning in LLMs. Our work is the first to provide an open-source
framework that explores the core techniques of OpenAI's o1 model with
reinforcement learning, achieving advanced reasoning capabilities beyond
traditional autoregressive methods. We demonstrate the efficacy of OpenR by
evaluating it on the MATH dataset, utilising publicly available data and search
methods. Our initial experiments confirm substantial gains, with relative
improvements in reasoning and performance driven by test-time computation and
reinforcement learning through process reward models. The OpenR framework,
including code, models, and datasets, is accessible at
https://openreasoner.github.io.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.09362v1' target='_blank'>SeRA: Self-Reviewing and Alignment of Large Language Models using
  Implicit Reward Margins</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jongwoo Ko, Saket Dingliwal, Bhavana Ganesh, Sailik Sengupta, Sravan Bodapati, Aram Galstyan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-12 04:17:28</h6>
<p class='card-text'>Direct alignment algorithms (DAAs), such as direct preference optimization
(DPO), have become popular alternatives for Reinforcement Learning from Human
Feedback (RLHF) due to their simplicity, efficiency, and stability. However,
the preferences used in DAAs are usually collected before the alignment
training begins and remain unchanged (off-policy). This can lead to two
problems where the policy model (1) picks up on spurious correlations in the
dataset (as opposed to learning the intended alignment expressed in the human
preference labels), and (2) overfits to feedback on off-policy trajectories
that have less likelihood of being generated by an updated policy model. To
address these issues, we introduce Self-Reviewing and Alignment (SeRA), a
cost-efficient and effective method that can be readily combined with existing
DAAs. SeRA comprises of two components: (1) sample selection using implicit
reward margins, which helps alleviate over-fitting to some undesired features,
and (2) preference bootstrapping using implicit rewards to augment preference
data with updated policy models in a cost-efficient manner. Extensive
experimentation, including some on instruction-following tasks, demonstrate the
effectiveness and generality of SeRA in training LLMs on offline preference
datasets with DAAs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.09302v2' target='_blank'>Enhancing Multi-Step Reasoning Abilities of Language Models through
  Direct Q-Function Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kaixuan Ji, Guanlin Liu, Ning Dai, Qingping Yang, Renjie Zheng, Zheng Wu, Chen Dun, Quanquan Gu, Lin Yan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-11 23:29:20</h6>
<p class='card-text'>Reinforcement Learning (RL) plays a crucial role in aligning large language
models (LLMs) with human preferences and improving their ability to perform
complex tasks. However, current approaches either require significant
computational resources due to the use of multiple models and extensive online
sampling for training (e.g., PPO) or are framed as bandit problems (e.g., DPO,
DRO), which often struggle with multi-step reasoning tasks, such as math
problem solving and complex reasoning that involve long chains of thought. To
overcome these limitations, we introduce Direct Q-function Optimization (DQO),
which formulates the response generation process as a Markov Decision Process
(MDP) and utilizes the soft actor-critic (SAC) framework to optimize a
Q-function directly parameterized by the language model. The MDP formulation of
DQO offers structural advantages over bandit-based methods, enabling more
effective process supervision. Experimental results on two math problem-solving
datasets, GSM8K and MATH, demonstrate that DQO outperforms previous methods,
establishing it as a promising offline reinforcement learning approach for
aligning language models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.09300v2' target='_blank'>Nudging: Inference-time Alignment via Model Collaboration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yu Fei, Yasaman Razeghi, Sameer Singh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-11 23:24:38</h6>
<p class='card-text'>Large language models (LLMs) require alignment, such as instruction-tuning or
reinforcement learning from human feedback, to effectively and safely follow
user instructions. This process necessitates training aligned versions for
every model size in each model family, resulting in significant computational
overhead. In this work, we propose nudging, a simple, plug-and-play, and
training-free algorithm that aligns any base model at inference time using a
small aligned model. Nudging is motivated by recent findings that alignment
primarily alters the model's behavior on a small subset of stylistic tokens,
such as "Sure" or "Thank". We find that base models are significantly more
uncertain when generating these tokens. Leveraging this observation, nudging
employs a small aligned model to generate nudging tokens to steer the large
base model's output toward desired directions when the base model's uncertainty
is high. We evaluate the effectiveness of nudging across 3 model families and
13 tasks, covering reasoning, general knowledge, instruction following, and
safety benchmarks. Without any additional training, nudging a large base model
with a 7x - 14x smaller aligned model achieves zero-shot performance comparable
to, and sometimes surpassing, that of large aligned models. For example,
nudging OLMo-7b with OLMo-1b-instruct, affecting less than 9% of tokens,
achieves a 10% absolute improvement on GSM8K over OLMo-7b-instruct. Unlike
prior inference-time tuning methods, nudging enables off-the-shelf
collaboration between model families. For instance, nudging Gemma-2-27b with
Llama-2-7b-chat outperforms Llama-2-70b-chat on various tasks. Overall, this
work introduces a simple yet powerful approach to token-level model
collaboration, offering a modular solution to LLM alignment. Our project
website: https://fywalter.github.io/nudging/ .</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.09286v1' target='_blank'>Language-Model-Assisted Bi-Level Programming for Reward Learning from
  Internet Videos</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Harsh Mahesheka, Zhixian Xie, Zhaoran Wang, Wanxin Jin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-11 22:31:39</h6>
<p class='card-text'>Learning from Demonstrations, particularly from biological experts like
humans and animals, often encounters significant data acquisition challenges.
While recent approaches leverage internet videos for learning, they require
complex, task-specific pipelines to extract and retarget motion data for the
agent. In this work, we introduce a language-model-assisted bi-level
programming framework that enables a reinforcement learning agent to directly
learn its reward from internet videos, bypassing dedicated data preparation.
The framework includes two levels: an upper level where a vision-language model
(VLM) provides feedback by comparing the learner's behavior with expert videos,
and a lower level where a large language model (LLM) translates this feedback
into reward updates. The VLM and LLM collaborate within this bi-level
framework, using a "chain rule" approach to derive a valid search direction for
reward learning. We validate the method for reward learning from YouTube
videos, and the results have shown that the proposed method enables efficient
reward design from expert videos of biological agents for complex behavior
synthesis.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.09186v1' target='_blank'>Learning Algorithms Made Simple</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Noorbakhsh Amiri Golilarz, Elias Hossain, Abdoljalil Addeh, Keyan Alexander Rahimi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-11 18:39:25</h6>
<p class='card-text'>In this paper, we discuss learning algorithms and their importance in
different types of applications which includes training to identify important
patterns and features in a straightforward, easy-to-understand manner. We will
review the main concepts of artificial intelligence (AI), machine learning
(ML), deep learning (DL), and hybrid models. Some important subsets of Machine
Learning algorithms such as supervised, unsupervised, and reinforcement
learning are also discussed in this paper. These techniques can be used for
some important tasks like prediction, classification, and segmentation.
Convolutional Neural Networks (CNNs) are used for image and video processing
and many more applications. We dive into the architecture of CNNs and how to
integrate CNNs with ML algorithms to build hybrid models. This paper explores
the vulnerability of learning algorithms to noise, leading to
misclassification. We further discuss the integration of learning algorithms
with Large Language Models (LLM) to generate coherent responses applicable to
many domains such as healthcare, marketing, and finance by learning important
patterns from large volumes of data. Furthermore, we discuss the next
generation of learning algorithms and how we may have an unified Adaptive and
Dynamic Network to perform important tasks. Overall, this article provides
brief overview of learning algorithms, exploring their current state,
applications and future direction.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.08854v3' target='_blank'>Hybrid LLM-DDQN based Joint Optimization of V2I Communication and
  Autonomous Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zijiang Yan, Hao Zhou, Hina Tabassum, Xue Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-11 14:30:04</h6>
<p class='card-text'>Large language models (LLMs) have received considerable interest recently due
to their outstanding reasoning and comprehension capabilities. This work
explores applying LLMs to vehicular networks, aiming to jointly optimize
vehicle-to-infrastructure (V2I) communications and autonomous driving (AD)
policies. We deploy LLMs for AD decision-making to maximize traffic flow and
avoid collisions for road safety, and a double deep Q-learning algorithm (DDQN)
is used for V2I optimization to maximize the received data rate and reduce
frequent handovers. In particular, for LLM-enabled AD, we employ the Euclidean
distance to identify previously explored AD experiences, and then LLMs can
learn from past good and bad decisions for further improvement. Then, LLM-based
AD decisions will become part of states in V2I problems, and DDQN will optimize
the V2I decisions accordingly. After that, the AD and V2I decisions are
iteratively optimized until convergence. Such an iterative optimization
approach can better explore the interactions between LLMs and conventional
reinforcement learning techniques, revealing the potential of using LLMs for
network optimization and management. Finally, the simulations demonstrate that
our proposed hybrid LLM-DDQN approach outperforms the conventional DDQN
algorithm, showing faster convergence and higher average rewards.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.08632v1' target='_blank'>Words as Beacons: Guiding RL Agents with High-Level Language Prompts</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Unai Ruiz-Gonzalez, Alain Andres, Pedro G. Bascoy, Javier Del Ser</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-11 08:54:45</h6>
<p class='card-text'>Sparse reward environments in reinforcement learning (RL) pose significant
challenges for exploration, often leading to inefficient or incomplete learning
processes. To tackle this issue, this work proposes a teacher-student RL
framework that leverages Large Language Models (LLMs) as "teachers" to guide
the agent's learning process by decomposing complex tasks into subgoals. Due to
their inherent capability to understand RL environments based on a textual
description of structure and purpose, LLMs can provide subgoals to accomplish
the task defined for the environment in a similar fashion to how a human would
do. In doing so, three types of subgoals are proposed: positional targets
relative to the agent, object representations, and language-based instructions
generated directly by the LLM. More importantly, we show that it is possible to
query the LLM only during the training phase, enabling agents to operate within
the environment without any LLM intervention. We assess the performance of this
proposed framework by evaluating three state-of-the-art open-source LLMs
(Llama, DeepSeek, Qwen) eliciting subgoals across various procedurally
generated environment of the MiniGrid benchmark. Experimental results
demonstrate that this curriculum-based approach accelerates learning and
enhances exploration in complex tasks, achieving up to 30 to 200 times faster
convergence in training steps compared to recent baselines designed for sparse
reward environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.08475v2' target='_blank'>GIVE: Structured Reasoning of Large Language Models with Knowledge Graph
  Inspired Veracity Extrapolation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiashu He, Mingyu Derek Ma, Jinxuan Fan, Dan Roth, Wei Wang, Alejandro Ribeiro</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-11 03:05:06</h6>
<p class='card-text'>Existing approaches based on context prompting or reinforcement learning (RL)
to improve the reasoning capacities of large language models (LLMs) depend on
the LLMs' internal knowledge to produce reliable Chain-Of-Thought (CoT).
However, no matter the size of LLMs, certain problems cannot be resolved in a
single forward pass. Meanwhile, agent-based reasoning systems require access to
a comprehensive nonparametric knowledge base, which is often costly or not
feasible for use in scientific and niche domains. We present Graph Inspired
Veracity Extrapolation (GIVE), a novel reasoning method that merges parametric
and non-parametric memories to improve accurate reasoning with minimal external
input. GIVE guides the LLM agent to select the most pertinent expert data
(observe), engage in query-specific divergent thinking (reflect), and then
synthesize this information to produce the final output (speak). Extensive
experiments demonstrated the following benefits of our framework: (1) GIVE
boosts the performance of LLMs across various sizes. (2) In some scenarios,
GIVE allows smaller LLMs to surpass larger, more sophisticated ones in
scientific tasks (GPT3.5T + GIVE > GPT4). (3) GIVE is effective on scientific
and open-domain assessments. (4) GIVE is a training-free method that enables
LLMs to tackle new problems that extend beyond their training data (up to 43.5%
-> 88.2%} accuracy improvement). (5) GIVE allows LLM agents to reason using
both restricted (very small) and noisy (very large) knowledge sources,
accommodating knowledge graphs (KG) ranging from 135 to more than 840k nodes.
(6) The reasoning process involved in GIVE is fully interpretable.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.08345v1' target='_blank'>Large Legislative Models: Towards Efficient AI Policymaking in Economic
  Simulations</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Henry Gasztowtt, Benjamin Smith, Vincent Zhu, Qinxun Bai, Edwin Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-10 20:04:58</h6>
<p class='card-text'>The improvement of economic policymaking presents an opportunity for broad
societal benefit, a notion that has inspired research towards AI-driven
policymaking tools. AI policymaking holds the potential to surpass human
performance through the ability to process data quickly at scale. However,
existing RL-based methods exhibit sample inefficiency, and are further limited
by an inability to flexibly incorporate nuanced information into their
decision-making processes. Thus, we propose a novel method in which we instead
utilize pre-trained Large Language Models (LLMs), as sample-efficient
policymakers in socially complex multi-agent reinforcement learning (MARL)
scenarios. We demonstrate significant efficiency gains, outperforming existing
methods across three environments. Our code is available at
https://github.com/hegasz/large-legislative-models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.08193v3' target='_blank'>GenARM: Reward Guided Generation with Autoregressive Reward Model for
  Test-time Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuancheng Xu, Udari Madhushani Sehwag, Alec Koppel, Sicheng Zhu, Bang An, Furong Huang, Sumitra Ganesh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-10 17:58:24</h6>
<p class='card-text'>Large Language Models (LLMs) exhibit impressive capabilities but require
careful alignment with human preferences. Traditional training-time methods
finetune LLMs using human preference datasets but incur significant training
costs and require repeated training to handle diverse user preferences.
Test-time alignment methods address this by using reward models (RMs) to guide
frozen LLMs without retraining. However, existing test-time approaches rely on
trajectory-level RMs which are designed to evaluate complete responses, making
them unsuitable for autoregressive text generation that requires computing
next-token rewards from partial responses. To address this, we introduce
GenARM, a test-time alignment approach that leverages the Autoregressive Reward
Model--a novel reward parametrization designed to predict next-token rewards
for efficient and effective autoregressive generation. Theoretically, we
demonstrate that this parametrization can provably guide frozen LLMs toward any
distribution achievable by traditional RMs within the KL-regularized
reinforcement learning framework. Experimental results show that GenARM
significantly outperforms prior test-time alignment baselines and matches the
performance of training-time methods. Additionally, GenARM enables efficient
weak-to-strong guidance, aligning larger LLMs with smaller RMs without the high
costs of training larger models. Furthermore, GenARM supports multi-objective
alignment, allowing real-time trade-offs between preference dimensions and
catering to diverse user preferences without retraining. Our project page is
available at: https://genarm.github.io.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.08146v1' target='_blank'>Rewarding Progress: Scaling Automated Process Verifiers for LLM
  Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, Aviral Kumar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-10 17:31:23</h6>
<p class='card-text'>A promising approach for improving reasoning in large language models is to
use process reward models (PRMs). PRMs provide feedback at each step of a
multi-step reasoning trace, potentially improving credit assignment over
outcome reward models (ORMs) that only provide feedback at the final step.
However, collecting dense, per-step human labels is not scalable, and training
PRMs from automatically-labeled data has thus far led to limited gains. To
improve a base policy by running search against a PRM or using it as dense
rewards for reinforcement learning (RL), we ask: "How should we design process
rewards?". Our key insight is that, to be effective, the process reward for a
step should measure progress: a change in the likelihood of producing a correct
response in the future, before and after taking the step, corresponding to the
notion of step-level advantages in RL. Crucially, this progress should be
measured under a prover policy distinct from the base policy. We theoretically
characterize the set of good provers and our results show that optimizing
process rewards from such provers improves exploration during test-time search
and online RL. In fact, our characterization shows that weak prover policies
can substantially improve a stronger base policy, which we also observe
empirically. We validate our claims by training process advantage verifiers
(PAVs) to predict progress under such provers, and show that compared to ORMs,
test-time search against PAVs is $>8\%$ more accurate, and $1.5-5\times$ more
compute-efficient. Online RL with dense rewards from PAVs enables one of the
first results with $5-6\times$ gain in sample efficiency, and $>6\%$ gain in
accuracy, over ORMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.08048v1' target='_blank'>VerifierQ: Enhancing LLM Test Time Compute with Q-Learning-based
  Verifiers</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jianing Qi, Hao Tang, Zhigang Zhu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-10 15:43:55</h6>
<p class='card-text'>Recent advancements in test time compute, particularly through the use of
verifier models, have significantly enhanced the reasoning capabilities of
Large Language Models (LLMs). This generator-verifier approach closely
resembles the actor-critic framework in reinforcement learning (RL). However,
current verifier models in LLMs often rely on supervised fine-tuning without
temporal difference learning such as Q-learning. This paper introduces
VerifierQ, a novel approach that integrates Offline Q-learning into LLM
verifier models. We address three key challenges in applying Q-learning to
LLMs: (1) handling utterance-level Markov Decision Processes (MDPs), (2)
managing large action spaces, and (3) mitigating overestimation bias. VerifierQ
introduces a modified Bellman update for bounded Q-values, incorporates
Implicit Q-learning (IQL) for efficient action space management, and integrates
a novel Conservative Q-learning (CQL) formulation for balanced Q-value
estimation. Our method enables parallel Q-value computation and improving
training efficiency. While recent work has explored RL techniques like MCTS for
generators, VerifierQ is among the first to investigate the verifier (critic)
aspect in LLMs through Q-learning. This integration of RL principles into
verifier models complements existing advancements in generator techniques,
potentially enabling more robust and adaptive reasoning in LLMs. Experimental
results on mathematical reasoning tasks demonstrate VerifierQ's superior
performance compared to traditional supervised fine-tuning approaches, with
improvements in efficiency, accuracy and robustness. By enhancing the synergy
between generation and evaluation capabilities, VerifierQ contributes to the
ongoing evolution of AI systems in addressing complex cognitive tasks across
various domains.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.07927v1' target='_blank'>Efficient Reinforcement Learning with Large Language Model Priors</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xue Yan, Yan Song, Xidong Feng, Mengyue Yang, Haifeng Zhang, Haitham Bou Ammar, Jun Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-10 13:54:11</h6>
<p class='card-text'>In sequential decision-making (SDM) tasks, methods like reinforcement
learning (RL) and heuristic search have made notable advances in specific
cases. However, they often require extensive exploration and face challenges in
generalizing across diverse environments due to their limited grasp of the
underlying decision dynamics. In contrast, large language models (LLMs) have
recently emerged as powerful general-purpose tools, due to their capacity to
maintain vast amounts of domain-specific knowledge. To harness this rich prior
knowledge for efficiently solving complex SDM tasks, we propose treating LLMs
as prior action distributions and integrating them into RL frameworks through
Bayesian inference methods, making use of variational inference and direct
posterior sampling. The proposed approaches facilitate the seamless
incorporation of fixed LLM priors into both policy-based and value-based RL
frameworks. Our experiments show that incorporating LLM-based action priors
significantly reduces exploration and optimization complexity, substantially
improving sample efficiency compared to traditional RL techniques, e.g., using
LLM priors decreases the number of required samples by over 90% in offline
learning scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.07797v1' target='_blank'>Rewriting Conversational Utterances with Instructed Large Language
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Elnara Galimzhanova, Cristina Ioana Muntean, Franco Maria Nardini, Raffaele Perego, Guido Rocchietti</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-10 10:30:28</h6>
<p class='card-text'>Many recent studies have shown the ability of large language models (LLMs) to
achieve state-of-the-art performance on many NLP tasks, such as question
answering, text summarization, coding, and translation. In some cases, the
results provided by LLMs are on par with those of human experts. These models'
most disruptive innovation is their ability to perform tasks via zero-shot or
few-shot prompting. This capability has been successfully exploited to train
instructed LLMs, where reinforcement learning with human feedback is used to
guide the model to follow the user's requests directly. In this paper, we
investigate the ability of instructed LLMs to improve conversational search
effectiveness by rewriting user questions in a conversational setting. We study
which prompts provide the most informative rewritten utterances that lead to
the best retrieval performance. Reproducible experiments are conducted on
publicly-available TREC CAST datasets. The results show that rewriting
conversational utterances with instructed LLMs achieves significant
improvements of up to 25.2% in MRR, 31.7% in Precision@1, 27% in NDCG@3, and
11.5% in Recall@500 over state-of-the-art techniques.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.07745v3' target='_blank'>StepTool: Enhancing Multi-Step Tool Usage in LLMs through Step-Grained
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuanqing Yu, Zhefan Wang, Weizhi Ma, Shuai Wang, Chuhan Wu, Zhiqiang Guo, Min Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-10 09:23:26</h6>
<p class='card-text'>Despite powerful text generation capabilities, large language models (LLMs)
still need to learn how to utilize external tools to solve complex tasks, a
process known as tool learning. Existing methods primarily rely on supervised
fine-tuning to enhance tool-use capabilities, treating tool learning as a
text-generation task while overlooking the decision-making complexities
inherent in multi-step contexts. In this work, we propose modeling tool
learning as a dynamic decision-making task and introduce StepTool, a novel
step-grained reinforcement learning framework that enhances the multi-step tool
use capabilities of LLMs. StepTool consists of two main components:
Step-grained Reward Shaping, which assigns rewards at each tool interaction
based on the success of tool invocation and its contribution to the task; and
Step-grained Optimization, which uses policy gradient methods to optimize the
model in a multi-step manner. Experimental results demonstrate that StepTool
significantly outperforms existing methods in multi-step, tool-based tasks,
offering a robust solution for tool learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.07652v1' target='_blank'>StablePrompt: Automatic Prompt Tuning using Reinforcement Learning for
  Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Minchan Kwon, Gaeun Kim, Jongsuk Kim, Haeil Lee, Junmo Kim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-10 06:35:51</h6>
<p class='card-text'>Finding appropriate prompts for the specific task has become an important
issue as the usage of Large Language Models (LLM) has expanded. Reinforcement
Learning (RL) is widely used for prompt tuning, but its inherent instability
and environmental dependency make it difficult to use in practice. In this
paper, we propose StablePrompt, which strikes a balance between training
stability and search space, mitigating the instability of RL and producing
high-performance prompts. We formulate prompt tuning as an online RL problem
between the agent and target LLM and introduce Adaptive Proximal Policy
Optimization (APPO). APPO introduces an LLM anchor model to adaptively adjust
the rate of policy updates. This allows for flexible prompt search while
preserving the linguistic ability of the pre-trained LLM. StablePrompt
outperforms previous methods on various tasks including text classification,
question answering, and text generation. Our code can be found in github.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.07571v2' target='_blank'>How Does Vision-Language Adaptation Impact the Safety of Vision Language
  Models?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Seongyun Lee, Geewook Kim, Jiyeon Kim, Hyunji Lee, Hoyeon Chang, Sue Hyun Park, Minjoon Seo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-10 03:12:03</h6>
<p class='card-text'>Vision-Language adaptation (VL adaptation) transforms Large Language Models
(LLMs) into Large Vision-Language Models (LVLMs) for multimodal tasks, but this
process often compromises the inherent safety capabilities embedded in the
original LLMs. Despite potential harmfulness due to weakened safety measures,
in-depth analysis on the effects of VL adaptation on safety remains
under-explored. This study examines how VL adaptation influences safety and
evaluates the impact of safety fine-tuning methods. Our analysis reveals that
safety degradation occurs during VL adaptation, even when the training data is
safe. While safety tuning techniques like supervised fine-tuning with safety
datasets or reinforcement learning from human feedback mitigate some risks,
they still lead to safety degradation and a reduction in helpfulness due to
over-rejection issues. Further analysis of internal model weights suggests that
VL adaptation may impact certain safety-related layers, potentially lowering
overall safety levels. Additionally, our findings demonstrate that the
objectives of VL adaptation and safety tuning are divergent, which often
results in their simultaneous application being suboptimal. To address this, we
suggest the weight merging approach as an optimal solution effectively reducing
safety degradation while maintaining helpfulness. These insights help guide the
development of more reliable and secure LVLMs for real-world applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.06491v1' target='_blank'>Honesty to Subterfuge: In-Context Reinforcement Learning Can Make Honest
  Models Reward Hack</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Leo McKee-Reid, Christoph Sträter, Maria Angelica Martinez, Joe Needham, Mikita Balesni</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-09 02:34:27</h6>
<p class='card-text'>Previous work has shown that training "helpful-only" LLMs with reinforcement
learning on a curriculum of gameable environments can lead models to generalize
to egregious specification gaming, such as editing their own reward function or
modifying task checklists to appear more successful. We show that gpt-4o,
gpt-4o-mini, o1-preview, and o1-mini - frontier models trained to be helpful,
harmless, and honest - can engage in specification gaming without training on a
curriculum of tasks, purely from in-context iterative reflection (which we call
in-context reinforcement learning, "ICRL"). We also show that using ICRL to
generate highly-rewarded outputs for expert iteration (compared to the standard
expert iteration reinforcement learning algorithm) may increase gpt-4o-mini's
propensity to learn specification-gaming policies, generalizing (in very rare
cases) to the most egregious strategy where gpt-4o-mini edits its own reward
function. Our results point toward the strong ability of in-context reflection
to discover rare specification-gaming strategies that models might not exhibit
zero-shot or with normal training, highlighting the need for caution when
relying on alignment of LLMs in zero-shot settings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.09097v2' target='_blank'>Recent advancements in LLM Red-Teaming: Techniques, Defenses, and
  Ethical Considerations</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tarun Raheja, Nilay Pochhi, F. D. C. M. Curie</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-09 01:35:38</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated remarkable capabilities in
natural language processing tasks, but their vulnerability to jailbreak attacks
poses significant security risks. This survey paper presents a comprehensive
analysis of recent advancements in attack strategies and defense mechanisms
within the field of Large Language Model (LLM) red-teaming. We analyze various
attack methods, including gradient-based optimization, reinforcement learning,
and prompt engineering approaches. We discuss the implications of these attacks
on LLM safety and the need for improved defense mechanisms. This work aims to
provide a thorough understanding of the current landscape of red-teaming
attacks and defenses on LLMs, enabling the development of more secure and
reliable language models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.06293v1' target='_blank'>Accelerated Preference Optimization for Large Language Model Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiafan He, Huizhuo Yuan, Quanquan Gu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-08 18:51:01</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal
tool for aligning large language models (LLMs) with human preferences. Direct
Preference Optimization (DPO), one of the most popular approaches, formulates
RLHF as a policy optimization problem without explicitly estimating the reward
function. It overcomes the stability and efficiency issues of two-step
approaches, which typically involve first estimating the reward function and
then optimizing the policy via proximal policy optimization (PPO). Since RLHF
is essentially an optimization problem, and it is well-known that momentum
techniques can accelerate optimization both theoretically and empirically, a
natural question arises: Can RLHF be accelerated by momentum? This paper
answers this question in the affirmative. In detail, we first show that the
iterative preference optimization method can be viewed as a proximal point
method. Based on this observation, we propose a general Accelerated Preference
Optimization (APO) framework, which unifies many existing preference
optimization algorithms and employs Nesterov's momentum technique to speed up
the alignment of LLMs. Theoretically, we demonstrate that APO can achieve a
faster convergence rate than the standard iterative preference optimization
methods, including DPO and Self-Play Preference Optimization (SPPO).
Empirically, we show the superiority of APO over DPO, iterative DPO, and other
strong baselines for RLHF on the AlpacaEval 2.0 benchmark.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.06238v1' target='_blank'>EVOLvE: Evaluating and Optimizing LLMs For Exploration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Allen Nie, Yi Su, Bo Chang, Jonathan N. Lee, Ed H. Chi, Quoc V. Le, Minmin Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-08 17:54:03</h6>
<p class='card-text'>Despite their success in many domains, large language models (LLMs) remain
under-studied in scenarios requiring optimal decision-making under uncertainty.
This is crucial as many real-world applications, ranging from personalized
recommendations to healthcare interventions, demand that LLMs not only predict
but also actively learn to make optimal decisions through exploration. In this
work, we measure LLMs' (in)ability to make optimal decisions in bandits, a
state-less reinforcement learning setting relevant to many applications. We
develop a comprehensive suite of environments, including both context-free and
contextual bandits with varying task difficulties, to benchmark LLMs'
performance. Motivated by the existence of optimal exploration algorithms, we
propose efficient ways to integrate this algorithmic knowledge into LLMs: by
providing explicit algorithm-guided support during inference; and through
algorithm distillation via in-context demonstrations and fine-tuning, using
synthetic data generated from these algorithms. Impressively, these techniques
allow us to achieve superior exploration performance with smaller models,
surpassing larger models on various tasks. We conducted an extensive ablation
study to shed light on various factors, such as task difficulty and data
representation, that influence the efficiency of LLM exploration. Additionally,
we conduct a rigorous analysis of the LLM's exploration efficiency using the
concept of regret, linking its ability to explore to the model size and
underlying algorithm.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.06101v2' target='_blank'>Coevolving with the Other You: Fine-Tuning LLM with Sequential
  Cooperative Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hao Ma, Tianyi Hu, Zhiqiang Pu, Boyin Liu, Xiaolin Ai, Yanyan Liang, Min Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-08 14:55:26</h6>
<p class='card-text'>Reinforcement learning (RL) has emerged as a pivotal technique for
fine-tuning large language models (LLMs) on specific tasks. However, prevailing
RL fine-tuning methods predominantly rely on PPO and its variants. Though these
algorithms are effective in general RL settings, they often exhibit suboptimal
performance and vulnerability to distribution collapse when applied to the
fine-tuning of LLMs. In this paper, we propose CORY, extending the RL
fine-tuning of LLMs to a sequential cooperative multi-agent reinforcement
learning framework, to leverage the inherent coevolution and emergent
capabilities of multi-agent systems. In CORY, the LLM to be fine-tuned is
initially duplicated into two autonomous agents: a pioneer and an observer. The
pioneer generates responses based on queries, while the observer generates
responses using both the queries and the pioneer's responses. The two agents
are trained together. During training, the agents exchange roles periodically,
fostering cooperation and coevolution between them. Experiments evaluate CORY's
performance by fine-tuning GPT-2 and Llama-2 under subjective and objective
reward functions on the IMDB Review and GSM8K datasets, respectively. Results
show that CORY outperforms PPO in terms of policy optimality, resistance to
distribution collapse, and training robustness, thereby underscoring its
potential as a superior methodology for refining LLMs in real-world
applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.05939v1' target='_blank'>RLRF4Rec: Reinforcement Learning from Recsys Feedback for Enhanced
  Recommendation Reranking</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chao Sun, Yaobo Liang, Yaming Yang, Shilin Xu, Tianmeng Yang, Yunhai Tong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-08 11:42:37</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated remarkable performance across
diverse domains, prompting researchers to explore their potential for use in
recommendation systems. Initial attempts have leveraged the exceptional
capabilities of LLMs, such as rich knowledge and strong generalization through
In-context Learning, which involves phrasing the recommendation task as
prompts. Nevertheless, the performance of LLMs in recommendation tasks remains
suboptimal due to a substantial disparity between the training tasks for LLMs
and recommendation tasks and inadequate recommendation data during
pre-training. This paper introduces RLRF4Rec, a novel framework integrating
Reinforcement Learning from Recsys Feedback for Enhanced Recommendation
Reranking(RLRF4Rec) with LLMs to address these challenges. Specifically, We
first have the LLM generate inferred user preferences based on user interaction
history, which is then used to augment traditional ID-based sequence
recommendation models. Subsequently, we trained a reward model based on
knowledge augmentation recommendation models to evaluate the quality of the
reasoning knowledge from LLM. We then select the best and worst responses from
the N samples to construct a dataset for LLM tuning. Finally, we design a
structure alignment strategy with Direct Preference Optimization(DPO). We
validate the effectiveness of RLRF4Rec through extensive experiments,
demonstrating significant improvements in recommendation re-ranking metrics
compared to baselines. This demonstrates that our approach significantly
improves the capability of LLMs to respond to instructions within recommender
systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.05821v1' target='_blank'>A Zero-Shot approach to the Conversational Tree Search Task</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dirk Väth, Ngoc Thang Vu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-08 08:51:44</h6>
<p class='card-text'>In sensitive domains, such as legal or medial domains, the correctness of
information given to users is critical. To address this, the recently
introduced task Conversational Tree Search (CTS) provides a graph-based
framework for controllable task-oriented dialog in sensitive domains. However,
a big drawback of state-of-the-art CTS agents is their long training time,
which is especially problematic as a new agent must be trained every time the
associated domain graph is updated. The goal of this paper is to eliminate the
need for training CTS agents altogether. To achieve this, we implement a novel
LLM-based method for zero-shot, controllable CTS agents. We show that these
agents significantly outperform state-of-the-art CTS agents (p<0.0001; Barnard
Exact test) in simulation. This generalizes to all available CTS domains.
Finally, we perform user evaluation to test the agent performance in the wild,
showing that our policy significantly (p<0.05; Barnard Exact) improves
task-success compared to the state-of-the-art Reinforcement Learning-based CTS
agent.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.07245v1' target='_blank'>AAAI Workshop on AI Planning for Cyber-Physical Systems -- CAIPI24</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Oliver Niggemann, Gautam Biswas, Alexander Diedrich, Jonas Ehrhardt, René Heesch, Niklas Widulle</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-08 05:52:00</h6>
<p class='card-text'>The workshop 'AI-based Planning for Cyber-Physical Systems', which took place
on February 26, 2024, as part of the 38th Annual AAAI Conference on Artificial
Intelligence in Vancouver, Canada, brought together researchers to discuss
recent advances in AI planning methods for Cyber-Physical Systems (CPS). CPS
pose a major challenge due to their complexity and data-intensive nature, which
often exceeds the capabilities of traditional planning algorithms. The workshop
highlighted new approaches such as neuro-symbolic architectures, large language
models (LLMs), deep reinforcement learning and advances in symbolic planning.
These techniques are promising when it comes to managing the complexity of CPS
and have potential for real-world applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.05656v1' target='_blank'>On the Modeling Capabilities of Large Language Models for Sequential
  Decision Making</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Martin Klissarov, Devon Hjelm, Alexander Toshev, Bogdan Mazoure</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-08 03:12:57</h6>
<p class='card-text'>Large pretrained models are showing increasingly better performance in
reasoning and planning tasks across different modalities, opening the
possibility to leverage them for complex sequential decision making problems.
In this paper, we investigate the capabilities of Large Language Models (LLMs)
for reinforcement learning (RL) across a diversity of interactive domains. We
evaluate their ability to produce decision-making policies, either directly, by
generating actions, or indirectly, by first generating reward models to train
an agent with RL. Our results show that, even without task-specific
fine-tuning, LLMs excel at reward modeling. In particular, crafting rewards
through artificial intelligence (AI) feedback yields the most generally
applicable approach and can enhance performance by improving credit assignment
and exploration. Finally, in environments with unfamiliar dynamics, we explore
how fine-tuning LLMs with synthetic data can significantly improve their reward
modeling capabilities while mitigating catastrophic forgetting, further
broadening their utility in sequential decision-making tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.05362v2' target='_blank'>LLMs Are In-Context Bandit Reinforcement Learners</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Giovanni Monea, Antoine Bosselut, Kianté Brantley, Yoav Artzi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-07 17:45:00</h6>
<p class='card-text'>Large Language Models (LLMs) excel at in-context learning (ICL), a supervised
learning technique that relies on adding annotated examples to the model
context. We investigate a contextual bandit version of in-context reinforcement
learning (ICRL), where models learn in-context, online, from external reward,
instead of supervised data. We show that LLMs effectively demonstrate such
learning, and provide a detailed study of the phenomena, experimenting with
challenging classification tasks and models of sizes from 500M to 70B
parameters. This includes identifying and addressing the instability of the
process, demonstrating learning with both semantic and abstract labels, and
showing scaling trends. Our findings highlight ICRL capabilities in LLMs, while
also underscoring fundamental limitations in their implicit reasoning about
errors.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.04834v2' target='_blank'>As Simple as Fine-tuning: LLM Alignment via Bidirectional Negative
  Feedback Loss</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xin Mao, Feng-Lin Li, Huimin Xu, Wei Zhang, Wang Chen, Anh Tuan Luu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-07 08:44:04</h6>
<p class='card-text'>Direct Preference Optimization (DPO) has emerged as a more computationally
efficient alternative to Reinforcement Learning from Human Feedback (RLHF) with
Proximal Policy Optimization (PPO), eliminating the need for reward models and
online sampling. Despite these benefits, DPO and its variants remain sensitive
to hyper-parameters and prone to instability, particularly on mathematical
datasets. We argue that these issues arise from the unidirectional
likelihood-derivative negative feedback inherent in the log-likelihood loss
function. To address this, we propose a novel LLM alignment loss that
establishes a stable Bidirectional Negative Feedback (BNF) during optimization.
Our proposed BNF loss eliminates the need for pairwise contrastive losses and
does not require any extra tunable hyper-parameters or pairwise preference
data, streamlining the alignment pipeline to be as simple as supervised
fine-tuning. We conduct extensive experiments across two challenging QA
benchmarks and four reasoning benchmarks. The experimental results show that
BNF achieves comparable performance to the best methods on QA benchmarks, while
its performance decrease on the four reasoning benchmarks is significantly
lower compared to the best methods, thus striking a better balance between
value alignment and reasoning ability. In addition, we further validate the
performance of BNF on non-pairwise datasets, and conduct in-depth analysis of
log-likelihood and logit shifts across different preference optimization
methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.04683v2' target='_blank'>Towards Measuring Goal-Directedness in AI Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dylan Xu, Juan-Pablo Rivera</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-07 01:34:42</h6>
<p class='card-text'>Recent advances in deep learning have brought attention to the possibility of
creating advanced, general AI systems that outperform humans across many tasks.
However, if these systems pursue unintended goals, there could be catastrophic
consequences. A key prerequisite for AI systems pursuing unintended goals is
whether they will behave in a coherent and goal-directed manner in the first
place, optimizing for some unknown goal; there exists significant research
trying to evaluate systems for said behaviors. However, the most rigorous
definitions of goal-directedness we currently have are difficult to compute in
real-world settings. Drawing upon this previous literature, we explore policy
goal-directedness within reinforcement learning (RL) environments. In our
findings, we propose a different family of definitions of the goal-directedness
of a policy that analyze whether it is well-modeled as near-optimal for many
(sparse) reward functions. We operationalize this preliminary definition of
goal-directedness and test it in toy Markov decision process (MDP)
environments. Furthermore, we explore how goal-directedness could be measured
in frontier large-language models (LLMs). Our contribution is a definition of
goal-directedness that is simpler and more easily computable in order to
approach the question of whether AI systems could pursue dangerous goals. We
recommend further exploration of measuring coherence and goal-directedness,
based on our findings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.04612v1' target='_blank'>Regressing the Relative Future: Efficient Policy Optimization for
  Multi-turn RLHF</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhaolin Gao, Wenhao Zhan, Jonathan D. Chang, Gokul Swamy, Kianté Brantley, Jason D. Lee, Wen Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-06 20:20:22</h6>
<p class='card-text'>Large Language Models (LLMs) have achieved remarkable success at tasks like
summarization that involve a single turn of interaction. However, they can
still struggle with multi-turn tasks like dialogue that require long-term
planning. Previous works on multi-turn dialogue extend single-turn
reinforcement learning from human feedback (RLHF) methods to the multi-turn
setting by treating all prior dialogue turns as a long context. Such approaches
suffer from covariate shift: the conversations in the training set have
previous turns generated by some reference policy, which means that low
training error may not necessarily correspond to good performance when the
learner is actually in the conversation loop. In response, we introduce
REgressing the RELative FUture (REFUEL), an efficient policy optimization
approach designed to address multi-turn RLHF in LLMs. REFUEL employs a single
model to estimate $Q$-values and trains on self-generated data, addressing the
covariate shift issue. REFUEL frames the multi-turn RLHF problem as a sequence
of regression tasks on iteratively collected datasets, enabling ease of
implementation. Theoretically, we prove that REFUEL can match the performance
of any policy covered by the training set. Empirically, we evaluate our
algorithm by using Llama-3.1-70B-it to simulate a user in conversation with our
model. REFUEL consistently outperforms state-of-the-art methods such as DPO and
REBEL across various settings. Furthermore, despite having only 8 billion
parameters, Llama-3-8B-it fine-tuned with REFUEL outperforms Llama-3.1-70B-it
on long multi-turn dialogues. Implementation of REFUEL can be found at
https://github.com/ZhaolinGao/REFUEL/, and models trained by REFUEL can be
found at https://huggingface.co/Cornell-AGI.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.04452v1' target='_blank'>MindScope: Exploring cognitive biases in large language models through
  Multi-Agent Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhentao Xie, Jiabao Zhao, Yilei Wang, Jinxin Shi, Yanhong Bai, Xingjiao Wu, Liang He</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-06 11:23:56</h6>
<p class='card-text'>Detecting cognitive biases in large language models (LLMs) is a fascinating
task that aims to probe the existing cognitive biases within these models.
Current methods for detecting cognitive biases in language models generally
suffer from incomplete detection capabilities and a restricted range of
detectable bias types. To address this issue, we introduced the 'MindScope'
dataset, which distinctively integrates static and dynamic elements. The static
component comprises 5,170 open-ended questions spanning 72 cognitive bias
categories. The dynamic component leverages a rule-based, multi-agent
communication framework to facilitate the generation of multi-round dialogues.
This framework is flexible and readily adaptable for various psychological
experiments involving LLMs. In addition, we introduce a multi-agent detection
method applicable to a wide range of detection tasks, which integrates
Retrieval-Augmented Generation (RAG), competitive debate, and a reinforcement
learning-based decision module. Demonstrating substantial effectiveness, this
method has shown to improve detection accuracy by as much as 35.10% compared to
GPT-4. Codes and appendix are available at
https://github.com/2279072142/MindScope.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.04346v1' target='_blank'>Ordinal Preference Optimization: Aligning Human Preferences via NDCG</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yang Zhao, Yixin Wang, Mingzhang Yin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-06 03:49:28</h6>
<p class='card-text'>Aligning Large Language Models (LLMs) with diverse human preferences is a
pivotal technique for controlling model behaviors and enhancing generation
quality. Reinforcement Learning from Human Feedback (RLHF), Direct Preference
Optimization (DPO), and their variants optimize language models by pairwise
comparisons. However, when multiple responses are available, these approaches
fall short of leveraging the extensive information in the ranking given by the
reward models or human feedback. In this work, we propose a novel listwise
approach named Ordinal Preference Optimization (OPO), which employs the
Normalized Discounted Cumulative Gain (NDCG), a widely-used ranking metric, to
better utilize relative proximity within ordinal multiple responses. We develop
an end-to-end preference optimization algorithm by approximating NDCG with a
differentiable surrogate loss. This approach builds a connection between
ranking models in information retrieval and the alignment problem. In aligning
multi-response datasets assigned with ordinal rewards, OPO outperforms existing
pairwise and listwise approaches on evaluation sets and general benchmarks like
AlpacaEval. Moreover, we demonstrate that increasing the pool of negative
samples can enhance model performance by reducing the adverse effects of
trivial negatives.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.05328v1' target='_blank'>Reward Learning From Preference With Ties</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jinsong Liu, Dongdong Ge, Ruihao Zhu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-05 21:02:57</h6>
<p class='card-text'>Reward learning plays a pivotal role in Reinforcement Learning from Human
Feedback (RLHF), ensuring the alignment of language models. The Bradley-Terry
(BT) model stands as the prevalent choice for capturing human preferences from
datasets containing pairs of chosen and rejected responses. In preference
modeling, the focus is not on absolute values but rather on the reward
difference between chosen and rejected responses, referred to as preference
strength. Thus, precise evaluation of preference strength holds paramount
importance in preference modeling. However, an easily overlooked factor
significantly affecting preference strength measurement is that human attitudes
towards two responses may not solely indicate a preference for one over the
other and ties are also a common occurrence. To address this, we propose the
adoption of the generalized Bradley-Terry model -- the Bradley-Terry model with
ties (BTT) -- to accommodate tied preferences, thus leveraging additional
information. We prove that even with the access to the true distributions of
prompt and response, disregarding ties can lead to a notable bias in preference
strength measurement. Comprehensive experiments further validate the advantages
of incorporating ties in preference modeling. Notably, fine-tuning with BTT
significantly outperforms fine-tuning with BT on synthetic preference datasets
with ties, labeled by state-of-the-art open-source LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.04112v1' target='_blank'>Exploring LLM-based Data Annotation Strategies for Medical Dialogue
  Preference Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chengfeng Dou, Ying Zhang, Zhi Jin, Wenpin Jiao, Haiyan Zhao, Yongqiang Zhao, Zhengwei Tao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-05 10:29:19</h6>
<p class='card-text'>This research examines the use of Reinforcement Learning from AI Feedback
(RLAIF) techniques to improve healthcare dialogue models, with the aim of
tackling the challenges of preference-aligned data annotation while reducing
the reliance on medical experts. We argue that the primary challenges in
current RLAIF research for healthcare are the limitations of automated
evaluation methods and the difficulties in accurately representing physician
preferences. To address these challenges, we present a new evaluation framework
based on standardized patient examinations. This framework is designed to
objectively assess the effectiveness of large language models (LLMs) in guiding
users and following instructions, enabling a comprehensive comparison across
different models. Furthermore, our investigation of effective ways to express
physician preferences using Constitutional AI algorithms highlighted the
particular effectiveness of flowcharts. Utilizing this finding, we introduce an
innovative agent-based approach for annotating preference data. This approach
autonomously creates medical dialogue flows tailored to the patient's
condition, demonstrates strong generalization abilities, and reduces the need
for expert involvement. Our results show that the agent-based approach
outperforms existing RLAIF annotation methods in standardized patient
examinations and surpasses current open source medical dialogue LLMs in various
test scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.04064v2' target='_blank'>Text2Chart31: Instruction Tuning for Chart Generation with Automatic
  Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fatemeh Pesaran Zadeh, Juyeon Kim, Jin-Hwa Kim, Gunhee Kim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-05 07:25:56</h6>
<p class='card-text'>Large language models (LLMs) have demonstrated strong capabilities across
various language tasks, notably through instruction-tuning methods. However,
LLMs face challenges in visualizing complex, real-world data through charts and
plots. Firstly, existing datasets rarely cover a full range of chart types,
such as 3D, volumetric, and gridded charts. Secondly, supervised fine-tuning
methods do not fully leverage the intricate relationships within rich datasets,
including text, code, and figures. To address these challenges, we propose a
hierarchical pipeline and a new dataset for chart generation. Our dataset,
Text2Chart31, includes 31 unique plot types referring to the Matplotlib
library, with 11.1K tuples of descriptions, code, data tables, and plots.
Moreover, we introduce a reinforcement learning-based instruction tuning
technique for chart generation tasks without requiring human feedback. Our
experiments show that this approach significantly enhances the model
performance, enabling smaller models to outperform larger open-source models
and be comparable to state-of-the-art proprietary models in data visualization
tasks. We make the code and dataset available at
https://github.com/fatemehpesaran310/Text2Chart31.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.03997v1' target='_blank'>YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuan Zhuang, Yi Shen, Zhili Zhang, Yuxiao Chen, Fei Miao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-05 01:44:11</h6>
<p class='card-text'>Advancements in deep multi-agent reinforcement learning (MARL) have
positioned it as a promising approach for decision-making in cooperative games.
However, it still remains challenging for MARL agents to learn cooperative
strategies for some game environments. Recently, large language models (LLMs)
have demonstrated emergent reasoning capabilities, making them promising
candidates for enhancing coordination among the agents. However, due to the
model size of LLMs, it can be expensive to frequently infer LLMs for actions
that agents can take. In this work, we propose You Only LLM Once for MARL
(YOLO-MARL), a novel framework that leverages the high-level task planning
capabilities of LLMs to improve the policy learning process of multi-agents in
cooperative games. Notably, for each game environment, YOLO-MARL only requires
one time interaction with LLMs in the proposed strategy generation, state
interpretation and planning function generation modules, before the MARL policy
training process. This avoids the ongoing costs and computational time
associated with frequent LLMs API calls during training. Moreover, the trained
decentralized normal-sized neural network-based policies operate independently
of the LLM. We evaluate our method across three different environments and
demonstrate that YOLO-MARL outperforms traditional MARL algorithms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.03642v2' target='_blank'>Aligning LLMs with Individual Preferences via Interaction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shujin Wu, May Fung, Cheng Qian, Jeonghwan Kim, Dilek Hakkani-Tur, Heng Ji</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-04 17:48:29</h6>
<p class='card-text'>As large language models (LLMs) demonstrate increasingly advanced
capabilities, aligning their behaviors with human values and preferences
becomes crucial for their wide adoption. While previous research focuses on
general alignment to principles such as helpfulness, harmlessness, and honesty,
the need to account for individual and diverse preferences has been largely
overlooked, potentially undermining customized human experiences. To address
this gap, we train LLMs that can ''interact to align'', essentially cultivating
the meta-skill of LLMs to implicitly infer the unspoken personalized
preferences of the current user through multi-turn conversations, and then
dynamically align their following behaviors and responses to these inferred
preferences. Our approach involves establishing a diverse pool of 3,310
distinct user personas by initially creating seed examples, which are then
expanded through iterative self-generation and filtering. Guided by distinct
user personas, we leverage multi-LLM collaboration to develop a multi-turn
preference dataset containing 3K+ multi-turn conversations in tree structures.
Finally, we apply supervised fine-tuning and reinforcement learning to enhance
LLMs using this dataset. For evaluation, we establish the ALOE (ALign With
CustOmized PrEferences) benchmark, consisting of 100 carefully selected
examples and well-designed metrics to measure the customized alignment
performance during conversations. Experimental results demonstrate the
effectiveness of our method in enabling dynamic, personalized alignment via
interaction.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.03439v2' target='_blank'>ToolGen: Unified Tool Retrieval and Calling via Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Renxi Wang, Xudong Han, Lei Ji, Shu Wang, Timothy Baldwin, Haonan Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-04 13:52:32</h6>
<p class='card-text'>As large language models (LLMs) advance, their inability to autonomously
execute tasks by directly interacting with external tools remains a critical
limitation. Traditional methods rely on inputting tool descriptions as context,
which is constrained by context length and requires separate, often
inefficient, retrieval mechanisms. We introduce ToolGen, a paradigm shift that
integrates tool knowledge directly into the LLM's parameters by representing
each tool as a unique token. This enables the LLM to generate tool calls and
arguments as part of its next token prediction capabilities, seamlessly
blending tool invocation with language generation. Our framework allows the LLM
to access and utilize a vast amount of tools with no additional retrieval step,
significantly enhancing both performance and scalability. Experimental results
with over 47,000 tools show that ToolGen not only achieves superior results in
both tool retrieval and autonomous task completion but also sets the stage for
a new era of AI agents that can adapt to tools across diverse domains. By
fundamentally transforming tool retrieval into a generative process, ToolGen
paves the way for more versatile, efficient, and autonomous AI systems. ToolGen
enables end-to-end tool learning and opens opportunities for integration with
other advanced techniques such as chain-of-thought and reinforcement learning,
thereby expanding the practical capabilities of LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.03212v1' target='_blank'>Data-Efficient Massive Tool Retrieval: A Reinforcement Learning Approach
  for Query-Tool Alignment with Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuxiang Zhang, Xin Fan, Junjie Wang, Chongxian Chen, Fan Mo, Tetsuya Sakai, Hayato Yamana</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-04 07:58:05</h6>
<p class='card-text'>Recent advancements in large language models (LLMs) integrated with external
tools and APIs have successfully addressed complex tasks by using in-context
learning or fine-tuning. Despite this progress, the vast scale of tool
retrieval remains challenging due to stringent input length constraints. In
response, we propose a pre-retrieval strategy from an extensive repository,
effectively framing the problem as the massive tool retrieval (MTR) task. We
introduce the MTRB (massive tool retrieval benchmark) to evaluate real-world
tool-augmented LLM scenarios with a large number of tools. This benchmark is
designed for low-resource scenarios and includes a diverse collection of tools
with descriptions refined for consistency and clarity. It consists of three
subsets, each containing 90 test samples and 10 training samples. To handle the
low-resource MTR task, we raise a new query-tool alignment (QTA) framework
leverages LLMs to enhance query-tool alignment by rewriting user queries
through ranking functions and the direct preference optimization (DPO) method.
This approach consistently outperforms existing state-of-the-art models in
top-5 and top-10 retrieval tasks across the MTRB benchmark, with improvements
up to 93.28% based on the metric Sufficiency@k, which measures the adequacy of
tool retrieval within the first k results. Furthermore, ablation studies
validate the efficacy of our framework, highlighting its capacity to optimize
performance even with limited annotated samples. Specifically, our framework
achieves up to 78.53% performance improvement in Sufficiency@k with just a
single annotated sample. Additionally, QTA exhibits strong cross-dataset
generalizability, emphasizing its potential for real-world applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.03145v1' target='_blank'>Margin Matching Preference Optimization: Enhanced Model Alignment with
  Granular Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kyuyoung Kim, Ah Jeong Seo, Hao Liu, Jinwoo Shin, Kimin Lee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-04 04:56:11</h6>
<p class='card-text'>Large language models (LLMs) fine-tuned with alignment techniques, such as
reinforcement learning from human feedback, have been instrumental in
developing some of the most capable AI systems to date. Despite their success,
existing methods typically rely on simple binary labels, such as those
indicating preferred outputs in pairwise preferences, which fail to capture the
subtle differences in relative quality between pairs. To address this
limitation, we introduce an approach called Margin Matching Preference
Optimization (MMPO), which incorporates relative quality margins into
optimization, leading to improved LLM policies and reward models. Specifically,
given quality margins in pairwise preferences, we design soft target
probabilities based on the Bradley-Terry model, which are then used to train
models with the standard cross-entropy objective. Experiments with both human
and AI feedback data demonstrate that MMPO consistently outperforms baseline
methods, often by a substantial margin, on popular benchmarks including
MT-bench and RewardBench. Notably, the 7B model trained with MMPO achieves
state-of-the-art performance on RewardBench as of June 2024, outperforming
other models of the same scale. Our analysis also shows that MMPO is more
robust to overfitting, leading to better-calibrated models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.03138v2' target='_blank'>Can LLMs Generate Diverse Molecules? Towards Alignment with Structural
  Diversity</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hyosoon Jang, Yunhui Jang, Jaehyung Kim, Sungsoo Ahn</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-04 04:25:36</h6>
<p class='card-text'>Recent advancements in large language models (LLMs) have demonstrated
impressive performance in molecular generation, which offers potential to
accelerate drug discovery. However, the current LLMs overlook a critical
requirement for drug discovery: proposing a diverse set of molecules. This
diversity is essential for improving the chances of finding a viable drug, as
it provides alternative molecules that may succeed where others fail in
real-world validations. Nevertheless, the LLMs often output structurally
similar molecules. While decoding schemes like diverse beam search may enhance
textual diversity, this often does not align with molecular structural
diversity. In response, we propose a new method for fine-tuning molecular
generative LLMs to autoregressively generate a set of structurally diverse
molecules, where each molecule is generated by conditioning on the previously
generated molecules. Our approach consists of two stages: (1) supervised
fine-tuning to adapt LLMs to autoregressively generate molecules in a sequence
and (2) reinforcement learning to maximize structural diversity within the
generated molecules. Our experiments show that the proposed approach enables
LLMs to generate diverse molecules better than existing approaches for diverse
sequence generation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.02884v2' target='_blank'>LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level
  Mathematical Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco Pavone, Yuqiang Li, Wanli Ouyang, Dongzhan Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-03 18:12:29</h6>
<p class='card-text'>This paper presents an advanced mathematical problem-solving framework,
LLaMA-Berry, for enhancing the mathematical reasoning ability of Large Language
Models (LLMs). The framework combines Monte Carlo Tree Search (MCTS) with
iterative Self-Refine to optimize the reasoning path and utilizes a pairwise
reward model to evaluate different paths globally. By leveraging the
self-critic and rewriting capabilities of LLMs, Self-Refine applied to MCTS
(SR-MCTS) overcomes the inefficiencies and limitations of conventional
step-wise and greedy search algorithms by fostering a more efficient
exploration of solution spaces. Pairwise Preference Reward Model~(PPRM),
inspired by Reinforcement Learning from Human Feedback (RLHF), is then used to
model pairwise preferences between solutions, utilizing an Enhanced Borda Count
(EBC) method to synthesize these preferences into a global ranking score to
find better answers. This approach addresses the challenges of scoring
variability and non-independent distributions in mathematical reasoning tasks.
The framework has been tested on general and advanced benchmarks, showing
superior performance in terms of search efficiency and problem-solving
capability compared to existing methods like ToT and rStar, particularly in
complex Olympiad-level benchmarks, including GPQA, AIME24 and AMC23.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.02743v2' target='_blank'>MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yekun Chai, Haoran Sun, Huang Fang, Shuohuan Wang, Yu Sun, Hua Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-03 17:55:13</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) has demonstrated
effectiveness in aligning large language models (LLMs) with human preferences.
However, token-level RLHF suffers from the credit assignment problem over long
sequences, where delayed rewards make it challenging for the model to discern
which actions contributed to preferred outcomes. This hinders learning
efficiency and slows convergence.In this paper, we propose MA-RLHF, a simple
yet effective RLHF framework that incorporates macro actions -- sequences of
tokens or higher-level language constructs -- into the learning process. By
operating at higher level of abstraction, our approach reduces the temporal
distance between actions and rewards, facilitating faster and more accurate
credit assignment. This results in more stable policy gradient estimates and
enhances learning efficiency within each episode, all without increasing
computational complexity during training or inference. We validate our approach
through extensive experiments across various model sizes and tasks, including
text summarization, dialogue generation, question answering, and program
synthesis. Our method achieves substantial performance improvements over
standard RLHF, with performance gains of up to 30% in text summarization and
code generation, 18% in dialogue, and 8% in question answering tasks. Notably,
our approach reaches parity with vanilla RLHF 1.7 ~ 2 times faster in terms of
training time and continues to outperform it with further training. We make our
code and data publicly available at https://github.com/ernie-research/MA-RLHF.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.02511v1' target='_blank'>Choices are More Important than Efforts: LLM Enables Efficient
  Multi-Agent Exploration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yun Qu, Boyuan Wang, Yuhang Jiang, Jianzhun Shao, Yixiu Mao, Cheems Wang, Chang Liu, Xiangyang Ji</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-03 14:21:23</h6>
<p class='card-text'>With expansive state-action spaces, efficient multi-agent exploration remains
a longstanding challenge in reinforcement learning. Although pursuing novelty,
diversity, or uncertainty attracts increasing attention, redundant efforts
brought by exploration without proper guidance choices poses a practical issue
for the community. This paper introduces a systematic approach, termed LEMAE,
choosing to channel informative task-relevant guidance from a knowledgeable
Large Language Model (LLM) for Efficient Multi-Agent Exploration. Specifically,
we ground linguistic knowledge from LLM into symbolic key states, that are
critical for task fulfillment, in a discriminative manner at low LLM inference
costs. To unleash the power of key states, we design Subspace-based Hindsight
Intrinsic Reward (SHIR) to guide agents toward key states by increasing reward
density. Additionally, we build the Key State Memory Tree (KSMT) to track
transitions between key states in a specific task for organized exploration.
Benefiting from diminishing redundant explorations, LEMAE outperforms existing
SOTA approaches on the challenging benchmarks (e.g., SMAC and MPE) by a large
margin, achieving a 10x acceleration in certain scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.02504v2' target='_blank'>Dual Active Learning for Reinforcement Learning from Human Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pangpang Liu, Chengchun Shi, Will Wei Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-03 14:09:58</h6>
<p class='card-text'>Aligning large language models (LLMs) with human preferences is critical to
recent advances in generative artificial intelligence. Reinforcement learning
from human feedback (RLHF) is widely applied to achieve this objective. A key
step in RLHF is to learn the reward function from human feedback. However,
human feedback is costly and time-consuming, making it essential to collect
high-quality conversation data for human teachers to label. Additionally,
different human teachers have different levels of expertise. It is thus
critical to query the most appropriate teacher for their opinions. In this
paper, we use offline reinforcement learning (RL) to formulate the alignment
problem. Motivated by the idea of $D$-optimal design, we first propose a dual
active reward learning algorithm for the simultaneous selection of
conversations and teachers. Next, we apply pessimistic RL to solve the
alignment problem, based on the learned reward estimator. Theoretically, we
show that the reward estimator obtained through our proposed adaptive selection
strategy achieves minimal generalized variance asymptotically, and prove that
the sub-optimality of our pessimistic policy scales as $O(1/\sqrt{T})$ with a
given sample budget $T$. Through simulations and experiments on LLMs, we
demonstrate the effectiveness of our algorithm and its superiority over
state-of-the-arts.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.02229v1' target='_blank'>CodePMP: Scalable Preference Model Pretraining for Large Language Model
  Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Huimu Yu, Xing Wu, Weidong Yin, Debing Zhang, Songlin Hu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-03 05:51:26</h6>
<p class='card-text'>Large language models (LLMs) have made significant progress in natural
language understanding and generation, driven by scalable pretraining and
advanced finetuning. However, enhancing reasoning abilities in LLMs,
particularly via reinforcement learning from human feedback (RLHF), remains
challenging due to the scarcity of high-quality preference data, which is
labor-intensive to annotate and crucial for reward model (RM) finetuning. To
alleviate this issue, we introduce CodePMP, a scalable preference model
pretraining (PMP) pipeline that utilizes a large corpus of synthesized
code-preference pairs from publicly available high-quality source code. CodePMP
improves RM finetuning efficiency by pretraining preference models on
large-scale synthesized code-preference pairs. We evaluate CodePMP on
mathematical reasoning tasks (GSM8K, MATH) and logical reasoning tasks (ReClor,
LogiQA2.0), consistently showing significant improvements in reasoning
performance of LLMs and highlighting the importance of scalable preference
model pretraining for efficient reward modeling.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.02141v3' target='_blank'>E2H: A Two-Stage Non-Invasive Neural Signal Driven Humanoid Robotic
  Whole-Body Control Framework</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yiqun Duan, Qiang Zhang, Jinzhao Zhou, Jingkai Sun, Xiaowei Jiang, Jiahang Cao, Jiaxu Wang, Yiqian Yang, Wen Zhao, Gang Han, Yijie Guo, Chin-Teng Lin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-03 01:58:34</h6>
<p class='card-text'>Recent advancements in humanoid robotics, including the integration of
hierarchical reinforcement learning-based control and the utilization of LLM
planning, have significantly enhanced the ability of robots to perform complex
tasks. In contrast to the highly developed humanoid robots, the human factors
involved remain relatively unexplored. Directly controlling humanoid robots
with the brain has already appeared in many science fiction novels, such as
Pacific Rim and Gundam. In this work, we present E2H (EEG-to-Humanoid), an
innovative framework that pioneers the control of humanoid robots using
high-frequency non-invasive neural signals. As the none-invasive signal quality
remains low in decoding precise spatial trajectory, we decompose the E2H
framework in an innovative two-stage formation: 1) decoding neural signals
(EEG) into semantic motion keywords, 2) utilizing LLM facilitated motion
generation with a precise motion imitation control policy to realize humanoid
robotics control. The method of directly driving robots with brainwave commands
offers a novel approach to human-machine collaboration, especially in
situations where verbal commands are impractical, such as in cases of speech
impairments, space exploration, or underwater exploration, unlocking
significant potential. E2H offers an exciting glimpse into the future, holding
immense potential for human-computer interaction.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.02089v2' target='_blank'>RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Quentin Carbonneaux, Taco Cohen, Gabriel Synnaeve</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-02 23:25:17</h6>
<p class='card-text'>Large language models (LLMs) deployed as agents solve user-specified tasks
over multiple steps while keeping the required manual engagement to a minimum.
Crucially, such LLMs need to ground their generations in any feedback obtained
to reliably achieve the desired outcomes. We propose an end-to-end
reinforcement learning method for teaching models to leverage execution
feedback in the realm of code synthesis, where state-of-the-art LLMs struggle
to improve code iteratively compared to independent sampling. We benchmark on
competitive programming tasks, where we achieve new state-of-the art results
with both small (8B parameters) and large (70B) models while reducing the
amount of samples required by an order of magnitude. Our analysis of
inference-time behavior demonstrates that our method produces LLMs that
effectively leverage automatic feedback over multiple steps.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.01929v1' target='_blank'>LLM-Augmented Symbolic Reinforcement Learning with Landmark-Based Task
  Decomposition</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alireza Kheirandish, Duo Xu, Faramarz Fekri</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-02 18:22:42</h6>
<p class='card-text'>One of the fundamental challenges in reinforcement learning (RL) is to take a
complex task and be able to decompose it to subtasks that are simpler for the
RL agent to learn. In this paper, we report on our work that would identify
subtasks by using some given positive and negative trajectories for solving the
complex task. We assume that the states are represented by first-order
predicate logic using which we devise a novel algorithm to identify the
subtasks. Then we employ a Large Language Model (LLM) to generate first-order
logic rule templates for achieving each subtask. Such rules were then further
fined tuned to a rule-based policy via an Inductive Logic Programming
(ILP)-based RL agent. Through experiments, we verify the accuracy of our
algorithm in detecting subtasks which successfully detect all of the subtasks
correctly. We also investigated the quality of the common-sense rules produced
by the language model to achieve the subtasks. Our experiments show that our
LLM-guided rule template generation can produce rules that are necessary for
solving a subtask, which leads to solving complex tasks with fewer assumptions
about predefined first-order logic predicates of the environment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.12832v1' target='_blank'>Generative Reward Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-Philipp Fränken, Chelsea Finn, Alon Albalak</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-02 17:58:39</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) has greatly improved the
performance of modern Large Language Models (LLMs). The RLHF process is
resource-intensive and technically challenging, generally requiring a large
collection of human preference labels over model-generated outputs.
Reinforcement Learning from AI Feedback (RLAIF) addresses this data collection
challenge by leveraging synthetic preferences generated by an LLM. However,
recent work has shown that synthetic preferences labels may not align well with
human preference judgments. To address this, we propose a hybrid approach that
unifies RLHF and RLAIF methodologies. We introduce GenRM, an iterative
algorithm that trains an LLM on self-generated reasoning traces, leading to
synthetic preference labels matching human preference judgments. Empirically,
we show that zero-shot LLM-based judgments under-perform compared to
Bradley-Terry reward models on in-distribution tasks (between 9-36%). In
contrast, GenRM achieves in-distribution accuracy comparable to Bradley-Terry
models, while significantly outperforming them on out-of-distribution tasks
(between 10-45%). Moreover, GenRM surpasses the performance of using LLMs as
judges on both in-distribution (by 9-31%) and out-of-distribution tasks (by 2-
6%). Our results show that combining the strengths of RLHF and RLAIF offers a
promising approach for improving the quality of synthetic preference labels.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.03768v1' target='_blank'>Hidden in Plain Text: Emergence & Mitigation of Steganographic Collusion
  in LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yohan Mathew, Ollie Matthews, Robert McCarthy, Joan Velja, Christian Schroeder de Witt, Dylan Cope, Nandi Schoots</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-02 16:18:33</h6>
<p class='card-text'>The rapid proliferation of frontier model agents promises significant
societal advances but also raises concerns about systemic risks arising from
unsafe interactions. Collusion to the disadvantage of others has been
identified as a central form of undesirable agent cooperation. The use of
information hiding (steganography) in agent communications could render
collusion practically undetectable. This underscores the need for evaluation
frameworks to monitor and mitigate steganographic collusion capabilities. We
address a crucial gap in the literature by demonstrating, for the first time,
that robust steganographic collusion in LLMs can arise indirectly from
optimization pressure. To investigate this problem we design two approaches --
a gradient-based reinforcement learning (GBRL) method and an in-context
reinforcement learning (ICRL) method -- for reliably eliciting sophisticated
LLM-generated linguistic text steganography. Importantly, we find that emergent
steganographic collusion can be robust to both passive steganalytic oversight
of model outputs and active mitigation through communication paraphrasing. We
contribute a novel model evaluation framework and discuss limitations and
future work. Our findings imply that effective risk mitigation from
steganographic collusion post-deployment requires innovation in passive and
active oversight techniques.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.01679v1' target='_blank'>VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit
  Assignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, Nicolas Le Roux</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-02 15:49:30</h6>
<p class='card-text'>Large language models (LLMs) are increasingly applied to complex reasoning
tasks that require executing several complex steps before receiving any reward.
Properly assigning credit to these steps is essential for enhancing model
performance. Proximal Policy Optimization (PPO), a state-of-the-art
reinforcement learning (RL) algorithm used for LLM finetuning, employs value
networks to tackle credit assignment. However, value networks face challenges
in predicting the expected cumulative rewards accurately in complex reasoning
tasks, often leading to high-variance updates and suboptimal performance. In
this work, we systematically evaluate the efficacy of value networks and reveal
their significant shortcomings in reasoning-heavy LLM tasks, showing that they
barely outperform a random baseline when comparing alternative steps. To
address this, we propose VinePPO, a straightforward approach that leverages the
flexibility of language environments to compute unbiased Monte Carlo-based
estimates, bypassing the need for large value networks. Our method consistently
outperforms PPO and other RL-free baselines across MATH and GSM8K datasets with
fewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These
results emphasize the importance of accurate credit assignment in RL finetuning
of LLM and demonstrate VinePPO's potential as a superior alternative.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.01639v3' target='_blank'>Moral Alignment for LLM Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Elizaveta Tennant, Stephen Hailes, Mirco Musolesi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-02 15:09:36</h6>
<p class='card-text'>Decision-making agents based on pre-trained Large Language Models (LLMs) are
increasingly being deployed across various domains of human activity. While
their applications are currently rather specialized, several research efforts
are under way to develop more generalist agents. As LLM-based systems become
more agentic, their influence on human activity will grow and the transparency
of this will decrease. Consequently, developing effective methods for aligning
them to human values is vital.
  The prevailing practice in alignment often relies on human preference data
(e.g., in RLHF or DPO), in which values are implicit and are essentially
deduced from relative preferences over different model outputs. In this work,
instead of relying on human feedback, we introduce the design of reward
functions that explicitly encode core human values for Reinforcement
Learning-based fine-tuning of foundation agent models. Specifically, we use
intrinsic rewards for the moral alignment of LLM agents.
  We evaluate our approach using the traditional philosophical frameworks of
Deontological Ethics and Utilitarianism, quantifying moral rewards for agents
in terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD)
environment. We also show how moral fine-tuning can be deployed to enable an
agent to unlearn a previously developed selfish strategy. Finally, we find that
certain moral strategies learned on the IPD game generalize to several other
matrix game environments. In summary, we demonstrate that fine-tuning with
intrinsic rewards is a promising general solution for aligning LLM agents to
human values, and it might represent a more transparent and cost-effective
alternative to currently predominant alignment techniques.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.01532v1' target='_blank'>Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for
  Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Angela Lopez-Cardona, Carlos Segura, Alexandros Karatzoglou, Sergi Abadal, Ioannis Arapakis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-02 13:24:56</h6>
<p class='card-text'>Advancements in Natural Language Processing (NLP), have led to the emergence
of Large Language Models (LLMs) such as GPT, Llama, Claude, and Gemini, which
excel across a range of tasks but require extensive fine-tuning to align their
outputs with human expectations. A widely used method for achieving this
alignment is Reinforcement Learning from Human Feedback (RLHF), which, despite
its success, faces challenges in accurately modelling human preferences. In
this paper, we introduce GazeReward, a novel framework that integrates implicit
feedback -- and specifically eye-tracking (ET) data -- into the Reward Model
(RM). In addition, we explore how ET-based features can provide insights into
user preferences. Through ablation studies we test our framework with different
integration methods, LLMs, and ET generator models, demonstrating that our
approach significantly improves the accuracy of the RM on established human
preference datasets. This work advances the ongoing discussion on optimizing AI
alignment with human values, exploring the potential of cognitive data for
shaping future NLP research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.01458v1' target='_blank'>From Reward Shaping to Q-Shaping: Achieving Unbiased Learning with
  LLM-Guided Knowledge</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiefeng Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-02 12:10:07</h6>
<p class='card-text'>Q-shaping is an extension of Q-value initialization and serves as an
alternative to reward shaping for incorporating domain knowledge to accelerate
agent training, thereby improving sample efficiency by directly shaping
Q-values. This approach is both general and robust across diverse tasks,
allowing for immediate impact assessment while guaranteeing optimality. We
evaluated Q-shaping across 20 different environments using a large language
model (LLM) as the heuristic provider. The results demonstrate that Q-shaping
significantly enhances sample efficiency, achieving a \textbf{16.87\%}
improvement over the best baseline in each environment and a \textbf{253.80\%}
improvement compared to LLM-based reward shaping methods. These findings
establish Q-shaping as a superior and unbiased alternative to conventional
reward shaping in reinforcement learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.01280v1' target='_blank'>Sparse Autoencoders Reveal Temporal Difference Learning in Large
  Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Can Demircan, Tankred Saanum, Akshay K. Jagadish, Marcel Binz, Eric Schulz</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-02 06:51:12</h6>
<p class='card-text'>In-context learning, the ability to adapt based on a few examples in the
input prompt, is a ubiquitous feature of large language models (LLMs). However,
as LLMs' in-context learning abilities continue to improve, understanding this
phenomenon mechanistically becomes increasingly important. In particular, it is
not well-understood how LLMs learn to solve specific classes of problems, such
as reinforcement learning (RL) problems, in-context. Through three different
tasks, we first show that Llama $3$ $70$B can solve simple RL problems
in-context. We then analyze the residual stream of Llama using Sparse
Autoencoders (SAEs) and find representations that closely match temporal
difference (TD) errors. Notably, these representations emerge despite the model
only being trained to predict the next token. We verify that these
representations are indeed causally involved in the computation of TD errors
and $Q$-values by performing carefully designed interventions on them. Taken
together, our work establishes a methodology for studying and manipulating
in-context learning with SAEs, paving the way for a more mechanistic
understanding.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.01268v2' target='_blank'>Deep Learning and Machine Learning, Advancing Big Data Analytics and
  Management: Unveiling AI's Potential Through Tools, Techniques, and
  Applications</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pohsun Feng, Ziqian Bi, Yizhu Wen, Xuanhe Pan, Benji Peng, Ming Liu, Jiawei Xu, Keyu Chen, Junyu Liu, Caitlyn Heqi Yin, Sen Zhang, Jinlang Wang, Qian Niu, Ming Li, Tianyang Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-02 06:24:51</h6>
<p class='card-text'>Artificial intelligence (AI), machine learning, and deep learning have become
transformative forces in big data analytics and management, enabling
groundbreaking advancements across diverse industries. This article delves into
the foundational concepts and cutting-edge developments in these fields, with a
particular focus on large language models (LLMs) and their role in natural
language processing, multimodal reasoning, and autonomous decision-making.
Highlighting tools such as ChatGPT, Claude, and Gemini, the discussion explores
their applications in data analysis, model design, and optimization.
  The integration of advanced algorithms like neural networks, reinforcement
learning, and generative models has enhanced the capabilities of AI systems to
process, visualize, and interpret complex datasets. Additionally, the emergence
of technologies like edge computing and automated machine learning (AutoML)
democratizes access to AI, empowering users across skill levels to engage with
intelligent systems. This work also underscores the importance of ethical
considerations, transparency, and fairness in the deployment of AI
technologies, paving the way for responsible innovation.
  Through practical insights into hardware configurations, software
environments, and real-world applications, this article serves as a
comprehensive resource for researchers and practitioners. By bridging
theoretical underpinnings with actionable strategies, it showcases the
potential of AI and LLMs to revolutionize big data management and drive
meaningful advancements across domains such as healthcare, finance, and
autonomous systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.00371v1' target='_blank'>AHA: A Vision-Language-Model for Detecting and Reasoning Over Failures
  in Robotic Manipulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiafei Duan, Wilbert Pumacay, Nishanth Kumar, Yi Ru Wang, Shulin Tian, Wentao Yuan, Ranjay Krishna, Dieter Fox, Ajay Mandlekar, Yijie Guo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-01 03:47:00</h6>
<p class='card-text'>Robotic manipulation in open-world settings requires not only task execution
but also the ability to detect and learn from failures. While recent advances
in vision-language models (VLMs) and large language models (LLMs) have improved
robots' spatial reasoning and problem-solving abilities, they still struggle
with failure recognition, limiting their real-world applicability. We introduce
AHA, an open-source VLM designed to detect and reason about failures in robotic
manipulation using natural language. By framing failure detection as a
free-form reasoning task, AHA identifies failures and provides detailed,
adaptable explanations across different robots, tasks, and environments. We
fine-tuned AHA using FailGen, a scalable framework that generates the first
large-scale dataset of robotic failure trajectories, the AHA dataset. FailGen
achieves this by procedurally perturbing successful demonstrations from
simulation. Despite being trained solely on the AHA dataset, AHA generalizes
effectively to real-world failure datasets, robotic systems, and unseen tasks.
It surpasses the second-best model (GPT-4o in-context learning) by 10.3% and
exceeds the average performance of six compared models including five
state-of-the-art VLMs by 35.3% across multiple metrics and datasets. We
integrate AHA into three manipulation frameworks that utilize LLMs/VLMs for
reinforcement learning, task and motion planning, and zero-shot trajectory
generation. AHA's failure feedback enhances these policies' performances by
refining dense reward functions, optimizing task planning, and improving
sub-task verification, boosting task success rates by an average of 21.4%
across all three tasks compared to GPT-4 models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.20370v1' target='_blank'>The Perfect Blend: Redefining RLHF with Mixture of Judges</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tengyu Xu, Eryk Helenowski, Karthik Abinav Sankararaman, Di Jin, Kaiyan Peng, Eric Han, Shaoliang Nie, Chen Zhu, Hejia Zhang, Wenxuan Zhou, Zhouhao Zeng, Yun He, Karishma Mandyam, Arya Talabzadeh, Madian Khabsa, Gabriel Cohen, Yuandong Tian, Hao Ma, Sinong Wang, Han Fang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-30 15:06:53</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) has become the leading
approach for fine-tuning large language models (LLM). However, RLHF has
limitations in multi-task learning (MTL) due to challenges of reward hacking
and extreme multi-objective optimization (i.e., trade-off of multiple and/or
sometimes conflicting objectives). Applying RLHF for MTL currently requires
careful tuning of the weights for reward model and data combinations. This is
often done via human intuition and does not generalize. In this work, we
introduce a novel post-training paradigm which we called Constrained Generative
Policy Optimization (CGPO). The core of CGPO is Mixture of Judges (MoJ) with
cost-efficient constrained policy optimization with stratification, which can
identify the perfect blend in RLHF in a principled manner. It shows strong
empirical results with theoretical guarantees, does not require extensive
hyper-parameter tuning, and is plug-and-play in common post-training pipelines.
Together, this can detect and mitigate reward hacking behaviors while reaching
a pareto-optimal point across an extremely large number of objectives.
  Our empirical evaluations demonstrate that CGPO significantly outperforms
standard RLHF algorithms like PPO and DPO across various tasks including
general chat, STEM questions, instruction following, and coding. Specifically,
CGPO shows improvements of 7.4% in AlpacaEval-2 (general chat), 12.5% in
Arena-Hard (STEM & reasoning), and consistent gains in other domains like math
and coding. Notably, PPO, while commonly used, is prone to severe reward
hacking in popular coding benchmarks, which CGPO successfully addresses. This
breakthrough in RLHF not only tackles reward hacking and extreme
multi-objective optimization challenges but also advances the state-of-the-art
in aligning general-purpose LLMs for diverse applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.19993v1' target='_blank'>Mitigating Backdoor Threats to Large Language Models: Advancement and
  Challenges</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qin Liu, Wenjie Mo, Terry Tong, Jiashu Xu, Fei Wang, Chaowei Xiao, Muhao Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-30 06:31:36</h6>
<p class='card-text'>The advancement of Large Language Models (LLMs) has significantly impacted
various domains, including Web search, healthcare, and software development.
However, as these models scale, they become more vulnerable to cybersecurity
risks, particularly backdoor attacks. By exploiting the potent memorization
capacity of LLMs, adversaries can easily inject backdoors into LLMs by
manipulating a small portion of training data, leading to malicious behaviors
in downstream applications whenever the hidden backdoor is activated by the
pre-defined triggers. Moreover, emerging learning paradigms like instruction
tuning and reinforcement learning from human feedback (RLHF) exacerbate these
risks as they rely heavily on crowdsourced data and human feedback, which are
not fully controlled. In this paper, we present a comprehensive survey of
emerging backdoor threats to LLMs that appear during LLM development or
inference, and cover recent advancement in both defense and detection
strategies for mitigating backdoor threats to LLMs. We also outline key
challenges in addressing these threats, highlighting areas for future research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.19817v1' target='_blank'>Calibrating Language Models with Adaptive Temperature Scaling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Johnathan Xie, Annie S. Chen, Yoonho Lee, Eric Mitchell, Chelsea Finn</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-29 22:54:31</h6>
<p class='card-text'>The effectiveness of large language models (LLMs) is not only measured by
their ability to generate accurate outputs but also by their calibration-how
well their confidence scores reflect the probability of their outputs being
correct. While unsupervised pre-training has been shown to yield LLMs with
well-calibrated conditional probabilities, recent studies have shown that after
fine-tuning with reinforcement learning from human feedback (RLHF), the
calibration of these models degrades significantly. In this work, we introduce
Adaptive Temperature Scaling (ATS), a post-hoc calibration method that predicts
a temperature scaling parameter for each token prediction. The predicted
temperature values adapt based on token-level features and are fit over a
standard supervised fine-tuning (SFT) dataset. The adaptive nature of ATS
addresses the varying degrees of calibration shift that can occur after RLHF
fine-tuning. ATS improves calibration by over 10-50% across three downstream
natural language evaluation benchmarks compared to prior calibration methods
and does not impede performance improvements from RLHF.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.19790v1' target='_blank'>Analysis on Riemann Hypothesis with Cross Entropy Optimization and
  Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kevin Li, Fulu Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-29 21:25:58</h6>
<p class='card-text'>In this paper, we present a novel framework for the analysis of Riemann
Hypothesis [27], which is composed of three key components: a) probabilistic
modeling with cross entropy optimization and reasoning; b) the application of
the law of large numbers; c) the application of mathematical inductions. The
analysis is mainly conducted by virtue of probabilistic modeling of cross
entropy optimization and reasoning with rare event simulation techniques. The
application of the law of large numbers [2, 3, 6] and the application of
mathematical inductions make the analysis of Riemann Hypothesis self-contained
and complete to make sure that the whole complex plane is covered as
conjectured in Riemann Hypothesis. We also discuss the method of enhanced top-p
sampling with large language models (LLMs) for reasoning, where next token
prediction is not just based on the estimated probabilities of each possible
token in the current round but also based on accumulated path probabilities
among multiple top-k chain of thoughts (CoTs) paths. The probabilistic modeling
of cross entropy optimization and reasoning may suit well with the analysis of
Riemann Hypothesis as Riemann Zeta functions are inherently dealing with the
sums of infinite components of a complex number series.
  We hope that our analysis in this paper could shed some light on some of the
insights of Riemann Hypothesis. The framework and techniques presented in this
paper, coupled with recent developments with chain of thought (CoT) or diagram
of thought (DoT) reasoning in large language models (LLMs) with reinforcement
learning (RL) [1, 7, 18, 21, 24, 34, 39-41], could pave the way for eventual
proof of Riemann Hypothesis [27].</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.19401v1' target='_blank'>Crafting Personalized Agents through Retrieval-Augmented Generation on
  Editable Memory Graphs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zheng Wang, Zhongyang Li, Zeren Jiang, Dandan Tu, Wei Shi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-28 16:22:53</h6>
<p class='card-text'>In the age of mobile internet, user data, often referred to as memories, is
continuously generated on personal devices. Effectively managing and utilizing
this data to deliver services to users is a compelling research topic. In this
paper, we introduce a novel task of crafting personalized agents powered by
large language models (LLMs), which utilize a user's smartphone memories to
enhance downstream applications with advanced LLM capabilities. To achieve this
goal, we introduce EMG-RAG, a solution that combines Retrieval-Augmented
Generation (RAG) techniques with an Editable Memory Graph (EMG). This approach
is further optimized using Reinforcement Learning to address three distinct
challenges: data collection, editability, and selectability. Extensive
experiments on a real-world dataset validate the effectiveness of EMG-RAG,
achieving an improvement of approximately 10% over the best existing approach.
Additionally, the personalized agents have been transferred into a real
smartphone AI assistant, which leads to enhanced usability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.19256v2' target='_blank'>HybridFlow: A Flexible and Efficient RLHF Framework</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, Chuan Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-28 06:20:03</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) is widely used in Large
Language Model (LLM) alignment. Traditional RL can be modeled as a dataflow,
where each node represents computation of a neural network (NN) and each edge
denotes data dependencies between the NNs. RLHF complicates the dataflow by
expanding each node into a distributed LLM training or generation program, and
each edge into a many-to-many multicast. Traditional RL frameworks execute the
dataflow using a single controller to instruct both intra-node computation and
inter-node communication, which can be inefficient in RLHF due to large control
dispatch overhead for distributed intra-node computation. Existing RLHF systems
adopt a multi-controller paradigm, which can be inflexible due to nesting
distributed computation and data communication. We propose HybridFlow, which
combines single-controller and multi-controller paradigms in a hybrid manner to
enable flexible representation and efficient execution of the RLHF dataflow. We
carefully design a set of hierarchical APIs that decouple and encapsulate
computation and data dependencies in the complex RLHF dataflow, allowing
efficient operation orchestration to implement RLHF algorithms and flexible
mapping of the computation onto various devices. We further design a
3D-HybridEngine for efficient actor model resharding between training and
generation phases, with zero memory redundancy and significantly reduced
communication overhead. Our experimental results demonstrate
1.53$\times$~20.57$\times$ throughput improvement when running various RLHF
algorithms using HybridFlow, as compared with state-of-the-art baselines.
HybridFlow source code will be available at https://github.com/volcengine/verl.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.18812v1' target='_blank'>LLMs4Synthesis: Leveraging Large Language Models for Scientific
  Synthesis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hamed Babaei Giglou, Jennifer D'Souza, Sören Auer</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-27 15:04:39</h6>
<p class='card-text'>In response to the growing complexity and volume of scientific literature,
this paper introduces the LLMs4Synthesis framework, designed to enhance the
capabilities of Large Language Models (LLMs) in generating high-quality
scientific syntheses. This framework addresses the need for rapid, coherent,
and contextually rich integration of scientific insights, leveraging both
open-source and proprietary LLMs. It also examines the effectiveness of LLMs in
evaluating the integrity and reliability of these syntheses, alleviating
inadequacies in current quantitative metrics. Our study contributes to this
field by developing a novel methodology for processing scientific papers,
defining new synthesis types, and establishing nine detailed quality criteria
for evaluating syntheses. The integration of LLMs with reinforcement learning
and AI feedback is proposed to optimize synthesis quality, ensuring alignment
with established criteria. The LLMs4Synthesis framework and its components are
made available, promising to enhance both the generation and evaluation
processes in scientific research synthesis.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.18417v2' target='_blank'>VickreyFeedback: Cost-efficient Data Construction for Reinforcement
  Learning from Human Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guoxi Zhang, Jiuding Duan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-27 03:15:07</h6>
<p class='card-text'>This paper addresses the cost-efficiency aspect of Reinforcement Learning
from Human Feedback (RLHF). RLHF leverages datasets of human preferences over
outputs of large language models (LLM)s to instill human expectations into
LLMs. Although preference annotation comes with a monetized cost, the economic
utility of a preference dataset has not been considered by far. What
exacerbates this situation is that, given complex intransitive or cyclic
relationships in preference datasets, existing algorithms for fine-tuning LLMs
are still far from capturing comprehensive preferences. This raises severe
cost-efficiency concerns in production environments, where preference data
accumulate over time. In this paper, we discuss the fine-tuning of LLMs as a
monetized economy and introduce an auction mechanism to improve the efficiency
of preference data collection in dollar terms. We show that introducing an
auction mechanism can play an essential role in enhancing the cost-efficiency
of RLHF, while maintaining satisfactory model performance. Experimental results
demonstrate that our proposed auction-based protocol is cost-effective for
fine-tuning LLMs concentrating on high-quality feedback.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.18382v1' target='_blank'>CurricuLLM: Automatic Task Curricula Design for Learning Complex Robot
  Skills using Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kanghyun Ryu, Qiayuan Liao, Zhongyu Li, Koushil Sreenath, Negar Mehr</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-27 01:48:16</h6>
<p class='card-text'>Curriculum learning is a training mechanism in reinforcement learning (RL)
that facilitates the achievement of complex policies by progressively
increasing the task difficulty during training. However, designing effective
curricula for a specific task often requires extensive domain knowledge and
human intervention, which limits its applicability across various domains. Our
core idea is that large language models (LLMs), with their extensive training
on diverse language data and ability to encapsulate world knowledge, present
significant potential for efficiently breaking down tasks and decomposing
skills across various robotics environments. Additionally, the demonstrated
success of LLMs in translating natural language into executable code for RL
agents strengthens their role in generating task curricula. In this work, we
propose CurricuLLM, which leverages the high-level planning and programming
capabilities of LLMs for curriculum design, thereby enhancing the efficient
learning of complex target tasks. CurricuLLM consists of: (Step 1) Generating
sequence of subtasks that aid target task learning in natural language form,
(Step 2) Translating natural language description of subtasks in executable
task code, including the reward code and goal distribution code, and (Step 3)
Evaluating trained policies based on trajectory rollout and subtask
description. We evaluate CurricuLLM in various robotics simulation
environments, ranging from manipulation, navigation, and locomotion, to show
that CurricuLLM can aid learning complex robot control tasks. In addition, we
validate humanoid locomotion policy learned through CurricuLLM in real-world.
The code is provided in https://github.com/labicon/CurricuLLM</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.18014v1' target='_blank'>Role-RL: Online Long-Context Processing with Role Reinforcement Learning
  for Distinct LLMs in Their Optimal Roles</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lewei He, Tianyu Shi, Pengran Huang, Bingzhi Chen, Qianglong Chen, Jiahui Pan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-26 16:22:59</h6>
<p class='card-text'>Large language models (LLMs) with long-context processing are still
challenging because of their implementation complexity, training efficiency and
data sparsity. To address this issue, a new paradigm named Online Long-context
Processing (OLP) is proposed when we process a document of unlimited length,
which typically occurs in the information reception and organization of diverse
streaming media such as automated news reporting, live e-commerce, and viral
short videos. Moreover, a dilemma was often encountered when we tried to select
the most suitable LLM from a large number of LLMs amidst explosive growth
aiming for outstanding performance, affordable prices, and short response
delays. In view of this, we also develop Role Reinforcement Learning (Role-RL)
to automatically deploy different LLMs in their respective roles within the OLP
pipeline according to their actual performance. Extensive experiments are
conducted on our OLP-MINI dataset and it is found that OLP with Role-RL
framework achieves OLP benchmark with an average recall rate of 93.2% and the
LLM cost saved by 79.4%. The code and dataset are publicly available at:
https://anonymous.4open.science/r/Role-RL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.17791v1' target='_blank'>Self-supervised Preference Optimization: Enhance Your Language Model
  with Preference Degree Awareness</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jian Li, Haojing Huang, Yujia Zhang, Pengfei Xu, Xi Chen, Rui Song, Lida Shi, Jingwen Wang, Hao Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-26 12:37:26</h6>
<p class='card-text'>Recently, there has been significant interest in replacing the reward model
in Reinforcement Learning with Human Feedback (RLHF) methods for Large Language
Models (LLMs), such as Direct Preference Optimization (DPO) and its variants.
These approaches commonly use a binary cross-entropy mechanism on pairwise
samples, i.e., minimizing and maximizing the loss based on preferred or
dis-preferred responses, respectively. However, while this training strategy
omits the reward model, it also overlooks the varying preference degrees within
different responses. We hypothesize that this is a key factor hindering LLMs
from sufficiently understanding human preferences. To address this problem, we
propose a novel Self-supervised Preference Optimization (SPO) framework, which
constructs a self-supervised preference degree loss combined with the alignment
loss, thereby helping LLMs improve their ability to understand the degree of
preference. Extensive experiments are conducted on two widely used datasets of
different tasks. The results demonstrate that SPO can be seamlessly integrated
with existing preference optimization methods and significantly boost their
performance to achieve state-of-the-art performance. We also conduct detailed
analyses to offer comprehensive insights into SPO, which verifies its
effectiveness. The code is available at https://github.com/lijian16/SPO.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.17407v1' target='_blank'>Post-hoc Reward Calibration: A Case Study on Length Bias</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zeyu Huang, Zihan Qiu, Zili Wang, Edoardo M. Ponti, Ivan Titov</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-25 22:30:42</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback aligns the outputs of Large
Language Models with human values and preferences. Central to this process is
the reward model (RM), which translates human feedback into training signals
for optimising LLM behaviour. However, RMs can develop biases by exploiting
spurious correlations in their training data, such as favouring outputs based
on length or style rather than true quality. These biases can lead to incorrect
output rankings, sub-optimal model evaluations, and the amplification of
undesirable behaviours in LLMs alignment. This paper addresses the challenge of
correcting such biases without additional data and training, introducing the
concept of Post-hoc Reward Calibration. We first propose an intuitive approach
to estimate the bias term and, thus, remove it to approximate the underlying
true reward. We then extend the approach to a more general and robust form with
the Locally Weighted Regression. Focusing on the prevalent length bias, we
validate our proposed approaches across three experimental settings,
demonstrating consistent improvements: (1) a 3.11 average performance gain
across 33 reward models on the RewardBench dataset; (2) enhanced alignment of
RM rankings with GPT-4 evaluations and human preferences based on the
AlpacaEval benchmark; and (3) improved Length-Controlled win rate of the RLHF
process in multiple LLM--RM combinations. Our method is computationally
efficient and generalisable to other types of bias and RMs, offering a scalable
and robust solution for mitigating biases in LLM alignment. Our code and
results are available at https://github.com/ZeroYuHuang/Reward-Calibration.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.17401v1' target='_blank'>Zeroth-Order Policy Gradient for Reinforcement Learning from Human
  Feedback without Reward Inference</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qining Zhang, Lei Ying</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-25 22:20:11</h6>
<p class='card-text'>Reward inference (learning a reward model from human preferences) is a
critical intermediate step in Reinforcement Learning from Human Feedback (RLHF)
for fine-tuning Large Language Models (LLMs) such as ChatGPT. In practice,
reward inference faces several fundamental challenges, including double problem
misspecification, reward model evaluation without ground truth, distribution
shift, and overfitting in joint reward model and policy training. An
alternative approach that avoids these pitfalls is direct policy optimization
without reward inference, such as Direct Preference Optimization (DPO), which
provides a much simpler pipeline and has shown empirical success in LLMs.
However, DPO utilizes the closed-form expression between the optimal policy and
the reward function, which only works under the bandit setting or deterministic
MDPs. This paper develops two RLHF algorithms without reward inference, which
work for general RL problems beyond bandits and deterministic MDPs, and general
preference models beyond the Bradely-Terry model. The key idea is to estimate
the local value function difference from human preferences and then approximate
the policy gradient with a zeroth-order gradient approximator. For both
algorithms, we establish rates of convergence in terms of the number of policy
gradient iterations, as well as the number of trajectory samples and human
preference queries per iteration. Our results show there exist provably
efficient methods to solve general RLHF problems without reward inference.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.17348v2' target='_blank'>Language Grounded Multi-agent Reinforcement Learning with
  Human-interpretable Communication</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Huao Li, Hossein Nourkhiz Mahjoub, Behdad Chalaki, Vaishnav Tadiparthi, Kwonjoon Lee, Ehsan Moradi-Pari, Charles Michael Lewis, Katia P Sycara</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-25 20:49:41</h6>
<p class='card-text'>Multi-Agent Reinforcement Learning (MARL) methods have shown promise in
enabling agents to learn a shared communication protocol from scratch and
accomplish challenging team tasks. However, the learned language is usually not
interpretable to humans or other agents not co-trained together, limiting its
applicability in ad-hoc teamwork scenarios. In this work, we propose a novel
computational pipeline that aligns the communication space between MARL agents
with an embedding space of human natural language by grounding agent
communications on synthetic data generated by embodied Large Language Models
(LLMs) in interactive teamwork scenarios. Our results demonstrate that
introducing language grounding not only maintains task performance but also
accelerates the emergence of communication. Furthermore, the learned
communication protocols exhibit zero-shot generalization capabilities in ad-hoc
teamwork scenarios with unseen teammates and novel task states. This work
presents a significant step toward enabling effective communication and
collaboration between artificial agents and humans in real-world teamwork
settings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.16909v2' target='_blank'>Enhancing Temporal Sensitivity and Reasoning for Time-Sensitive Question
  Answering</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wanqi Yang, Yanda Li, Meng Fang, Ling Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-25 13:13:21</h6>
<p class='card-text'>Time-Sensitive Question Answering (TSQA) demands the effective utilization of
specific temporal contexts, encompassing multiple time-evolving facts, to
address time-sensitive questions. This necessitates not only the parsing of
temporal information within questions but also the identification and
understanding of time-evolving facts to generate accurate answers. However,
current large language models still have limited sensitivity to temporal
information and their inadequate temporal reasoning capabilities. In this
paper, we propose a novel framework that enhances temporal awareness and
reasoning through Temporal Information-Aware Embedding and Granular Contrastive
Reinforcement Learning. Experimental results on four TSQA datasets demonstrate
that our framework significantly outperforms existing LLMs in TSQA tasks,
marking a step forward in bridging the performance gap between machine and
human temporal understanding and reasoning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.16783v1' target='_blank'>Holistic Automated Red Teaming for Large Language Models through
  Top-Down Test Case Generation and Multi-turn Interaction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jinchuan Zhang, Yan Zhou, Yaxin Liu, Ziming Li, Songlin Hu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-25 09:44:48</h6>
<p class='card-text'>Automated red teaming is an effective method for identifying misaligned
behaviors in large language models (LLMs). Existing approaches, however, often
focus primarily on improving attack success rates while overlooking the need
for comprehensive test case coverage. Additionally, most of these methods are
limited to single-turn red teaming, failing to capture the multi-turn dynamics
of real-world human-machine interactions. To overcome these limitations, we
propose HARM (Holistic Automated Red teaMing), which scales up the diversity of
test cases using a top-down approach based on an extensible, fine-grained risk
taxonomy. Our method also leverages a novel fine-tuning strategy and
reinforcement learning techniques to facilitate multi-turn adversarial probing
in a human-like manner. Experimental results demonstrate that our framework
enables a more systematic understanding of model vulnerabilities and offers
more targeted guidance for the alignment process.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.16371v1' target='_blank'>Do the Right Thing, Just Debias! Multi-Category Bias Mitigation Using
  LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Amartya Roy, Danush Khanna, Devanshu Mahapatra, Vasanthakumar, Avirup Das, Kripabandhu Ghosh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-24 18:05:10</h6>
<p class='card-text'>This paper tackles the challenge of building robust and generalizable bias
mitigation models for language. Recognizing the limitations of existing
datasets, we introduce ANUBIS, a novel dataset with 1507 carefully curated
sentence pairs encompassing nine social bias categories. We evaluate
state-of-the-art models like T5, utilizing Supervised Fine-Tuning (SFT),
Reinforcement Learning (PPO, DPO), and In-Context Learning (ICL) for effective
bias mitigation. Our analysis focuses on multi-class social bias reduction,
cross-dataset generalizability, and environmental impact of the trained models.
ANUBIS and our findings offer valuable resources for building more equitable AI
systems and contribute to the development of responsible and unbiased
technologies with broad societal impact.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.15637v2' target='_blank'>Synatra: Turning Indirect Knowledge into Direct Demonstrations for
  Digital Agents at Scale</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tianyue Ou, Frank F. Xu, Aman Madaan, Jiarui Liu, Robert Lo, Abishek Sridhar, Sudipta Sengupta, Dan Roth, Graham Neubig, Shuyan Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-24 00:51:45</h6>
<p class='card-text'>LLMs can now act as autonomous agents that interact with digital environments
and complete specific objectives (e.g., arranging an online meeting). However,
accuracy is still far from satisfactory, partly due to a lack of large-scale,
direct demonstrations for digital tasks. Obtaining supervised data from humans
is costly, and automatic data collection through exploration or reinforcement
learning relies on complex environmental and content setup, resulting in
datasets that lack comprehensive coverage of various scenarios. On the other
hand, there is abundant knowledge that may indirectly assist task completion,
such as online tutorials that were created for human consumption. In this work,
we present Synatra, an approach that effectively transforms this indirect
knowledge into direct supervision at scale. We define different types of
indirect knowledge, and carefully study the available sources to obtain it,
methods to encode the structure of direct demonstrations, and finally methods
to transform indirect knowledge into direct demonstrations. We use 100k such
synthetically-created demonstrations to finetune a 7B CodeLlama, and
demonstrate that the resulting agent surpasses all comparably sized models on
three web-based task benchmarks Mind2Web, MiniWoB++ and WebArena, as well as
surpassing GPT-3.5 on WebArena and Mind2Web. In addition, while synthetic
demonstrations prove to be only 3% the cost of human demonstrations (at $0.031
each), we show that the synthetic demonstrations can be more effective than an
identical number of human demonstrations collected from limited domains.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.15277v1' target='_blank'>A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yunfei Xie, Juncheng Wu, Haoqin Tu, Siwei Yang, Bingchen Zhao, Yongshuo Zong, Qiao Jin, Cihang Xie, Yuyin Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-23 17:59:43</h6>
<p class='card-text'>Large language models (LLMs) have exhibited remarkable capabilities across
various domains and tasks, pushing the boundaries of our knowledge in learning
and cognition. The latest model, OpenAI's o1, stands out as the first LLM with
an internalized chain-of-thought technique using reinforcement learning
strategies. While it has demonstrated surprisingly strong capabilities on
various general language tasks, its performance in specialized fields such as
medicine remains unknown. To this end, this report provides a comprehensive
exploration of o1 on different medical scenarios, examining 3 key aspects:
understanding, reasoning, and multilinguality. Specifically, our evaluation
encompasses 6 tasks using data from 37 medical datasets, including two newly
constructed and more challenging question-answering (QA) tasks based on
professional medical quizzes from the New England Journal of Medicine (NEJM)
and The Lancet. These datasets offer greater clinical relevance compared to
standard medical QA benchmarks such as MedQA, translating more effectively into
real-world clinical utility. Our analysis of o1 suggests that the enhanced
reasoning ability of LLMs may (significantly) benefit their capability to
understand various medical instructions and reason through complex clinical
scenarios. Notably, o1 surpasses the previous GPT-4 in accuracy by an average
of 6.2% and 6.6% across 19 datasets and two newly created complex QA scenarios.
But meanwhile, we identify several weaknesses in both the model capability and
the existing evaluation protocols, including hallucination, inconsistent
multilingual ability, and discrepant metrics for evaluation. We release our raw
data and model outputs at https://ucsc-vlaa.github.io/o1_medicine/ for future
research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.14826v3' target='_blank'>ToolPlanner: A Tool Augmented LLM for Multi Granularity Instructions
  with Path Planning and Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qinzhuo Wu, Wei Liu, Jian Luan, Bin Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-23 08:58:48</h6>
<p class='card-text'>Recently, tool-augmented LLMs have gained increasing attention. Given an
instruction, tool-augmented LLMs can interact with various external tools in
multiple rounds and provide a final answer. However, previous LLMs were trained
on overly detailed instructions, which included API names or parameters, while
real users would not explicitly mention these API details. This leads to a gap
between trained LLMs and real-world scenarios. In addition, most works ignore
whether the interaction process follows the instruction. To address these
issues, we constructed a training dataset called MGToolBench, which contains
statement and category-level instructions to better reflect real-world
scenarios. In addition, we propose ToolPlanner, a two-stage reinforcement
learning framework that utilizes path planning and two feedback mechanisms to
enhance the LLM's task completion and instruction-following capabilities.
Experimental results show that ToolPlanner significantly improves the Match
Rate, Pass Rate and Win Rate by 26.8%, 20.2%, and 5.6% compared to the SOTA
model. Human evaluation verifies that the multi-granularity instructions can
better align with users' usage habits. Our data and code will be released upon
acceptance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.14583v2' target='_blank'>Evaluating Gender, Racial, and Age Biases in Large Language Models: A
  Comparative Analysis of Occupational and Crime Scenarios</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Vishal Mirza, Rahul Kulkarni, Aakanksha Jadhav</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-22 20:21:20</h6>
<p class='card-text'>Recent advancements in Large Language Models(LLMs) have been notable, yet
widespread enterprise adoption remains limited due to various constraints. This
paper examines bias in LLMs-a crucial issue affecting their usability,
reliability, and fairness. Researchers are developing strategies to mitigate
bias, including debiasing layers, specialized reference datasets like
Winogender and Winobias, and reinforcement learning with human feedback (RLHF).
These techniques have been integrated into the latest LLMs. Our study evaluates
gender bias in occupational scenarios and gender, age, and racial bias in crime
scenarios across four leading LLMs released in 2024: Gemini 1.5 Pro, Llama 3
70B, Claude 3 Opus, and GPT-4o. Findings reveal that LLMs often depict female
characters more frequently than male ones in various occupations, showing a 37%
deviation from US BLS data. In crime scenarios, deviations from US FBI data are
54% for gender, 28% for race, and 17% for age. We observe that efforts to
reduce gender and racial bias often lead to outcomes that may over-index one
sub-class, potentially exacerbating the issue. These results highlight the
limitations of current bias mitigation techniques and underscore the need for
more effective approaches.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.14177v2' target='_blank'>PathSeeker: Exploring LLM Security Vulnerabilities with a Reinforcement
  Learning-Based Jailbreak Approach</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhihao Lin, Wei Ma, Mingyi Zhou, Yanjie Zhao, Haoyu Wang, Yang Liu, Jun Wang, Li Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-21 15:36:26</h6>
<p class='card-text'>In recent years, Large Language Models (LLMs) have gained widespread use,
raising concerns about their security. Traditional jailbreak attacks, which
often rely on the model internal information or have limitations when exploring
the unsafe behavior of the victim model, limiting their reducing their general
applicability. In this paper, we introduce PathSeeker, a novel black-box
jailbreak method, which is inspired by the game of rats escaping a maze. We
think that each LLM has its unique "security maze", and attackers attempt to
find the exit learning from the received feedback and their accumulated
experience to compromise the target LLM's security defences. Our approach
leverages multi-agent reinforcement learning, where smaller models collaborate
to guide the main LLM in performing mutation operations to achieve the attack
objectives. By progressively modifying inputs based on the model's feedback,
our system induces richer, harmful responses. During our manual attempts to
perform jailbreak attacks, we found that the vocabulary of the response of the
target model gradually became richer and eventually produced harmful responses.
Based on the observation, we also introduce a reward mechanism that exploits
the expansion of vocabulary richness in LLM responses to weaken security
constraints. Our method outperforms five state-of-the-art attack techniques
when tested across 13 commercial and open-source LLMs, achieving high attack
success rates, especially in strongly aligned commercial models like
GPT-4o-mini, Claude-3.5, and GLM-4-air with strong safety alignment. This study
aims to improve the understanding of LLM security vulnerabilities and we hope
that this sturdy can contribute to the development of more robust defenses.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.14065v2' target='_blank'>Temporally Consistent Factuality Probing for Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ashutosh Bajpai, Aaryan Goyal, Atif Anwer, Tanmoy Chakraborty</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-21 08:41:08</h6>
<p class='card-text'>The prolific use of Large Language Models (LLMs) as an alternate knowledge
base requires them to be factually consistent, necessitating both correctness
and consistency traits for paraphrased queries. Recently, significant attempts
have been made to benchmark datasets and metrics to evaluate LLMs for these
traits. However, structural simplicity (subject-relation-object) and
contemporary association in their query formulation limit the broader
definition of factuality and consistency. In this study, we introduce TeCFaP, a
novel Temporally Consistent Factuality Probe task to expand the consistent
factuality probe in the temporal dimension. To this end, we propose TEMP-COFAC,
a high-quality dataset of prefix-style English query paraphrases. Subsequently,
we extend the definitions of existing metrics to represent consistent
factuality across temporal dimension. We experiment with a diverse set of LLMs
and find most of them performing poorly on TeCFaP. Next, we propose a novel
solution CoTSeLF (Consistent-Time-Sensitive Learning Framework) combining
multi-task instruction tuning (MT-IT) with consistent-time-sensitive
reinforcement learning (CTSRL) to improve temporally consistent factuality in
LLMs. Our experiments demonstrate the efficacy of CoTSeLF over several
baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.13445v1' target='_blank'>Selective Exploration and Information Gathering in Search and Rescue
  Using Hierarchical Learning Guided by Natural Language Input</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dimitrios Panagopoulos, Adolfo Perrusquia, Weisi Guo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-20 12:27:47</h6>
<p class='card-text'>In recent years, robots and autonomous systems have become increasingly
integral to our daily lives, offering solutions to complex problems across
various domains. Their application in search and rescue (SAR) operations,
however, presents unique challenges. Comprehensively exploring the
disaster-stricken area is often infeasible due to the vastness of the terrain,
transformed environment, and the time constraints involved. Traditional robotic
systems typically operate on predefined search patterns and lack the ability to
incorporate and exploit ground truths provided by human stakeholders, which can
be the key to speeding up the learning process and enhancing triage. Addressing
this gap, we introduce a system that integrates social interaction via large
language models (LLMs) with a hierarchical reinforcement learning (HRL)
framework. The proposed system is designed to translate verbal inputs from
human stakeholders into actionable RL insights and adjust its search strategy.
By leveraging human-provided information through LLMs and structuring task
execution through HRL, our approach not only bridges the gap between autonomous
capabilities and human intelligence but also significantly improves the agent's
learning efficiency and decision-making process in environments characterised
by long horizons and sparse rewards.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.13221v2' target='_blank'>RLHFuse: Efficient RLHF Training for Large Language Models with Inter-
  and Intra-Stage Fusion</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yinmin Zhong, Zili Zhang, Bingyang Wu, Shengyu Liu, Yukun Chen, Changyi Wan, Hanpeng Hu, Lei Xia, Ranchen Ming, Yibo Zhu, Xin Jin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-20 05:15:38</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) enhances the alignment
between LLMs and human preference. The workflow of RLHF typically involves
several models and tasks in a series of distinct stages. Existing RLHF training
systems view each task as the smallest execution unit thus overlooking the
opportunities for subtask-level optimizations. Due to the intrinsic nature of
RLHF training, i.e., the data skewness in the generation stage, and the
pipeline bubbles in the training stage, existing RLHF systems suffer from low
GPU utilization in production deployments.
  RLHFuse breaks the traditional view of RLHF workflow as a composition of
individual tasks, splitting each task into finer-grained subtasks, and
performing stage fusion to improve GPU utilization. RLHFuse contains two key
ideas. First, for generation and inference tasks, RLHFuse splits them into
sample-level subtasks, enabling efficient inter-stage fusion to mitigate the
original generation bottleneck dominated by long-tailed samples. Second, for
training tasks, RLHFuse breaks them into subtasks of micro-batches. By
leveraging the intuition that pipeline execution can be essentially
complemented by another pipeline, RLHFuse performs intra-stage fusion to
concurrently execute these subtasks in the training stage with a fused pipeline
schedule, resulting in fewer pipeline bubbles. In addition, RLHFuse
incorporates a series of system optimizations tailored for each stage of RLHF,
making it efficient and scalable for our internal product usage. We evaluate
RLHFuse on various popular LLMs and the results show that RLHFuse increases the
training throughput by up to 3.7x, compared to existing state-of-the-art
systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.13035v3' target='_blank'>TACO-RL: Task Aware Prompt Compression Optimization with Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shivam Shandilya, Menglin Xia, Supriyo Ghosh, Huiqiang Jiang, Jue Zhang, Qianhui Wu, Victor Rühle</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-19 18:11:59</h6>
<p class='card-text'>The increasing prevalence of large language models (LLMs) such as GPT-4 in
various applications has led to a surge in the size of prompts required for
optimal performance, leading to challenges in computational efficiency. Prompt
compression aims to reduce the inference cost by minimizing input tokens
without compromising on the task performance. However, existing prompt
compression techniques either rely on sub-optimal metrics such as information
entropy or model it as a task-agnostic token classification problem that fails
to capture task-specific information. To address these issues, we propose a
novel and efficient reinforcement learning (RL) based task-aware prompt
compression method. To ensure low latency requirements, we leverage existing
Transformer encoder-based token classification model while guiding the learning
process with task-specific reward signals using lightweight REINFORCE
algorithm. We evaluate the performance of our method on three diverse and
challenging tasks including text summarization, question answering and code
summarization. We demonstrate that our RL-guided compression method improves
the task performance by 8% - 189% across these three scenarios over
state-of-the-art compression techniques while satisfying the same compression
rate and latency requirements.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.12917v2' target='_blank'>Training Language Models to Self-Correct via Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei M Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal Behbahani, Aleksandra Faust</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-19 17:16:21</h6>
<p class='card-text'>Self-correction is a highly desirable capability of large language models
(LLMs), yet it has consistently been found to be largely ineffective in modern
LLMs. Current methods for training self-correction typically depend on either
multiple models, a more advanced model, or additional forms of supervision. To
address these shortcomings, we develop a multi-turn online reinforcement
learning (RL) approach, SCoRe, that significantly improves an LLM's
self-correction ability using entirely self-generated data. To build SCoRe, we
first show that variants of supervised fine-tuning (SFT) on offline
model-generated correction traces are often insufficient for instilling
self-correction behavior. In particular, we observe that training via SFT falls
prey to either a distribution mismatch between mistakes made by the
data-collection policy and the model's own responses, or to behavior collapse,
where learning implicitly prefers only a certain mode of correction behavior
that is often not effective at self-correction on test problems. SCoRe
addresses these challenges by training under the model's own distribution of
self-generated correction traces and using appropriate regularization to steer
the learning process into learning a self-correction behavior that is effective
at test time as opposed to fitting high-reward responses for a given prompt.
This regularization process includes an initial phase of multi-turn RL on a
base model to generate a policy initialization that is less susceptible to
collapse, followed by using a reward bonus to amplify self-correction. With
Gemini 1.0 Pro and 1.5 Flash models, we find that SCoRe achieves
state-of-the-art self-correction performance, improving the base models'
self-correction by 15.6% and 9.1% respectively on MATH and HumanEval.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.12914v3' target='_blank'>Evaluating Defences against Unsafe Feedback in RLHF</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Domenic Rosati, Giles Edkins, Harsh Raj, David Atanasov, Subhabrata Majumdar, Janarthanan Rajendran, Frank Rudzicz, Hassan Sajjad</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-19 17:10:34</h6>
<p class='card-text'>While there has been progress towards aligning Large Language Models (LLMs)
with human values and ensuring safe behaviour at inference time, safety guards
can easily be removed when fine tuned on unsafe and harmful datasets. While
this setting has been treated extensively, another popular training paradigm,
learning from unsafe feedback with reinforcement learning, has previously been
unexplored. This is concerning due to the widespread deployment of feedback
collection systems. We address this gap by providing an analysis of learning
settings where feedback is harmful, i.e. that unsafe samples are preferred over
safe ones despite model developers goal to maintain safety. We find that
safety-aligned LLMs easily explore unsafe action spaces via generating harmful
text and optimize for reward that violates safety constraints indicating that
current safety guards are not enough to prevent learning from unsafe feedback.
In order to protect against this vulnerability, we adapt a number of both
"implict" and "explicit" harmful fine-tuning defences to evaluate whether they
are effective as learning constraints in an RLHF setting finding that no method
is generally effective pointing to the need for more defence research. We end
the paper with the observation that some defences work by performing "harmless
reward hacking" for which we provide a theoretical explanation drawn from the
theory of Constrained Markov Decision Processes and provide some direction for
future defence development.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.12889v2' target='_blank'>Can VLMs Play Action Role-Playing Games? Take Black Myth Wukong as a
  Study Case</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Peng Chen, Pi Bu, Jun Song, Yuan Gao, Bo Zheng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-19 16:30:25</h6>
<p class='card-text'>Recently, large language model (LLM)-based agents have made significant
advances across various fields. One of the most popular research areas involves
applying these agents to video games. Traditionally, these methods have relied
on game APIs to access in-game environmental and action data. However, this
approach is limited by the availability of APIs and does not reflect how humans
play games. With the advent of vision language models (VLMs), agents now have
enhanced visual understanding capabilities, enabling them to interact with
games using only visual inputs. Despite these advances, current approaches
still face challenges in action-oriented tasks, particularly in action
role-playing games (ARPGs), where reinforcement learning methods are prevalent
but suffer from poor generalization and require extensive training. To address
these limitations, we select an ARPG, ``Black Myth: Wukong'', as a research
platform to explore the capability boundaries of existing VLMs in scenarios
requiring visual-only input and complex action output. We define 12 tasks
within the game, with 75% focusing on combat, and incorporate several
state-of-the-art VLMs into this benchmark. Additionally, we will release a
human operation dataset containing recorded gameplay videos and operation logs,
including mouse and keyboard actions. Moreover, we propose a novel VARP (Vision
Action Role-Playing) agent framework, consisting of an action planning system
and a visual trajectory system. Our framework demonstrates the ability to
perform basic tasks and succeed in 90% of easy and medium-level combat
scenarios. This research aims to provide new insights and directions for
applying multimodal agents in complex action game environments. The code and
datasets will be made available at https://varp-agent.github.io/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.12798v1' target='_blank'>Assessing the Zero-Shot Capabilities of LLMs for Action Evaluation in RL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Eduardo Pignatelli, Johan Ferret, Tim Rockäschel, Edward Grefenstette, Davide Paglieri, Samuel Coward, Laura Toni</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-19 14:08:09</h6>
<p class='card-text'>The temporal credit assignment problem is a central challenge in
Reinforcement Learning (RL), concerned with attributing the appropriate
influence to each actions in a trajectory for their ability to achieve a goal.
However, when feedback is delayed and sparse, the learning signal is poor, and
action evaluation becomes harder. Canonical solutions, such as reward shaping
and options, require extensive domain knowledge and manual intervention,
limiting their scalability and applicability. In this work, we lay the
foundations for Credit Assignment with Language Models (CALM), a novel approach
that leverages Large Language Models (LLMs) to automate credit assignment via
reward shaping and options discovery. CALM uses LLMs to decompose a task into
elementary subgoals and assess the achievement of these subgoals in
state-action transitions. Every time an option terminates, a subgoal is
achieved, and CALM provides an auxiliary reward. This additional reward signal
can enhance the learning process when the task reward is sparse and delayed
without the need for human-designed rewards. We provide a preliminary
evaluation of CALM using a dataset of human-annotated demonstrations from
MiniHack, suggesting that LLMs can be effective in assigning credit in
zero-shot settings, without examples or LLM fine-tuning. Our preliminary
results indicate that the knowledge of LLMs is a promising prior for credit
assignment in RL, facilitating the transfer of human knowledge into value
functions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.11704v1' target='_blank'>From Lists to Emojis: How Format Bias Affects Model Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xuanchang Zhang, Wei Xiong, Lichang Chen, Tianyi Zhou, Heng Huang, Tong Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-18 05:13:18</h6>
<p class='card-text'>In this paper, we study format biases in reinforcement learning from human
feedback (RLHF). We observe that many widely-used preference models, including
human evaluators, GPT-4, and top-ranking models on the RewardBench benchmark,
exhibit strong biases towards specific format patterns, such as lists, links,
bold text, and emojis. Furthermore, large language models (LLMs) can exploit
these biases to achieve higher rankings on popular benchmarks like AlpacaEval
and LMSYS Chatbot Arena. One notable example of this is verbosity bias, where
current preference models favor longer responses that appear more
comprehensive, even when their quality is equal to or lower than shorter,
competing responses. However, format biases beyond verbosity remain largely
underexplored in the literature. In this work, we extend the study of biases in
preference learning beyond the commonly recognized length bias, offering a
comprehensive analysis of a wider range of format biases. Additionally, we show
that with a small amount of biased data (less than 1%), we can inject
significant bias into the reward model. Moreover, these format biases can also
be easily exploited by downstream alignment algorithms, such as best-of-n
sampling and online iterative DPO, as it is usually easier to manipulate the
format than to improve the quality of responses. Our findings emphasize the
need to disentangle format and content both for designing alignment algorithms
and evaluating models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.15360v3' target='_blank'>Reward-Robust RLHF in LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuzi Yan, Xingzhou Lou, Jialian Li, Yiping Zhang, Jian Xie, Chao Yu, Yu Wang, Dong Yan, Yuan Shen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-18 02:35:41</h6>
<p class='card-text'>As Large Language Models (LLMs) continue to progress toward more advanced
forms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is
increasingly seen as a key pathway toward achieving Artificial General
Intelligence (AGI). However, the reliance on reward-model-based (RM-based)
alignment methods introduces significant challenges due to the inherent
instability and imperfections of Reward Models (RMs), which can lead to
critical issues such as reward hacking and misalignment with human intentions.
In this paper, we introduce a reward-robust RLHF framework aimed at addressing
these fundamental challenges, paving the way for more reliable and resilient
learning in LLMs. Our approach introduces a novel optimization objective that
carefully balances performance and robustness by incorporating Bayesian Reward
Model Ensembles (BRME) to model the uncertainty set of reward functions. This
allows the framework to integrate both nominal performance and minimum reward
signals, ensuring more stable learning even with imperfect RMs. Empirical
results demonstrate that our framework consistently outperforms baselines
across diverse benchmarks, showing improved accuracy and long-term stability.
We also provide a theoretical analysis, demonstrating that reward-robust RLHF
approaches the stability of constant reward settings, which proves to be
acceptable even in a stochastic-case analysis. Together, these contributions
highlight the framework potential to enhance both the performance and stability
of LLM alignment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.11239v2' target='_blank'>LLM-as-a-Judge & Reward Model: What They Can and Cannot Do</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guijin Son, Hyunwoo Ko, Hoyoung Lee, Yewon Kim, Seunghyeok Hong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-17 14:40:02</h6>
<p class='card-text'>LLM-as-a-Judge and reward models are widely used alternatives of
multiple-choice questions or human annotators for large language model (LLM)
evaluation. Their efficacy shines in evaluating long-form responses, serving a
critical role as evaluators of leaderboards and as proxies to align LLMs via
reinforcement learning. However, despite their popularity, their effectiveness
in diverse contexts, such as non-English prompts, factual verification, or
challenging questions, remains unexplored. In this paper, we conduct a
comprehensive analysis of automated evaluators, reporting several key findings
on their behavior. First, we discover that English evaluation capabilities
significantly influence language-specific evaluation capabilities, often more
than the language proficiency itself, enabling evaluators trained in English to
easily transfer their skills to other languages. Second, we identify critical
shortcomings, where LLMs fail to detect and penalize errors, such as factual
inaccuracies, cultural misrepresentations, and the presence of unwanted
language. Finally, we find that state-of-the-art evaluators struggle with
challenging prompts, in either English or Korean, underscoring their
limitations in assessing or generating complex reasoning questions. We release
the dataset and codes used.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.10372v3' target='_blank'>Instigating Cooperation among LLM Agents Using Adaptive Information
  Modulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qiliang Chen, Sepehr Ilami, Nunzio Lore, Babak Heydari</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-16 15:15:51</h6>
<p class='card-text'>This paper introduces a novel framework combining LLM agents as proxies for
human strategic behavior with reinforcement learning (RL) to engage these
agents in evolving strategic interactions within team environments. Our
approach extends traditional agent-based simulations by using strategic LLM
agents (SLA) and introducing dynamic and adaptive governance through a
pro-social promoting RL agent (PPA) that modulates information access across
agents in a network, optimizing social welfare and promoting pro-social
behavior. Through validation in iterative games, including the prisoner
dilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.
The PPA agent effectively learns to adjust information transparency, resulting
in enhanced cooperation rates. This framework offers significant insights into
AI-mediated social dynamics, contributing to the deployment of AI in real-world
team settings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.10289v2' target='_blank'>ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for
  Empathetic Response Generation via a RL-Diffusion Framework</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiahao Yuan, Zixiang Di, Zhiqing Cui, Guisong Yang, Usman Naseem</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-16 13:56:17</h6>
<p class='card-text'>Empathetic response generation necessitates the integration of emotional and
intentional dynamics to foster meaningful interactions. Existing research
either neglects the intricate interplay between emotion and intent, leading to
suboptimal controllability of empathy, or resorts to large language models
(LLMs), which incur significant computational overhead. In this paper, we
introduce ReflectDiffu, a lightweight and comprehensive framework for
empathetic response generation. This framework incorporates emotion contagion
to augment emotional expressiveness and employs an emotion-reasoning mask to
pinpoint critical emotional elements. Additionally, it integrates intent
mimicry within reinforcement learning for refinement during diffusion. By
harnessing an intent twice reflect the mechanism of
Exploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional
decision-making into precise intent actions, thereby addressing empathetic
response misalignments stemming from emotional misrecognition. Through
reflection, the framework maps emotional states to intents, markedly enhancing
both response empathy and flexibility. Comprehensive experiments reveal that
ReflectDiffu outperforms existing models regarding relevance, controllability,
and informativeness, achieving state-of-the-art results in both automatic and
human evaluations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.10188v1' target='_blank'>Enhancing RL Safety with Counterfactual LLM Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dennis Gross, Helge Spieker</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-16 11:30:39</h6>
<p class='card-text'>Reinforcement learning (RL) policies may exhibit unsafe behavior and are hard
to explain. We use counterfactual large language model reasoning to enhance RL
policy safety post-training. We show that our approach improves and helps to
explain the RL policy safety.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.10164v1' target='_blank'>Quantile Regression for Distributional Reward Models in RLHF</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nicolai Dorka</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-16 10:54:04</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) has become a key method for
aligning large language models (LLMs) with human preferences through the use of
reward models. However, traditional reward models typically generate point
estimates, which oversimplify the diversity and complexity of human values and
preferences. In this paper, we introduce Quantile Reward Models (QRMs), a novel
approach to reward modeling that learns a distribution over rewards instead of
a single scalar value. Our method uses quantile regression to estimate a full,
potentially multimodal distribution over preferences, providing a more powerful
and nuanced representation of preferences. This distributional approach can
better capture the diversity of human values, addresses label noise, and
accommodates conflicting preferences by modeling them as distinct modes in the
distribution. Our experimental results show that QRM outperforms comparable
traditional point-estimate models on RewardBench. Furthermore, we demonstrate
that the additional information provided by the distributional estimates can be
utilized in downstream applications, such as risk-aware reinforcement learning,
resulting in LLM policies that generate fewer extremely negative responses. Our
code and model are released at https://github.com/Nicolinho/QRM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.09343v1' target='_blank'>Generative AI in Data Center Networking: Fundamentals, Perspectives, and
  Case Study</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yinqiu Liu, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Yonggang Wen, Dong In Kim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-14 07:19:18</h6>
<p class='card-text'>Generative AI (GenAI), exemplified by Large Language Models (LLMs) such as
OpenAI's ChatGPT, is revolutionizing various fields. Central to this
transformation is Data Center Networking (DCN), which not only provides the
computational power necessary for GenAI training and inference but also
delivers GenAI-driven services to users. This article examines an interplay
between GenAI and DCNs, highlighting their symbiotic relationship and mutual
advancements. We begin by reviewing current challenges within DCNs and discuss
how GenAI contributes to enhancing DCN capabilities through innovations, such
as data augmentation, process automation, and domain transfer. We then focus on
analyzing the distinctive characteristics of GenAI workloads on DCNs, gaining
insights that catalyze the evolution of DCNs to more effectively support GenAI
and LLMs. Moreover, to illustrate the seamless integration of GenAI with DCNs,
we present a case study on full-lifecycle DCN digital twins. In this study, we
employ LLMs equipped with Retrieval Augmented Generation (RAG) to formulate
optimization problems for DCNs and adopt Diffusion-Deep Reinforcement Learning
(DRL) for optimizing the RAG knowledge placement strategy. This approach not
only demonstrates the application of advanced GenAI methods within DCNs but
also positions the digital twin as a pivotal GenAI service operating on DCNs.
We anticipate that this article can promote further research into enhancing the
virtuous interaction between GenAI and DCNs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.08904v2' target='_blank'>AnyBipe: An End-to-End Framework for Training and Deploying Bipedal
  Robots Guided by Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yifei Yao, Wentao He, Chenyu Gu, Jiaheng Du, Fuwei Tan, Zhen Zhu, Junguo Lu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-13 15:15:45</h6>
<p class='card-text'>Training and deploying reinforcement learning (RL) policies for robots,
especially in accomplishing specific tasks, presents substantial challenges.
Recent advancements have explored diverse reward function designs, training
techniques, simulation-to-reality (sim-to-real) transfers, and performance
analysis methodologies, yet these still require significant human intervention.
This paper introduces an end-to-end framework for training and deploying RL
policies, guided by Large Language Models (LLMs), and evaluates its
effectiveness on bipedal robots. The framework consists of three interconnected
modules: an LLM-guided reward function design module, an RL training module
leveraging prior work, and a sim-to-real homomorphic evaluation module. This
design significantly reduces the need for human input by utilizing only
essential simulation and deployment platforms, with the option to incorporate
human-engineered strategies and historical data. We detail the construction of
these modules, their advantages over traditional approaches, and demonstrate
the framework's capability to autonomously develop and refine controlling
strategies for bipedal robot locomotion, showcasing its potential to operate
independently of human intervention.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.08642v2' target='_blank'>CPL: Critical Plan Step Learning Boosts LLM Generalization in Reasoning
  Tasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tianlong Wang, Junzhe Chen, Xueting Han, Jing Bai</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-13 08:59:31</h6>
<p class='card-text'>Post-training, particularly reinforcement learning (RL) using
self-play-generated data, has become a new learning paradigm for large language
models (LLMs). However, scaling RL to develop a general reasoner remains a
research challenge, as existing methods focus on task-specific reasoning
without adequately addressing generalization across a broader range of tasks.
Moreover, unlike traditional RL with limited action space, LLMs operate in an
infinite space, making it crucial to search for valuable and diverse strategies
to solve problems effectively. To address this, we propose searching within the
action space on high-level abstract plans to enhance model generalization and
introduce Critical Plan Step Learning (CPL), comprising: 1) searching on plan,
using Monte Carlo Tree Search (MCTS) to explore diverse plan steps in
multi-step reasoning tasks, and 2) learning critical plan steps through
Step-level Advantage Preference Optimization (Step-APO), which integrates
advantage estimates for step preference obtained via MCTS into Direct
Preference Optimization (DPO). This combination helps the model effectively
learn critical plan steps, enhancing both reasoning capabilities and
generalization. Experimental results demonstrate that our method, trained
exclusively on GSM8K and MATH, not only significantly improves performance on
GSM8K (+10.5%) and MATH (+6.5%), but also enhances out-of-domain reasoning
benchmarks, such as HumanEval (+12.2%), GPQA (+8.6%), ARC-C (+4.0%), MMLU-STEM
(+2.2%), and BBH (+1.8%).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.07604v1' target='_blank'>Multilingual Prompts in LLM-Based Recommenders: Performance Across
  Languages</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Makbule Gulcin Ozsoy</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-11 20:31:42</h6>
<p class='card-text'>Large language models (LLMs) are increasingly used in natural language
processing tasks. Recommender systems traditionally use methods such as
collaborative filtering and matrix factorization, as well as advanced
techniques like deep learning and reinforcement learning. Although language
models have been applied in recommendation, the recent trend have focused on
leveraging the generative capabilities of LLMs for more personalized
suggestions. While current research focuses on English due to its resource
richness, this work explores the impact of non-English prompts on
recommendation performance. Using OpenP5, a platform for developing and
evaluating LLM-based recommendations, we expanded its English prompt templates
to include Spanish and Turkish. Evaluation on three real-world datasets, namely
ML1M, LastFM, and Amazon-Beauty, showed that usage of non-English prompts
generally reduce performance, especially in less-resourced languages like
Turkish. We also retrained an LLM-based recommender model with multilingual
prompts to analyze performance variations. Retraining with multilingual prompts
resulted in more balanced performance across languages, but slightly reduced
English performance. This work highlights the need for diverse language support
in LLM-based recommenders and suggests future research on creating evaluation
datasets, using newer models and additional languages.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.06957v2' target='_blank'>Policy Filtration in RLHF to Fine-Tune LLM for Code Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wei Shen, Chuheng Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-11 02:40:38</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) is one of the key
techniques that helps large language models (LLMs) to follow instructions and
provide helpful and harmless responses. While direct policy optimization
methods exist, state-of-the-art LLMs adopt RL-based methods (usually PPO) in
RLHF to train the policy to generate good responses guided by a reward model
learned from preference data. The main challenge of these methods is the
inaccuracy of the intermediate reward model, especially in code generation
tasks that require long and complex reasoning to score a response. We find that
the reliability of the reward model varies across responses assigned with
different rewards. This motivates us to filter the samples whose rewards may be
unreliable to improve signal-to-noise ratio during policy learning, resulting
in Policy Filtration for Proximal Policy Optimization (PF-PPO). To choose a
proper policy filtration strategy for a given reward model, the coefficient of
determination ($R^2$) between rewards and actual scores on filtered samples
serves as a good metrics and helps us find several promising strategies. We
provide extensive experiments to validate the effectiveness of PF-PPO in code
generation tasks, and find that some variants of PF-PPO are highly effective
and achieve new state-of-the-art performance across 7-billion-parameter models
on HumanEval, MBPP, and a new and more challenging LeetCode Contest benchmark.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.06903v1' target='_blank'>Semi-Supervised Reward Modeling via Iterative Self-Training</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yifei He, Haoxiang Wang, Ziyan Jiang, Alexandros Papangelis, Han Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-10 22:57:58</h6>
<p class='card-text'>Reward models (RM) capture the values and preferences of humans and play a
central role in Reinforcement Learning with Human Feedback (RLHF) to align
pretrained large language models (LLMs). Traditionally, training these models
relies on extensive human-annotated preference data, which poses significant
challenges in terms of scalability and cost. To overcome these limitations, we
propose Semi-Supervised Reward Modeling (SSRM), an approach that enhances RM
training using unlabeled data. Given an unlabeled dataset, SSRM involves three
key iterative steps: pseudo-labeling unlabeled examples, selecting
high-confidence examples through a confidence threshold, and supervised
finetuning on the refined dataset. Across extensive experiments on various
model configurations, we demonstrate that SSRM significantly improves reward
models without incurring additional labeling costs. Notably, SSRM can achieve
performance comparable to models trained entirely on labeled data of equivalent
volumes. Overall, SSRM substantially reduces the dependency on large volumes of
human-annotated data, thereby decreasing the overall cost and time involved in
training effective reward models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.06411v2' target='_blank'>Length Desensitization in Direct Preference Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wei Liu, Yang Bai, Chengcheng Han, Rongxiang Weng, Jun Xu, Xuezhi Cao, Jingang Wang, Xunliang Cai</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-10 10:49:38</h6>
<p class='card-text'>Direct Preference Optimization (DPO) is widely utilized in the Reinforcement
Learning from Human Feedback (RLHF) phase to align Large Language Models (LLMs)
with human preferences, thereby enhancing both their harmlessness and efficacy.
However, it has been observed that DPO tends to over-optimize for verbosity,
which can detrimentally affect both performance and user experience. In this
paper, we conduct an in-depth theoretical analysis of DPO's optimization
objective and reveal a strong correlation between its implicit reward and data
length. This correlation misguides the optimization direction, resulting in
length sensitivity during the DPO training and leading to verbosity. To address
this issue, we propose a length-desensitization improvement method for DPO,
termed LD-DPO. The proposed method aims to desensitize DPO to data length by
decoupling explicit length preference, which is relatively insignificant, from
the other implicit preferences, thereby enabling more effective learning of the
intrinsic preferences. We utilized two settings (Base and Instruct) of
Llama2-13B, Llama3-8B, and Qwen2-7B for experimental validation on various
benchmarks including MT-Bench and AlpacaEval 2. The experimental results
indicate that LD-DPO consistently outperforms DPO and other baseline methods,
achieving more concise responses with a 10-40% reduction in length compared to
DPO. We conducted in-depth experimental analyses to demonstrate that LD-DPO can
indeed achieve length desensitization and align the model more closely with
human-like preferences.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.04965v1' target='_blank'>Enhancing Socially-Aware Robot Navigation through Bidirectional Natural
  Language Conversation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Congcong Wen, Yifan Liu, Geeta Chandra Raju Bethala, Zheng Peng, Hui Lin, Yu-Shen Liu, Yi Fang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-08 04:04:21</h6>
<p class='card-text'>Robot navigation is an important research field with applications in various
domains. However, traditional approaches often prioritize efficiency and
obstacle avoidance, neglecting a nuanced understanding of human behavior or
intent in shared spaces. With the rise of service robots, there's an increasing
emphasis on endowing robots with the capability to navigate and interact in
complex real-world environments. Socially aware navigation has recently become
a key research area. However, existing work either predicts pedestrian
movements or simply emits alert signals to pedestrians, falling short of
facilitating genuine interactions between humans and robots. In this paper, we
introduce the Hybrid Soft Actor-Critic with Large Language Model (HSAC-LLM), an
innovative model designed for socially-aware navigation in robots. This model
seamlessly integrates deep reinforcement learning with large language models,
enabling it to predict both continuous and discrete actions for navigation.
Notably, HSAC-LLM facilitates bidirectional interaction based on natural
language with pedestrian models. When a potential collision with pedestrians is
detected, the robot can initiate or respond to communications with pedestrians,
obtaining and executing subsequent avoidance strategies. Experimental results
in 2D simulation, the Gazebo environment, and the real-world environment
demonstrate that HSAC-LLM not only efficiently enables interaction with humans
but also exhibits superior performance in navigation and obstacle avoidance
compared to state-of-the-art DRL algorithms. We believe this innovative
paradigm opens up new avenues for effective and socially aware human-robot
interactions in dynamic environments. Videos are available at
https://hsacllm.github.io/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.04744v1' target='_blank'>LMGT: Optimizing Exploration-Exploitation Balance in Reinforcement
  Learning through Language Model Guided Trade-offs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yongxin Deng, Xihe Qiu, Xiaoyu Tan, Wei Chu, Yinghui Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-07 07:40:43</h6>
<p class='card-text'>The uncertainty inherent in the environmental transition model of
Reinforcement Learning (RL) necessitates a careful balance between exploration
and exploitation to optimize the use of computational resources for accurately
estimating an agent's expected reward. Achieving balance in control systems is
particularly challenging in scenarios with sparse rewards. However, given the
extensive prior knowledge available for many environments, it is redundant to
begin learning from scratch in such settings. To address this, we introduce
\textbf{L}anguage \textbf{M}odel \textbf{G}uided \textbf{T}rade-offs (i.e.,
\textbf{LMGT}), a novel, sample-efficient framework that leverages the
comprehensive prior knowledge embedded in Large Language Models (LLMs) and
their adeptness at processing non-standard data forms, such as wiki tutorials.
LMGT proficiently manages the exploration-exploitation trade-off by employing
reward shifts guided by LLMs, which direct agents' exploration endeavors,
thereby improving sample efficiency. We have thoroughly tested LMGT across
various RL tasks and deployed it in industrial-grade RL recommendation systems,
where it consistently outperforms baseline methods. The results indicate that
our framework can significantly reduce the time cost required during the
training phase in RL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.04421v2' target='_blank'>RLPF: Reinforcement Learning from Prediction Feedback for User
  Summarization with LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiaxing Wu, Lin Ning, Luyang Liu, Harrison Lee, Neo Wu, Chao Wang, Sushant Prakash, Shawn O'Banion, Bradley Green, Jun Xie</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-06 17:30:45</h6>
<p class='card-text'>LLM-powered personalization agent systems employ Large Language Models (LLMs)
to predict users' behavior from their past activities. However, their
effectiveness often hinges on the ability to effectively leverage extensive,
long user historical data due to its inherent noise and length of such data.
Existing pretrained LLMs may generate summaries that are concise but lack the
necessary context for downstream tasks, hindering their utility in
personalization systems. To address these challenges, we introduce
Reinforcement Learning from Prediction Feedback (RLPF). RLPF fine-tunes LLMs to
generate concise, human-readable user summaries that are optimized for
downstream task performance. By maximizing the usefulness of the generated
summaries, RLPF effectively distills extensive user history data while
preserving essential information for downstream tasks. Our empirical evaluation
demonstrates significant improvements in both extrinsic downstream task utility
and intrinsic summary quality, surpassing baseline methods by up to 22% on
downstream task performance and achieving an up to 84.59% win rate on
Factuality, Abstractiveness, and Readability. RLPF also achieves a remarkable
74% reduction in context length while improving performance on 16 out of 19
unseen tasks and/or datasets, showcasing its generalizability. This approach
offers a promising solution for enhancing LLM personalization by effectively
transforming long, noisy user histories into informative and human-readable
representations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.03650v2' target='_blank'>On the Limited Generalization Capability of the Implicit Reward Model
  Induced by Direct Preference Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yong Lin, Skyler Seto, Maartje ter Hoeve, Katherine Metcalf, Barry-John Theobald, Xuan Wang, Yizhe Zhang, Chen Huang, Tong Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-05 16:08:19</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) is an effective approach
for aligning language models to human preferences. Central to RLHF is learning
a reward function for scoring human preferences. Two main approaches for
learning a reward model are 1) training an EXplicit Reward Model (EXRM) as in
RLHF, and 2) using an implicit reward learned from preference data through
methods such as Direct Preference Optimization (DPO). Prior work has shown that
the implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in
the limit. DPORM's effectiveness directly implies the optimality of the learned
policy, and also has practical implication for LLM alignment methods including
iterative DPO. However, it is unclear how well DPORM empirically matches the
performance of EXRM. This work studies the accuracy at distinguishing preferred
and rejected answers for both DPORM and EXRM. Our findings indicate that even
though DPORM fits the training dataset comparably, it generalizes less
effectively than EXRM, especially when the validation datasets contain
distribution shifts. Across five out-of-distribution settings, DPORM has a mean
drop in accuracy of 3% and a maximum drop of 7%. These findings highlight that
DPORM has limited generalization ability and substantiates the integration of
an explicit reward model in iterative DPO approaches.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.09063v1' target='_blank'>TS-EoH: An Edge Server Task Scheduling Algorithm Based on Evolution of
  Heuristic</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wang Yatong, Pei Yuchen, Zhao Yuqi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-04 10:00:32</h6>
<p class='card-text'>With the widespread adoption of 5G and Internet of Things (IoT) technologies,
the low latency provided by edge computing has great importance for real-time
processing. However, managing numerous simultaneous service requests poses a
significant challenge to maintaining low latency. Current edge server task
scheduling methods often fail to balance multiple optimization goals
effectively. This paper introduces a novel task-scheduling approach based on
Evolutionary Computing (EC) theory and heuristic algorithms. We model service
requests as task sequences and evaluate various scheduling schemes during each
evolutionary process using Large Language Models (LLMs) services. Experimental
results show that our task-scheduling algorithm outperforms existing heuristic
and traditional reinforcement learning methods. Additionally, we investigate
the effects of different heuristic strategies and compare the evolutionary
outcomes across various LLM services.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.02428v3' target='_blank'>Large Language Models as Efficient Reward Function Searchers for
  Custom-Environment Multi-Objective Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guanwen Xie, Jingzehua Xu, Yiyuan Yang, Yimian Ding, Shuai Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-04 04:15:14</h6>
<p class='card-text'>Achieving the effective design and improvement of reward functions in
reinforcement learning (RL) tasks with complex custom environments and multiple
requirements presents considerable challenges. In this paper, we propose ERFSL,
an efficient reward function searcher using LLMs, which enables LLMs to be
effective white-box searchers and highlights their advanced semantic
understanding capabilities. Specifically, we generate reward components for
each numerically explicit user requirement and employ a reward critic to
identify the correct code form. Then, LLMs assign weights to the reward
components to balance their values and iteratively adjust the weights without
ambiguity and redundant adjustments by flexibly adopting directional mutation
and crossover strategies, similar to genetic algorithms, based on the context
provided by the training log analyzer. We applied the framework to an
underwater data collection RL task without direct human feedback or reward
examples (zero-shot learning). The reward critic successfully corrects the
reward code with only one feedback instance for each requirement, effectively
preventing unrectifiable errors. The initialization of weights enables the
acquisition of different reward functions within the Pareto solution set
without the need for weight search. Even in cases where a weight is 500 times
off, on average, only 5.2 iterations are needed to meet user requirements. The
ERFSL also works well with most prompts utilizing GPT-4o mini, as we decompose
the weight searching process to reduce the requirement for numerical and
long-context understanding capabilities</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.01552v1' target='_blank'>Self-Instructed Derived Prompt Generation Meets In-Context Learning:
  Unlocking New Potential of Black-Box LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhuo Li, Yuhao Du, Jinpeng Hu, Xiang Wan, Anningzhe Gao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-03 02:42:39</h6>
<p class='card-text'>Large language models (LLMs) have shown success in generating high-quality
responses. In order to achieve better alignment with LLMs with human
preference, various works are proposed based on specific optimization process,
which, however, is not suitable to Black-Box LLMs like GPT-4, due to
inaccessible parameters. In Black-Box LLMs case, their performance is highly
dependent on the quality of the provided prompts. Existing methods to enhance
response quality often involve a prompt refinement model, yet these approaches
potentially suffer from semantic inconsistencies between the refined and
original prompts, and typically overlook the relationship between them. To
address these challenges, we introduce a self-instructed in-context learning
framework that empowers LLMs to deliver more effective responses by generating
reliable derived prompts to construct informative contextual environments. Our
approach incorporates a self-instructed reinforcement learning mechanism,
enabling direct interaction with the response model during derived prompt
generation for better alignment. We then formulate querying as an in-context
learning task, using responses from LLMs combined with the derived prompts to
establish a contextual demonstration for the original prompt. This strategy
ensures alignment with the original query, reduces discrepancies from refined
prompts, and maximizes the LLMs' in-context learning capability. Extensive
experiments demonstrate that the proposed method not only generates more
reliable derived prompts but also significantly enhances LLMs' ability to
deliver more effective responses, including Black-Box models such as GPT-4.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.01369v2' target='_blank'>Imitating Language via Scalable Inverse Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Markus Wulfmeier, Michael Bloesch, Nino Vieillard, Arun Ahuja, Jorg Bornschein, Sandy Huang, Artem Sokolov, Matt Barnes, Guillaume Desjardins, Alex Bewley, Sarah Maria Elisabeth Bechtle, Jost Tobias Springenberg, Nikola Momchev, Olivier Bachem, Matthieu Geist, Martin Riedmiller</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-02 16:48:57</h6>
<p class='card-text'>The majority of language model training builds on imitation learning. It
covers pretraining, supervised fine-tuning, and affects the starting conditions
for reinforcement learning from human feedback (RLHF). The simplicity and
scalability of maximum likelihood estimation (MLE) for next token prediction
led to its role as predominant paradigm. However, the broader field of
imitation learning can more effectively utilize the sequential structure
underlying autoregressive generation. We focus on investigating the inverse
reinforcement learning (IRL) perspective to imitation, extracting rewards and
directly optimizing sequences instead of individual token likelihoods and
evaluate its benefits for fine-tuning large language models. We provide a new
angle, reformulating inverse soft-Q-learning as a temporal difference
regularized extension of MLE. This creates a principled connection between MLE
and IRL and allows trading off added complexity with increased performance and
diversity of generations in the supervised fine-tuning (SFT) setting. We find
clear advantages for IRL-based imitation, in particular for retaining diversity
while maximizing task performance, rendering IRL a strong alternative on fixed
SFT datasets even without online data generation. Our analysis of IRL-extracted
reward functions further indicates benefits for more robust reward functions
via tighter integration of supervised and preference-based LLM post-training.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.01326v1' target='_blank'>Grounding Language Models in Autonomous Loco-manipulation Tasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jin Wang, Nikos Tsagarakis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-02 15:27:48</h6>
<p class='card-text'>Humanoid robots with behavioral autonomy have consistently been regarded as
ideal collaborators in our daily lives and promising representations of
embodied intelligence. Compared to fixed-based robotic arms, humanoid robots
offer a larger operational space while significantly increasing the difficulty
of control and planning. Despite the rapid progress towards general-purpose
humanoid robots, most studies remain focused on locomotion ability with few
investigations into whole-body coordination and tasks planning, thus limiting
the potential to demonstrate long-horizon tasks involving both mobility and
manipulation under open-ended verbal instructions. In this work, we propose a
novel framework that learns, selects, and plans behaviors based on tasks in
different scenarios. We combine reinforcement learning (RL) with whole-body
optimization to generate robot motions and store them into a motion library. We
further leverage the planning and reasoning features of the large language
model (LLM), constructing a hierarchical task graph that comprises a series of
motion primitives to bridge lower-level execution with higher-level planning.
Experiments in simulation and real-world using the CENTAURO robot show that the
language model based planner can efficiently adapt to new loco-manipulation
tasks, demonstrating high autonomy from free-text commands in unstructured
scenes.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.00985v1' target='_blank'>Co-Learning: Code Learning for Multi-Agent Reinforcement Collaborative
  Framework with Conversational Natural Language Interfaces</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiapeng Yu, Yuqian Wu, Yajing Zhan, Wenhao Guo, Zhou Xu, Raymond Lee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-02 07:03:22</h6>
<p class='card-text'>Online question-and-answer (Q\&A) systems based on the Large Language Model
(LLM) have progressively diverged from recreational to professional use. This
paper proposed a Multi-Agent framework with environmentally reinforcement
learning (E-RL) for code correction called Code Learning (Co-Learning)
community, assisting beginners to correct code errors independently. It
evaluates the performance of multiple LLMs from an original dataset with 702
error codes, uses it as a reward or punishment criterion for E-RL; Analyzes
input error codes by the current agent; selects the appropriate LLM-based agent
to achieve optimal error correction accuracy and reduce correction time.
Experiment results showed that 3\% improvement in Precision score and 15\%
improvement in time cost as compared with no E-RL method respectively. Our
source code is available at: https://github.com/yuqian2003/Co_Learning</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.00855v1' target='_blank'>LanguaShrink: Reducing Token Overhead with Psycholinguistics</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xuechen Liang, Meiling Tao, Yinghui Xia, Tianyu Shi, Jun Wang, JingSong Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-01 22:09:20</h6>
<p class='card-text'>As large language models (LLMs) improve their capabilities in handling
complex tasks, the issues of computational cost and efficiency due to long
prompts are becoming increasingly prominent. To accelerate model inference and
reduce costs, we propose an innovative prompt compression framework called
LanguaShrink. Inspired by the observation that LLM performance depends on the
density and position of key information in the input prompts, LanguaShrink
leverages psycholinguistic principles and the Ebbinghaus memory curve to
achieve task-agnostic prompt compression. This effectively reduces prompt
length while preserving essential information. We referred to the training
method of OpenChat.The framework introduces part-of-speech priority compression
and data distillation techniques, using smaller models to learn compression
targets and employing a KL-regularized reinforcement learning strategy for
training.\cite{wang2023openchat} Additionally, we adopt a chunk-based
compression algorithm to achieve adjustable compression rates. We evaluate our
method on multiple datasets, including LongBench, ZeroScrolls, Arxiv Articles,
and a newly constructed novel test set. Experimental results show that
LanguaShrink maintains semantic similarity while achieving up to 26 times
compression. Compared to existing prompt compression methods, LanguaShrink
improves end-to-end latency by 1.43 times.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.00571v1' target='_blank'>Enhancing Source Code Security with LLMs: Demystifying The Challenges
  and Generating Reliable Repairs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nafis Tanveer Islam, Joseph Khoury, Andrew Seong, Elias Bou-Harb, Peyman Najafirad</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-01 00:41:40</h6>
<p class='card-text'>With the recent unprecedented advancements in Artificial Intelligence (AI)
computing, progress in Large Language Models (LLMs) is accelerating rapidly,
presenting challenges in establishing clear guidelines, particularly in the
field of security. That being said, we thoroughly identify and describe three
main technical challenges in the security and software engineering literature
that spans the entire LLM workflow, namely; \textbf{\textit{(i)}} Data
Collection and Labeling; \textbf{\textit{(ii)}} System Design and Learning; and
\textbf{\textit{(iii)}} Performance Evaluation. Building upon these challenges,
this paper introduces \texttt{SecRepair}, an instruction-based LLM system
designed to reliably \textit{identify}, \textit{describe}, and automatically
\textit{repair} vulnerable source code. Our system is accompanied by a list of
actionable guides on \textbf{\textit{(i)}} Data Preparation and Augmentation
Techniques; \textbf{\textit{(ii)}} Selecting and Adapting state-of-the-art LLM
Models; \textbf{\textit{(iii)}} Evaluation Procedures. \texttt{SecRepair} uses
a reinforcement learning-based fine-tuning with a semantic reward that caters
to the functionality and security aspects of the generated code. Our empirical
analysis shows that \texttt{SecRepair} achieves a \textit{12}\% improvement in
security code repair compared to other LLMs when trained using reinforcement
learning. Furthermore, we demonstrate the capabilities of \texttt{SecRepair} in
generating reliable, functional, and compilable security code repairs against
real-world test cases using automated evaluation metrics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.00162v1' target='_blank'>Sequence to Sequence Reward Modeling: Improving RLHF by Language
  Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiayi Zhou, Jiaming Ji, Juntao Dai, Yaodong Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-30 16:14:35</h6>
<p class='card-text'>Aligning the behavior of Large language models (LLMs) with human intentions
and values remains a critical challenge. Reinforcement learning from human
feedback (RLHF) aligns LLMs by training a reward model (RM) on human
preferences and fine-tuning the LLMs to maximize RM feedback. Despite its
effectiveness and popularity, RLHF is prone to biased local optimization. It
means RM fails to provide feedback that accurately aligns with human
preference, causing LLMs to explore unexpected generalizations, and failing to
achieve alignment objectives. To mitigate this issue, we propose a novel
\textit{sequence-to-sequence (seq2seq) reward modeling} method. Its key insight
is that learning from language feedback rather than scalar feedback improves
RLHF without additional annotations. We replaced the reward modeling target
from binary maximum likelihood estimation (MLE) with sequence MLE. This method
enables richer and fine-grained language feedback without additional
annotations, models, or training stages. Our experiments demonstrated its
effectiveness, specifically, reducing the refusal-to-response paradigm in
single-turn safety dialogues and the long-response bias in text summarization
tasks. We provide further analysis that seq2seq RM improves RLHF performance
across 2B and 7B LLMs on 3 NLP tasks, achieving an average win rate of 76.9\%.
We further show that seq2seq RM can still improve the performance of RLHF under
out-of-distribution prompts.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.00147v1' target='_blank'>MultiMath: Bridging Visual and Mathematical Reasoning for Large Language
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shuai Peng, Di Fu, Liangcai Gao, Xiuqin Zhong, Hongguang Fu, Zhi Tang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-30 07:37:38</h6>
<p class='card-text'>The rapid development of large language models (LLMs) has spurred extensive
research into their domain-specific capabilities, particularly mathematical
reasoning. However, most open-source LLMs focus solely on mathematical
reasoning, neglecting the integration with visual injection, despite the fact
that many mathematical tasks rely on visual inputs such as geometric diagrams,
charts, and function plots. To fill this gap, we introduce
\textbf{MultiMath-7B}, a multimodal large language model that bridges the gap
between math and vision. \textbf{MultiMath-7B} is trained through a four-stage
process, focusing on vision-language alignment, visual and math
instruction-tuning, and process-supervised reinforcement learning. We also
construct a novel, diverse and comprehensive multimodal mathematical dataset,
\textbf{MultiMath-300K}, which spans K-12 levels with image captions and
step-wise solutions. MultiMath-7B achieves state-of-the-art (SOTA) performance
among open-source models on existing multimodal mathematical benchmarks and
also excels on text-only mathematical benchmarks. Our model and dataset are
available at
{\textcolor{blue}{\url{https://github.com/pengshuai-rin/MultiMath}}}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.15950v2' target='_blank'>Atari-GPT: Benchmarking Multimodal Large Language Models as Low-Level
  Policies in Atari Games</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nicholas R. Waytowich, Devin White, MD Sunbeam, Vinicius G. Goecks</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-28 17:08:56</h6>
<p class='card-text'>Recent advancements in large language models (LLMs) have expanded their
capabilities beyond traditional text-based tasks to multimodal domains,
integrating visual, auditory, and textual data. While multimodal LLMs have been
extensively explored for high-level planning in domains like robotics and
games, their potential as low-level controllers remains largely untapped. In
this paper, we introduce a novel benchmark aimed at testing the emergent
capabilities of multimodal LLMs as low-level policies in Atari games. Unlike
traditional reinforcement learning (RL) methods that require training for each
new environment and reward function specification, these LLMs utilize
pre-existing multimodal knowledge to directly engage with game environments.
Our study assesses the performances of multiple multimodal LLMs against
traditional RL agents, human players, and random agents, focusing on their
ability to understand and interact with complex visual scenes and formulate
strategic responses. Our results show that these multimodal LLMs are not yet
capable of being zero-shot low-level policies. Furthermore, we see that this
is, in part, due to their visual and spatial reasoning. Additional results and
videos are available on our project webpage:
https://dev1nw.github.io/atari-gpt/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.16032v1' target='_blank'>An Extremely Data-efficient and Generative LLM-based Reinforcement
  Learning Agent for Recommenders</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shuang Feng, Grace Feng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-28 10:31:50</h6>
<p class='card-text'>Recent advancements in large language models (LLMs) have enabled
understanding webpage contexts, product details, and human instructions.
Utilizing LLMs as the foundational architecture for either reward models or
policies in reinforcement learning has gained popularity -- a notable
achievement is the success of InstructGPT. RL algorithms have been instrumental
in maximizing long-term customer satisfaction and avoiding short-term, myopic
goals in industrial recommender systems, which often rely on deep learning
models to predict immediate clicks or purchases.
  In this project, several RL methods are implemented and evaluated using the
WebShop benchmark environment, data, simulator, and pre-trained model
checkpoints. The goal is to train an RL agent to maximize the purchase reward
given a detailed human instruction describing a desired product. The RL agents
are developed by fine-tuning a pre-trained BERT model with various objectives,
learning from preferences without a reward model, and employing contemporary
training techniques such as Proximal Policy Optimization (PPO) as used in
InstructGPT, and Direct Preference Optimization (DPO). This report also
evaluates the RL agents trained using generative trajectories. Evaluations were
conducted using Thompson sampling in the WebShop simulator environment.
  The simulated online experiments demonstrate that agents trained on generated
trajectories exhibited comparable task performance to those trained using human
trajectories. This has demonstrated an example of an extremely low-cost
data-efficient way of training reinforcement learning agents. Also, with
limited training time (<2hours), without utilizing any images, a DPO agent
achieved a 19% success rate after approximately 3000 steps or 30 minutes of
training on T4 GPUs, compared to a PPO agent, which reached a 15% success rate.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.15313v1' target='_blank'>Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in
  Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenxuan Zhang, Philip H. S. Torr, Mohamed Elhoseiny, Adel Bibi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-27 17:31:21</h6>
<p class='card-text'>Fine-tuning large language models (LLMs) on human preferences, typically
through reinforcement learning from human feedback (RLHF), has proven
successful in enhancing their capabilities. However, ensuring the safety of
LLMs during the fine-tuning remains a critical concern, and mitigating the
potential conflicts in safety and helpfulness is costly in RLHF. To address
this issue, we propose a supervised learning framework called Bi-Factorial
Preference Optimization (BFPO), which re-parameterizes a joint RLHF objective
of both safety and helpfulness into a single supervised learning objective. In
the supervised optimization, a labeling function is used to capture global
preferences ranking to balance both safety and helpfulness. To evaluate BFPO,
we develop a benchmark including comprehensive discriminative and generative
tasks for helpfulness and harmlessness. The results indicate that our method
significantly outperforms existing approaches in both safety and helpfulness.
Moreover, BFPO eliminates the need for human prompting and annotation in LLM
fine-tuning while achieving the same level of safety as methods that heavily
rely on human labor, with less than 10% of the computational resources. The
training recipes and models will be released.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.00105v2' target='_blank'>Negation Blindness in Large Language Models: Unveiling the NO Syndrome
  in Image Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mohammad Nadeem, Shahab Saquib Sohail, Erik Cambria, Björn W. Schuller, Amir Hussain</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-27 14:40:16</h6>
<p class='card-text'>Foundational Large Language Models (LLMs) have changed the way we perceive
technology. They have been shown to excel in tasks ranging from poem writing
and coding to essay generation and puzzle solving. With the incorporation of
image generation capability, they have become more comprehensive and versatile
AI tools. At the same time, researchers are striving to identify the
limitations of these tools to improve them further. Currently identified flaws
include hallucination, biases, and bypassing restricted commands to generate
harmful content. In the present work, we have identified a fundamental
limitation related to the image generation ability of LLMs, and termed it The
NO Syndrome. This negation blindness refers to LLMs inability to correctly
comprehend NO related natural language prompts to generate the desired images.
Interestingly, all tested LLMs including GPT-4, Gemini, and Copilot were found
to be suffering from this syndrome. To demonstrate the generalization of this
limitation, we carried out simulation experiments and conducted entropy-based
and benchmark statistical analysis tests on various LLMs in multiple languages,
including English, Hindi, and French. We conclude that the NO syndrome is a
significant flaw in current LLMs that needs to be addressed. A related finding
of this study showed a consistent discrepancy between image and textual
responses as a result of this NO syndrome. We posit that the introduction of a
negation context-aware reinforcement learning based feedback loop between the
LLMs textual response and generated image could help ensure the generated text
is based on both the LLMs correct contextual understanding of the negation
query and the generated visual output.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.14853v2' target='_blank'>Atoxia: Red-teaming Large Language Models with Target Toxic Answers</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuhao Du, Zhuo Li, Pengyu Cheng, Xiang Wan, Anningzhe Gao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-27 08:12:08</h6>
<p class='card-text'>Despite the substantial advancements in artificial intelligence, large
language models (LLMs) remain being challenged by generation safety. With
adversarial jailbreaking prompts, one can effortlessly induce LLMs to output
harmful content, causing unexpected negative social impacts. This vulnerability
highlights the necessity for robust LLM red-teaming strategies to identify and
mitigate such risks before large-scale application. To detect specific types of
risks, we propose a novel red-teaming method that $\textbf{A}$ttacks LLMs with
$\textbf{T}$arget $\textbf{Toxi}$c $\textbf{A}$nswers ($\textbf{Atoxia}$).
Given a particular harmful answer, Atoxia generates a corresponding user query
and a misleading answer opening to examine the internal defects of a given LLM.
The proposed attacker is trained within a reinforcement learning scheme with
the LLM outputting probability of the target answer as the reward. We verify
the effectiveness of our method on various red-teaming benchmarks, such as
AdvBench and HH-Harmless. The empirical results demonstrate that Atoxia can
successfully detect safety risks in not only open-source models but also
state-of-the-art black-box models such as GPT-4o.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.00092v1' target='_blank'>PatentGPT: A Large Language Model for Patent Drafting Using
  Knowledge-based Fine-tuning Method</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Runtao Ren, Jian Ma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-26 12:00:29</h6>
<p class='card-text'>As humanity stands on the brink of a new era of technological innovation, the
ability to rapidly transform creative ideas into protected intellectual
property (IP) is more crucial than ever. However, the conventional processes
for patent drafting are fraught with challenges, demanding a nuanced
understanding of advanced field knowledge and technical concepts. Existing
large language models (LLMs), while powerful, often fall short in this IP
creation domain due to their lack of specialized knowledge and
context-awareness necessary for generating technically accurate patent
documents. To bridge this critical gap, we propose a groundbreaking framework
for Knowledge Fine-Tuning (KFT) of LLMs, designed to endow AI with the ability
to autonomously mine, understand, and apply domain-specific knowledge. Our
model, PatentGPT leverages a unique combination of knowledge graph-based
pre-training, domain-specific supervised fine-tuning (SFT), and reinforcement
learning from human feedback (RLHF). Through extensive evaluation, PatentGPT
has demonstrated outstanding performance, scoring up to approximately 400%
higher in patent related benchmark tests compared to state-of-the-art models.
By KFT method the model's capability to not only assist but also augment human
creativity and innovation, our approach sets a new standard for AI-driven
intellectual property generation, paving the way for more efficient and
effective invention processes.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.13510v2' target='_blank'>Intelligent Router for LLM Workloads: Improving Performance Through
  Workload-Aware Load Balancing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kunal Jain, Anjaly Parayil, Ankur Mallick, Esha Choukse, Xiaoting Qin, Jue Zhang, Íñigo Goiri, Rujia Wang, Chetan Bansal, Victor Rühle, Anoop Kulkarni, Steve Kofsky, Saravan Rajmohan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-24 08:12:22</h6>
<p class='card-text'>Large Language Model (LLM) workloads have distinct prefill and decode phases
with different compute and memory requirements which should ideally be
accounted for when scheduling input queries across different LLM instances in a
cluster. However existing scheduling algorithms treat LLM workloads as
monolithic jobs without considering the distinct characteristics of the two
phases in each workload. This leads to sub-optimal scheduling and increased
response latency. In this work, we start by characterizing factors affecting
the response latency during LLM inference serving. We establish that better
load balancing of inference requests across the available LLM instances can
improve the end-to-end latency to a larger extent than merely focusing on
optimizing the instance-level scheduler. Motivated by our findings, we propose
a heuristic-guided reinforcement learning-based intelligent router for
data-driven and workload-aware scheduling. Our router schedules queries across
LLM instances by leveraging a trainable response-length predictor, and a novel
formulation for estimating the impact of mixing different workloads and
achieves over 11% lower end-to-end latency than existing approaches on a mix of
public datasets and 7.8% lower end-to-end latency on real workload data with
diverse input and output trends from Cloud Provider X. Additionally, the
proposed framework can also serve as a standard for benchmarking different LLM
inference schedulers since it provides the best latency for a given model,
hardware, and instance-level scheduler combination.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.13071v1' target='_blank'>Guiding IoT-Based Healthcare Alert Systems with Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yulan Gao, Ziqiang Ye, Ming Xiao, Yue Xiao, Dong In Kim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-23 13:55:36</h6>
<p class='card-text'>Healthcare alert systems (HAS) are undergoing rapid evolution, propelled by
advancements in artificial intelligence (AI), Internet of Things (IoT)
technologies, and increasing health consciousness. Despite significant
progress, a fundamental challenge remains: balancing the accuracy of
personalized health alerts with stringent privacy protection in HAS
environments constrained by resources. To address this issue, we introduce a
uniform framework, LLM-HAS, which incorporates Large Language Models (LLM) into
HAS to significantly boost the accuracy, ensure user privacy, and enhance
personalized health service, while also improving the subjective quality of
experience (QoE) for users. Our innovative framework leverages a Mixture of
Experts (MoE) approach, augmented with LLM, to analyze users' personalized
preferences and potential health risks from additional textual job
descriptions. This analysis guides the selection of specialized Deep
Reinforcement Learning (DDPG) experts, tasked with making precise health
alerts. Moreover, LLM-HAS can process Conversational User Feedback, which not
only allows fine-tuning of DDPG but also deepen user engagement, thereby
enhancing both the accuracy and personalization of health management
strategies. Simulation results validate the effectiveness of the LLM-HAS
framework, highlighting its potential as a groundbreaking approach for
employing generative AI (GAI) to provide highly accurate and reliable alerts.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.13028v1' target='_blank'>In-Context Learning with Reinforcement Learning for Incomplete Utterance
  Rewriting</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haowei Du, Dongyan Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-23 12:32:12</h6>
<p class='card-text'>In-context learning (ICL) of large language models (LLMs) has attracted
increasing attention in the community where LLMs make predictions only based on
instructions augmented with a few examples. Existing example selection methods
for ICL utilize sparse or dense retrievers and derive effective performance.
However, these methods do not utilize direct feedback of LLM to train the
retriever and the examples selected can not necessarily improve the analogy
ability of LLM. To tackle this, we propose our policy-based reinforcement
learning framework for example selection (RLS), which consists of a language
model (LM) selector and an LLM generator. The LM selector encodes the candidate
examples into dense representations and selects the top-k examples into the
demonstration for LLM. The outputs of LLM are adopted to compute the reward and
policy gradient to optimize the LM selector. We conduct experiments on
different datasets and significantly outperform existing example selection
methods. Moreover, our approach shows advantages over supervised finetuning
(SFT) models in few shot setting. Further experiments show the balance of
abundance and the similarity with the test case of examples is important for
ICL performance of LLM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.12775v2' target='_blank'>Intelligent OPC Engineer Assistant for Semiconductor Manufacturing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guojin Chen, Haoyu Yang, Bei Yu, Haoxing Ren</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-23 00:49:36</h6>
<p class='card-text'>Advancements in chip design and manufacturing have enabled the processing of
complex tasks such as deep learning and natural language processing, paving the
way for the development of artificial general intelligence (AGI). AI, on the
other hand, can be leveraged to innovate and streamline semiconductor
technology from planning and implementation to manufacturing. In this paper, we
present \textit{Intelligent OPC Engineer Assistant}, an AI/LLM-powered
methodology designed to solve the core manufacturing-aware optimization problem
known as optical proximity correction (OPC). The methodology involves a
reinforcement learning-based OPC recipe search and a customized multi-modal
agent system for recipe summarization. Experiments demonstrate that our
methodology can efficiently build OPC recipes on various chip designs with
specially handled design topologies, a task that typically requires the
full-time effort of OPC engineers with years of experience.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.12599v1' target='_blank'>Controllable Text Generation for Large Language Models: A Survey</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xun Liang, Hanyu Wang, Yezhaohui Wang, Shichao Song, Jiawei Yang, Simin Niu, Jie Hu, Dan Liu, Shunyu Yao, Feiyu Xiong, Zhiyu Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-22 17:59:04</h6>
<p class='card-text'>In Natural Language Processing (NLP), Large Language Models (LLMs) have
demonstrated high text generation quality. However, in real-world applications,
LLMs must meet increasingly complex requirements. Beyond avoiding misleading or
inappropriate content, LLMs are also expected to cater to specific user needs,
such as imitating particular writing styles or generating text with poetic
richness. These varied demands have driven the development of Controllable Text
Generation (CTG) techniques, which ensure that outputs adhere to predefined
control conditions--such as safety, sentiment, thematic consistency, and
linguistic style--while maintaining high standards of helpfulness, fluency, and
diversity.
  This paper systematically reviews the latest advancements in CTG for LLMs,
offering a comprehensive definition of its core concepts and clarifying the
requirements for control conditions and text quality. We categorize CTG tasks
into two primary types: content control and attribute control. The key methods
are discussed, including model retraining, fine-tuning, reinforcement learning,
prompt engineering, latent space manipulation, and decoding-time intervention.
We analyze each method's characteristics, advantages, and limitations,
providing nuanced insights for achieving generation control. Additionally, we
review CTG evaluation methods, summarize its applications across domains, and
address key challenges in current research, including reduced fluency and
practicality. We also propose several appeals, such as placing greater emphasis
on real-world applications in future research. This paper aims to offer
valuable guidance to researchers and developers in the field. Our reference
list and Chinese version are open-sourced at
https://github.com/IAAR-Shanghai/CTGSurvey.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.12214v2' target='_blank'>Bridging Large Language Models and Optimization: A Unified Framework for
  Text-attributed Combinatorial Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xia Jiang, Yaoxin Wu, Yuan Wang, Yingqian Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-22 08:42:44</h6>
<p class='card-text'>To advance capabilities of large language models (LLMs) in solving
combinatorial optimization problems (COPs), this paper presents the
Language-based Neural COP Solver (LNCS), a novel framework that is unified for
the end-to-end resolution of diverse text-attributed COPs. LNCS leverages LLMs
to encode problem instances into a unified semantic space, and integrates their
embeddings with a Transformer-based solution generator to produce high-quality
solutions. By training the solution generator with conflict-free multi-task
reinforcement learning, LNCS effectively enhances LLM performance in tackling
COPs of varying types and sizes, achieving state-of-the-art results across
diverse problems. Extensive experiments validate the effectiveness and
generalizability of the LNCS, highlighting its potential as a unified and
practical framework for real-world COP applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.12112v3' target='_blank'>Balancing Act: Prioritization Strategies for LLM-Designed Restless
  Bandit Rewards</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shresth Verma, Niclas Boehmer, Lingkai Kong, Milind Tambe</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-22 03:54:08</h6>
<p class='card-text'>LLMs are increasingly used to design reward functions based on human
preferences in Reinforcement Learning (RL). We focus on LLM-designed rewards
for Restless Multi-Armed Bandits, a framework for allocating limited resources
among agents. In applications such as public health, this approach empowers
grassroots health workers to tailor automated allocation decisions to community
needs. In the presence of multiple agents, altering the reward function based
on human preferences can impact subpopulations very differently, leading to
complex tradeoffs and a multi-objective resource allocation problem. We are the
first to present a principled method termed Social Choice Language Model for
dealing with these tradeoffs for LLM-designed rewards for multiagent planners
in general and restless bandits in particular. The novel part of our model is a
transparent and configurable selection component, called an adjudicator,
external to the LLM that controls complex tradeoffs via a user-selected social
welfare function. Our experiments demonstrate that our model reliably selects
more effective, aligned, and balanced reward functions compared to purely
LLM-based approaches.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.11791v1' target='_blank'>Critique-out-Loud Reward Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan D. Chang, Prithviraj Ammanabrolu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-21 17:24:15</h6>
<p class='card-text'>Traditionally, reward models used for reinforcement learning from human
feedback (RLHF) are trained to directly predict preference scores without
leveraging the generation capabilities of the underlying large language model
(LLM). This limits the capabilities of reward models as they must reason
implicitly about the quality of a response, i.e., preference modeling must be
performed in a single forward pass through the model. To enable reward models
to reason explicitly about the quality of a response, we introduce
Critique-out-Loud (CLoud) reward models. CLoud reward models operate by first
generating a natural language critique of the assistant's response that is then
used to predict a scalar reward for the quality of the response. We demonstrate
the success of CLoud reward models for both Llama-3-8B and 70B base models:
compared to classic reward models CLoud reward models improve pairwise
preference classification accuracy on RewardBench by 4.65 and 5.84 percentage
points for the 8B and 70B base models respectively. Furthermore, CLoud reward
models lead to a Pareto improvement for win rate on ArenaHard when used as the
scoring model for Best-of-N. Finally, we explore how to exploit the dynamic
inference compute capabilities of CLoud reward models by performing
self-consistency decoding for reward prediction.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.10642v1' target='_blank'>Minor SFT loss for LLM fine-tune to increase performance and reduce
  model deviation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shiming Xie, Hong Chen, Fred Yu, Zeye Sun, Xiuyu Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-20 08:32:44</h6>
<p class='card-text'>Instruct LLM provide a paradigm used in large scale language model to align
LLM to human preference. The paradigm contains supervised fine tuning and
reinforce learning from human feedback. This paradigm is also used in
downstream scenarios to adapt LLM to specific corpora and applications.
Comparing to SFT, there are many efforts focused on RLHF and several algorithms
being proposed, such as PPO, DPO, IPO, KTO, MinorDPO and etc. Meanwhile most
efforts for SFT are focused on how to collect, filter and mix high quality
data. In this article with insight from DPO and MinorDPO, we propose a training
metric for SFT to measure the discrepancy between the optimized model and the
original model, and a loss function MinorSFT that can increase the training
effectiveness, and reduce the discrepancy between the optimized LLM and
original LLM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.10635v2' target='_blank'>Strategist: Learning Strategic Skills by LLMs via Bi-Level Tree Search</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jonathan Light, Min Cai, Weiqin Chen, Guanzhi Wang, Xiusi Chen, Wei Cheng, Yisong Yue, Ziniu Hu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-20 08:22:04</h6>
<p class='card-text'>In this paper, we propose a new method STRATEGIST that utilizes LLMs to
acquire new skills for playing multi-agent games through a self-improvement
process. Our method gathers quality feedback through self-play simulations with
Monte Carlo tree search and LLM-based reflection, which can then be used to
learn high-level strategic skills such as how to evaluate states that guide the
low-level execution. We showcase how our method can be used in both action
planning and dialogue generation in the context of games, achieving good
performance on both tasks. Specifically, we demonstrate that our method can
help train agents with better performance than both traditional reinforcement
learning-based approaches and other LLM-based skill learning approaches in
games including the Game of Pure Strategy (GOPS) and The Resistance: Avalon.
STRATEGIST helps bridge the gap between foundation models and symbolic
decision-making methods through its bi-level approach, leading to more robust
decision-making.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.10504v1' target='_blank'>QPO: Query-dependent Prompt Optimization via Multi-Loop Offline
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yilun Kong, Hangyu Mao, Qi Zhao, Bin Zhang, Jingqing Ruan, Li Shen, Yongzhe Chang, Xueqian Wang, Rui Zhao, Dacheng Tao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-20 03:06:48</h6>
<p class='card-text'>Prompt engineering has demonstrated remarkable success in enhancing the
performance of large language models (LLMs) across diverse tasks. However, most
existing prompt optimization methods only focus on the task-level performance,
overlooking the importance of query-preferred prompts, which leads to
suboptimal performances. Additionally, these methods rely heavily on frequent
interactions with LLMs to obtain feedback for guiding the optimization process,
incurring substantial redundant interaction costs. In this paper, we introduce
Query-dependent Prompt Optimization (QPO), which leverages multi-loop offline
reinforcement learning to iteratively fine-tune a small pretrained language
model to generate optimal prompts tailored to the input queries, thus
significantly improving the prompting effect on the large target LLM. We derive
insights from offline prompting demonstration data, which already exists in
large quantities as a by-product of benchmarking diverse prompts on
open-sourced tasks, thereby circumventing the expenses of online interactions.
Furthermore, we continuously augment the offline dataset with the generated
prompts in each loop, as the prompts from the fine-tuned model are supposed to
outperform the source prompts in the original dataset. These iterative loops
bootstrap the model towards generating optimal prompts. Experiments on various
LLM scales and diverse NLP and math tasks demonstrate the efficacy and
cost-efficiency of our method in both zero-shot and few-shot scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.09834v3' target='_blank'>Minor DPO reject penalty to increase training robustness</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shiming Xie, Hong Chen, Fred Yu, Zeye Sun, Xiuyu Wu, Yingfan Hu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-19 09:29:31</h6>
<p class='card-text'>Learning from human preference is a paradigm used in large-scale language
model (LLM) fine-tuning step to better align pretrained LLM to human preference
for downstream task. In the past it uses reinforcement learning from human
feedback (RLHF) algorithm to optimize the LLM policy to align with these
preferences and not to draft too far from the original model. Recently, Direct
Preference Optimization (DPO) has been proposed to solve the alignment problem
with a simplified RL-free method. Using preference pairs of chosen and reject
data, DPO models the relative log probability as implicit reward function and
optimize LLM policy using a simple binary cross entropy objective directly. DPO
is quite straight forward and easy to be understood. It perform efficiently and
well in most cases. In this article, we analyze the working mechanism of
$\beta$ in DPO, disclose its syntax difference between RL algorithm and DPO,
and understand the potential shortage brought by the DPO simplification. With
these insights, we propose MinorDPO, which is better aligned to the original RL
algorithm, and increase the stability of preference optimization process.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.09385v2' target='_blank'>Reward Difference Optimization For Sample Reweighting In Offline RLHF</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shiqi Wang, Zhengze Zhang, Rui Zhao, Fei Tan, Cam Tu Nguyen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-18 07:04:16</h6>
<p class='card-text'>With the rapid advances in Large Language Models (LLMs), aligning LLMs with
human preferences become increasingly important. Although Reinforcement
Learning with Human Feedback (RLHF) proves effective, it is complicated and
highly resource-intensive. As such, offline RLHF has been introduced as an
alternative solution, which directly optimizes LLMs with ranking losses on a
fixed preference dataset. Current offline RLHF only captures the "ordinal
relationship" between responses, overlooking the crucial aspect of how much one
is preferred over the others. To address this issue, we propose a simple yet
effective solution called Reward Difference Optimization, shorted as RDO.
Specifically, we introduce reward difference coefficients to reweigh sample
pairs in offline RLHF. We then develop a difference model which captures rich
interactions between a pair of responses for predicting these difference
coefficients. Experiments with 7B LLMs on the HH and TL;DR datasets
substantiate the effectiveness of our method in both automatic metrics and
human evaluation, thereby highlighting its potential for aligning LLMs with
human intent and values</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.08781v1' target='_blank'>Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation
  Instructions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bhuvanashree Murugadoss, Christian Poelitz, Ian Drosos, Vu Le, Nick McKenna, Carina Suzana Negreanu, Chris Parnin, Advait Sarkar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-16 14:49:35</h6>
<p class='card-text'>LLMs-as-a-judge is a recently popularized method which replaces human
judgements in task evaluation (Zheng et al. 2024) with automatic evaluation
using LLMs. Due to widespread use of RLHF (Reinforcement Learning from Human
Feedback), state-of-the-art LLMs like GPT4 and Llama3 are expected to have
strong alignment with human preferences when prompted for a quality judgement,
such as the coherence of a text. While this seems beneficial, it is not clear
whether the assessments by an LLM-as-a-judge constitute only an evaluation
based on the instructions in the prompts, or reflect its preference for
high-quality data similar to its fine-tune data. To investigate how much
influence prompting the LLMs-as-a-judge has on the alignment of AI judgements
to human judgements, we analyze prompts with increasing levels of instructions
about the target quality of an evaluation, for several LLMs-as-a-judge.
Further, we compare to a prompt-free method using model perplexity as a quality
measure instead. We aggregate a taxonomy of quality criteria commonly used
across state-of-the-art evaluations with LLMs and provide this as a rigorous
benchmark of models as judges. Overall, we show that the LLMs-as-a-judge
benefit only little from highly detailed instructions in prompts and that
perplexity can sometimes align better with human judgements than prompting,
especially on textual quality.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.08676v1' target='_blank'>Fine-tuning LLMs for Autonomous Spacecraft Control: A Case Study Using
  Kerbal Space Program</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alejandro Carrasco, Victor Rodriguez-Fernandez, Richard Linares</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-16 11:43:31</h6>
<p class='card-text'>Recent trends are emerging in the use of Large Language Models (LLMs) as
autonomous agents that take actions based on the content of the user text
prompt. This study explores the use of fine-tuned Large Language Models (LLMs)
for autonomous spacecraft control, using the Kerbal Space Program Differential
Games suite (KSPDG) as a testing environment. Traditional Reinforcement
Learning (RL) approaches face limitations in this domain due to insufficient
simulation capabilities and data. By leveraging LLMs, specifically fine-tuning
models like GPT-3.5 and LLaMA, we demonstrate how these models can effectively
control spacecraft using language-based inputs and outputs. Our approach
integrates real-time mission telemetry into textual prompts processed by the
LLM, which then generate control actions via an agent. The results open a
discussion about the potential of LLMs for space operations beyond their
nominal use for text-related tasks. Future work aims to expand this methodology
to other space control tasks and evaluate the performance of different LLM
families. The code is available at this URL:
\texttt{https://github.com/ARCLab-MIT/kspdg}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.07806v2' target='_blank'>From Decision to Action in Surgical Autonomy: Multi-Modal Large Language
  Models for Robot-Assisted Blood Suction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sadra Zargarzadeh, Maryam Mirzaei, Yafei Ou, Mahdi Tavakoli</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-14 20:30:34</h6>
<p class='card-text'>The rise of Large Language Models (LLMs) has impacted research in robotics
and automation. While progress has been made in integrating LLMs into general
robotics tasks, a noticeable void persists in their adoption in more specific
domains such as surgery, where critical factors such as reasoning,
explainability, and safety are paramount. Achieving autonomy in robotic
surgery, which entails the ability to reason and adapt to changes in the
environment, remains a significant challenge. In this work, we propose a
multi-modal LLM integration in robot-assisted surgery for autonomous blood
suction. The reasoning and prioritization are delegated to the higher-level
task-planning LLM, and the motion planning and execution are handled by the
lower-level deep reinforcement learning model, creating a distributed agency
between the two components. As surgical operations are highly dynamic and may
encounter unforeseen circumstances, blood clots and active bleeding were
introduced to influence decision-making. Results showed that using a
multi-modal LLM as a higher-level reasoning unit can account for these surgical
complexities to achieve a level of reasoning previously unattainable in
robot-assisted surgeries. These findings demonstrate the potential of
multi-modal LLMs to significantly enhance contextual understanding and
decision-making in robotic-assisted surgeries, marking a step toward autonomous
surgical systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.07505v2' target='_blank'>Large Language Models Know What Makes Exemplary Contexts</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Quanyu Long, Jianda Chen, Wenya Wang, Sinno Jialin Pan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-14 12:32:41</h6>
<p class='card-text'>In-context learning (ICL) has proven to be a significant capability with the
advancement of Large Language models (LLMs). By instructing LLMs using few-shot
demonstrative examples, ICL enables them to perform a wide range of tasks
without needing to update millions of parameters. This paper presents a unified
framework for LLMs that allows them to self-select influential in-context
examples to compose their contexts; self-rank candidates with different
demonstration compositions; self-optimize the demonstration selection and
ordering through reinforcement learning. Specifically, our method designs a
parameter-efficient retrieval head that generates the optimized demonstration
after training with rewards from LLM's own preference. Experimental results
validate the proposed method's effectiveness in enhancing ICL performance.
Additionally, our approach effectively identifies and selects the most
representative examples for the current task, and includes more diversity in
retrieval.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.07465v1' target='_blank'>Large Language Models Prompting With Episodic Memory</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dai Do, Quan Tran, Svetha Venkatesh, Hung Le</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-14 11:19:28</h6>
<p class='card-text'>Prompt optimization is essential for enhancing the performance of Large
Language Models (LLMs) in a range of Natural Language Processing (NLP) tasks,
particularly in scenarios of few-shot learning where training examples are
incorporated directly into the prompt. Despite the growing interest in
optimizing prompts with few-shot examples, existing methods for prompt
optimization are often resource-intensive or perform inadequately. In this
work, we propose PrOmpting with Episodic Memory (POEM), a novel prompt
optimization technique that is simple, efficient, and demonstrates strong
generalization capabilities. We approach prompt optimization as a Reinforcement
Learning (RL) challenge, using episodic memory to archive combinations of input
data, permutations of few-shot examples, and the rewards observed during
training. In the testing phase, we optimize the sequence of examples for each
test query by selecting the sequence that yields the highest total rewards from
the top-k most similar training examples in the episodic memory. Our results
show that POEM outperforms recent techniques like TEMPERA and RLPrompt by over
5.3% in various text classification tasks. Furthermore, our approach adapts
well to broader language understanding tasks, consistently outperforming
conventional heuristic methods for ordering examples.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.06993v1' target='_blank'>LLMs can Schedule</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Henrik Abgaryan, Ararat Harutyunyan, Tristan Cazenave</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-13 15:53:58</h6>
<p class='card-text'>The job shop scheduling problem (JSSP) remains a significant hurdle in
optimizing production processes. This challenge involves efficiently allocating
jobs to a limited number of machines while minimizing factors like total
processing time or job delays. While recent advancements in artificial
intelligence have yielded promising solutions, such as reinforcement learning
and graph neural networks, this paper explores the potential of Large Language
Models (LLMs) for JSSP. We introduce the very first supervised 120k dataset
specifically designed to train LLMs for JSSP. Surprisingly, our findings
demonstrate that LLM-based scheduling can achieve performance comparable to
other neural approaches. Furthermore, we propose a sampling method that
enhances the effectiveness of LLMs in tackling JSSP.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.06520v2' target='_blank'>Retrieval-Augmented Hierarchical in-Context Reinforcement Learning and
  Hindsight Modular Reflections for Task Planning with LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chuanneng Sun, Songjun Huang, Dario Pompili</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-12 22:40:01</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated remarkable abilities in
various language tasks, making them promising candidates for decision-making in
robotics. Inspired by Hierarchical Reinforcement Learning (HRL), we propose
Retrieval-Augmented in-context reinforcement Learning (RAHL), a novel framework
that decomposes complex tasks into sub-tasks using an LLM-based high-level
policy, in which a complex task is decomposed into sub-tasks by a high-level
policy on-the-fly. The sub-tasks, defined by goals, are assigned to the
low-level policy to complete. To improve the agent's performance in
multi-episode execution, we propose Hindsight Modular Reflection (HMR), where,
instead of reflecting on the full trajectory, we let the agent reflect on
shorter sub-trajectories to improve reflection efficiency. We evaluated the
decision-making ability of the proposed RAHL in three benchmark
environments--ALFWorld, Webshop, and HotpotQA. The results show that RAHL can
achieve an improvement in performance in 9%, 42%, and 10% in 5 episodes of
execution in strong baselines. Furthermore, we also implemented RAHL on the
Boston Dynamics SPOT robot. The experiment shows that the robot can scan the
environment, find entrances, and navigate to new rooms controlled by the LLM
policy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.06087v1' target='_blank'>Building Decision Making Models Through Language Model Regime</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yu Zhang, Haoxiang Liu, Feijun Jiang, Weihua Luo, Kaifu Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-12 12:04:14</h6>
<p class='card-text'>We propose a novel approach for decision making problems leveraging the
generalization capabilities of large language models (LLMs). Traditional
methods such as expert systems, planning algorithms, and reinforcement learning
often exhibit limited generalization, typically requiring the training of new
models for each unique task. In contrast, LLMs demonstrate remarkable success
in generalizing across varied language tasks, inspiring a new strategy for
training decision making models. Our approach, referred to as "Learning then
Using" (LTU), entails a two-stage process. Initially, the \textit{learning}
phase develops a robust foundational decision making model by integrating
diverse knowledge from various domains and decision making contexts. The
subsequent \textit{using} phase refines this foundation model for specific
decision making scenarios. Distinct from other studies that employ LLMs for
decision making through supervised learning, our LTU method embraces a
versatile training methodology that combines broad pre-training with targeted
fine-tuning. Experiments in e-commerce domains such as advertising and search
optimization have shown that LTU approach outperforms traditional supervised
learning regimes in decision making capabilities and generalization. The LTU
approach is the first practical training architecture for both single-step and
multi-step decision making tasks combined with LLMs, which can be applied
beyond game and robot domains. It provides a robust and adaptable framework for
decision making, enhances the effectiveness and flexibility of various systems
in tackling various challenges.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.05568v1' target='_blank'>Metacognitive Myopia in Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Florian Scholten, Tobias R. Rebholz, Mandy Hütter</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-10 14:43:57</h6>
<p class='card-text'>Large Language Models (LLMs) exhibit potentially harmful biases that
reinforce culturally inherent stereotypes, cloud moral judgments, or amplify
positive evaluations of majority groups. Previous explanations mainly
attributed bias in LLMs to human annotators and the selection of training data.
Consequently, they have typically been addressed with bottom-up approaches such
as reinforcement learning or debiasing corpora. However, these methods only
treat the effects of LLM biases by indirectly influencing the model
architecture, but do not address the underlying causes in the computational
process. Here, we propose metacognitive myopia as a cognitive-ecological
framework that can account for a conglomerate of established and emerging LLM
biases and provide a lever to address problems in powerful but vulnerable
tools. Our theoretical framework posits that a lack of the two components of
metacognition, monitoring and control, causes five symptoms of metacognitive
myopia in LLMs: integration of invalid tokens and embeddings, susceptibility to
redundant information, neglect of base rates in conditional computation,
decision rules based on frequency, and inappropriate higher-order statistical
inference for nested data structures. As a result, LLMs produce erroneous
output that reaches into the daily high-stakes decisions of humans. By
introducing metacognitive regulatory processes into LLMs, engineers and
scientists can develop precise remedies for the underlying causes of these
biases. Our theory sheds new light on flawed human-machine interactions and
raises ethical concerns regarding the increasing, imprudent implementation of
LLMs in organizational structures.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.04835v1' target='_blank'>Next-Generation Wi-Fi Networks with Generative AI: Design and Insights</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jingyu Wang, Xuming Fang, Dusit Niyato, Tie Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-09 03:12:53</h6>
<p class='card-text'>Generative artificial intelligence (GAI), known for its powerful capabilities
in image and text processing, also holds significant promise for the design and
performance enhancement of future wireless networks. In this article, we
explore the transformative potential of GAI in next-generation Wi-Fi networks,
exploiting its advanced capabilities to address key challenges and improve
overall network performance. We begin by reviewing the development of major
Wi-Fi generations and illustrating the challenges that future Wi-Fi networks
may encounter. We then introduce typical GAI models and detail their potential
capabilities in Wi-Fi network optimization, performance enhancement, and other
applications. Furthermore, we present a case study wherein we propose a
retrieval-augmented LLM (RA-LLM)-enabled Wi-Fi design framework that aids in
problem formulation, which is subsequently solved using a generative diffusion
model (GDM)-based deep reinforcement learning (DRL) framework to optimize
various network parameters. Numerical results demonstrate the effectiveness of
our proposed algorithm in high-density deployment scenarios. Finally, we
provide some potential future research directions for GAI-assisted Wi-Fi
networks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.03562v1' target='_blank'>A Comparison of LLM Finetuning Methods & Evaluation Metrics with Travel
  Chatbot Use Case</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sonia Meyer, Shreya Singh, Bertha Tam, Christopher Ton, Angel Ren</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-07 05:52:00</h6>
<p class='card-text'>This research compares large language model (LLM) fine-tuning methods,
including Quantized Low Rank Adapter (QLoRA), Retrieval Augmented fine-tuning
(RAFT), and Reinforcement Learning from Human Feedback (RLHF), and additionally
compared LLM evaluation methods including End to End (E2E) benchmark method of
"Golden Answers", traditional natural language processing (NLP) metrics, RAG
Assessment (Ragas), OpenAI GPT-4 evaluation metrics, and human evaluation,
using the travel chatbot use case. The travel dataset was sourced from the the
Reddit API by requesting posts from travel-related subreddits to get
travel-related conversation prompts and personalized travel experiences, and
augmented for each fine-tuning method. We used two pretrained LLMs utilized for
fine-tuning research: LLaMa 2 7B, and Mistral 7B. QLoRA and RAFT are applied to
the two pretrained models. The inferences from these models are extensively
evaluated against the aforementioned metrics. The best model according to human
evaluation and some GPT-4 metrics was Mistral RAFT, so this underwent a
Reinforcement Learning from Human Feedback (RLHF) training pipeline, and
ultimately was evaluated as the best model. Our main findings are that: 1)
quantitative and Ragas metrics do not align with human evaluation, 2) Open AI
GPT-4 evaluation most aligns with human evaluation, 3) it is essential to keep
humans in the loop for evaluation because, 4) traditional NLP metrics
insufficient, 5) Mistral generally outperformed LLaMa, 6) RAFT outperforms
QLoRA, but still needs postprocessing, 7) RLHF improves model performance
significantly. Next steps include improving data quality, increasing data
quantity, exploring RAG methods, and focusing data collection on a specific
city, which would improve data quality by narrowing the focus, while creating a
useful product.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.02861v1' target='_blank'>A Framework for Fine-Tuning LLMs using Heterogeneous Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ryan Aponte, Ryan A. Rossi, Shunan Guo, Franck Dernoncourt, Tong Yu, Xiang Chen, Subrata Mitra, Nedim Lipka</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-05 23:20:32</h6>
<p class='card-text'>Large language models (LLMs) have been applied to a wide range of tasks,
including text summarization, web navigation, and chatbots. They have
benefitted from supervised fine-tuning (SFT) and reinforcement learning from
human feedback (RLHF) following an unsupervised pretraining. These datasets can
be difficult to collect, limited in scope, and vary in sample quality.
Additionally, datasets can vary extensively in supervision format, from
numerical to binary as well as multi-dimensional with many different values. We
present a framework for fine-tuning LLMs using heterogeneous feedback, which
has two main components. First, we combine the heterogeneous feedback data into
a single supervision format, compatible with methods like SFT and RLHF. Next,
given this unified feedback dataset, we extract a high-quality and diverse
subset to obtain performance increases potentially exceeding the full dataset.
We conduct extensive experiments to understand the effectiveness of these
techniques for incorporating heterogeneous feedback, and demonstrate
improvements from using a high-quality and diverse subset of the data. We find
that our framework is able to improve models in multiple areas simultaneously,
such as in instruction following and bias reduction.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.02651v1' target='_blank'>Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large
  Language Models?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mohammad Bahrami Karkevandi, Nishant Vishwamitra, Peyman Najafirad</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-05 17:27:29</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated impressive capabilities in
natural language tasks, but their safety and morality remain contentious due to
their training on internet text corpora. To address these concerns, alignment
techniques have been developed to improve the public usability and safety of
LLMs. Yet, the potential for generating harmful content through these models
seems to persist. This paper explores the concept of jailbreaking
LLMs-reversing their alignment through adversarial triggers. Previous methods,
such as soft embedding prompts, manually crafted prompts, and gradient-based
automatic prompts, have had limited success on black-box models due to their
requirements for model access and for producing a low variety of manually
crafted prompts, making them susceptible to being blocked. This paper
introduces a novel approach using reinforcement learning to optimize
adversarial triggers, requiring only inference API access to the target model
and a small surrogate model. Our method, which leverages a BERTScore-based
reward function, enhances the transferability and effectiveness of adversarial
triggers on new black-box models. We demonstrate that this approach improves
the performance of adversarial triggers on a previously untested language
model.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.02599v2' target='_blank'>Progressively Label Enhancement for Large Language Model Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Biao Liu, Ning Xu, Xin Geng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-05 16:21:17</h6>
<p class='card-text'>Large Language Models (LLM) alignment aims to prevent models from producing
content that misaligns with human expectations, which can lead to ethical and
legal concerns. In the last few years, Reinforcement Learning from Human
Feedback (RLHF) has been the most prominent method for achieving alignment. Due
to challenges in stability and scalability with RLHF stages, which arise from
the complex interactions between multiple models, researchers are exploring
alternative methods to achieve effects comparable to those of RLHF. However,
these methods often rely on large high-quality datasets. Despite some methods
considering the generation of additional data to expand datasets, they often
treat model training and data generation as separate and static processes,
overlooking the fact that these processes are highly interdependent, leading to
inefficient utilization of the generated data. To deal with this problem, we
propose PLE, i.e., Progressively Label Enhancement for LLM Alignment, a
framework that dynamically adjusts the model's training process based on the
evolving quality of the generated data. Specifically, we prompt the model to
generate responses for both the original query and the query guided by a set of
carefully designed principles, and then utilize a dynamic threshold to
determine the appropriate training approach for both responses based on their
corresponding reward scores. Experimental results demonstrate the effectiveness
of PLE compared to existing LLM alignment methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.02559v1' target='_blank'>Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan:
  A Multi-Player Cooperative Game under Imperfect Information</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yauwai Yim, Chunkit Chan, Tianyu Shi, Zheye Deng, Wei Fan, Tianshi Zheng, Yangqiu Song</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-05 15:36:46</h6>
<p class='card-text'>Large language models (LLMs) have shown success in handling simple games with
imperfect information and enabling multi-agent coordination, but their ability
to facilitate practical collaboration against other agents in complex,
imperfect information environments, especially in a non-English environment,
still needs to be explored. This study investigates the applicability of
knowledge acquired by open-source and API-based LLMs to sophisticated
text-based games requiring agent collaboration under imperfect information,
comparing their performance to established baselines using other types of
agents. We propose a Theory of Mind (ToM) planning technique that allows LLM
agents to adapt their strategy against various adversaries using only game
rules, current state, and historical context as input. An external tool was
incorporated to mitigate the challenge of dynamic and extensive action spaces
in this card game. Our results show that although a performance gap exists
between current LLMs and state-of-the-art reinforcement learning (RL) models,
LLMs demonstrate ToM capabilities in this game setting. It consistently
improves their performance against opposing agents, suggesting their ability to
understand the actions of allies and adversaries and establish collaboration
with allies. To encourage further research and understanding, we have made our
codebase openly accessible.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.04655v2' target='_blank'>Strong and weak alignment of large language models with human values</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mehdi Khamassi, Marceau Nahon, Raja Chatila</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-05 11:27:51</h6>
<p class='card-text'>Minimizing negative impacts of Artificial Intelligent (AI) systems on human
societies without human supervision requires them to be able to align with
human values. However, most current work only addresses this issue from a
technical point of view, e.g., improving current methods relying on
reinforcement learning from human feedback, neglecting what it means and is
required for alignment to occur. Here, we propose to distinguish strong and
weak value alignment. Strong alignment requires cognitive abilities (either
human-like or different from humans) such as understanding and reasoning about
agents' intentions and their ability to causally produce desired effects. We
argue that this is required for AI systems like large language models (LLMs) to
be able to recognize situations presenting a risk that human values may be
flouted. To illustrate this distinction, we present a series of prompts showing
ChatGPT's, Gemini's and Copilot's failures to recognize some of these
situations. We moreover analyze word embeddings to show that the nearest
neighbors of some human values in LLMs differ from humans' semantic
representations. We then propose a new thought experiment that we call "the
Chinese room with a word transition dictionary", in extension of John Searle's
famous proposal. We finally mention current promising research directions
towards a weak alignment, which could produce statistically satisfying answers
in a number of common situations, however so far without ensuring any truth
value.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.00214v1' target='_blank'>Large Language Model (LLM)-enabled In-context Learning for Wireless
  Network Optimization: A Case Study of Power Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hao Zhou, Chengming Hu, Dun Yuan, Ye Yuan, Di Wu, Xue Liu, Charlie Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-01 00:53:02</h6>
<p class='card-text'>Large language model (LLM) has recently been considered a promising technique
for many fields. This work explores LLM-based wireless network optimization via
in-context learning. To showcase the potential of LLM technologies, we consider
the base station (BS) power control as a case study, a fundamental but crucial
technique that is widely investigated in wireless networks. Different from
existing machine learning (ML) methods, our proposed in-context learning
algorithm relies on LLM's inference capabilities. It avoids the complexity of
tedious model training and hyper-parameter fine-tuning, which is a well-known
bottleneck of many ML algorithms. Specifically, the proposed algorithm first
describes the target task via formatted natural language, and then designs the
in-context learning framework and demonstration examples. After that, it
considers two cases, namely discrete-state and continuous-state problems, and
proposes state-based and ranking-based methods to select appropriate examples
for these two cases, respectively. Finally, the simulations demonstrate that
the proposed algorithm can achieve comparable performance as conventional deep
reinforcement learning (DRL) techniques without dedicated model training or
fine-tuning. Such an efficient and low-complexity approach has great potential
for future wireless network optimization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.20164v1' target='_blank'>Language-Conditioned Offline RL for Multi-Robot Navigation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Steven Morad, Ajay Shankar, Jan Blumenkamp, Amanda Prorok</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-29 16:49:30</h6>
<p class='card-text'>We present a method for developing navigation policies for multi-robot teams
that interpret and follow natural language instructions. We condition these
policies on embeddings from pretrained Large Language Models (LLMs), and train
them via offline reinforcement learning with as little as 20 minutes of
randomly-collected data. Experiments on a team of five real robots show that
these policies generalize well to unseen commands, indicating an understanding
of the LLM latent space. Our method requires no simulators or environment
models, and produces low-latency control policies that can be deployed directly
to real robots without finetuning. We provide videos of our experiments at
https://sites.google.com/view/llm-marl.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.19280v1' target='_blank'>Large Language Models for Human-like Autonomous Driving: A Survey</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yun Li, Kai Katsumata, Ehsan Javanmardi, Manabu Tsukada</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-27 15:24:11</h6>
<p class='card-text'>Large Language Models (LLMs), AI models trained on massive text corpora with
remarkable language understanding and generation capabilities, are transforming
the field of Autonomous Driving (AD). As AD systems evolve from rule-based and
optimization-based methods to learning-based techniques like deep reinforcement
learning, they are now poised to embrace a third and more advanced category:
knowledge-based AD empowered by LLMs. This shift promises to bring AD closer to
human-like AD. However, integrating LLMs into AD systems poses challenges in
real-time inference, safety assurance, and deployment costs. This survey
provides a comprehensive and critical review of recent progress in leveraging
LLMs for AD, focusing on their applications in modular AD pipelines and
end-to-end AD systems. We highlight key advancements, identify pressing
challenges, and propose promising research directions to bridge the gap between
LLMs and AD, thereby facilitating the development of more human-like AD
systems. The survey first introduces LLMs' key features and common training
schemes, then delves into their applications in modular AD pipelines and
end-to-end AD, respectively, followed by discussions on open challenges and
future directions. Through this in-depth analysis, we aim to provide insights
and inspiration for researchers and practitioners working at the intersection
of AI and autonomous vehicles, ultimately contributing to safer, smarter, and
more human-centric AD technologies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.18676v1' target='_blank'>Right Now, Wrong Then: Non-Stationary Direct Preference Optimization
  under Preference Drift</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Seongho Son, William Bankes, Sayak Ray Chowdhury, Brooks Paige, Ilija Bogunovic</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-26 11:38:18</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) aligns Large Language
Models (LLMs) with human preferences. However, these preferences can often
change over time due to external factors (e.g. environment change and societal
influence). Consequently, what was wrong then might be right now. Current
preference optimization algorithms do not account for temporal preference drift
in their modeling, which can lead to severe misalignment. To address this
limitation, we use a Dynamic Bradley-Terry model that models preferences via
time-dependent reward functions, and propose Non-Stationary Direct Preference
Optimisation (NS-DPO). By introducing a discount parameter in the loss
function, NS-DPO applies exponential weighting, which proportionally focuses
learning on more time-relevant datapoints. We theoretically analyse the
convergence of NS-DPO in the offline setting, providing upper bounds on the
estimation error caused by non-stationary preferences. Finally, we demonstrate
the effectiveness of NS-DPO1 for fine-tuning LLMs in scenarios with drifting
preferences. By simulating preference drift using renowned reward models and
modifying popular LLM datasets accordingly, we show that NS-DPO fine-tuned LLMs
remain robust under non-stationarity, significantly outperforming baseline
algorithms that ignore temporal preference changes, without sacrificing
performance in stationary cases.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.18219v2' target='_blank'>Recursive Introspection: Teaching Language Model Agents How to
  Self-Improve</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuxiao Qu, Tianjun Zhang, Naman Garg, Aviral Kumar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-25 17:35:59</h6>
<p class='card-text'>A central piece in enabling intelligent agentic behavior in foundation models
is to make them capable of introspecting upon their behavior, reasoning, and
correcting their mistakes as more computation or interaction is available. Even
the strongest proprietary large language models (LLMs) do not quite exhibit the
ability of continually improving their responses sequentially, even in
scenarios where they are explicitly told that they are making a mistake. In
this paper, we develop RISE: Recursive IntroSpEction, an approach for
fine-tuning LLMs to introduce this capability, despite prior work hypothesizing
that this capability may not be possible to attain. Our approach prescribes an
iterative fine-tuning procedure, which attempts to teach the model how to alter
its response after having executed previously unsuccessful attempts to solve a
hard test-time problem, with optionally additional environment feedback. RISE
poses fine-tuning for a single-turn prompt as solving a multi-turn Markov
decision process (MDP), where the initial state is the prompt. Inspired by
principles in online imitation learning and reinforcement learning, we propose
strategies for multi-turn data collection and training so as to imbue an LLM
with the capability to recursively detect and correct its previous mistakes in
subsequent iterations. Our experiments show that RISE enables Llama2, Llama3,
and Mistral models to improve themselves with more turns on math reasoning
tasks, outperforming several single-turn strategies given an equal amount of
inference-time computation. We also find that RISE scales well, often attaining
larger benefits with more capable models. Our analysis shows that RISE makes
meaningful improvements to responses to arrive at the correct solution for
challenging prompts, without disrupting one-turn abilities as a result of
expressing more complex distributions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.18271v2' target='_blank'>Large Language Model for Verilog Generation with Golden Code Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ning Wang, Bingkun Yao, Jie Zhou, Xi Wang, Zhe Jiang, Nan Guan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-21 11:25:21</h6>
<p class='card-text'>Recent advancements in large language models (LLMs) have catalyzed
significant interest in the automatic generation of Register-Transfer Level
(RTL) code, particularly Verilog, from natural language instructions. While
commercial LLMs like ChatGPT have dominated this domain, open-source
alternatives have lagged considerably in performance, limiting the flexibility
and data privacy of this emerging technology. This study introduces a novel
approach utilizing reinforcement learning with golden code feedback to enhance
the performance of pre-trained models. Leveraging open-source data and base
models, we have achieved state-of-the-art (SOTA) results with a substantial
margin. Notably, our 6.7B parameter model \ours{} demonstrates superior
performance compared to current best-in-class 13B and 16B models. Furthermore,
through a comprehensive analysis of the limitations in direct fine-tuning and
the training dynamics of reinforcement learning, we posit that the development
of comprehensive supervisory signals, which are align with the inherent
parallel semantics of Verilog code, is critical to effective generation. The
code and data associated with this research are publicly available at
\url{https://github.com/CatIIIIIIII/veriseek}. The model weights can be
accessed at \url{https://huggingface.co/WANGNingroci/VeriSeek}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.15050v1' target='_blank'>Arondight: Red Teaming Large Vision Language Models with Auto-generated
  Multi-modal Jailbreak Prompts</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yi Liu, Chengjun Cai, Xiaoli Zhang, Xingliang Yuan, Cong Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-21 04:37:11</h6>
<p class='card-text'>Large Vision Language Models (VLMs) extend and enhance the perceptual
abilities of Large Language Models (LLMs). Despite offering new possibilities
for LLM applications, these advancements raise significant security and ethical
concerns, particularly regarding the generation of harmful content. While LLMs
have undergone extensive security evaluations with the aid of red teaming
frameworks, VLMs currently lack a well-developed one. To fill this gap, we
introduce Arondight, a standardized red team framework tailored specifically
for VLMs. Arondight is dedicated to resolving issues related to the absence of
visual modality and inadequate diversity encountered when transitioning
existing red teaming methodologies from LLMs to VLMs. Our framework features an
automated multi-modal jailbreak attack, wherein visual jailbreak prompts are
produced by a red team VLM, and textual prompts are generated by a red team LLM
guided by a reinforcement learning agent. To enhance the comprehensiveness of
VLM security evaluation, we integrate entropy bonuses and novelty reward
metrics. These elements incentivize the RL agent to guide the red team LLM in
creating a wider array of diverse and previously unseen test cases. Our
evaluation of ten cutting-edge VLMs exposes significant security
vulnerabilities, particularly in generating toxic images and aligning
multi-modal prompts. In particular, our Arondight achieves an average attack
success rate of 84.5\% on GPT-4 in all fourteen prohibited scenarios defined by
OpenAI in terms of generating toxic text. For a clearer comparison, we also
categorize existing VLMs based on their safety levels and provide corresponding
reinforcement recommendations. Our multimodal prompt dataset and red team code
will be released after ethics committee approval. CONTENT WARNING: THIS PAPER
CONTAINS HARMFUL MODEL RESPONSES.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.14622v1' target='_blank'>BOND: Aligning LLMs with Best-of-N Distillation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pier Giuseppe Sessa, Robert Dadashi, Léonard Hussenot, Johan Ferret, Nino Vieillard, Alexandre Ramé, Bobak Shariari, Sarah Perrin, Abe Friesen, Geoffrey Cideron, Sertan Girgin, Piotr Stanczyk, Andrea Michi, Danila Sinopalnikov, Sabela Ramos, Amélie Héliou, Aliaksei Severyn, Matt Hoffman, Nikola Momchev, Olivier Bachem</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-19 18:38:25</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) is a key driver of quality
and safety in state-of-the-art large language models. Yet, a surprisingly
simple and strong inference-time strategy is Best-of-N sampling that selects
the best generation among N candidates. In this paper, we propose Best-of-N
Distillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but
without its significant computational overhead at inference time. Specifically,
BOND is a distribution matching algorithm that forces the distribution of
generations from the policy to get closer to the Best-of-N distribution. We use
the Jeffreys divergence (a linear combination of forward and backward KL) to
balance between mode-covering and mode-seeking behavior, and derive an
iterative formulation that utilizes a moving anchor for efficiency. We
demonstrate the effectiveness of our approach and several design choices
through experiments on abstractive summarization and Gemma models. Aligning
Gemma policies with BOND outperforms other RLHF algorithms by improving results
on several benchmarks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.13237v1' target='_blank'>LLM-Empowered State Representation for Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Boyuan Wang, Yun Qu, Yuhang Jiang, Jianzhun Shao, Chang Liu, Wenming Yang, Xiangyang Ji</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-18 07:47:51</h6>
<p class='card-text'>Conventional state representations in reinforcement learning often omit
critical task-related details, presenting a significant challenge for value
networks in establishing accurate mappings from states to task rewards.
Traditional methods typically depend on extensive sample learning to enrich
state representations with task-specific information, which leads to low sample
efficiency and high time costs. Recently, surging knowledgeable large language
models (LLM) have provided promising substitutes for prior injection with
minimal human intervention. Motivated by this, we propose LLM-Empowered State
Representation (LESR), a novel approach that utilizes LLM to autonomously
generate task-related state representation codes which help to enhance the
continuity of network mappings and facilitate efficient training. Experimental
results demonstrate LESR exhibits high sample efficiency and outperforms
state-of-the-art baselines by an average of 29% in accumulated reward in Mujoco
tasks and 30% in success rates in Gym-Robotics tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.12532v1' target='_blank'>Towards Collaborative Intelligence: Propagating Intentions and Reasoning
  for Multi-Agent Coordination with Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xihe Qiu, Haoyu Wang, Xiaoyu Tan, Chao Qu, Yujie Xiong, Yuan Cheng, Yinghui Xu, Wei Chu, Yuan Qi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-17 13:14:00</h6>
<p class='card-text'>Effective collaboration in multi-agent systems requires communicating goals
and intentions between agents. Current agent frameworks often suffer from
dependencies on single-agent execution and lack robust inter-module
communication, frequently leading to suboptimal multi-agent reinforcement
learning (MARL) policies and inadequate task coordination. To address these
challenges, we present a framework for training large language models (LLMs) as
collaborative agents to enable coordinated behaviors in cooperative MARL. Each
agent maintains a private intention consisting of its current goal and
associated sub-tasks. Agents broadcast their intentions periodically, allowing
other agents to infer coordination tasks. A propagation network transforms
broadcast intentions into teammate-specific communication messages, sharing
relevant goals with designated teammates. The architecture of our framework is
structured into planning, grounding, and execution modules. During execution,
multiple agents interact in a downstream environment and communicate intentions
to enable coordinated behaviors. The grounding module dynamically adapts
comprehension strategies based on emerging coordination patterns, while
feedback from execution agents influnces the planning module, enabling the
dynamic re-planning of sub-tasks. Results in collaborative environment
simulation demonstrate intention propagation reduces miscoordination errors by
aligning sub-task dependencies between agents. Agents learn when to communicate
intentions and which teammates require task details, resulting in emergent
coordinated behaviors. This demonstrates the efficacy of intention sharing for
cooperative multi-agent RL based on LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.12296v1' target='_blank'>A foundation model approach to guide antimicrobial peptide design in the
  era of artificial intelligence driven scientific discovery</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jike Wang, Jianwen Feng, Yu Kang, Peichen Pan, Jingxuan Ge, Yan Wang, Mingyang Wang, Zhenxing Wu, Xingcai Zhang, Jiameng Yu, Xujun Zhang, Tianyue Wang, Lirong Wen, Guangning Yan, Yafeng Deng, Hui Shi, Chang-Yu Hsieh, Zhihui Jiang, Tingjun Hou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-17 03:36:33</h6>
<p class='card-text'>We propose AMP-Designer, an LLM-based foundation model approach for the rapid
design of novel antimicrobial peptides (AMPs) with multiple desired properties.
Within 11 days, AMP-Designer enables de novo design of 18 novel candidates with
broad-spectrum potency against Gram-negative bacteria. Subsequent in vitro
validation experiments demonstrate that almost all in silico recommended
candidates exhibit notable antibacterial activity, yielding a 94.4% positive
rate. Two of these candidates exhibit exceptional activity, minimal
hemotoxicity, substantial stability in human plasma, and a low propensity of
inducing antibiotic resistance as observed in murine lung infection
experiments, showcasing their significant efficacy in reducing bacterial load
by approximately one hundredfold. The entire process, from in silico design to
in vitro and in vivo validation, is completed within a timeframe of 48 days.
Moreover, AMP-Designer demonstrates its remarkable capability in designing
specific AMPs to target strains with extremely limited labeled datasets. The
most outstanding candidate against Propionibacterium acnes suggested by
AMP-Designer exhibits an in vitro minimum inhibitory concentration value of 2.0
$\mu$g/ml. Through the integration of advanced machine learning methodologies
such as contrastive prompt tuning, knowledge distillation, and reinforcement
learning within the AMP-Designer framework, the process of designing AMPs
demonstrates exceptional efficiency. This efficiency remains conspicuous even
in the face of challenges posed by constraints arising from a scarcity of
labeled data. These findings highlight the tremendous potential of AMP-Designer
as a promising approach in combating the global health threat of antibiotic
resistance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.11511v1' target='_blank'>Reasoning with Large Language Models, a Survey</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, Thomas Back</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-16 08:49:35</h6>
<p class='card-text'>Scaling up language models to billions of parameters has opened up
possibilities for in-context learning, allowing instruction tuning and few-shot
learning on tasks that the model was not specifically trained for. This has
achieved breakthrough performance on language tasks such as translation,
summarization, and question-answering. Furthermore, in addition to these
associative "System 1" tasks, recent advances in Chain-of-thought prompt
learning have demonstrated strong "System 2" reasoning abilities, answering a
question in the field of artificial general intelligence whether LLMs can
reason. The field started with the question whether LLMs can solve grade school
math word problems. This paper reviews the rapidly expanding field of
prompt-based reasoning with LLMs. Our taxonomy identifies different ways to
generate, evaluate, and control multi-step reasoning. We provide an in-depth
coverage of core approaches and open problems, and we propose a research agenda
for the near future. Finally, we highlight the relation between reasoning and
prompt-based learning, and we discuss the relation between reasoning,
sequential decision processes, and reinforcement learning. We find that
self-improvement, self-reflection, and some metacognitive abilities of the
reasoning processes are possible through the judicious use of prompts. True
self-improvement and self-reasoning, to go from reasoning with LLMs to
reasoning by LLMs, remains future work.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.11384v2' target='_blank'>InvAgent: A Large Language Model based Multi-Agent System for Inventory
  Management in Supply Chains</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yinzhu Quan, Zefang Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-16 04:55:17</h6>
<p class='card-text'>Supply chain management (SCM) involves coordinating the flow of goods,
information, and finances across various entities to deliver products
efficiently. Effective inventory management is crucial in today's volatile and
uncertain world. Previous research has demonstrated the superiority of
heuristic methods and reinforcement learning applications in inventory
management. However, the application of large language models (LLMs) as
autonomous agents in multi-agent systems for inventory management remains
underexplored. This study introduces a novel approach using LLMs to manage
multi-agent inventory systems. Leveraging their zero-shot learning
capabilities, our model, InvAgent, enhances resilience and improves efficiency
across the supply chain network. Our contributions include utilizing LLMs for
zero-shot learning to enable adaptive and informed decision-making without
prior training, providing explainability and clarity through chain-of-thought,
and demonstrating dynamic adaptability to varying demand scenarios while
reducing costs and preventing stockouts. Extensive evaluations across different
scenarios highlight the efficiency of our model in SCM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.10627v1' target='_blank'>Arena Learning: Build Data Flywheel for LLMs Post-training via Simulated
  Chatbot Arena</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Qingwei Lin, Jianguang Lou, Shifeng Chen, Yansong Tang, Weizhu Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-15 11:26:07</h6>
<p class='card-text'>Assessing the effectiveness of large language models (LLMs) presents
substantial challenges. The method of conducting human-annotated battles in an
online Chatbot Arena is a highly effective evaluative technique. However, this
approach is limited by the costs and time required for human annotation. In
this paper, we introduce Arena Learning, an innovative offline strategy
designed to simulate these arena battles using AI-driven annotations to
evaluate battle outcomes, thus facilitating the continuous improvement of the
target model through both supervised fine-tuning and reinforcement learning.
Arena Learning comprises two key elements. First, it ensures precise
evaluations and maintains consistency between offline simulations and online
competitions via WizardArena, a pipeline developed to accurately predict the
Elo rankings of various models using a meticulously designed offline test set.
Our results demonstrate that WizardArena's predictions closely align with those
from the online Arena. Second, it involves the continuous improvement of
training data based on the battle results and the refined model. We establish a
data flywheel to iteratively update the training data by highlighting the
weaknesses of the target model based on its battle results, enabling it to
learn from the strengths of multiple different models. We apply Arena Learning
to train our target model, WizardLM-$\beta$, and demonstrate significant
performance enhancements across various metrics. This fully automated training
and evaluation pipeline sets the stage for continuous advancements in various
LLMs via post-training. Notably, Arena Learning plays a pivotal role in the
success of WizardLM-2, and this paper serves both as an exploration of its
efficacy and a foundational study for future discussions related to WizardLM-2
and its derivatives.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.09447v3' target='_blank'>ASTPrompter: Weakly Supervised Automated Language Model Red-Teaming to
  Identify Low-Perplexity Toxic Prompts</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Amelia F. Hardy, Houjun Liu, Bernard Lange, Duncan Eddy, Mykel J. Kochenderfer</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-12 17:33:34</h6>
<p class='card-text'>Conventional approaches for the automated red-teaming of large language
models (LLMs) aim to identify prompts that elicit toxic outputs from a frozen
language model (the defender). This often results in the prompting model (the
adversary) producing text that is unlikely to arise during autoregression. In
response, we propose a reinforcement learning formulation of LLM red-teaming
designed to discover prompts that both (1) elicit toxic outputs from a defender
and (2) have low perplexity as scored by that defender. These prompts are the
most pertinent in a red-teaming setting because the defender generates them
with high probability. We solve this formulation with an online and weakly
supervised form of Identity Preference Optimization (IPO), attacking models
ranging from 137M to 7.8B parameters. Our policy performs competitively,
producing prompts that induce defender toxicity at a rate of 2-23 times higher
than baseline across model scales. Importantly, these prompts have lower
perplexity than both automatically generated and human-written attacks.
Furthermore, our method creates black-box attacks with 5.4-14 times increased
toxicity. To assess the downstream utility of our method, we use rollouts from
our policy as negative examples for downstream toxicity tuning and demonstrate
improved safety.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.09287v1' target='_blank'>Instruction Following with Goal-Conditioned Reinforcement Learning in
  Virtual Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zoya Volovikova, Alexey Skrynnik, Petr Kuderov, Aleksandr I. Panov</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-12 14:19:36</h6>
<p class='card-text'>In this study, we address the issue of enabling an artificial intelligence
agent to execute complex language instructions within virtual environments. In
our framework, we assume that these instructions involve intricate linguistic
structures and multiple interdependent tasks that must be navigated
successfully to achieve the desired outcomes. To effectively manage these
complexities, we propose a hierarchical framework that combines the deep
language comprehension of large language models with the adaptive
action-execution capabilities of reinforcement learning agents. The language
module (based on LLM) translates the language instruction into a high-level
action plan, which is then executed by a pre-trained reinforcement learning
agent. We have demonstrated the effectiveness of our approach in two different
environments: in IGLU, where agents are instructed to build structures, and in
Crafter, where agents perform tasks and interact with objects in the
surrounding environment according to language commands.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.09024v2' target='_blank'>Aligning Diffusion Behaviors with Q-functions for Efficient Continuous
  Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Huayu Chen, Kaiwen Zheng, Hang Su, Jun Zhu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-12 06:32:36</h6>
<p class='card-text'>Drawing upon recent advances in language model alignment, we formulate
offline Reinforcement Learning as a two-stage optimization problem: First
pretraining expressive generative policies on reward-free behavior datasets,
then fine-tuning these policies to align with task-specific annotations like
Q-values. This strategy allows us to leverage abundant and diverse behavior
data to enhance generalization and enable rapid adaptation to downstream tasks
using minimal annotations. In particular, we introduce Efficient Diffusion
Alignment (EDA) for solving continuous control problems. EDA utilizes diffusion
models for behavior modeling. However, unlike previous approaches, we represent
diffusion policies as the derivative of a scalar neural network with respect to
action inputs. This representation is critical because it enables direct
density calculation for diffusion models, making them compatible with existing
LLM alignment theories. During policy fine-tuning, we extend preference-based
alignment methods like Direct Preference Optimization (DPO) to align diffusion
behaviors with continuous Q-functions. Our evaluation on the D4RL benchmark
shows that EDA exceeds all baseline methods in overall performance. Notably,
EDA maintains about 95\% of performance and still outperforms several baselines
given only 1\% of Q-labelled data during fine-tuning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.08770v2' target='_blank'>Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Huanqian Wang, Yang Yue, Rui Lu, Jingxin Shi, Andrew Zhao, Shenzhi Wang, Shiji Song, Gao Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-11 17:52:03</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated great potential as generalist
assistants, showcasing powerful task understanding and problem-solving
capabilities. To deploy LLMs as AI assistants, it is crucial that these models
exhibit desirable behavioral traits, such as non-toxicity and resilience
against jailbreak attempts. Current approaches for detoxification or preventing
jailbreaking usually involve Supervised Fine-Tuning (SFT) or Reinforcement
Learning from Human Feedback (RLHF), which requires finetuning billions of
parameters through gradient descent with substantial computational cost.
Furthermore, models modified through SFT and RLHF may deviate from the
pretrained models, potentially leading to a degradation in foundational LLM
capabilities. In this paper, we observe that surprisingly, directly editing a
small subset of parameters can effectively modulate specific behaviors of LLMs,
such as detoxification and resistance to jailbreaking, with only
inference-level computational resources. Experiments demonstrate that in the
detoxification task, our approach achieves reductions of up to 90.0% in
toxicity on the RealToxicityPrompts dataset and 49.2% on ToxiGen, while
maintaining the LLM's general capabilities in areas such as common sense,
question answering, and mathematics</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.08626v1' target='_blank'>RoboMorph: Evolving Robot Morphology using Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kevin Qiu, Krzysztof Ciebiera, Paweł Fijałkowski, Marek Cygan, Łukasz Kuciński</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-11 16:05:56</h6>
<p class='card-text'>We introduce RoboMorph, an automated approach for generating and optimizing
modular robot designs using large language models (LLMs) and evolutionary
algorithms. In this framework, we represent each robot design as a grammar and
leverage the capabilities of LLMs to navigate the extensive robot design space,
which is traditionally time-consuming and computationally demanding. By
integrating automatic prompt design and a reinforcement learning based control
algorithm, RoboMorph iteratively improves robot designs through feedback loops.
Our experimental results demonstrate that RoboMorph can successfully generate
nontrivial robots that are optimized for a single terrain while showcasing
improvements in morphology over successive evolutions. Our approach
demonstrates the potential of using LLMs for data-driven and modular robot
design, providing a promising methodology that can be extended to other domains
with similar design frameworks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.08213v2' target='_blank'>PrefCLM: Enhancing Preference-based Reinforcement Learning with
  Crowdsourced Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ruiqi Wang, Dezhong Zhao, Ziqin Yuan, Ike Obi, Byung-Cheol Min</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-11 06:30:46</h6>
<p class='card-text'>Preference-based reinforcement learning (PbRL) is emerging as a promising
approach to teaching robots through human comparative feedback, sidestepping
the need for complex reward engineering. However, the substantial volume of
feedback required in existing PbRL methods often lead to reliance on synthetic
feedback generated by scripted teachers. This approach necessitates intricate
reward engineering again and struggles to adapt to the nuanced preferences
particular to human-robot interaction (HRI) scenarios, where users may have
unique expectations toward the same task. To address these challenges, we
introduce PrefCLM, a novel framework that utilizes crowdsourced large language
models (LLMs) as simulated teachers in PbRL. We utilize Dempster-Shafer Theory
to fuse individual preferences from multiple LLM agents at the score level,
efficiently leveraging their diversity and collective intelligence. We also
introduce a human-in-the-loop pipeline that facilitates collective refinements
based on user interactive feedback. Experimental results across various general
RL tasks show that PrefCLM achieves competitive performance compared to
traditional scripted teachers and excels in facilitating more more natural and
efficient behaviors. A real-world user study (N=10) further demonstrates its
capability to tailor robot behaviors to individual user preferences,
significantly enhancing user satisfaction in HRI scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.07966v1' target='_blank'>A Comprehensive Survey on the Security of Smart Grid: Challenges,
  Mitigations, and Future Research Opportunities</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Arastoo Zibaeirad, Farnoosh Koleini, Shengping Bi, Tao Hou, Tao Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-10 18:03:24</h6>
<p class='card-text'>In this study, we conduct a comprehensive review of smart grid security,
exploring system architectures, attack methodologies, defense strategies, and
future research opportunities. We provide an in-depth analysis of various
attack vectors, focusing on new attack surfaces introduced by advanced
components in smart grids. The review particularly includes an extensive
analysis of coordinated attacks that incorporate multiple attack strategies and
exploit vulnerabilities across various smart grid components to increase their
adverse impact, demonstrating the complexity and potential severity of these
threats. Following this, we examine innovative detection and mitigation
strategies, including game theory, graph theory, blockchain, and machine
learning, discussing their advancements in counteracting evolving threats and
associated research challenges. In particular, our review covers a thorough
examination of widely used machine learning-based mitigation strategies,
analyzing their applications and research challenges spanning across
supervised, unsupervised, semi-supervised, ensemble, and reinforcement
learning. Further, we outline future research directions and explore new
techniques and concerns. We first discuss the research opportunities for
existing and emerging strategies, and then explore the potential role of new
techniques, such as large language models (LLMs), and the emerging threat of
adversarial machine learning in the future of smart grid security.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.07930v2' target='_blank'>Token-Mol 1.0: Tokenized drug design with large language model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jike Wang, Rui Qin, Mingyang Wang, Meijing Fang, Yangyang Zhang, Yuchen Zhu, Qun Su, Qiaolin Gou, Chao Shen, Odin Zhang, Zhenxing Wu, Dejun Jiang, Xujun Zhang, Huifeng Zhao, Xiaozhe Wan, Zhourui Wu, Liwei Liu, Yu Kang, Chang-Yu Hsieh, Tingjun Hou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-10 07:22:15</h6>
<p class='card-text'>Significant interests have recently risen in leveraging sequence-based large
language models (LLMs) for drug design. However, most current applications of
LLMs in drug discovery lack the ability to comprehend three-dimensional (3D)
structures, thereby limiting their effectiveness in tasks that explicitly
involve molecular conformations. In this study, we introduced Token-Mol, a
token-only 3D drug design model. This model encodes all molecular information,
including 2D and 3D structures, as well as molecular property data, into
tokens, which transforms classification and regression tasks in drug discovery
into probabilistic prediction problems, thereby enabling learning through a
unified paradigm. Token-Mol is built on the transformer decoder architecture
and trained using random causal masking techniques. Additionally, we proposed
the Gaussian cross-entropy (GCE) loss function to overcome the challenges in
regression tasks, significantly enhancing the capacity of LLMs to learn
continuous numerical values. Through a combination of fine-tuning and
reinforcement learning (RL), Token-Mol achieves performance comparable to or
surpassing existing task-specific methods across various downstream tasks,
including pocket-based molecular generation, conformation generation, and
molecular property prediction. Compared to existing molecular pre-trained
models, Token-Mol exhibits superior proficiency in handling a wider range of
downstream tasks essential for drug design. Notably, our approach improves
regression task accuracy by approximately 30% compared to similar token-only
methods. Token-Mol overcomes the precision limitations of token-only models and
has the potential to integrate seamlessly with general models such as ChatGPT,
paving the way for the development of a universal artificial intelligence drug
design model that facilitates rapid and high-quality drug design by experts.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.07086v2' target='_blank'>Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks
  with Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Logan Cross, Violet Xiang, Agam Bhatia, Daniel LK Yamins, Nick Haber</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-09 17:57:15</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) methods struggle with the
non-stationarity of multi-agent systems and fail to adaptively learn online
when tested with novel agents. Here, we leverage large language models (LLMs)
to create an autonomous agent that can handle these challenges. Our agent,
Hypothetical Minds, consists of a cognitively-inspired architecture, featuring
modular components for perception, memory, and hierarchical planning over two
levels of abstraction. We introduce the Theory of Mind module that scaffolds
the high-level planning process by generating hypotheses about other agents'
strategies in natural language. It then evaluates and iteratively refines these
hypotheses by reinforcing hypotheses that make correct predictions about the
other agents' behavior. Hypothetical Minds significantly improves performance
over previous LLM-agent and RL baselines on a range of competitive, mixed
motive, and collaborative domains in the Melting Pot benchmark, including both
dyadic and population-based environments. Additionally, comparisons against
LLM-agent baselines and ablations reveal the importance of hypothesis
evaluation and refinement for succeeding on complex scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.06902v1' target='_blank'>Learning From Crowdsourced Noisy Labels: A Signal Processing Perspective</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shahana Ibrahim, Panagiotis A. Traganitis, Xiao Fu, Georgios B. Giannakis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-09 14:34:40</h6>
<p class='card-text'>One of the primary catalysts fueling advances in artificial intelligence (AI)
and machine learning (ML) is the availability of massive, curated datasets. A
commonly used technique to curate such massive datasets is crowdsourcing, where
data are dispatched to multiple annotators. The annotator-produced labels are
then fused to serve downstream learning and inference tasks. This annotation
process often creates noisy labels due to various reasons, such as the limited
expertise, or unreliability of annotators, among others. Therefore, a core
objective in crowdsourcing is to develop methods that effectively mitigate the
negative impact of such label noise on learning tasks. This feature article
introduces advances in learning from noisy crowdsourced labels. The focus is on
key crowdsourcing models and their methodological treatments, from classical
statistical models to recent deep learning-based approaches, emphasizing
analytical insights and algorithmic developments. In particular, this article
reviews the connections between signal processing (SP) theory and methods, such
as identifiability of tensor and nonnegative matrix factorization, and novel,
principled solutions of longstanding challenges in crowdsourcing -- showing how
SP perspectives drive the advancements of this field. Furthermore, this article
touches upon emerging topics that are critical for developing cutting-edge
AI/ML systems, such as crowdsourcing in reinforcement learning with human
feedback (RLHF) and direct preference optimization (DPO) that are key
techniques for fine-tuning large language models (LLMs).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.06025v1' target='_blank'>iLLM-TSC: Integration reinforcement learning and large language model
  for traffic signal control policy improvement</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aoyu Pang, Maonan Wang, Man-On Pun, Chung Shue Chen, Xi Xiong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-08 15:22:49</h6>
<p class='card-text'>Urban congestion remains a critical challenge, with traffic signal control
(TSC) emerging as a potent solution. TSC is often modeled as a Markov Decision
Process problem and then solved using reinforcement learning (RL), which has
proven effective. However, the existing RL-based TSC system often overlooks
imperfect observations caused by degraded communication, such as packet loss,
delays, and noise, as well as rare real-life events not included in the reward
function, such as unconsidered emergency vehicles. To address these
limitations, we introduce a novel integration framework that combines a large
language model (LLM) with RL. This framework is designed to manage overlooked
elements in the reward function and gaps in state information, thereby
enhancing the policies of RL agents. In our approach, RL initially makes
decisions based on observed data. Subsequently, LLMs evaluate these decisions
to verify their reasonableness. If a decision is found to be unreasonable, it
is adjusted accordingly. Additionally, this integration approach can be
seamlessly integrated with existing RL-based TSC systems without necessitating
modifications. Extensive testing confirms that our approach reduces the average
waiting time by $17.5\%$ in degraded communication conditions as compared to
traditional RL methods, underscoring its potential to advance practical RL
applications in intelligent transportation systems. The related code can be
found at \url{https://github.com/Traffic-Alpha/iLLM-TSC}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.05580v1' target='_blank'>$\mathrm{E^{2}CFD}$: Towards Effective and Efficient Cost Function
  Design for Safe Reinforcement Learning via Large Language Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zepeng Wang, Chao Ma, Linjiang Zhou, Libing Wu, Lei Yang, Xiaochuan Shi, Guojun Peng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-08 03:30:25</h6>
<p class='card-text'>Different classes of safe reinforcement learning algorithms have shown
satisfactory performance in various types of safety requirement scenarios.
However, the existing methods mainly address one or several classes of specific
safety requirement scenario problems and cannot be applied to arbitrary safety
requirement scenarios. In addition, the optimization objectives of existing
reinforcement learning algorithms are misaligned with the task requirements.
Based on the need to address these issues, we propose $\mathrm{E^{2}CFD}$, an
effective and efficient cost function design framework. $\mathrm{E^{2}CFD}$
leverages the capabilities of a large language model (LLM) to comprehend
various safety scenarios and generate corresponding cost functions. It
incorporates the \textit{fast performance evaluation (FPE)} method to
facilitate rapid and iterative updates to the generated cost function. Through
this iterative process, $\mathrm{E^{2}CFD}$ aims to obtain the most suitable
cost function for policy training, tailored to the specific tasks within the
safety scenario. Experiments have proven that the performance of policies
trained using this framework is superior to traditional safe reinforcement
learning algorithms and policies trained with carefully designed cost
functions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.06227v2' target='_blank'>Communication and Control Co-Design in 6G: Sequential Decision-Making
  with LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xianfu Chen, Celimuge Wu, Yi Shen, Yusheng Ji, Tsutomu Yoshinaga, Qiang Ni, Charilaos C. Zarakovitis, Honggang Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-06 14:49:46</h6>
<p class='card-text'>This article investigates a control system within the context of
six-generation wireless networks. The control performance optimization
confronts the technical challenges that arise from the intricate interactions
between communication and control sub-systems, asking for a co-design.
Accounting for the system dynamics, we formulate the sequential co-design
decision-makings of communication and control over the discrete time horizon as
a Markov decision process, for which a practical offline learning framework is
proposed. Our proposed framework integrates large language models into the
elements of reinforcement learning. We present a case study on the age of
semantics-aware communication and control co-design to showcase the potentials
from our proposed learning framework. Furthermore, we discuss the open issues
remaining to make our proposed offline learning framework feasible for
real-world implementations, and highlight the research directions for future
explorations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.04181v1' target='_blank'>Orchestrating LLMs with Different Personalizations</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jin Peng Zhou, Katie Z Luo, Jingwen Gu, Jason Yuan, Kilian Q. Weinberger, Wen Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-04 22:55:02</h6>
<p class='card-text'>This paper presents a novel approach to aligning large language models (LLMs)
with individual human preferences, sometimes referred to as Reinforcement
Learning from \textit{Personalized} Human Feedback (RLPHF). Given stated
preferences along multiple dimensions, such as helpfulness, conciseness, or
humor, the goal is to create an LLM without re-training that best adheres to
this specification. Starting from specialized expert LLMs, each trained for one
such particular preference dimension, we propose a black-box method that merges
their outputs on a per-token level. We train a lightweight Preference Control
Model (PCM) that dynamically translates the preference description and current
context into next-token prediction weights. By combining the expert models'
outputs at the token level, our approach dynamically generates text that
optimizes the given preference. Empirical tests show that our method matches or
surpasses existing preference merging techniques, providing a scalable,
efficient alternative to fine-tuning LLMs for individual personalization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.03964v1' target='_blank'>Improving Sample Efficiency of Reinforcement Learning with Background
  Knowledge from Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fuxiang Zhang, Junyou Li, Yi-Chen Li, Zongzhang Zhang, Yang Yu, Deheng Ye</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-04 14:33:47</h6>
<p class='card-text'>Low sample efficiency is an enduring challenge of reinforcement learning
(RL). With the advent of versatile large language models (LLMs), recent works
impart common-sense knowledge to accelerate policy learning for RL processes.
However, we note that such guidance is often tailored for one specific task but
loses generalizability. In this paper, we introduce a framework that harnesses
LLMs to extract background knowledge of an environment, which contains general
understandings of the entire environment, making various downstream RL tasks
benefit from one-time knowledge representation. We ground LLMs by feeding a few
pre-collected experiences and requesting them to delineate background knowledge
of the environment. Afterward, we represent the output knowledge as potential
functions for potential-based reward shaping, which has a good property for
maintaining policy optimality from task rewards. We instantiate three variants
to prompt LLMs for background knowledge, including writing code, annotating
preferences, and assigning goals. Our experiments show that these methods
achieve significant sample efficiency improvements in a spectrum of downstream
tasks from Minigrid and Crafter domains.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.03856v3' target='_blank'>Q-Adapter: Customizing Pre-trained LLMs to New Preferences with
  Forgetting Mitigation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yi-Chen Li, Fuxiang Zhang, Wenjie Qiu, Lei Yuan, Chengxing Jia, Zongzhang Zhang, Yang Yu, Bo An</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-04 11:42:36</h6>
<p class='card-text'>Large Language Models (LLMs), trained on a large amount of corpus, have
demonstrated remarkable abilities. However, it may not be sufficient to
directly apply open-source LLMs like Llama to certain real-world scenarios,
since most of them are trained for \emph{general} purposes. Thus, the demands
for customizing publicly available LLMs emerge, but are currently
under-studied. In this work, we consider customizing pre-trained LLMs with new
human preferences. Specifically, the LLM should not only meet the new
preference but also preserve its original capabilities after customization.
Drawing inspiration from the observation that human preference can be expressed
as a reward model, we propose to cast LLM customization as optimizing the sum
of two reward functions, one of which (denoted as $r_1$) was used to pre-train
the LLM while the other (denoted as $r_2$) characterizes the new human
preference. The obstacle here is that both reward functions are unknown, making
the application of modern reinforcement learning methods infeasible. Thanks to
the residual Q-learning framework, we can restore the customized LLM with the
pre-trained LLM and the \emph{residual Q-function} without the reward function
$r_1$. Moreover, we find that for a fixed pre-trained LLM, the reward function
$r_2$ can be derived from the residual Q-function, enabling us to directly
learn the residual Q-function from the new human preference data upon the
Bradley-Terry model. We name our method Q-Adapter as it introduces an adapter
module to approximate the residual Q-function for customizing the pre-trained
LLM towards the new preference. Experiments based on the Llama-3.1 model on the
DSP dataset and HH-RLHF dataset illustrate the superior effectiveness of
Q-Adapter on both retaining existing knowledge and learning new preferences.
Code is available at \url{https://github.com/mansicer/Q-Adapter}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.03051v2' target='_blank'>Improving Conversational Abilities of Quantized Large Language Models
  via Direct Preference Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Janghwan Lee, Seongmin Park, Sukjin Hong, Minsoo Kim, Du-Seong Chang, Jungwook Choi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-03 12:19:06</h6>
<p class='card-text'>The rapid advancement of large language models (LLMs) has facilitated their
transformation into conversational chatbots that can grasp contextual nuances
and generate pertinent sentences, closely mirroring human values through
advanced techniques such as instruction tuning and reinforcement learning from
human feedback (RLHF). However, the computational efficiency required for LLMs,
achieved through techniques like post-training quantization (PTQ), presents
challenges such as token-flipping that can impair chatbot performance. In
response, we propose a novel preference alignment approach, quantization-aware
direct preference optimization (QDPO), that aligns quantized LLMs with their
full-precision counterparts, improving conversational abilities. Evaluated on
two instruction-tuned LLMs in various languages, QDPO demonstrated superior
performance in improving conversational abilities compared to established PTQ
and knowledge-distillation fine-tuning techniques, marking a significant step
forward in the development of efficient and effective conversational LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.03038v2' target='_blank'>Towards Federated RLHF with Aggregated Client Preference for LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Feijie Wu, Xiaoze Liu, Haoyu Wang, Xingchen Wang, Lu Su, Jing Gao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-03 12:02:24</h6>
<p class='card-text'>Reinforcement learning with human feedback (RLHF) fine-tunes a pretrained
large language model (LLM) using user preference data, enabling it to generate
content aligned with human preferences. However, due to privacy concerns, users
may be reluctant to share sensitive preference data. To address this, we
propose utilizing Federated Learning (FL) techniques, allowing large-scale
preference collection from diverse real-world users without requiring them to
transmit data to a central server. Our federated RLHF methods (i.e., FedBis and
FedBiscuit) encode each client's preferences into binary selectors and
aggregate them to capture common preferences. In particular, FedBiscuit
overcomes key challenges, such as preference heterogeneity and reward hacking,
through innovative solutions like grouping clients with similar preferences to
reduce heterogeneity and using multiple binary selectors to enhance LLM output
quality. To evaluate the performance of the proposed methods, we establish the
first federated RLHF benchmark with a heterogeneous human preference dataset.
Experimental results show that by integrating the LLM with aggregated client
preferences, FedBis and FedBiscuit significantly enhance the professionalism
and readability of the generated content.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.02586v1' target='_blank'>Improving Visual Storytelling with Multimodal Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaochuan Lin, Xiangyong Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-02 18:13:55</h6>
<p class='card-text'>Visual storytelling is an emerging field that combines images and narratives
to create engaging and contextually rich stories. Despite its potential,
generating coherent and emotionally resonant visual stories remains challenging
due to the complexity of aligning visual and textual information. This paper
presents a novel approach leveraging large language models (LLMs) and large
vision-language models (LVLMs) combined with instruction tuning to address
these challenges. We introduce a new dataset comprising diverse visual stories,
annotated with detailed captions and multimodal elements. Our method employs a
combination of supervised and reinforcement learning to fine-tune the model,
enhancing its narrative generation capabilities. Quantitative evaluations using
GPT-4 and qualitative human assessments demonstrate that our approach
significantly outperforms existing models, achieving higher scores in narrative
coherence, relevance, emotional depth, and overall quality. The results
underscore the effectiveness of instruction tuning and the potential of
LLMs/LVLMs in advancing visual storytelling.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.02354v1' target='_blank'>Talking to Machines: do you read me?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lina M. Rojas-Barahona</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-02 15:19:46</h6>
<p class='card-text'>In this dissertation I would like to guide the reader to the research on
dialogue but more precisely the research I have conducted during my career
since my PhD thesis. Starting from modular architectures with machine
learning/deep learning and reinforcement learning to end-to-end deep neural
networks. Besides my work as research associate, I also present the work I have
supervised in the last years.
  I review briefly the state of the art and highlight the open research
problems on conversational agents. Afterwards, I present my contribution to
Task-Oriented Dialogues (TOD), both as research associate and as the industrial
supervisor of CIFRE theses. I discuss conversational QA. Particularly, I
present the work of two PhD candidates Thibault Cordier and Sebastien Montella;
as well as the work of the young researcher Quentin Brabant. Finally, I present
the scientific project, where I discuss about Large Language Models (LLMs) for
Task-Oriented Dialogue and Multimodal Task-Oriented Dialogue.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.17482v2' target='_blank'>Reinforcement Learning from Human Feedback: Whose Culture, Whose Values,
  Whose Perspectives?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kristian González Barman, Simon Lohse, Henk de Regt</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-02 08:07:27</h6>
<p class='card-text'>We argue for the epistemic and ethical advantages of pluralism in
Reinforcement Learning from Human Feedback (RLHF) in the context of Large
Language Models (LLM). Drawing on social epistemology and pluralist philosophy
of science, we suggest ways in which RHLF can be made more responsive to human
needs and how we can address challenges along the way. The paper concludes with
an agenda for change, i.e. concrete, actionable steps to improve LLM
development.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.01887v3' target='_blank'>Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fanzeng Xia, Hao Liu, Yisong Yue, Tongxin Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-02 02:18:14</h6>
<p class='card-text'>In-context reinforcement learning (ICRL) is a frontier paradigm for solving
reinforcement learning problems in the foundation model era. While ICRL
capabilities have been demonstrated in transformers through task-specific
training, the potential of Large Language Models (LLMs) out-of-the-box remains
largely unexplored. Recent findings highlight that LLMs often face challenges
when dealing with numerical contexts, and limited attention has been paid to
evaluating their performance through preference feedback generated by the
environment. This paper is the first to investigate LLMs as in-context
decision-makers under the problem of Dueling Bandits (DB), a stateless
preference-based reinforcement learning setting that extends the classic
Multi-Armed Bandit (MAB) model by querying for preference feedback. We compare
GPT-3.5 Turbo, GPT-4, GPT-4 Turbo, Llama 3.1, and o1-Preview against nine
well-established DB algorithms. Our results reveal that our top-performing LLM,
GPT-4 Turbo, has the zero-shot relative decision-making ability to achieve
surprisingly low weak regret across all the DB environment instances by quickly
including the best arm in duels. However, an optimality gap exists between LLMs
and classic DB algorithms in terms of strong regret. LLMs struggle to converge
and consistently exploit even when explicitly prompted to do so, and are
sensitive to prompt variations. To bridge this gap, we propose an agentic flow
framework: LLM with Enhanced Algorithmic Dueling (LEAD), which integrates
off-the-shelf DB algorithms with LLM agents through fine-grained adaptive
interplay. We show that LEAD has theoretical guarantees inherited from classic
DB algorithms on both weak and strong regret. We validate its efficacy and
robustness even with noisy and adversarial prompts. The design of our framework
sheds light on how to enhance the trustworthiness of LLMs used for in-context
decision-making.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.01470v2' target='_blank'>DogeRM: Equipping Reward Models with Domain Knowledge through Model
  Merging</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tzu-Han Lin, Chen-An Li, Hung-yi Lee, Yun-Nung Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-01 17:01:54</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) is a popular strategy for
aligning large language models (LLMs) with desired behaviors. Reward modeling
is a crucial step in RLHF. However, collecting paired preference data for
training reward models is often costly and time-consuming, especially for
domain-specific preferences requiring expert annotation. To address this
challenge, we propose the \textbf{Do}main knowled\textbf{ge} merged
\textbf{R}eward \textbf{M}odel (DogeRM), a novel framework that integrates
domain-specific knowledge into a general reward model by model merging. The
experiments demonstrate that DogeRM enhances performance across different
benchmarks and provide a detailed analysis showcasing the effects of model
merging, showing the great potential of facilitating model alignment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.01461v2' target='_blank'>Enhancing the Capability and Robustness of Large Language Models through
  Reinforcement Learning-Driven Query Refinement</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zisu Huang, Xiaohua Wang, Feiran Zhang, Zhibo Xu, Cenyuan Zhang, Qi Qian, Xiaoqing Zheng, Xuanjing Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-01 16:55:28</h6>
<p class='card-text'>The capacity of large language models (LLMs) to generate honest, harmless,
and helpful responses heavily relies on the quality of user prompts. However,
these prompts often tend to be brief and vague, thereby significantly limiting
the full potential of LLMs. Moreover, harmful prompts can be meticulously
crafted and manipulated by adversaries to jailbreak LLMs, inducing them to
produce potentially toxic content. To enhance the capabilities of LLMs while
maintaining strong robustness against harmful jailbreak inputs, this study
proposes a transferable and pluggable framework that refines user prompts
before they are input into LLMs. This strategy improves the quality of the
queries, empowering LLMs to generate more truthful, benign and useful
responses. Specifically, a lightweight query refinement model is introduced and
trained using a specially designed reinforcement learning approach that
incorporates multiple objectives to enhance particular capabilities of LLMs.
Extensive experiments demonstrate that the refinement model not only improves
the quality of responses but also strengthens their robustness against
jailbreak attacks. Code is available at:
https://github.com/Huangzisu/query-refinement .</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.12036v2' target='_blank'>Exploring Advanced Large Language Models with LLMsuite</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Giorgio Roffo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-01 05:37:17</h6>
<p class='card-text'>This tutorial explores the advancements and challenges in the development of
Large Language Models (LLMs) such as ChatGPT and Gemini. It addresses inherent
limitations like temporal knowledge cutoffs, mathematical inaccuracies, and the
generation of incorrect information, proposing solutions like Retrieval
Augmented Generation (RAG), Program-Aided Language Models (PAL), and frameworks
such as ReAct and LangChain. The integration of these techniques enhances LLM
performance and reliability, especially in multi-step reasoning and complex
task execution. The paper also covers fine-tuning strategies, including
instruction fine-tuning, parameter-efficient methods like LoRA, and
Reinforcement Learning from Human Feedback (RLHF) as well as Reinforced
Self-Training (ReST). Additionally, it provides a comprehensive survey of
transformer architectures and training techniques for LLMs. The source code can
be accessed by contacting the author via email for a request.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.00978v2' target='_blank'>Hybrid RAG-empowered Multi-modal LLM for Secure Data Management in
  Internet of Medical Things: A Diffusion-based Contract Approach</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Cheng Su, Jinbo Wen, Jiawen Kang, Yonghua Wang, Yuanjia Su, Hudan Pan, Zishao Zhong, M. Shamim Hossain</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-01 05:28:40</h6>
<p class='card-text'>Secure data management and effective data sharing have become paramount in
the rapidly evolving healthcare landscape, especially with the growing
integration of the Internet of Medical Things (IoMT). The rise of generative
artificial intelligence has further elevated Multi-modal Large Language Models
(MLLMs) as essential tools for managing and optimizing healthcare data in IoMT.
MLLMs can support multi-modal inputs and generate diverse types of content by
leveraging large-scale training on vast amounts of multi-modal data. However,
critical challenges persist in developing medical MLLMs, including security and
freshness issues of healthcare data, affecting the output quality of MLLMs. To
this end, in this paper, we propose a hybrid Retrieval-Augmented Generation
(RAG)-empowered medical MLLM framework for healthcare data management. This
framework leverages a hierarchical cross-chain architecture to facilitate
secure data training. Moreover, it enhances the output quality of MLLMs through
hybrid RAG, which employs multi-modal metrics to filter various unimodal RAG
results and incorporates these retrieval results as additional inputs to MLLMs.
Additionally, we employ age of information to indirectly evaluate the data
freshness impact of MLLMs and utilize contract theory to incentivize healthcare
data holders to share their fresh data, mitigating information asymmetry during
data sharing. Finally, we utilize a generative diffusion model-based deep
reinforcement learning algorithm to identify the optimal contract for efficient
data sharing. Numerical results demonstrate the effectiveness of the proposed
schemes, which achieve secure and efficient healthcare data management.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.00808v1' target='_blank'>Exploring a Physics-Informed Decision Transformer for Distribution
  System Restoration: Methodology and Performance Analysis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hong Zhao, Jin Wei-Kocsis, Adel Heidari Akhijahani, Karen L Butler-Purry</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-30 19:27:06</h6>
<p class='card-text'>Driven by advancements in sensing and computing, deep reinforcement learning
(DRL)-based methods have demonstrated significant potential in effectively
tackling distribution system restoration (DSR) challenges under uncertain
operational scenarios. However, the data-intensive nature of DRL poses
obstacles in achieving satisfactory DSR solutions for large-scale, complex
distribution systems. Inspired by the transformative impact of emerging
foundation models, including large language models (LLMs), across various
domains, this paper explores an innovative approach harnessing LLMs' powerful
computing capabilities to address scalability challenges inherent in
conventional DRL methods for solving DSR. To our knowledge, this study
represents the first exploration of foundation models, including LLMs, in
revolutionizing conventional DRL applications in power system operations. Our
contributions are twofold: 1) introducing a novel LLM-powered Physics-Informed
Decision Transformer (PIDT) framework that leverages LLMs to transform
conventional DRL methods for DSR operations, and 2) conducting comparative
studies to assess the performance of the proposed LLM-powered PIDT framework at
its initial development stage for solving DSR problems. While our primary focus
in this paper is on DSR operations, the proposed PIDT framework can be
generalized to optimize sequential decision-making across various power system
operations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.00617v3' target='_blank'>Iterative Nash Policy Optimization: Aligning LLMs with General
  Preferences via No-Regret Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuheng Zhang, Dian Yu, Baolin Peng, Linfeng Song, Ye Tian, Mingyue Huo, Nan Jiang, Haitao Mi, Dong Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-30 08:00:34</h6>
<p class='card-text'>Reinforcement Learning with Human Feedback (RLHF) has achieved great success
in aligning large language models (LLMs) with human preferences. Prevalent RLHF
approaches are reward-based, following the Bradley-Terry (BT) model assumption,
which may not fully capture the complexity of human preferences. In this paper,
we explore RLHF under a general preference framework and approach it from a
game-theoretic perspective. Specifically, we formulate the problem as a
two-player game and propose a novel online algorithm, iterative Nash policy
optimization (INPO). The key idea is to let the policy play against itself via
no-regret learning, thereby approximating the Nash policy. Unlike previous
methods, INPO bypasses the need for estimating the expected win rate for
individual responses, which typically incurs high computational or annotation
costs. Instead, we introduce a new loss objective that is directly minimized
over a preference dataset. We provide theoretical analysis for our approach and
demonstrate its effectiveness through experiments on various representative
benchmarks. With an LLaMA-3-8B-based SFT model, INPO achieves a 42.6%
length-controlled win rate on AlpacaEval 2.0 and a 37.8% win rate on
Arena-Hard, showing substantial improvement over the state-of-the-art online
RLHF algorithms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.00215v1' target='_blank'>LLM Critics Help Catch LLM Bugs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, Jan Leike</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-28 19:53:17</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) is fundamentally limited by
the capacity of humans to correctly evaluate model output. To improve human
evaluation ability and overcome that limitation this work trains "critic"
models that help humans to more accurately evaluate model-written code. These
critics are themselves LLMs trained with RLHF to write natural language
feedback highlighting problems in code from real-world assistant tasks. On code
containing naturally occurring LLM errors model-written critiques are preferred
over human critiques in 63% of cases, and human evaluation finds that models
catch more bugs than human contractors paid for code review. We further confirm
that our fine-tuned LLM critics can successfully identify hundreds of errors in
ChatGPT training data rated as "flawless", even though the majority of those
tasks are non-code tasks and thus out-of-distribution for the critic model.
Critics can have limitations of their own, including hallucinated bugs that
could mislead humans into making mistakes they might have otherwise avoided,
but human-machine teams of critics and contractors catch similar numbers of
bugs to LLM critics while hallucinating less than LLMs alone.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.20060v1' target='_blank'>Applying RLAIF for Code Generation with API-usage in Lightweight LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sujan Dutta, Sayantan Mahinder, Raviteja Anantha, Bortik Bandyopadhyay</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-28 17:16:03</h6>
<p class='card-text'>Reinforcement Learning from AI Feedback (RLAIF) has demonstrated significant
potential across various domains, including mitigating harm in LLM outputs,
enhancing text summarization, and mathematical reasoning. This paper introduces
an RLAIF framework for improving the code generation abilities of lightweight
(<1B parameters) LLMs. We specifically focus on code generation tasks that
require writing appropriate API calls, which is challenging due to the
well-known issue of hallucination in LLMs. Our framework extracts AI feedback
from a larger LLM (e.g., GPT-3.5) through a specialized prompting strategy and
uses this data to train a reward model towards better alignment from smaller
LLMs. We run our experiments on the Gorilla dataset and meticulously assess the
quality of the model-generated code across various metrics, including AST,
ROUGE, and Code-BLEU, and develop a pipeline to compute its executability rate
accurately. Our approach significantly enhances the fine-tuned LLM baseline's
performance, achieving a 4.5% improvement in executability rate. Notably, a
smaller LLM model (780M parameters) trained with RLAIF surpasses a much larger
fine-tuned baseline with 7B parameters, achieving a 1.0% higher code
executability rate.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.19644v2' target='_blank'>Beyond Human Preferences: Exploring Reinforcement Learning Trajectory
  Evaluation and Improvement through LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zichao Shen, Tianchen Zhu, Qingyun Sun, Shiqi Gao, Jianxin Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-28 04:21:24</h6>
<p class='card-text'>Reinforcement learning (RL) faces challenges in evaluating policy
trajectories within intricate game tasks due to the difficulty in designing
comprehensive and precise reward functions. This inherent difficulty curtails
the broader application of RL within game environments characterized by diverse
constraints. Preference-based reinforcement learning (PbRL) presents a
pioneering framework that capitalizes on human preferences as pivotal reward
signals, thereby circumventing the need for meticulous reward engineering.
However, obtaining preference data from human experts is costly and
inefficient, especially under conditions marked by complex constraints. To
tackle this challenge, we propose a LLM-enabled automatic preference generation
framework named LLM4PG , which harnesses the capabilities of large language
models (LLMs) to abstract trajectories, rank preferences, and reconstruct
reward functions to optimize conditioned policies. Experiments on tasks with
complex language constraints demonstrated the effectiveness of our LLM-enabled
reward functions, accelerating RL convergence and overcoming stagnation caused
by slow or absent progress under original reward structures. This approach
mitigates the reliance on specialized human knowledge and demonstrates the
potential of LLMs to enhance RL's effectiveness in complex environments in the
wild.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.19188v1' target='_blank'>Averaging log-likelihoods in direct alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nathan Grinsztajn, Yannis Flet-Berliac, Mohammad Gheshlaghi Azar, Florian Strub, Bill Wu, Eugene Choi, Chris Cremer, Arash Ahmadian, Yash Chandak, Olivier Pietquin, Matthieu Geist</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-27 14:07:38</h6>
<p class='card-text'>To better align Large Language Models (LLMs) with human judgment,
Reinforcement Learning from Human Feedback (RLHF) learns a reward model and
then optimizes it using regularized RL. Recently, direct alignment methods were
introduced to learn such a fine-tuned model directly from a preference dataset
without computing a proxy reward function. These methods are built upon
contrastive losses involving the log-likelihood of (dis)preferred completions
according to the trained model. However, completions have various lengths, and
the log-likelihood is not length-invariant. On the other side, the
cross-entropy loss used in supervised training is length-invariant, as batches
are typically averaged token-wise. To reconcile these approaches, we introduce
a principled approach for making direct alignment length-invariant. Formally,
we introduce a new averaging operator, to be composed with the optimality
operator giving the best policy for the underlying RL problem. It translates
into averaging the log-likelihood within the loss. We empirically study the
effect of such averaging, observing a trade-off between the length of
generations and their scores.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.19185v2' target='_blank'>Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a
  supervised-friendly fashion</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yannis Flet-Berliac, Nathan Grinsztajn, Florian Strub, Bill Wu, Eugene Choi, Chris Cremer, Arash Ahmadian, Yash Chandak, Mohammad Gheshlaghi Azar, Olivier Pietquin, Matthieu Geist</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-27 14:03:49</h6>
<p class='card-text'>Reinforcement Learning (RL) has been used to finetune Large Language Models
(LLMs) using a reward model trained from preference data, to better align with
human judgment. The recently introduced direct alignment methods, which are
often simpler, more stable, and computationally lighter, can more directly
achieve this. However, these approaches cannot optimize arbitrary rewards, and
the preference-based ones are not the only rewards of interest for LLMs (eg.,
unit tests for code generation or textual entailment for summarization, among
others). RL-finetuning is usually done with a variation of policy gradient,
which calls for on-policy or near-on-policy samples, requiring costly
generations. We introduce Contrastive Policy Gradient, or CoPG, a simple and
mathematically principled new RL algorithm that can estimate the optimal policy
even from off-policy data. It can be seen as an off-policy policy gradient
approach that does not rely on important sampling techniques and highlights the
importance of using (the right) state baseline. We show this approach to
generalize the direct alignment method IPO (identity preference optimization)
and classic policy gradient. We experiment with the proposed CoPG on a toy
bandit problem to illustrate its properties, as well as for finetuning LLMs on
a summarization task, using a learned reward function considered as ground
truth for the purpose of the experiments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.18505v1' target='_blank'>Mental Modeling of Reinforcement Learning Agents by Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenhao Lu, Xufeng Zhao, Josua Spisak, Jae Hee Lee, Stefan Wermter</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-26 17:14:45</h6>
<p class='card-text'>Can emergent language models faithfully model the intelligence of
decision-making agents? Though modern language models exhibit already some
reasoning ability, and theoretically can potentially express any probable
distribution over tokens, it remains underexplored how the world knowledge
these pretrained models have memorized can be utilized to comprehend an agent's
behaviour in the physical world. This study empirically examines, for the first
time, how well large language models (LLMs) can build a mental model of agents,
termed agent mental modelling, by reasoning about an agent's behaviour and its
effect on states from agent interaction history. This research may unveil the
potential of leveraging LLMs for elucidating RL agent behaviour, addressing a
key challenge in eXplainable reinforcement learning (XRL). To this end, we
propose specific evaluation metrics and test them on selected RL task datasets
of varying complexity, reporting findings on agent mental model establishment.
Our results disclose that LLMs are not yet capable of fully mental modelling
agents through inference alone without further innovations. This work thus
provides new insights into the capabilities and limitations of modern LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.18346v1' target='_blank'>AI Alignment through Reinforcement Learning from Human Feedback?
  Contradictions and Limitations</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Adam Dahlgren Lindström, Leila Methnani, Lea Krause, Petter Ericson, Íñigo Martínez de Rituerto de Troya, Dimitri Coelho Mollo, Roel Dobbe</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-26 13:42:13</h6>
<p class='card-text'>This paper critically evaluates the attempts to align Artificial Intelligence
(AI) systems, especially Large Language Models (LLMs), with human values and
intentions through Reinforcement Learning from Feedback (RLxF) methods,
involving either human feedback (RLHF) or AI feedback (RLAIF). Specifically, we
show the shortcomings of the broadly pursued alignment goals of honesty,
harmlessness, and helpfulness. Through a multidisciplinary sociotechnical
critique, we examine both the theoretical underpinnings and practical
implementations of RLxF techniques, revealing significant limitations in their
approach to capturing the complexities of human ethics and contributing to AI
safety. We highlight tensions and contradictions inherent in the goals of RLxF.
In addition, we discuss ethically-relevant issues that tend to be neglected in
discussions about alignment and RLxF, among which the trade-offs between
user-friendliness and deception, flexibility and interpretability, and system
safety. We conclude by urging researchers and practitioners alike to critically
assess the sociotechnical ramifications of RLxF, advocating for a more nuanced
and reflective approach to its application in AI development.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.17840v2' target='_blank'>Human-Object Interaction from Human-Level Instructions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhen Wu, Jiaman Li, Pei Xu, C. Karen Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-25 17:46:28</h6>
<p class='card-text'>Intelligent agents must autonomously interact with the environments to
perform daily tasks based on human-level instructions. They need a foundational
understanding of the world to accurately interpret these instructions, along
with precise low-level movement and interaction skills to execute the derived
actions. In this work, we propose the first complete system for synthesizing
physically plausible, long-horizon human-object interactions for object
manipulation in contextual environments, driven by human-level instructions. We
leverage large language models (LLMs) to interpret the input instructions into
detailed execution plans. Unlike prior work, our system is capable of
generating detailed finger-object interactions, in seamless coordination with
full-body movements. We also train a policy to track generated motions in
physics simulation via reinforcement learning (RL) to ensure physical
plausibility of the motion. Our experiments demonstrate the effectiveness of
our system in synthesizing realistic interactions with diverse objects in
complex environments, highlighting its potential for real-world applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.00087v2' target='_blank'>ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for
  Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ju-Seung Byun, Jiyun Chun, Jihyung Kil, Andrew Perrault</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-25 07:20:11</h6>
<p class='card-text'>Large Multimodal Models (LMMs) excel at comprehending human instructions and
demonstrate remarkable results across a broad spectrum of tasks. Reinforcement
Learning from Human Feedback (RLHF) and AI Feedback (RLAIF) further refine LLMs
by aligning them with specific preferences. These methods primarily use
ranking-based feedback for entire generations. With advanced AI models
(Teacher), such as GPT-4 and Claude 3 Opus, we can request various types of
detailed feedback that are expensive for humans to provide. We propose a
two-stage algorithm ARES that Alternates REinforcement Learning (RL) and
Supervised Fine-Tuning (SFT). First, we request the Teacher to score how much
each sentence contributes to solving the problem in a Chain-of-Thought (CoT).
This sentence-level feedback allows us to consider individual valuable
segments, providing more granular rewards for the RL procedure. Second, we ask
the Teacher to correct the wrong reasoning after the RL stage. The RL procedure
requires massive efforts for hyperparameter tuning and often generates errors
like repetitive words and incomplete sentences. With the correction feedback,
we stabilize the RL fine-tuned model through SFT. We conduct experiments on
multi-model dataset ScienceQA and A-OKVQA to demonstrate the effectiveness of
our proposal. ARES rationale reasoning achieves around 70% win rate against
baseline models judged by GPT-4o. Additionally, we observe that the improved
rationale reasoning leads to a 2.5% increase in inference answer accuracy on
average for the multi-modal datasets.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.16768v1' target='_blank'>WARP: On the Benefits of Weight Averaged Rewarded Policies</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alexandre Ramé, Johan Ferret, Nino Vieillard, Robert Dadashi, Léonard Hussenot, Pierre-Louis Cedoz, Pier Giuseppe Sessa, Sertan Girgin, Arthur Douillard, Olivier Bachem</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-24 16:24:34</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) aligns large language
models (LLMs) by encouraging their generations to have high rewards, using a
reward model trained on human preferences. To prevent the forgetting of
pre-trained knowledge, RLHF usually incorporates a KL regularization; this
forces the policy to remain close to its supervised fine-tuned initialization,
though it hinders the reward optimization. To tackle the trade-off between KL
and reward, in this paper we introduce a novel alignment strategy named Weight
Averaged Rewarded Policies (WARP). WARP merges policies in the weight space at
three distinct stages. First, it uses the exponential moving average of the
policy as a dynamic anchor in the KL regularization. Second, it applies
spherical interpolation to merge independently fine-tuned policies into a new
enhanced one. Third, it linearly interpolates between this merged model and the
initialization, to recover features from pre-training. This procedure is then
applied iteratively, with each iteration's final model used as an advanced
initialization for the next, progressively refining the KL-reward Pareto front,
achieving superior rewards at fixed KL. Experiments with GEMMA policies
validate that WARP improves their quality and alignment, outperforming other
open-source LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.16748v1' target='_blank'>OCALM: Object-Centric Assessment with Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Timo Kaufmann, Jannis Blüml, Antonia Wüst, Quentin Delfosse, Kristian Kersting, Eyke Hüllermeier</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-24 15:57:48</h6>
<p class='card-text'>Properly defining a reward signal to efficiently train a reinforcement
learning (RL) agent is a challenging task. Designing balanced objective
functions from which a desired behavior can emerge requires expert knowledge,
especially for complex environments. Learning rewards from human feedback or
using large language models (LLMs) to directly provide rewards are promising
alternatives, allowing non-experts to specify goals for the agent. However,
black-box reward models make it difficult to debug the reward. In this work, we
propose Object-Centric Assessment with Language Models (OCALM) to derive
inherently interpretable reward functions for RL agents from natural language
task descriptions. OCALM uses the extensive world-knowledge of LLMs while
leveraging the object-centric nature common to many environments to derive
reward functions focused on relational concepts, providing RL agents with the
ability to derive policies from task descriptions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.16743v1' target='_blank'>Adversarial Contrastive Decoding: Boosting Safety Alignment of Large
  Language Models via Opposite Prompt Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhengyue Zhao, Xiaoyun Zhang, Kaidi Xu, Xing Hu, Rui Zhang, Zidong Du, Qi Guo, Yunji Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-24 15:51:30</h6>
<p class='card-text'>With the widespread application of Large Language Models (LLMs), it has
become a significant concern to ensure their safety and prevent harmful
responses. While current safe-alignment methods based on instruction
fine-tuning and Reinforcement Learning from Human Feedback (RLHF) can
effectively reduce harmful responses from LLMs, they often require high-quality
datasets and heavy computational overhead during model training. Another way to
align language models is to modify the logit of tokens in model outputs without
heavy training. Recent studies have shown that contrastive decoding can enhance
the performance of language models by reducing the likelihood of confused
tokens. However, these methods require the manual selection of contrastive
models or instruction templates. To this end, we propose Adversarial
Contrastive Decoding (ACD), an optimization-based framework to generate two
opposite system prompts for prompt-based contrastive decoding. ACD only needs
to apply a lightweight prompt tuning on a rather small anchor dataset (< 3 min
for each model) without training the target model. Experiments conducted on
extensive models and benchmarks demonstrate that the proposed method achieves
much better safety performance than previous model training-free decoding
methods without sacrificing its original generation ability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.16486v1' target='_blank'>Towards Comprehensive Preference Data Collection for Reward Modeling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yulan Hu, Qingyang Li, Sheng Ouyang, Ge Chen, Kaihui Chen, Lijun Mei, Xucheng Ye, Fuzheng Zhang, Yong Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-24 09:40:39</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment
of large language models (LLMs) with human preferences, thereby enhancing the
quality of responses generated. A critical component of RLHF is the reward
model, which is trained on preference data and outputs a scalar reward during
the inference stage. However, the collection of preference data still lacks
thorough investigation. Recent studies indicate that preference data is
collected either by AI or humans, where chosen and rejected instances are
identified among pairwise responses. We question whether this process
effectively filters out noise and ensures sufficient diversity in collected
data. To address these concerns, for the first time, we propose a comprehensive
framework for preference data collection, decomposing the process into four
incremental steps: Prompt Generation, Response Generation, Response Filtering,
and Human Labeling. This structured approach ensures the collection of
high-quality preferences while reducing reliance on human labor. We conducted
comprehensive experiments based on the data collected at different stages,
demonstrating the effectiveness of the proposed data collection method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.16382v1' target='_blank'>UNO Arena for Evaluating Sequential Decision-Making Capability of Large
  Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhanyue Qin, Haochuan Wang, Deyuan Liu, Ziyang Song, Cunhang Fan, Zhao Lv, Jinlin Wu, Zhen Lei, Zhiying Tu, Dianhui Chu, Xiaoyan Yu, Dianbo Sui</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-24 07:47:34</h6>
<p class='card-text'>Sequential decision-making refers to algorithms that take into account the
dynamics of the environment, where early decisions affect subsequent decisions.
With large language models (LLMs) demonstrating powerful capabilities between
tasks, we can't help but ask: Can Current LLMs Effectively Make Sequential
Decisions? In order to answer this question, we propose the UNO Arena based on
the card game UNO to evaluate the sequential decision-making capability of LLMs
and explain in detail why we choose UNO. In UNO Arena, We evaluate the
sequential decision-making capability of LLMs dynamically with novel metrics
based Monte Carlo methods. We set up random players, DQN-based reinforcement
learning players, and LLM players (e.g. GPT-4, Gemini-pro) for comparison
testing. Furthermore, in order to improve the sequential decision-making
capability of LLMs, we propose the TUTRI player, which can involves having LLMs
reflect their own actions wtih the summary of game history and the game
strategy. Numerous experiments demonstrate that the TUTRI player achieves a
notable breakthrough in the performance of sequential decision-making compared
to the vanilla LLM player.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.17807v3' target='_blank'>Enhancing Commentary Strategies for Imperfect Information Card Games: A
  Study of Large Language Models in Guandan Commentary</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Meiling Tao, Xuechen Liang, Ziyi Wang, Yiling Tao, Tianyu Shi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-23 11:58:26</h6>
<p class='card-text'>Recent advancements in large language models (LLMs) have unlocked the
potential for generating high-quality game commentary. However, producing
insightful and engaging commentary for complex games with incomplete
information remains a significant challenge. In this paper, we introduce a
novel commentary method that combine Reinforcement Learning (RL) and LLMs,
tailored specifically for the Chinese card game \textit{Guandan}. Our system
leverages RL to generate intricate card-playing scenarios and employs LLMs to
generate corresponding commentary text, effectively emulating the strategic
analysis and narrative prowess of professional commentators. The framework
comprises a state commentary guide, a Theory of Mind (ToM)-based strategy
analyzer, and a style retrieval module, which seamlessly collaborate to deliver
detailed and context-relevant game commentary in the Chinese language
environment. We empower LLMs with ToM capabilities and refine both retrieval
and information filtering mechanisms. This facilitates the generation of
personalized commentary content. Our experimental results showcase the
substantial enhancement in performance achieved by the proposed commentary
framework when applied to open-source LLMs, surpassing the performance of GPT-4
across multiple evaluation metrics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.15599v2' target='_blank'>Pareto-Optimal Learning from Preferences with Hidden Context</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ryan Bahlous-Boldi, Li Ding, Lee Spector, Scott Niekum</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-21 18:57:38</h6>
<p class='card-text'>Ensuring AI models align with human values is essential for their safety and
functionality. Reinforcement learning from human feedback (RLHF) leverages
human preferences to achieve this alignment. However, when preferences are
sourced from diverse populations, point estimates of reward can result in
suboptimal performance or be unfair to specific groups. We propose Pareto
Optimal Preference Learning (POPL), which enables pluralistic alignment by
framing discrepant group preferences as objectives with potential trade-offs,
aiming for policies that are Pareto-optimal on the preference dataset. POPL
utilizes lexicase selection, an iterative process that selects diverse and
Pareto-optimal solutions. Our theoretical and empirical evaluations demonstrate
that POPL surpasses baseline methods in learning sets of reward functions and
policies, effectively catering to distinct groups without access to group
numbers or membership labels. We verify the performance of POPL on a stateless
preference learning setting, a Minigrid RL domain, Metaworld robotics
benchmarks, as well as large language model (LLM) fine-tuning. We illustrate
that POPL can also serve as a foundation for techniques optimizing specific
notions of group fairness, ensuring safe and equitable AI model alignment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.15568v2' target='_blank'>Robust Reinforcement Learning from Corrupted Human Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alexander Bukharin, Ilgee Hong, Haoming Jiang, Zichong Li, Qingru Zhang, Zixuan Zhang, Tuo Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-21 18:06:30</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) provides a principled
framework for aligning AI systems with human preference data. For various
reasons, e.g., personal bias, context ambiguity, lack of training, etc, human
annotators may give incorrect or inconsistent preference labels. To tackle this
challenge, we propose a robust RLHF approach -- $R^3M$, which models the
potentially corrupted preference label as sparse outliers. Accordingly, we
formulate the robust reward learning as an $\ell_1$-regularized maximum
likelihood estimation problem. Computationally, we develop an efficient
alternating optimization algorithm, which only incurs negligible computational
overhead compared with the standard RLHF approach. Theoretically, we prove that
under proper regularity conditions, $R^3M$ can consistently learn the
underlying reward and identify outliers, provided that the number of outlier
labels scales sublinearly with the preference sample size. Furthermore, we
remark that $R^3M$ is versatile and can be extended to various preference
optimization methods, including direct preference optimization (DPO). Our
experiments on robotic control and natural language generation with large
language models (LLMs) show that $R^3M$ improves robustness of the reward
against several types of perturbations to the preference data.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.15567v1' target='_blank'>SAIL: Self-Improving Efficient Online Alignment of Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mucong Ding, Souradip Chakraborty, Vibhu Agrawal, Zora Che, Alec Koppel, Mengdi Wang, Amrit Bedi, Furong Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-21 18:05:35</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) is a key method for
aligning large language models (LLMs) with human preferences. However, current
offline alignment approaches like DPO, IPO, and SLiC rely heavily on fixed
preference datasets, which can lead to sub-optimal performance. On the other
hand, recent literature has focused on designing online RLHF methods but still
lacks a unified conceptual formulation and suffers from distribution shift
issues. To address this, we establish that online LLM alignment is underpinned
by bilevel optimization. By reducing this formulation to an efficient
single-level first-order method (using the reward-policy equivalence), our
approach generates new samples and iteratively refines model alignment by
exploring responses and regulating preference labels. In doing so, we permit
alignment methods to operate in an online and self-improving manner, as well as
generalize prior online RLHF methods as special cases. Compared to
state-of-the-art iterative RLHF methods, our approach significantly improves
alignment performance on open-sourced datasets with minimal computational
overhead.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.14868v5' target='_blank'>Direct Multi-Turn Preference Optimization for Language Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wentao Shi, Mengqi Yuan, Junkang Wu, Qifan Wang, Fuli Feng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-21 05:13:20</h6>
<p class='card-text'>Adapting Large Language Models (LLMs) for agent tasks is critical in
developing language agents. Direct Preference Optimization (DPO) is a promising
technique for this adaptation with the alleviation of compounding errors,
offering a means to directly optimize Reinforcement Learning (RL) objectives.
However, applying DPO to multi-turn tasks presents challenges due to the
inability to cancel the partition function. Overcoming this obstacle involves
making the partition function independent of the current state and addressing
length disparities between preferred and dis-preferred trajectories. In this
light, we replace the policy constraint with the state-action occupancy measure
constraint in the RL objective and add length normalization to the
Bradley-Terry model, yielding a novel loss function named DMPO for multi-turn
agent tasks with theoretical explanations. Extensive experiments on three
multi-turn agent task datasets confirm the effectiveness and superiority of the
DMPO loss. The code is available at https://github.com/swt-user/DMPO.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.14739v1' target='_blank'>Learning to Retrieve Iteratively for In-Context Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yunmo Chen, Tongfei Chen, Harsh Jhamtani, Patrick Xia, Richard Shin, Jason Eisner, Benjamin Van Durme</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-20 21:07:55</h6>
<p class='card-text'>We introduce iterative retrieval, a novel framework that empowers retrievers
to make iterative decisions through policy optimization. Finding an optimal
portfolio of retrieved items is a combinatorial optimization problem, generally
considered NP-hard. This approach provides a learned approximation to such a
solution, meeting specific task requirements under a given family of large
language models (LLMs). We propose a training procedure based on reinforcement
learning, incorporating feedback from LLMs. We instantiate an iterative
retriever for composing in-context learning (ICL) exemplars and apply it to
various semantic parsing tasks that demand synthesized programs as outputs. By
adding only 4M additional parameters for state encoding, we convert an
off-the-shelf dense retriever into a stateful iterative retriever,
outperforming previous methods in selecting ICL exemplars on semantic parsing
datasets such as CalFlow, TreeDST, and MTOP. Additionally, the trained
iterative retriever generalizes across different inference LLMs beyond the one
used during training.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.14662v3' target='_blank'>Advantage Alignment Algorithms</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Juan Agustin Duque, Milad Aghajohari, Tim Cooijmans, Razvan Ciuca, Tianyu Zhang, Gauthier Gidel, Aaron Courville</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-20 18:30:09</h6>
<p class='card-text'>Artificially intelligent agents are increasingly being integrated into human
decision-making: from large language model (LLM) assistants to autonomous
vehicles. These systems often optimize their individual objective, leading to
conflicts, particularly in general-sum games where naive reinforcement learning
agents empirically converge to Pareto-suboptimal Nash equilibria. To address
this issue, opponent shaping has emerged as a paradigm for finding socially
beneficial equilibria in general-sum games. In this work, we introduce
Advantage Alignment, a family of algorithms derived from first principles that
perform opponent shaping efficiently and intuitively. We achieve this by
aligning the advantages of interacting agents, increasing the probability of
mutually beneficial actions when their interaction has been positive. We prove
that existing opponent shaping methods implicitly perform Advantage Alignment.
Compared to these methods, Advantage Alignment simplifies the mathematical
formulation of opponent shaping, reduces the computational burden and extends
to continuous action domains. We demonstrate the effectiveness of our
algorithms across a range of social dilemmas, achieving state-of-the-art
cooperation and robustness against exploitation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.14655v1' target='_blank'>HYPERmotion: Learning Hybrid Behavior Planning for Autonomous
  Loco-manipulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jin Wang, Rui Dai, Weijie Wang, Luca Rossini, Francesco Ruscelli, Nikos Tsagarakis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-20 18:21:24</h6>
<p class='card-text'>Enabling robots to autonomously perform hybrid motions in diverse
environments can be beneficial for long-horizon tasks such as material
handling, household chores, and work assistance. This requires extensive
exploitation of intrinsic motion capabilities, extraction of affordances from
rich environmental information, and planning of physical interaction behaviors.
Despite recent progress has demonstrated impressive humanoid whole-body control
abilities, they struggle to achieve versatility and adaptability for new tasks.
In this work, we propose HYPERmotion, a framework that learns, selects and
plans behaviors based on tasks in different scenarios. We combine reinforcement
learning with whole-body optimization to generate motion for 38 actuated joints
and create a motion library to store the learned skills. We apply the planning
and reasoning features of the large language models (LLMs) to complex
loco-manipulation tasks, constructing a hierarchical task graph that comprises
a series of primitive behaviors to bridge lower-level execution with
higher-level planning. By leveraging the interaction of distilled spatial
geometry and 2D observation with a visual language model (VLM) to ground
knowledge into a robotic morphology selector to choose appropriate actions in
single- or dual-arm, legged or wheeled locomotion. Experiments in simulation
and real-world show that learned motions can efficiently adapt to new tasks,
demonstrating high autonomy from free-text commands in unstructured scenes.
Videos and website: hy-motion.github.io/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.14532v1' target='_blank'>RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math
  Reasoning by Eight-Fold</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Amrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, Aviral Kumar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-20 17:45:54</h6>
<p class='card-text'>Training on model-generated synthetic data is a promising approach for
finetuning LLMs, but it remains unclear when it helps or hurts. In this paper,
we investigate this question for math reasoning via an empirical study,
followed by building a conceptual understanding of our observations. First, we
find that while the typical approach of finetuning a model on synthetic correct
or positive problem-solution pairs generated by capable models offers modest
performance gains, sampling more correct solutions from the finetuned learner
itself followed by subsequent fine-tuning on this self-generated data
$\textbf{doubles}$ the efficiency of the same synthetic problems. At the same
time, training on model-generated positives can amplify various spurious
correlations, resulting in flat or even inverse scaling trends as the amount of
data increases. Surprisingly, we find that several of these issues can be
addressed if we also utilize negative responses, i.e., model-generated
responses that are deemed incorrect by a final answer verifier. Crucially,
these negatives must be constructed such that the training can appropriately
recover the utility or advantage of each intermediate step in the negative
response. With this per-step scheme, we are able to attain consistent gains
over only positive data, attaining performance similar to amplifying the amount
of synthetic data by $\mathbf{8 \times}$. We show that training on per-step
negatives can help to unlearn spurious correlations in the positive data, and
is equivalent to advantage-weighted reinforcement learning (RL), implying that
it inherits robustness benefits of RL over imitating positive data alone.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.14088v1' target='_blank'>ReaLHF: Optimized RLHF Training for Large Language Models through
  Parameter Reallocation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiyu Mei, Wei Fu, Kaiwei Li, Guangju Wang, Huanchen Zhang, Yi Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-20 08:04:07</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) stands as a pivotal
technique in empowering large language model (LLM) applications. Since RLHF
involves diverse computational workloads and intricate dependencies among
multiple LLMs, directly adopting parallelization techniques from supervised
training can result in sub-optimal performance. To overcome this limitation, we
propose a novel approach named parameter ReaLlocation, which dynamically
redistributes LLM parameters in the cluster and adapts parallelization
strategies during training. Building upon this idea, we introduce ReaLHF, a
pioneering system capable of automatically discovering and running efficient
execution plans for RLHF training given the desired algorithmic and hardware
configurations. ReaLHF formulates the execution plan for RLHF as an augmented
dataflow graph. Based on this formulation, ReaLHF employs a tailored search
algorithm with a lightweight cost estimator to discover an efficient execution
plan. Subsequently, the runtime engine deploys the selected plan by effectively
parallelizing computations and redistributing parameters. We evaluate ReaLHF on
the LLaMA-2 models with up to $4\times70$ billion parameters and 128 GPUs. The
experiment results showcase ReaLHF's substantial speedups of $2.0-10.6\times$
compared to baselines. Furthermore, the execution plans generated by ReaLHF
exhibit an average of $26\%$ performance improvement over heuristic approaches
based on Megatron-LM. The source code of ReaLHF is publicly available at
https://github.com/openpsi-project/ReaLHF .</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.14024v4' target='_blank'>LLM Critics Help Catch Bugs in Mathematics: Towards a Better
  Mathematical Verifier with Natural Language Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bofei Gao, Zefan Cai, Runxin Xu, Peiyi Wang, Ce Zheng, Runji Lin, Keming Lu, Dayiheng Liu, Chang Zhou, Wen Xiao, Junjie Hu, Tianyu Liu, Baobao Chang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-20 06:42:27</h6>
<p class='card-text'>In recent progress, mathematical verifiers have achieved success in
mathematical reasoning tasks by validating the correctness of solutions
generated by policy models. However, existing verifiers are trained with binary
classification labels, which are not informative enough for the model to
accurately assess the solutions. To mitigate the aforementioned insufficiency
of binary labels, we introduce step-wise natural language feedback as rationale
labels, that is, the correctness of each step and the detailed explanations. In
this paper, we propose Math-Minos, a natural language feedback-enhanced
verifier by constructing automatically generated training data and a two-stage
training paradigm for effective training and efficient inference. Our
experiments reveal that a small set of natural language feedback can
significantly boost the performance of the verifier in both verification and
reinforcement learning. We have released the code and data for further
exploration.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.15508v1' target='_blank'>What Teaches Robots to Walk, Teaches Them to Trade too -- Regime
  Adaptive Execution using Informed Data and LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Raeid Saqur</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-20 00:17:28</h6>
<p class='card-text'>Machine learning techniques applied to the problem of financial market
forecasting struggle with dynamic regime switching, or underlying correlation
and covariance shifts in true (hidden) market variables. Drawing inspiration
from the success of reinforcement learning in robotics, particularly in agile
locomotion adaptation of quadruped robots to unseen terrains, we introduce an
innovative approach that leverages world knowledge of pretrained LLMs (aka.
'privileged information' in robotics) and dynamically adapts them using
intrinsic, natural market rewards using LLM alignment technique we dub as
"Reinforcement Learning from Market Feedback" (**RLMF**). Strong empirical
results demonstrate the efficacy of our method in adapting to regime shifts in
financial markets, a challenge that has long plagued predictive models in this
domain. The proposed algorithmic framework outperforms best-performing SOTA LLM
models on the existing (FLARE) benchmark stock-movement (SM) tasks by more than
15\% improved accuracy. On the recently proposed NIFTY SM task, our adaptive
policy outperforms the SOTA best performing trillion parameter models like
GPT-4. The paper details the dual-phase, teacher-student architecture and
implementation of our model, the empirical results obtained, and an analysis of
the role of language embeddings in terms of Information Gain.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.13885v1' target='_blank'>Knowledge Tagging System on Math Questions via LLMs with Flexible
  Demonstration Retriever</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hang Li, Tianlong Xu, Jiliang Tang, Qingsong Wen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-19 23:30:01</h6>
<p class='card-text'>Knowledge tagging for questions plays a crucial role in contemporary
intelligent educational applications, including learning progress diagnosis,
practice question recommendations, and course content organization.
Traditionally, these annotations are always conducted by pedagogical experts,
as the task requires not only a strong semantic understanding of both question
stems and knowledge definitions but also deep insights into connecting
question-solving logic with corresponding knowledge concepts. With the recent
emergence of advanced text encoding algorithms, such as pre-trained language
models, many researchers have developed automatic knowledge tagging systems
based on calculating the semantic similarity between the knowledge and question
embeddings. In this paper, we explore automating the task using Large Language
Models (LLMs), in response to the inability of prior encoding-based methods to
deal with the hard cases which involve strong domain knowledge and complicated
concept definitions. By showing the strong performance of zero- and few-shot
results over math questions knowledge tagging tasks, we demonstrate LLMs' great
potential in conquering the challenges faced by prior methods. Furthermore, by
proposing a reinforcement learning-based demonstration retriever, we
successfully exploit the great potential of different-sized LLMs in achieving
better performance results while keeping the in-context demonstration usage
efficiency high.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.13542v3' target='_blank'>Self-play with Execution Feedback: Improving Instruction-following
  Capabilities of Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, Jingren Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-19 13:29:53</h6>
<p class='card-text'>One core capability of large language models (LLMs) is to follow natural
language instructions. However, the issue of automatically constructing
high-quality training data to enhance the complex instruction-following
abilities of LLMs without manual annotation remains unresolved. In this paper,
we introduce AutoIF, the first scalable and reliable method for automatically
generating instruction-following training data. AutoIF transforms the
validation of instruction-following data quality into code verification,
requiring LLMs to generate instructions, the corresponding code to check the
correctness of the instruction responses, and unit test samples to verify the
code's correctness. Then, execution feedback-based rejection sampling can
generate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from
Human Feedback (RLHF) training. AutoIF achieves significant improvements across
three training algorithms, SFT, Offline DPO, and Online DPO, when applied to
the top open-source LLMs, Qwen2 and LLaMA3, in self-alignment and
strong-to-weak distillation settings. Our code is publicly available at
https://github.com/QwenLM/AutoIF.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.13399v1' target='_blank'>VELO: A Vector Database-Assisted Cloud-Edge Collaborative LLM QoS
  Optimization Framework</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhi Yao, Zhiqing Tang, Jiong Lou, Ping Shen, Weijia Jia</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-19 09:41:37</h6>
<p class='card-text'>The Large Language Model (LLM) has gained significant popularity and is
extensively utilized across various domains. Most LLM deployments occur within
cloud data centers, where they encounter substantial response delays and incur
high costs, thereby impacting the Quality of Services (QoS) at the network
edge. Leveraging vector database caching to store LLM request results at the
edge can substantially mitigate response delays and cost associated with
similar requests, which has been overlooked by previous research. Addressing
these gaps, this paper introduces a novel Vector database-assisted cloud-Edge
collaborative LLM QoS Optimization (VELO) framework. Firstly, we propose the
VELO framework, which ingeniously employs vector database to cache the results
of some LLM requests at the edge to reduce the response time of subsequent
similar requests. Diverging from direct optimization of the LLM, our VELO
framework does not necessitate altering the internal structure of LLM and is
broadly applicable to diverse LLMs. Subsequently, building upon the VELO
framework, we formulate the QoS optimization problem as a Markov Decision
Process (MDP) and devise an algorithm grounded in Multi-Agent Reinforcement
Learning (MARL) to decide whether to request the LLM in the cloud or directly
return the results from the vector database at the edge. Moreover, to enhance
request feature extraction and expedite training, we refine the policy network
of MARL and integrate expert demonstrations. Finally, we implement the proposed
algorithm within a real edge system. Experimental findings confirm that our
VELO framework substantially enhances user satisfaction by concurrently
diminishing delay and resource consumption for edge users utilizing LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.12845v1' target='_blank'>Interpretable Preferences via Multi-Objective Reward Modeling and
  Mixture-of-Experts</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, Tong Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-18 17:58:28</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) has emerged as the primary
method for aligning large language models (LLMs) with human preferences. The
RLHF process typically starts by training a reward model (RM) using human
preference data. Conventional RMs are trained on pairwise responses to the same
user request, with relative ratings indicating which response humans prefer.
The trained RM serves as a proxy for human preferences. However, due to the
black-box nature of RMs, their outputs lack interpretability, as humans cannot
intuitively understand why an RM thinks a response is good or not. As RMs act
as human preference proxies, we believe they should be human-interpretable to
ensure that their internal decision processes are consistent with human
preferences and to prevent reward hacking in LLM alignment. To build RMs with
interpretable preferences, we propose a two-stage approach: i) train an
Absolute-Rating Multi-Objective Reward Model (ArmoRM) with multi-dimensional
absolute-rating data, each dimension corresponding to a human-interpretable
objective (e.g., honesty, verbosity, safety); ii) employ a Mixture-of-Experts
(MoE) strategy with a gating network that automatically selects the most
suitable reward objectives based on the context. We efficiently trained an
ArmoRM with Llama-3 8B and a gating network consisting of a shallow MLP on top
of the ArmoRM. Our trained model, ArmoRM-Llama3-8B, obtains state-of-the-art
performance on RewardBench, a benchmark evaluating RMs for language modeling.
Notably, the performance of our model surpasses the LLM-as-a-judge method with
GPT-4 judges by a margin, and approaches the performance of the much larger
Nemotron-4 340B reward model.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.12566v3' target='_blank'>RichRAG: Crafting Rich Responses for Multi-faceted Queries in
  Retrieval-Augmented Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shuting Wang, Xin Yu, Mang Wang, Weipeng Chen, Yutao Zhu, Zhicheng Dou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-18 12:52:51</h6>
<p class='card-text'>Retrieval-augmented generation (RAG) effectively addresses issues of static
knowledge and hallucination in large language models. Existing studies mostly
focus on question scenarios with clear user intents and concise answers.
However, it is prevalent that users issue broad, open-ended queries with
diverse sub-intents, for which they desire rich and long-form answers covering
multiple relevant aspects. To tackle this important yet underexplored problem,
we propose a novel RAG framework, namely RichRAG. It includes a sub-aspect
explorer to identify potential sub-aspects of input questions, a multi-faceted
retriever to build a candidate pool of diverse external documents related to
these sub-aspects, and a generative list-wise ranker, which is a key module to
provide the top-k most valuable documents for the final generator. These ranked
documents sufficiently cover various query aspects and are aware of the
generator's preferences, hence incentivizing it to produce rich and
comprehensive responses for users. The training of our ranker involves a
supervised fine-tuning stage to ensure the basic coverage of documents, and a
reinforcement learning stage to align downstream LLM's preferences to the
ranking of documents. Experimental results on two publicly available datasets
prove that our framework effectively and efficiently provides comprehensive and
satisfying responses to users.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.12221v4' target='_blank'>On-Policy Self-Alignment with Fine-grained Knowledge Feedback for
  Hallucination Mitigation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xueru Wen, Jie Lou, Xinyu Lu, Ji Yuqiu, Xinyan Guan, Yaojie Lu, Hongyu Lin, Ben He, Xianpei Han, Debing Zhang, Le Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-18 02:43:49</h6>
<p class='card-text'>Hallucination occurs when large language models exhibit behavior that
deviates from the boundaries of their knowledge during response generation. To
address this critical issue, previous learning-based methods attempt to
finetune models but are limited by off-policy sampling and coarse-grained
feedback. In this paper, we present \textit{\b{R}einforcement \b{L}earning
\b{f}or \b{H}allucination} (RLFH), an on-policy self-alignment approach that
enables LLMs to actively explore their knowledge boundaries and self-correct
generation behavior through fine-grained feedback signals. RLFH introduces a
self-assessment framework where the policy serves as its own judge. Through
this framework, responses are automatically decomposed into atomic facts and
their truthfulness and informativeness are assessed against external knowledge
sources. The resulting fine-grained feedback at the statement level are then
converted into token-level dense reward signals. This enables online
reinforcement learning to achieve precise and timely optimization without human
intervention. Comprehensive evaluations on HotpotQA, SQuADv2, and Biography
benchmarks validate RLFH's effectiveness in hallucination mitigation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.12182v1' target='_blank'>Aqulia-Med LLM: Pioneering Full-Process Open-Source Medical Language
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lulu Zhao, Weihao Zeng, Xiaofeng Shi, Hua Zhou, Donglin Hao, Yonghua Lin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-18 01:30:07</h6>
<p class='card-text'>Recently, both closed-source LLMs and open-source communities have made
significant strides, outperforming humans in various general domains. However,
their performance in specific professional fields such as medicine, especially
within the open-source community, remains suboptimal due to the complexity of
medical knowledge. We propose Aquila-Med, a bilingual medical LLM based on
Aquila, addressing these challenges through continue pre-training, supervised
fine-tuning (SFT), and reinforcement learning from human feedback (RLHF). We
construct a large-scale Chinese and English medical dataset for continue
pre-training and a high-quality SFT dataset, covering extensive medical
specialties. Additionally, we develop a high-quality Direct Preference
Optimization (DPO) dataset for further alignment. Aquila-Med achieves notable
results across single-turn, multi-turn dialogues, and medical multiple-choice
questions, demonstrating the effectiveness of our approach. We open-source the
datasets and the entire training process, contributing valuable resources to
the research community. Our models and datasets will released at
https://huggingface.co/BAAI/AquilaMed-RL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.12091v3' target='_blank'>Is poisoning a real threat to LLM alignment? Maybe more so than you
  think</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pankayaraj Pathmanathan, Souradip Chakraborty, Xiangyu Liu, Yongyuan Liang, Furong Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-17 21:06:00</h6>
<p class='card-text'>Recent advancements in Reinforcement Learning with Human Feedback (RLHF) have
significantly impacted the alignment of Large Language Models (LLMs). The
sensitivity of reinforcement learning algorithms such as Proximal Policy
Optimization (PPO) has led to new line work on Direct Policy Optimization
(DPO), which treats RLHF in a supervised learning framework. The increased
practical use of these RLHF methods warrants an analysis of their
vulnerabilities. In this work, we investigate the vulnerabilities of DPO to
poisoning attacks under different scenarios and compare the effectiveness of
preference poisoning, a first of its kind. We comprehensively analyze DPO's
vulnerabilities under different types of attacks, i.e., backdoor and
non-backdoor attacks, and different poisoning methods across a wide array of
language models, i.e., LLama 7B, Mistral 7B, and Gemma 7B. We find that unlike
PPO-based methods, which, when it comes to backdoor attacks, require at least
4\% of the data to be poisoned to elicit harmful behavior, we exploit the true
vulnerabilities of DPO more simply so we can poison the model with only as much
as 0.5\% of the data. We further investigate the potential reasons behind the
vulnerability and how well this vulnerability translates into backdoor vs
non-backdoor attacks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.11827v2' target='_blank'>WPO: Enhancing RLHF with Weighted Preference Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenxuan Zhou, Ravi Agrawal, Shujian Zhang, Sathish Reddy Indurthi, Sanqiang Zhao, Kaiqiang Song, Silei Xu, Chenguang Zhu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-17 17:59:13</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) is a promising solution to
align large language models (LLMs) more closely with human values. Off-policy
preference optimization, where the preference data is obtained from other
models, is widely adopted due to its cost efficiency and scalability. However,
off-policy preference optimization often suffers from a distributional gap
between the policy used for data collection and the target policy, leading to
suboptimal optimization. In this paper, we propose a novel strategy to mitigate
this problem by simulating on-policy learning with off-policy preference data.
Our Weighted Preference Optimization (WPO) method adapts off-policy data to
resemble on-policy data more closely by reweighting preference pairs according
to their probability under the current policy. This method not only addresses
the distributional gap problem but also enhances the optimization process
without incurring additional costs. We validate our method on instruction
following benchmarks including Alpaca Eval 2 and MT-bench. WPO not only
outperforms Direct Preference Optimization (DPO) by up to 5.6% on Alpaca Eval 2
but also establishes a remarkable length-controlled winning rate against
GPT-4-turbo of 76.7% based on Gemma-2-9b-it. We release the code and models at
https://github.com/wzhouad/WPO.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.11555v1' target='_blank'>Input Conditioned Graph Generation for Language Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lukas Vierling, Jie Fu, Kai Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-17 13:53:15</h6>
<p class='card-text'>Recent progress in Large Language Models (LLMs) and language agents has
demonstrated significant promise for various future applications across
multiple disciplines. While traditional approaches to language agents often
rely on fixed, handcrafted designs, our research aims to develop both learnable
and dynamic agents. Our method uses an existing framework that abstracts
language agents as graphs. Within this graph framework, we aim to learn a model
that can generate edges for every given input to the language agent. This
allows us to generate edges that represent the flow of communication within the
graph based on the given input, thereby adjusting the internal communication of
a language agent. We learn to generate these edges using a pretrained LLM that
is fine-tuned with reinforcement learning. This LLM can be fine-tuned on
several datasets simultaneously, and we hypothesize that the model learns to
adapt to these different domains during training, achieving good overall
performance when encountering data from different domains during deployment. We
demonstrate that our approach surpasses the previous static approach by nearly
6% accuracy on a combined dataset of MMLU and CMMLU, and by more than 10% when
trained with a sparsity-inducing loss. It also performs superior in additional
experiments conducted with the MMLU and Mini Crossword Puzzles datasets. The
code is available at https://github.com/lukasVierling/DynamicGPTSwarm.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.11455v2' target='_blank'>Adaptive Reinforcement Learning Planning: Harnessing Large Language
  Models for Complex Information Extraction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zepeng Ding, Ruiyang Ke, Wenhao Huang, Guochao Jiang, Yanda Li, Deqing Yang, Jiaqing Liang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-17 12:11:01</h6>
<p class='card-text'>Existing research on large language models (LLMs) shows that they can solve
information extraction tasks through multi-step planning. However, their
extraction behavior on complex sentences and tasks is unstable, emerging issues
such as false positives and missing elements. We observe that decomposing
complex extraction tasks and extracting them step by step can effectively
improve LLMs' performance, and the extraction orders of entities significantly
affect the final results of LLMs. This paper proposes a two-stage multi-step
method for LLM-based information extraction and adopts the RL framework to
execute the multi-step planning. We regard sequential extraction as a Markov
decision process, build an LLM-based extraction environment, design a decision
module to adaptively provide the optimal order for sequential entity extraction
on different sentences, and utilize the DDQN algorithm to train the decision
model. We also design the rewards and evaluation metrics suitable for the
extraction results of LLMs. We conduct extensive experiments on multiple public
datasets to demonstrate the effectiveness of our method in improving the
information extraction capabilities of LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.11285v2' target='_blank'>Self and Cross-Model Distillation for LLMs: Effective Methods for
  Refusal Pattern Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jie Li, Yi Liu, Chongyang Liu, Xiaoning Ren, Ling Shi, Weisong Sun, Yinxing Xue</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-17 07:46:45</h6>
<p class='card-text'>Large Language Models (LLMs) like OpenAI's GPT series, Anthropic's Claude,
and Meta's LLaMa have shown remarkable capabilities in text generation.
However, their susceptibility to toxic prompts presents significant security
challenges. This paper investigates alignment techniques, including Supervised
Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), to
mitigate these risks. We conduct an empirical study on refusal patterns across
nine LLMs, revealing that models with uniform refusal patterns, such as
Claude3, exhibit higher security. Based on these findings, we propose
self-distilling and cross-model distilling methods to enhance LLM security. Our
results show that these methods significantly improve refusal rates and reduce
unsafe content, with cross-model distilling achieving refusal rates close to
Claude3's 94.51%. These findings underscore the potential of distillation-based
alignment in securing LLMs against toxic prompts.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.11190v1' target='_blank'>Aligning Large Language Models from Self-Reference AI Feedback with one
  General Principle</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rong Bao, Rui Zheng, Shihan Dou, Xiao Wang, Enyu Zhou, Bo Wang, Qi Zhang, Liang Ding, Dacheng Tao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-17 03:51:46</h6>
<p class='card-text'>In aligning large language models (LLMs), utilizing feedback from existing
advanced AI rather than humans is an important method to scale supervisory
signals. However, it is highly challenging for AI to understand human
intentions and societal values, and provide accurate preference feedback based
on these. Current AI feedback methods rely on powerful LLMs, carefully designed
specific principles to describe human intentions, and are easily influenced by
position bias. To address these issues, we propose a self-reference-based AI
feedback framework that enables a 13B Llama2-Chat to provide high-quality
feedback under simple and general principles such as ``best for humanity``.
Specifically, we allow the AI to first respond to the user's instructions, then
generate criticism of other answers based on its own response as a reference,
and finally determine which answer better fits human preferences according to
the criticism. Additionally, we use a self-consistency method to further reduce
the impact of position bias, and employ semantic perplexity to calculate the
preference strength differences between different answers. Experimental results
show that our method enables 13B and 70B Llama2-Chat annotators to provide
high-quality preference feedback, and the policy models trained based on these
preference data achieve significant advantages in benchmark datasets through
reinforcement learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.10977v1' target='_blank'>Toward Optimal LLM Alignments Using Two-Player Games</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rui Zheng, Hongyi Guo, Zhihan Liu, Xiaoying Zhang, Yuanshun Yao, Xiaojun Xu, Zhaoran Wang, Zhiheng Xi, Tao Gui, Qi Zhang, Xuanjing Huang, Hang Li, Yang Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-16 15:24:50</h6>
<p class='card-text'>The standard Reinforcement Learning from Human Feedback (RLHF) framework
primarily focuses on optimizing the performance of large language models using
pre-collected prompts. However, collecting prompts that provide comprehensive
coverage is both tedious and challenging, and often fails to include scenarios
that LLMs need to improve on the most. In this paper, we investigate alignment
through the lens of two-agent games, involving iterative interactions between
an adversarial and a defensive agent. The adversarial agent's task at each step
is to generate prompts that expose the weakness of the defensive agent. In
return, the defensive agent seeks to improve its responses to these newly
identified prompts it struggled with, based on feedback from the reward model.
We theoretically demonstrate that this iterative reinforcement learning
optimization converges to a Nash Equilibrium for the game induced by the
agents. Experimental results in safety scenarios demonstrate that learning in
such a competitive environment not only fully trains agents but also leads to
policies with enhanced generalization capabilities for both adversarial and
defensive agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.10957v4' target='_blank'>Eliminating Biased Length Reliance of Direct Preference Optimization via
  Down-Sampled KL Divergence</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Junru Lu, Jiazheng Li, Siyu An, Meng Zhao, Yulan He, Di Yin, Xing Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-16 14:24:30</h6>
<p class='card-text'>Direct Preference Optimization (DPO) has emerged as a prominent algorithm for
the direct and robust alignment of Large Language Models (LLMs) with human
preferences, offering a more straightforward alternative to the complex
Reinforcement Learning from Human Feedback (RLHF). Despite its promising
efficacy, DPO faces a notable drawback: "verbosity", a common over-optimization
phenomenon also observed in RLHF. While previous studies mainly attributed
verbosity to biased labels within the data, we propose that the issue also
stems from an inherent algorithmic length reliance in DPO. Specifically, we
suggest that the discrepancy between sequence-level Kullback-Leibler (KL)
divergences between chosen and rejected sequences, used in DPO, results in
overestimated or underestimated rewards due to varying token lengths.
Empirically, we utilize datasets with different label lengths to demonstrate
the presence of biased rewards. We then introduce an effective downsampling
approach, named SamPO, to eliminate potential length reliance. Our experimental
evaluations, conducted across three LLMs of varying scales and a diverse array
of conditional and open-ended benchmarks, highlight the efficacy of SamPO in
mitigating verbosity, achieving improvements of 5% to 12% over DPO through
debaised rewards. Our codes can be accessed at:
https://github.com/LuJunru/SamPO/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.10858v2' target='_blank'>Step-level Value Preference Optimization for Mathematical Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guoxin Chen, Minpeng Liao, Chengxi Li, Kai Fan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-16 09:06:17</h6>
<p class='card-text'>Direct Preference Optimization (DPO) using an implicit reward model has
proven to be an effective alternative to reinforcement learning from human
feedback (RLHF) for fine-tuning preference aligned large language models
(LLMs). However, the overall preference annotations of responses do not fully
capture the fine-grained quality of model outputs in complex multi-step
reasoning tasks, such as mathematical reasoning. To address this limitation, we
introduce a novel algorithm called Step-level Value Preference Optimization
(SVPO). Our approach employs Monte Carlo Tree Search (MCTS) to automatically
annotate step-level preferences for multi-step reasoning. Furthermore, from the
perspective of learning-to-rank, we train an explicit value model to replicate
the behavior of the implicit reward model, complementing standard preference
optimization. This value model enables the LLM to generate higher reward
responses with minimal cost during inference. Experimental results demonstrate
that our method achieves state-of-the-art performance on both in-domain and
out-of-domain mathematical reasoning benchmarks. Our code is available at
\url{https://github.com/MARIO-Math-Reasoning/Super_MARIO}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.10813v1' target='_blank'>Self-Evolution Fine-Tuning for Policy Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ruijun Chen, Jiehao Liang, Shiping Gao, Fanqi Wan, Xiaojun Quan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-16 06:38:02</h6>
<p class='card-text'>The alignment of large language models (LLMs) is crucial not only for
unlocking their potential in specific tasks but also for ensuring that
responses meet human expectations and adhere to safety and ethical principles.
Current alignment methodologies face considerable challenges. For instance,
supervised fine-tuning (SFT) requires extensive, high-quality annotated
samples, while reinforcement learning from human feedback (RLHF) is complex and
often unstable. In this paper, we introduce self-evolution fine-tuning (SEFT)
for policy optimization, with the aim of eliminating the need for annotated
samples while retaining the stability and efficiency of SFT. SEFT first trains
an adaptive reviser to elevate low-quality responses while maintaining
high-quality ones. The reviser then gradually guides the policy's optimization
by fine-tuning it with enhanced responses. One of the prominent features of
this method is its ability to leverage unlimited amounts of unannotated data
for policy optimization through supervised fine-tuning. Our experiments on
AlpacaEval 2.0 and MT-Bench demonstrate the effectiveness of SEFT. We also
provide a comprehensive analysis of its advantages over existing alignment
techniques.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.10540v1' target='_blank'>Generating and Evolving Reward Functions for Highway Driving with Large
  Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xu Han, Qiannan Yang, Xianda Chen, Xiaowen Chu, Meixin Zhu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-15 07:50:10</h6>
<p class='card-text'>Reinforcement Learning (RL) plays a crucial role in advancing autonomous
driving technologies by maximizing reward functions to achieve the optimal
policy. However, crafting these reward functions has been a complex, manual
process in many practices. To reduce this complexity, we introduce a novel
framework that integrates Large Language Models (LLMs) with RL to improve
reward function design in autonomous driving. This framework utilizes the
coding capabilities of LLMs, proven in other areas, to generate and evolve
reward functions for highway scenarios. The framework starts with instructing
LLMs to create an initial reward function code based on the driving environment
and task descriptions. This code is then refined through iterative cycles
involving RL training and LLMs' reflection, which benefits from their ability
to review and improve the output. We have also developed a specific prompt
template to improve LLMs' understanding of complex driving simulations,
ensuring the generation of effective and error-free code. Our experiments in a
highway driving simulator across three traffic configurations show that our
method surpasses expert handcrafted reward functions, achieving a 22% higher
average success rate. This not only indicates safer driving but also suggests
significant gains in development productivity.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.10216v2' target='_blank'>Regularizing Hidden States Enables Learning Generalizable Reward Model
  for LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, Tong Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-14 17:49:59</h6>
<p class='card-text'>Reward models trained on human preference data have been proven to
effectively align Large Language Models (LLMs) with human intent within the
framework of reinforcement learning from human feedback (RLHF). However,
current reward models have limited generalization capabilities to unseen
prompts and responses, which can lead to an unexpected phenomenon known as
reward over-optimization, resulting in a decline in actual performance due to
excessive optimization of rewards. While previous research has advocated for
constraining policy optimization, our study introduces a novel approach to
enhance the reward model's generalization ability against distribution shifts
by regularizing the hidden states. Specifically, we retain the base model's
language model head and incorporate a suite of text-generation losses to
preserve the hidden states' text-generation capabilities, while concurrently
learning a reward head behind the same hidden states. Our experimental results
demonstrate that the introduced regularization technique markedly improves the
accuracy of learned reward models across a variety of out-of-distribution (OOD)
tasks and effectively alleviates the over-optimization issue in RLHF, offering
a more reliable and robust preference learning paradigm.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.10162v3' target='_blank'>Sycophancy to Subterfuge: Investigating Reward-Tampering in Large
  Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Carson Denison, Monte MacDiarmid, Fazl Barez, David Duvenaud, Shauna Kravec, Samuel Marks, Nicholas Schiefer, Ryan Soklaski, Alex Tamkin, Jared Kaplan, Buck Shlegeris, Samuel R. Bowman, Ethan Perez, Evan Hubinger</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-14 16:26:20</h6>
<p class='card-text'>In reinforcement learning, specification gaming occurs when AI systems learn
undesired behaviors that are highly rewarded due to misspecified training
goals. Specification gaming can range from simple behaviors like sycophancy to
sophisticated and pernicious behaviors like reward-tampering, where a model
directly modifies its own reward mechanism. However, these more pernicious
behaviors may be too complex to be discovered via exploration. In this paper,
we study whether Large Language Model (LLM) assistants which find easily
discovered forms of specification gaming will generalize to perform rarer and
more blatant forms, up to and including reward-tampering. We construct a
curriculum of increasingly sophisticated gameable environments and find that
training on early-curriculum environments leads to more specification gaming on
remaining environments. Strikingly, a small but non-negligible proportion of
the time, LLM assistants trained on the full curriculum generalize zero-shot to
directly rewriting their own reward function. Retraining an LLM not to game
early-curriculum environments mitigates, but does not eliminate,
reward-tampering in later environments. Moreover, adding harmlessness training
to our gameable environments does not prevent reward-tampering. These results
demonstrate that LLMs can generalize from common forms of specification gaming
to more pernicious reward tampering and that such behavior may be nontrivial to
remove.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.09760v1' target='_blank'>Bootstrapping Language Models with DPO Implicit Rewards</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Changyu Chen, Zichen Liu, Chao Du, Tianyu Pang, Qian Liu, Arunesh Sinha, Pradeep Varakantham, Min Lin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-14 06:57:18</h6>
<p class='card-text'>Human alignment in large language models (LLMs) is an active area of
research. A recent groundbreaking work, direct preference optimization (DPO),
has greatly simplified the process from past work in reinforcement learning
from human feedback (RLHF) by bypassing the reward learning stage in RLHF. DPO,
after training, provides an implicit reward model. In this work, we make a
novel observation that this implicit reward model can by itself be used in a
bootstrapping fashion to further align the LLM. Our approach is to use the
rewards from a current LLM model to construct a preference dataset, which is
then used in subsequent DPO rounds. We incorporate refinements that debias the
length of the responses and improve the quality of the preference dataset to
further improve our approach. Our approach, named self-alignment with DPO
ImpliCit rEwards (DICE), shows great improvements in alignment and achieves
superior performance than Gemini Pro on AlpacaEval 2, reaching 27.55%
length-controlled win rate against GPT-4 Turbo, but with only 8B parameters and
no external feedback. Our code is available at https://github.com/sail-sg/dice.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.10307v1' target='_blank'>What is the best model? Application-driven Evaluation for Large Language
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shiguo Lian, Kaikai Zhao, Xinhui Liu, Xuejiao Lei, Bikun Yang, Wenjing Zhang, Kai Wang, Zhaoxiang Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-14 04:52:15</h6>
<p class='card-text'>General large language models enhanced with supervised fine-tuning and
reinforcement learning from human feedback are increasingly popular in academia
and industry as they generalize foundation models to various practical tasks in
a prompt manner. To assist users in selecting the best model in practical
application scenarios, i.e., choosing the model that meets the application
requirements while minimizing cost, we introduce A-Eval, an application-driven
LLMs evaluation benchmark for general large language models. First, we
categorize evaluation tasks into five main categories and 27 sub-categories
from a practical application perspective. Next, we construct a dataset
comprising 678 question-and-answer pairs through a process of collecting,
annotating, and reviewing. Then, we design an objective and effective
evaluation method and evaluate a series of LLMs of different scales on A-Eval.
Finally, we reveal interesting laws regarding model scale and task difficulty
level and propose a feasible method for selecting the best model. Through
A-Eval, we provide clear empirical and engineer guidance for selecting the best
model, reducing barriers to selecting and using LLMs and promoting their
application and development. Our benchmark is publicly available at
https://github.com/UnicomAI/DataSet/tree/main/TestData/GeneralAbility.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.10305v2' target='_blank'>Unlock the Correlation between Supervised Fine-Tuning and Reinforcement
  Learning in Training Code Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jie Chen, Xintian Han, Yu Ma, Xun Zhou, Liang Xiang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-14 03:39:01</h6>
<p class='card-text'>Automatic code generation has been a longstanding research topic. With the
advancement of general-purpose large language models (LLMs), the ability to
code stands out as one important measure to the model's reasoning performance.
Usually, a two-stage training paradigm is implemented to obtain a Code LLM,
namely the pretraining and the fine-tuning. Within the fine-tuning, supervised
fine-tuning (SFT), and reinforcement learning (RL) are often used to improve
the model's zero-shot ability. A large number of work has been conducted to
improve the model's performance on code-related benchmarks with either
modifications to the algorithm or refinement of the dataset. However, we still
lack a deep insight into the correlation between SFT and RL. For instance, what
kind of dataset should be used to ensure generalization, or what if we abandon
the SFT phase in fine-tuning. In this work, we make an attempt to understand
the correlation between SFT and RL. To facilitate our research, we manually
craft 100 basis python functions, called atomic functions, and then a
synthesizing pipeline is deployed to create a large number of synthetic
functions on top of the atomic ones. In this manner, we ensure that the train
and test sets remain distinct, preventing data contamination. Through
comprehensive ablation study, we find: (1) Both atomic and synthetic functions
are indispensable for SFT's generalization, and only a handful of synthetic
functions are adequate; (2) Through RL, the SFT's generalization to target
domain can be greatly enhanced, even with the same training prompts; (3)
Training RL from scratch can alleviate the over-fitting issue introduced in the
SFT phase.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.09397v1' target='_blank'>Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks
  and Algorithms</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Miaosen Zhang, Yixuan Wei, Zhen Xing, Yifei Ma, Zuxuan Wu, Ji Li, Zheng Zhang, Qi Dai, Chong Luo, Xin Geng, Baining Guo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-13 17:59:20</h6>
<p class='card-text'>Modern vision models are trained on very large noisy datasets. While these
models acquire strong capabilities, they may not follow the user's intent to
output the desired results in certain aspects, e.g., visual aesthetic,
preferred style, and responsibility. In this paper, we target the realm of
visual aesthetics and aim to align vision models with human aesthetic standards
in a retrieval system. Advanced retrieval systems usually adopt a cascade of
aesthetic models as re-rankers or filters, which are limited to low-level
features like saturation and perform poorly when stylistic, cultural or
knowledge contexts are involved. We find that utilizing the reasoning ability
of large language models (LLMs) to rephrase the search query and extend the
aesthetic expectations can make up for this shortcoming. Based on the above
findings, we propose a preference-based reinforcement learning method that
fine-tunes the vision models to distill the knowledge from both LLMs reasoning
and the aesthetic models to better align the vision models with human
aesthetics. Meanwhile, with rare benchmarks designed for evaluating retrieval
systems, we leverage large multi-modality model (LMM) to evaluate the aesthetic
performance with their strong abilities. As aesthetic assessment is one of the
most subjective tasks, to validate the robustness of LMM, we further propose a
novel dataset named HPIR to benchmark the alignment with human aesthetics.
Experiments demonstrate that our method significantly enhances the aesthetic
behaviors of the vision models, under several metrics. We believe the proposed
algorithm can be a general practice for aligning vision models with human
values.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.08811v2' target='_blank'>Mixture-of-Skills: Learning to Optimize Data Usage for Fine-Tuning Large
  Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Minghao Wu, Thuy-Trang Vu, Lizhen Qu, Gholamreza Haffari</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-13 05:01:28</h6>
<p class='card-text'>Large language models (LLMs) are typically fine-tuned on diverse and
extensive datasets sourced from various origins to develop a comprehensive
range of skills, such as writing, reasoning, chatting, coding, and more. Each
skill has unique characteristics, and these datasets are often heterogeneous
and imbalanced, making the fine-tuning process highly challenging. Balancing
the development of each skill while ensuring the model maintains its overall
performance requires sophisticated techniques and careful dataset curation. In
this work, we propose a general, model-agnostic, reinforcement learning
framework, Mixture-of-Skills (MoS), that learns to optimize data usage
automatically during the fine-tuning process. This framework ensures the
optimal comprehensive skill development of LLMs by dynamically adjusting the
focus on different datasets based on their current learning state. To validate
the effectiveness of MoS, we conduct extensive experiments using three diverse
LLM backbones on two widely used benchmarks and demonstrate that MoS
substantially enhances model performance. Building on the success of MoS, we
propose MoSpec, an adaptation for task-specific fine-tuning, which harnesses
the utilities of various datasets for a specific purpose. Our work underlines
the significance of dataset rebalancing and present MoS as a powerful, general
solution for optimizing data usage in the fine-tuning of LLMs for various
purposes.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.08725v1' target='_blank'>RL-JACK: Reinforcement Learning-powered Black-box Jailbreaking Attack
  against LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xuan Chen, Yuzhou Nie, Lu Yan, Yunshu Mao, Wenbo Guo, Xiangyu Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-13 01:05:22</h6>
<p class='card-text'>Modern large language model (LLM) developers typically conduct a safety
alignment to prevent an LLM from generating unethical or harmful content.
Recent studies have discovered that the safety alignment of LLMs can be
bypassed by jailbreaking prompts. These prompts are designed to create specific
conversation scenarios with a harmful question embedded. Querying an LLM with
such prompts can mislead the model into responding to the harmful question. The
stochastic and random nature of existing genetic methods largely limits the
effectiveness and efficiency of state-of-the-art (SOTA) jailbreaking attacks.
In this paper, we propose RL-JACK, a novel black-box jailbreaking attack
powered by deep reinforcement learning (DRL). We formulate the generation of
jailbreaking prompts as a search problem and design a novel RL approach to
solve it. Our method includes a series of customized designs to enhance the RL
agent's learning efficiency in the jailbreaking context. Notably, we devise an
LLM-facilitated action space that enables diverse action variations while
constraining the overall search space. We propose a novel reward function that
provides meaningful dense rewards for the agent toward achieving successful
jailbreaking. Through extensive evaluations, we demonstrate that RL-JACK is
overall much more effective than existing jailbreaking attacks against six SOTA
LLMs, including large open-source models and commercial models. We also show
the RL-JACK's resiliency against three SOTA defenses and its transferability
across different models. Finally, we validate the insensitivity of RL-JACK to
the variations in key hyper-parameters.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.08705v4' target='_blank'>When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided
  Search</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xuan Chen, Yuzhou Nie, Wenbo Guo, Xiangyu Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-13 00:04:15</h6>
<p class='card-text'>Recent studies developed jailbreaking attacks, which construct jailbreaking
prompts to fool LLMs into responding to harmful questions. Early-stage
jailbreaking attacks require access to model internals or significant human
efforts. More advanced attacks utilize genetic algorithms for automatic and
black-box attacks. However, the random nature of genetic algorithms
significantly limits the effectiveness of these attacks. In this paper, we
propose RLbreaker, a black-box jailbreaking attack driven by deep reinforcement
learning (DRL). We model jailbreaking as a search problem and design an RL
agent to guide the search, which is more effective and has less randomness than
stochastic search, such as genetic algorithms. Specifically, we design a
customized DRL system for the jailbreaking problem, including a novel reward
function and a customized proximal policy optimization (PPO) algorithm. Through
extensive experiments, we demonstrate that RLbreaker is much more effective
than existing jailbreaking attacks against six state-of-the-art (SOTA) LLMs. We
also show that RLbreaker is robust against three SOTA defenses and its trained
agents can transfer across different LLMs. We further validate the key design
choices of RLbreaker via a comprehensive ablation study.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.10278v1' target='_blank'>Prompt-Based Length Controlled Generation with Multiple Control Types</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Renlong Jie, Xiaojun Meng, Lifeng Shang, Xin Jiang, Qun Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-12 01:49:54</h6>
<p class='card-text'>Large language models (LLMs) have attracted great attention given their
strong performance on a wide range of NLP tasks. In practice, users often
expect generated texts to fall within a specific length range, making length
controlled generation an important topic, especially for GPT-style models.
Existing length control methods mostly focus on a simple control type of "equal
to" a target length. Different from them, we propose a prompt-based method to
achieve length controlled generation under different control types with high
accuracy. In particular, we adopt reinforcement learning (RL) and sample
filtering with the reward signal given by rule-based reward models, which
enhances the length control ability of models by rewarding outputs that follow
certain control instructions. In addition, we introduce a standard prompt
extractor to parse arbitrary users' input into standard control instructions.
Experiments show that our method significantly improves the accuracy of
prompt-based length control on popular summarization datasets like CNNDM and
NYT under multiple control types. Moreover, both the standard prompt extractor
and RL-tuned model show strong generalization to unseen control prompt
templates.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.07780v2' target='_blank'>A Critical Look At Tokenwise Reward-Guided Text Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ahmad Rashid, Ruotian Wu, Julia Grosse, Agustinus Kristiadi, Pascal Poupart</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-12 00:19:40</h6>
<p class='card-text'>Large language models (LLMs) can be improved by aligning with human
preferences through fine-tuning -- the so-called reinforcement learning from
human feedback (RLHF). However, the cost of fine-tuning an LLM is prohibitive
for many users. Due to their ability to bypass LLM fine-tuning, prediction-time
tokenwise reward-guided text generation (RGTG) methods have recently been
proposed. They use a reward model trained on full sequences to score partial
sequences during decoding in a bid to steer the generation towards sequences
with high rewards. However, these methods have so far been only heuristically
motivated and poorly analyzed. In this work, we show that reward models trained
on full sequences are not compatible with scoring partial sequences. To
alleviate this issue, we propose to train a Bradley-Terry reward model on
partial sequences explicitly, and autoregressively sample from the implied
tokenwise policy during decoding time. We study the properties of this reward
model and the resulting policy: we show that this policy is proportional to the
ratio of two distinct RLHF policies. Our simple approach outperforms previous
RGTG methods and performs similarly to strong offline baselines without
large-scale LLM finetuning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.07657v1' target='_blank'>OPTune: Efficient Online Preference Tuning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lichang Chen, Jiuhai Chen, Chenxi Liu, John Kirchenbauer, Davit Soselia, Chen Zhu, Tom Goldstein, Tianyi Zhou, Heng Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-11 18:55:04</h6>
<p class='card-text'>Reinforcement learning with human feedback~(RLHF) is critical for aligning
Large Language Models (LLMs) with human preference. Compared to the widely
studied offline version of RLHF, \emph{e.g.} direct preference optimization
(DPO), recent works have shown that the online variants achieve even better
alignment. However, online alignment requires on-the-fly generation of new
training data, which is costly, hard to parallelize, and suffers from varying
quality and utility. In this paper, we propose a more efficient data
exploration strategy for online preference tuning (OPTune), which does not rely
on human-curated or pre-collected teacher responses but dynamically samples
informative responses for on-policy preference alignment. During data
generation, OPTune only selects prompts whose (re)generated responses can
potentially provide more informative and higher-quality training signals than
the existing responses. In the training objective, OPTune reweights each
generated response (pair) by its utility in improving the alignment so that
learning can be focused on the most helpful samples. Throughout our
evaluations, OPTune'd LLMs maintain the instruction-following benefits provided
by standard preference tuning whilst enjoying 1.27-1.56x faster training speed
due to the efficient data exploration strategy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.07455v2' target='_blank'>Reinforcement Learning from Human Feedback without Reward Inference:
  Model-Free Algorithm and Instance-Dependent Analysis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qining Zhang, Honghao Wei, Lei Ying</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-11 17:01:41</h6>
<p class='card-text'>In this paper, we study reinforcement learning from human feedback (RLHF)
under an episodic Markov decision process with a general trajectory-wise reward
model. We developed a model-free RLHF best policy identification algorithm,
called $\mathsf{BSAD}$, without explicit reward model inference, which is a
critical intermediate step in the contemporary RLHF paradigms for training
large language models (LLM). The algorithm identifies the optimal policy
directly from human preference information in a backward manner, employing a
dueling bandit sub-routine that constantly duels actions to identify the
superior one. $\mathsf{BSAD}$ adopts a reward-free exploration and
best-arm-identification-like adaptive stopping criteria to equalize the
visitation among all states in the same decision step while moving to the
previous step as soon as the optimal action is identifiable, leading to a
provable, instance-dependent sample complexity
$\tilde{\mathcal{O}}(c_{\mathcal{M}}SA^3H^3M\log\frac{1}{\delta})$ which
resembles the result in classic RL, where $c_{\mathcal{M}}$ is the
instance-dependent constant and $M$ is the batch size. Moreover,
$\mathsf{BSAD}$ can be transformed into an explore-then-commit algorithm with
logarithmic regret and generalized to discounted MDPs using a frame-based
approach. Our results show: (i) sample-complexity-wise, RLHF is not
significantly harder than classic RL and (ii) end-to-end RLHF may deliver
improved performance by avoiding pitfalls in reward inferring such as overfit
and distribution shift.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.07381v1' target='_blank'>World Models with Hints of Large Language Models for Goal Achieving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zeyuan Liu, Ziyu Huan, Xiyao Wang, Jiafei Lyu, Jian Tao, Xiu Li, Furong Huang, Huazhe Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-11 15:49:08</h6>
<p class='card-text'>Reinforcement learning struggles in the face of long-horizon tasks and sparse
goals due to the difficulty in manual reward specification. While existing
methods address this by adding intrinsic rewards, they may fail to provide
meaningful guidance in long-horizon decision-making tasks with large state and
action spaces, lacking purposeful exploration. Inspired by human cognition, we
propose a new multi-modal model-based RL approach named Dreaming with Large
Language Models (DLLM). DLLM integrates the proposed hinting subgoals from the
LLMs into the model rollouts to encourage goal discovery and reaching in
challenging tasks. By assigning higher intrinsic rewards to samples that align
with the hints outlined by the language model during model rollouts, DLLM
guides the agent toward meaningful and efficient exploration. Extensive
experiments demonstrate that the DLLM outperforms recent methods in various
challenging, sparse-reward environments such as HomeGrid, Crafter, and
Minecraft by 27.7\%, 21.1\%, and 9.9\%, respectively.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.06874v3' target='_blank'>Learning Reward and Policy Jointly from Demonstration and Preference
  Improves Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chenliang Li, Siliang Zeng, Zeyi Liao, Jiaxiang Li, Dongyeop Kang, Alfredo Garcia, Mingyi Hong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-11 01:20:53</h6>
<p class='card-text'>Aligning human preference and value is an important requirement for building
contemporary foundation models and embodied AI. However, popular approaches
such as reinforcement learning with human feedback (RLHF) break down the task
into successive stages, such as supervised fine-tuning (SFT), reward modeling
(RM), and reinforcement learning (RL), each performing one specific learning
task. Such a sequential approach results in serious issues such as significant
under-utilization of data and distribution mismatch between the learned reward
model and generated policy, which eventually lead to poor alignment
performance. We develop a single stage approach named Alignment with Integrated
Human Feedback (AIHF), capable of integrating both human preference and
demonstration to train reward models and the policy. The proposed approach
admits a suite of efficient algorithms, which can easily reduce to, and
leverage, popular alignment algorithms such as RLHF and Directly Policy
Optimization (DPO), and only requires minor changes to the existing alignment
pipelines. We demonstrate the efficiency of the proposed solutions with
extensive experiments involving alignment problems in LLMs and robotic control
problems in MuJoCo. We observe that the proposed solutions outperform the
existing alignment algorithms such as RLHF and DPO by large margins, especially
when the amount of high-quality preference data is relatively limited.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.06059v2' target='_blank'>LLM-Based Intent Processing and Network Optimization Using
  Attention-Based Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Md Arafat Habib, Pedro Enrique Iturria Rivera, Yigit Ozcan, Medhat Elsayed, Majid Bavand, Raimundus Gaigalas, Melike Erol-Kantarci</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-10 07:13:54</h6>
<p class='card-text'>Intent-based network automation is a promising tool to enable easier network
management however certain challenges need to be effectively addressed. These
are: 1) processing intents, i.e., identification of logic and necessary
parameters to fulfill an intent, 2) validating an intent to align it with
current network status, and 3) satisfying intents via network optimizing
functions like xApps and rApps in O-RAN. This paper addresses these points via
a three-fold strategy to introduce intent-based automation for O-RAN. First,
intents are processed via a lightweight Large Language Model (LLM). Secondly,
once an intent is processed, it is validated against future incoming traffic
volume profiles (high or low). Finally, a series of network optimization
applications (rApps and xApps) have been developed. With their machine
learning-based functionalities, they can improve certain key performance
indicators such as throughput, delay, and energy efficiency. In this final
stage, using an attention-based hierarchical reinforcement learning algorithm,
these applications are optimally initiated to satisfy the intent of an
operator. Our simulations show that the proposed method can achieve at least
12% increase in throughput, 17.1% increase in energy efficiency, and 26.5%
decrease in network delay compared to the baseline algorithms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.05881v2' target='_blank'>LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Utsav Singh, Pramit Bhattacharyya, Vinay P. Namboodiri</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-09 18:40:24</h6>
<p class='card-text'>Developing interactive systems that leverage natural language instructions to
solve complex robotic control tasks has been a long-desired goal in the
robotics community. Large Language Models (LLMs) have demonstrated exceptional
abilities in handling complex tasks, including logical reasoning, in-context
learning, and code generation. However, predicting low-level robotic actions
using LLMs poses significant challenges. Additionally, the complexity of such
tasks usually demands the acquisition of policies to execute diverse subtasks
and combine them to attain the ultimate objective. Hierarchical Reinforcement
Learning (HRL) is an elegant approach for solving such tasks, which provides
the intuitive benefits of temporal abstraction and improved exploration.
However, HRL faces the recurring issue of non-stationarity due to unstable
lower primitive behaviour. In this work, we propose LGR2, a novel HRL framework
that leverages language instructions to generate a stationary reward function
for the higher-level policy. Since the language-guided reward is unaffected by
the lower primitive behaviour, LGR2 mitigates non-stationarity and is thus an
elegant method for leveraging language instructions to solve robotic control
tasks. To analyze the efficacy of our approach, we perform empirical analysis
and demonstrate that LGR2 effectively alleviates non-stationarity in HRL. Our
approach attains success rates exceeding 70$\%$ in challenging, sparse-reward
robotic navigation and manipulation environments where the baselines fail to
achieve any significant progress. Additionally, we conduct real-world robotic
manipulation experiments and demonstrate that CRISP shows impressive
generalization in real-world scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.05673v4' target='_blank'>Flow of Reasoning:Training LLMs for Divergent Problem Solving with
  Minimal Examples</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, Lianhui Qin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-09 07:06:58</h6>
<p class='card-text'>The ability to generate diverse solutions to a given problem is a hallmark of
human creativity. This divergent reasoning is also crucial for machines,
enhancing their robustness and enabling them to assist humans in many
applications such as scientific discovery. However, existing approaches to
multi-step reasoning with large language models (LLMs) have mostly focused only
on reasoning accuracy, without further discovering more diverse valid
solutions. For example, supervised fine-tuning can improve LLM reasoning
quality, but requires extensive supervised data to capture the full range of
possible solutions. Reward-maximization reinforcement learning aims to find
limited highest-reward solutions while neglecting the solution diversity. To
fill this gap, we propose Flow of Reasoning (FoR), an efficient
diversity-seeking LLM finetuning method aimed at improving reasoning quality
and diversity with minimal data. FoR formulates multi-step LLM reasoning as a
Markovian flow on a DAG-structured reasoning graph. This formulation allows us
to incorporate and adapt principled GFlowNet approaches, for finetuning LLMs to
sample divergent paths with probabilities proportional to the (unnormalized)
reward of target problems. Extensive experiments show that, with limited
training examples (e.g., 15 examples), FoR enables the discovery of diverse,
creative, high-quality solutions, greatly outperforming a wide range of
existing inference and training methods across six challenging reasoning tasks,
including BlocksWorld (embodied reasoning), Game24 (math puzzle solving),
Rubik's Cube (spatial reasoning), 1D-ARC (abstraction reasoning), GSM8k (math
reasoning), and ProntoQA (logical reasoning). Code is available at
https://github.com/Yu-Fangxu/FoR.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.05587v1' target='_blank'>Creativity Has Left the Chat: The Price of Debiasing Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Behnam Mohammadi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-08 22:14:51</h6>
<p class='card-text'>Large Language Models (LLMs) have revolutionized natural language processing
but can exhibit biases and may generate toxic content. While alignment
techniques like Reinforcement Learning from Human Feedback (RLHF) reduce these
issues, their impact on creativity, defined as syntactic and semantic
diversity, remains unexplored. We investigate the unintended consequences of
RLHF on the creativity of LLMs through three experiments focusing on the
Llama-2 series. Our findings reveal that aligned models exhibit lower entropy
in token predictions, form distinct clusters in the embedding space, and
gravitate towards "attractor states", indicating limited output diversity. Our
findings have significant implications for marketers who rely on LLMs for
creative tasks such as copywriting, ad creation, and customer persona
generation. The trade-off between consistency and creativity in aligned models
should be carefully considered when selecting the appropriate model for a given
application. We also discuss the importance of prompt engineering in harnessing
the creative potential of base models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.05374v1' target='_blank'>Planning Like Human: A Dual-process Framework for Dialogue Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tao He, Lizi Liao, Yixin Cao, Yuanxing Liu, Ming Liu, Zerui Chen, Bing Qin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-08 06:52:47</h6>
<p class='card-text'>In proactive dialogue, the challenge lies not just in generating responses
but in steering conversations toward predetermined goals, a task where Large
Language Models (LLMs) typically struggle due to their reactive nature.
Traditional approaches to enhance dialogue planning in LLMs, ranging from
elaborate prompt engineering to the integration of policy networks, either face
efficiency issues or deliver suboptimal performance. Inspired by the
dualprocess theory in psychology, which identifies two distinct modes of
thinking - intuitive (fast) and analytical (slow), we propose the Dual-Process
Dialogue Planning (DPDP) framework. DPDP embodies this theory through two
complementary planning systems: an instinctive policy model for familiar
contexts and a deliberative Monte Carlo Tree Search (MCTS) mechanism for
complex, novel scenarios. This dual strategy is further coupled with a novel
two-stage training regimen: offline Reinforcement Learning for robust initial
policy model formation followed by MCTS-enhanced on-the-fly learning, which
ensures a dynamic balance between efficiency and strategic depth. Our empirical
evaluations across diverse dialogue tasks affirm DPDP's superiority in
achieving both high-quality dialogues and operational efficiency, outpacing
existing methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.05107v1' target='_blank'>LINX: A Language Driven Generative System for Goal-Oriented Automated
  Data Exploration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tavor Lipman, Tova Milo, Amit Somech, Tomer Wolfson, Oz Zafar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-07 17:37:05</h6>
<p class='card-text'>Data exploration is a challenging process in which users examine a dataset by
iteratively employing a series of queries. While in some cases the user
explores a new dataset to become familiar with it, more often, the exploration
process is conducted with a specific analysis goal or question in mind. To
assist users in exploring a new dataset, Automated Data Exploration (ADE)
systems have been devised in previous work. These systems aim to auto-generate
a full exploration session, containing a sequence of queries that showcase
interesting elements of the data. However, existing ADE systems are often
constrained by a predefined objective function, thus always generating the same
session for a given dataset. Therefore, their effectiveness in goal-oriented
exploration, in which users need to answer specific questions about the data,
are extremely limited.
  To this end, this paper presents LINX, a generative system augmented with a
natural language interface for goal-oriented ADE. Given an input dataset and an
analytical goal described in natural language, LINX generates a personalized
exploratory session that is relevant to the user's goal. LINX utilizes a Large
Language Model (LLM) to interpret the input analysis goal, and then derive a
set of specifications for the desired output exploration session. These
specifications are then transferred to a novel, modular ADE engine based on
Constrained Deep Reinforcement Learning (CDRL), which can adapt its output
according to the specified instructions.
  To validate LINX's effectiveness, we introduce a new benchmark dataset for
goal-oriented exploration and conduct an extensive user study. Our analysis
underscores LINX's superior capability in producing exploratory notebooks that
are significantly more relevant and beneficial than those generated by existing
solutions, including ChatGPT, goal-agnostic ADE, and commercial systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.11875v1' target='_blank'>ChatPCG: Large Language Model-Driven Reward Design for Procedural
  Content Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:In-Chang Baek, Tae-Hwa Park, Jin-Ha Noh, Cheong-Mok Bae, Kyung-Joong Kim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-07 08:18:42</h6>
<p class='card-text'>Driven by the rapid growth of machine learning, recent advances in game
artificial intelligence (AI) have significantly impacted productivity across
various gaming genres. Reward design plays a pivotal role in training game AI
models, wherein researchers implement concepts of specific reward functions.
However, despite the presence of AI, the reward design process predominantly
remains in the domain of human experts, as it is heavily reliant on their
creativity and engineering skills. Therefore, this paper proposes ChatPCG, a
large language model (LLM)-driven reward design framework.It leverages
human-level insights, coupled with game expertise, to generate rewards tailored
to specific game features automatically. Moreover, ChatPCG is integrated with
deep reinforcement learning, demonstrating its potential for multiplayer game
content generation tasks. The results suggest that the proposed LLM exhibits
the capability to comprehend game mechanics and content generation tasks,
enabling tailored content generation for a specified game. This study not only
highlights the potential for improving accessibility in content generation but
also aims to streamline the game AI development process.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.04583v1' target='_blank'>Extroversion or Introversion? Controlling The Personality of Your Large
  Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yanquan Chen, Zhen Wu, Junjie Guo, Shujian Huang, Xinyu Dai</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-07 02:11:49</h6>
<p class='card-text'>Large language models (LLMs) exhibit robust capabilities in text generation
and comprehension, mimicking human behavior and exhibiting synthetic
personalities. However, some LLMs have displayed offensive personality,
propagating toxic discourse. Existing literature neglects the origin and
evolution of LLM personalities, as well as the effective personality control.
To fill these gaps, our study embarked on a comprehensive investigation into
LLM personality control. We investigated several typical methods to influence
LLMs, including three training methods: Continual Pre-training, Supervised
Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF), along
with inference phase considerations (prompts). Our investigation revealed a
hierarchy of effectiveness in control: Prompt > SFT > RLHF > Continual
Pre-train. Notably, SFT exhibits a higher control success rate compared to
prompt induction. While prompts prove highly effective, we found that
prompt-induced personalities are less robust than those trained, making them
more prone to showing conflicting personalities under reverse personality
prompt induction. Besides, harnessing the strengths of both SFT and prompt, we
proposed $\underline{\text{P}}$rompt $\underline{\text{I}}$nduction post
$\underline{\text{S}}$upervised $\underline{\text{F}}$ine-tuning (PISF), which
emerges as the most effective and robust strategy for controlling LLMs'
personality, displaying high efficacy, high success rates, and high robustness.
Even under reverse personality prompt induction, LLMs controlled by PISF still
exhibit stable and robust personalities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.04523v1' target='_blank'>Proofread: Fixes All Errors with One Tap</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Renjie Liu, Yanxiang Zhang, Yun Zhu, Haicheng Sun, Yuanbo Zhang, Michael Xuelin Huang, Shanqing Cai, Lei Meng, Shumin Zhai</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-06 21:38:08</h6>
<p class='card-text'>The impressive capabilities in Large Language Models (LLMs) provide a
powerful approach to reimagine users' typing experience. This paper
demonstrates Proofread, a novel Gboard feature powered by a server-side LLM in
Gboard, enabling seamless sentence-level and paragraph-level corrections with a
single tap. We describe the complete system in this paper, from data
generation, metrics design to model tuning and deployment. To obtain models
with sufficient quality, we implement a careful data synthetic pipeline
tailored to online use cases, design multifaceted metrics, employ a two-stage
tuning approach to acquire the dedicated LLM for the feature: the Supervised
Fine Tuning (SFT) for foundational quality, followed by the Reinforcement
Learning (RL) tuning approach for targeted refinement. Specifically, we find
sequential tuning on Rewrite and proofread tasks yields the best quality in SFT
stage, and propose global and direct rewards in the RL tuning stage to seek
further improvement. Extensive experiments on a human-labeled golden set showed
our tuned PaLM2-XS model achieved 85.56\% good ratio. We launched the feature
to Pixel 8 devices by serving the model on TPU v5 in Google Cloud, with
thousands of daily active users. Serving latency was significantly reduced by
quantization, bucket inference, text segmentation, and speculative decoding.
Our demo could be seen in \href{https://youtu.be/4ZdcuiwFU7I}{Youtube}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.04481v1' target='_blank'>Optimizing Autonomous Driving for Safety: A Human-Centric Approach with
  LLM-Enhanced RLHF</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuan Sun, Navid Salami Pargoo, Peter J. Jin, Jorge Ortiz</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-06 20:10:34</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) is popular in large
language models (LLMs), whereas traditional Reinforcement Learning (RL) often
falls short. Current autonomous driving methods typically utilize either human
feedback in machine learning, including RL, or LLMs. Most feedback guides the
car agent's learning process (e.g., controlling the car). RLHF is usually
applied in the fine-tuning step, requiring direct human "preferences," which
are not commonly used in optimizing autonomous driving models. In this
research, we innovatively combine RLHF and LLMs to enhance autonomous driving
safety. Training a model with human guidance from scratch is inefficient. Our
framework starts with a pre-trained autonomous car agent model and implements
multiple human-controlled agents, such as cars and pedestrians, to simulate
real-life road environments. The autonomous car model is not directly
controlled by humans. We integrate both physical and physiological feedback to
fine-tune the model, optimizing this process using LLMs. This multi-agent
interactive environment ensures safe, realistic interactions before real-world
application. Finally, we will validate our model using data gathered from
real-life testbeds located in New Jersey and New York City.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.04274v1' target='_blank'>Self-Play with Adversarial Critic: Provable and Scalable Offline
  Alignment for Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiang Ji, Sanjeev Kulkarni, Mengdi Wang, Tengyang Xie</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-06 17:23:49</h6>
<p class='card-text'>This work studies the challenge of aligning large language models (LLMs) with
offline preference data. We focus on alignment by Reinforcement Learning from
Human Feedback (RLHF) in particular. While popular preference optimization
methods exhibit good empirical performance in practice, they are not
theoretically guaranteed to converge to the optimal policy and can provably
fail when the data coverage is sparse by classical offline reinforcement
learning (RL) results. On the other hand, a recent line of work has focused on
theoretically motivated preference optimization methods with provable
guarantees, but these are not computationally efficient for large-scale
applications like LLM alignment. To bridge this gap, we propose SPAC, a new
offline preference optimization method with self-play, inspired by the
on-average pessimism technique from the offline RL literature, to be the first
provable and scalable approach to LLM alignment. We both provide theoretical
analysis for its convergence under single-policy concentrability for the
general function approximation setting and demonstrate its competitive
empirical performance for LLM alignment on a 7B Mistral model with Open LLM
Leaderboard evaluations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.06606v2' target='_blank'>Prototypical Reward Network for Data-Efficient RLHF</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jinghan Zhang, Xiting Wang, Yiqiao Jin, Changyu Chen, Xinhao Zhang, Kunpeng Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-06 15:23:30</h6>
<p class='card-text'>The reward model for Reinforcement Learning from Human Feedback (RLHF) has
proven effective in fine-tuning Large Language Models (LLMs). Notably,
collecting human feedback for RLHF can be resource-intensive and lead to
scalability issues for LLMs and complex tasks. Our proposed framework Proto-RM
leverages prototypical networks to enhance reward models under limited human
feedback. By enabling stable and reliable structural learning from fewer
samples, Proto-RM significantly enhances LLMs' adaptability and accuracy in
interpreting human preferences. Extensive experiments on various datasets
demonstrate that Proto-RM significantly improves the performance of reward
models and LLMs in human feedback tasks, achieving comparable and usually
better results than traditional methods, while requiring significantly less
data. in data-limited scenarios. This research offers a promising direction for
enhancing the efficiency of reward models and optimizing the fine-tuning of
language models under restricted feedback conditions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.03997v1' target='_blank'>HackAtari: Atari Learning Environments for Robust and Continual
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Quentin Delfosse, Jannis Blüml, Bjarne Gregori, Kristian Kersting</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-06 12:17:05</h6>
<p class='card-text'>Artificial agents' adaptability to novelty and alignment with intended
behavior is crucial for their effective deployment. Reinforcement learning (RL)
leverages novelty as a means of exploration, yet agents often struggle to
handle novel situations, hindering generalization. To address these issues, we
propose HackAtari, a framework introducing controlled novelty to the most
common RL benchmark, the Atari Learning Environment. HackAtari allows us to
create novel game scenarios (including simplification for curriculum learning),
to swap the game elements' colors, as well as to introduce different reward
signals for the agent. We demonstrate that current agents trained on the
original environments include robustness failures, and evaluate HackAtari's
efficacy in enhancing RL agents' robustness and aligning behavior through
experiments using C51 and PPO. Overall, HackAtari can be used to improve the
robustness of current and future RL algorithms, allowing Neuro-Symbolic RL,
curriculum RL, causal RL, as well as LLM-driven RL. Our work underscores the
significance of developing interpretable in RL agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.03949v2' target='_blank'>UltraMedical: Building Specialized Generalists in Biomedicine</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding, Zhang-Ren Chen, Zhiyuan Ma, Haoxin Li, Ganqu Cui, Biqing Qi, Xuekai Zhu, Xingtai Lv, Hu Jinfang, Zhiyuan Liu, Bowen Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-06 10:50:26</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated remarkable capabilities across
various domains and are moving towards more specialized areas. Recent advanced
proprietary models such as GPT-4 and Gemini have achieved significant
advancements in biomedicine, which have also raised privacy and security
challenges. The construction of specialized generalists hinges largely on
high-quality datasets, enhanced by techniques like supervised fine-tuning and
reinforcement learning from human or AI feedback, and direct preference
optimization. However, these leading technologies (e.g., preference learning)
are still significantly limited in the open source community due to the
scarcity of specialized data. In this paper, we present the UltraMedical
collections, which consist of high-quality manual and synthetic datasets in the
biomedicine domain, featuring preference annotations across multiple advanced
LLMs. By utilizing these datasets, we fine-tune a suite of specialized medical
models based on Llama-3 series, demonstrating breathtaking capabilities across
various medical benchmarks. Moreover, we develop powerful reward models skilled
in biomedical and general reward benchmark, enhancing further online preference
learning within the biomedical LLM community. Datasets and models are available
at https://github.com/TsinghuaC3I/UltraMedical</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.03816v3' target='_blank'>ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, Jie Tang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-06 07:40:00</h6>
<p class='card-text'>Recent methodologies in LLM self-training mostly rely on LLM generating
responses and filtering those with correct output answers as training data.
This approach often yields a low-quality fine-tuning training set (e.g.,
incorrect plans or intermediate reasoning). In this paper, we develop a
reinforced self-training approach, called ReST-MCTS*, based on integrating
process reward guidance with tree search MCTS* for collecting higher-quality
reasoning traces as well as per-step value to train policy and reward models.
ReST-MCTS* circumvents the per-step manual annotation typically used to train
process rewards by tree-search-based reinforcement learning: Given oracle final
correct answers, ReST-MCTS* is able to infer the correct process rewards by
estimating the probability this step can help lead to the correct answer. These
inferred rewards serve dual purposes: they act as value targets for further
refining the process reward model and also facilitate the selection of
high-quality traces for policy model self-training. We first show that the
tree-search policy in ReST-MCTS* achieves higher accuracy compared with prior
LLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same
search budget. We then show that by using traces searched by this tree-search
policy as training data, we can continuously enhance the three language models
for multiple iterations, and outperform other self-training algorithms such as
ReST$^\text{EM}$ and Self-Rewarding LM. We release all code at
https://github.com/THUDM/ReST-MCTS.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.03600v1' target='_blank'>Knowledge-Infused Legal Wisdom: Navigating LLM Consultation through the
  Lens of Diagnostics and Positive-Unlabeled Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yang Wu, Chenghao Wang, Ece Gumusel, Xiaozhong Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-05 19:47:35</h6>
<p class='card-text'>The integration of generative Large Language Models (LLMs) into various
applications, including the legal domain, has been accelerated by their
expansive and versatile nature. However, when facing a legal case, users
without a legal background often struggle to formulate professional queries and
may inadvertently overlook critical legal factors when presenting their case
narrative to LLMs. To address this issue, we propose the Diagnostic Legal Large
Language Model (D3LM), which utilizes adaptive lawyer-like diagnostic questions
to collect additional case information and then provides high-quality feedback.
D3LM incorporates an innovative graph-based Positive-Unlabeled Reinforcement
Learning (PURL) algorithm, enabling the generation of critical questions and
enhancing user-LLM interactions. Moreover, an integrated LLM-based stopping
criterion facilitates precise Court Views Generation (CVG). Our research also
introduces a new English-language CVG dataset based on the US case law
database, enriching the realm of LLM research and deployment with a vital
dimension. D3LM surpasses classical LLMs by delivering outstanding performance
and a remarkable user experience in the legal domain.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.03397v1' target='_blank'>Automating Turkish Educational Quiz Generation Using Large Language
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kamyar Zeinalipour, Yusuf Gökberk Keptiğ, Marco Maggini, Marco Gori</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-05 15:54:50</h6>
<p class='card-text'>Crafting quizzes from educational content is a pivotal activity that benefits
both teachers and students by reinforcing learning and evaluating
understanding. In this study, we introduce a novel approach to generate quizzes
from Turkish educational texts, marking a pioneering endeavor in educational
technology specifically tailored to the Turkish educational context. We present
a specialized dataset, named the Turkish-Quiz-Instruct, comprising an extensive
collection of Turkish educational texts accompanied by multiple-choice and
short-answer quizzes. This research leverages the capabilities of Large
Language Models (LLMs), including GPT-4-Turbo, GPT-3.5-Turbo,
Llama-2-7b-chat-hf, and Llama-2-13b-chat-hf, to automatically generate quiz
questions and answers from the Turkish educational content. Our work delineates
the methodology for employing these LLMs in the context of Turkish educational
material, thereby opening new avenues for automated Turkish quiz generation.
The study not only demonstrates the efficacy of using such models for
generating coherent and relevant quiz content but also sets a precedent for
future research in the domain of automated educational content creation for
languages other than English. The Turkish-Quiz-Instruct dataset is introduced
as a valuable resource for researchers and practitioners aiming to explore the
boundaries of educational technology and language-specific applications of LLMs
in Turkish. By addressing the challenges of quiz generation in a non-English
context specifically Turkish, this study contributes significantly to the field
of Turkish educational technology, providing insights into the potential of
leveraging LLMs for educational purposes across diverse linguistic landscapes.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.03363v1' target='_blank'>LLM-based Rewriting of Inappropriate Argumentation using Reinforcement
  Learning from Machine Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Timon Ziegenbein, Gabriella Skitalinskaya, Alireza Bayat Makou, Henning Wachsmuth</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-05 15:18:08</h6>
<p class='card-text'>Ensuring that online discussions are civil and productive is a major
challenge for social media platforms. Such platforms usually rely both on users
and on automated detection tools to flag inappropriate arguments of other
users, which moderators then review. However, this kind of post-hoc moderation
is expensive and time-consuming, and moderators are often overwhelmed by the
amount and severity of flagged content. Instead, a promising alternative is to
prevent negative behavior during content creation. This paper studies how
inappropriate language in arguments can be computationally mitigated. We
propose a reinforcement learning-based rewriting approach that balances content
preservation and appropriateness based on existing classifiers, prompting an
instruction-finetuned large language model (LLM) as our initial policy. Unlike
related style transfer tasks, rewriting inappropriate arguments allows deleting
and adding content permanently. It is therefore tackled on document level
rather than sentence level. We evaluate different weighting schemes for the
reward function in both absolute and relative human assessment studies.
Systematic experiments on non-parallel data provide evidence that our approach
can mitigate the inappropriateness of arguments while largely preserving their
content. It significantly outperforms competitive baselines, including few-shot
learning, prompting, and humans.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.03030v1' target='_blank'>From Tarzan to Tolkien: Controlling the Language Proficiency Level of
  LLMs for Content Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ali Malik, Stephen Mayhew, Chris Piech, Klinton Bicknell</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-05 07:57:17</h6>
<p class='card-text'>We study the problem of controlling the difficulty level of text generated by
Large Language Models (LLMs) for contexts where end-users are not fully
proficient, such as language learners. Using a novel framework, we evaluate the
effectiveness of several key approaches for this task, including few-shot
prompting, supervised finetuning, and reinforcement learning (RL), utilising
both GPT-4 and open source alternatives like LLama2-7B and Mistral-7B.
  Our findings reveal a large performance gap between GPT-4 and the open source
models when using prompt-based strategies. However, we show how to bridge this
gap with a careful combination of finetuning and RL alignment. Our best model,
CALM (CEFR-Aligned Language Model), surpasses the performance of GPT-4 and
other strategies, at only a fraction of the cost. We further validate the
quality of our results through a small-scale human study.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.02903v1' target='_blank'>Open Grounded Planning: Challenges and Benchmark Construction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shiguang Guo, Ziliang Deng, Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-05 03:46:52</h6>
<p class='card-text'>The emergence of large language models (LLMs) has increasingly drawn
attention to the use of LLMs for human-like planning. Existing work on
LLM-based planning either focuses on leveraging the inherent language
generation capabilities of LLMs to produce free-style plans, or employs
reinforcement learning approaches to learn decision-making for a limited set of
actions within restricted environments. However, both approaches exhibit
significant discrepancies from the open and executable requirements in
real-world planning. In this paper, we propose a new planning task--open
grounded planning. The primary objective of open grounded planning is to ask
the model to generate an executable plan based on a variable action set,
thereby ensuring the executability of the produced plan. To this end, we
establishes a benchmark for open grounded planning spanning a wide range of
domains. Then we test current state-of-the-art LLMs along with five planning
approaches, revealing that existing LLMs and methods still struggle to address
the challenges posed by grounded planning in open domains. The outcomes of this
paper define and establish a foundational dataset for open grounded planning,
and shed light on the potential challenges and future directions of LLM-based
planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.02900v2' target='_blank'>Scaling Laws for Reward Model Overoptimization in Direct Alignment
  Algorithms</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rafael Rafailov, Yaswanth Chittepu, Ryan Park, Harshit Sikchi, Joey Hejna, Bradley Knox, Chelsea Finn, Scott Niekum</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-05 03:41:37</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) has been crucial to the
recent success of Large Language Models (LLMs), however, it is often a complex
and brittle process. In the classical RLHF framework, a reward model is first
trained to represent human preferences, which is in turn used by an online
reinforcement learning (RL) algorithm to optimize the LLM. A prominent issue
with such methods is reward over-optimization or reward hacking, where
performance as measured by the learned proxy reward model increases, but true
quality plateaus or even deteriorates. Direct Alignment Algorithms (DDAs) like
Direct Preference Optimization have emerged as alternatives to the classical
RLHF pipeline by circumventing the reward modeling phase. However, although
DAAs do not use a separate proxy reward model, they still commonly deteriorate
from over-optimization. While the so-called reward hacking phenomenon is not
well-defined for DAAs, we still uncover similar trends: at higher KL budgets,
DAA algorithms exhibit similar degradation patterns to their classic RLHF
counterparts. In particular, we find that DAA methods deteriorate not only
across a wide range of KL budgets but also often before even a single epoch of
the dataset is completed. Through extensive empirical experimentation, this
work formulates and formalizes the reward over-optimization or hacking problem
for DAAs and explores its consequences across objectives, training regimes, and
model scales.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.02764v1' target='_blank'>Adaptive Preference Scaling for Reinforcement Learning with Human
  Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ilgee Hong, Zichong Li, Alexander Bukharin, Yixiao Li, Haoming Jiang, Tianbao Yang, Tuo Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-04 20:33:22</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) is a prevalent approach to
align AI systems with human values by learning rewards from human preference
data. Due to various reasons, however, such data typically takes the form of
rankings over pairs of trajectory segments, which fails to capture the varying
strengths of preferences across different pairs. In this paper, we propose a
novel adaptive preference loss, underpinned by distributionally robust
optimization (DRO), designed to address this uncertainty in preference
strength. By incorporating an adaptive scaling parameter into the loss for each
pair, our method increases the flexibility of the reward function.
Specifically, it assigns small scaling parameters to pairs with ambiguous
preferences, leading to more comparable rewards, and large scaling parameters
to those with clear preferences for more distinct rewards. Computationally, our
proposed loss function is strictly convex and univariate with respect to each
scaling parameter, enabling its efficient optimization through a simple
second-order algorithm. Our method is versatile and can be readily adapted to
various preference optimization frameworks, including direct preference
optimization (DPO). Our experiments with robotic control and natural language
generation with large language models (LLMs) show that our method not only
improves policy performance but also aligns reward function selection more
closely with policy optimization, simplifying the hyperparameter tuning
process.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.02756v1' target='_blank'>Aligning Large Language Models via Fine-grained Supervision</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dehong Xu, Liang Qiu, Minseok Kim, Faisal Ladhak, Jaeyoung Do</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-04 20:21:45</h6>
<p class='card-text'>Pre-trained large-scale language models (LLMs) excel at producing coherent
articles, yet their outputs may be untruthful, toxic, or fail to align with
user expectations. Current approaches focus on using reinforcement learning
with human feedback (RLHF) to improve model alignment, which works by
transforming coarse human preferences of LLM outputs into a feedback signal
that guides the model learning process. However, because this approach operates
on sequence-level feedback, it lacks the precision to identify the exact parts
of the output affecting user preferences. To address this gap, we propose a
method to enhance LLM alignment through fine-grained token-level supervision.
Specifically, we ask annotators to minimally edit less preferred responses
within the standard reward modeling dataset to make them more favorable,
ensuring changes are made only where necessary while retaining most of the
original content. The refined dataset is used to train a token-level reward
model, which is then used for training our fine-grained Proximal Policy
Optimization (PPO) model. Our experiment results demonstrate that this approach
can achieve up to an absolute improvement of $5.1\%$ in LLM performance, in
terms of win rate against the reference model, compared with the traditional
PPO model.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.01931v2' target='_blank'>Dishonesty in Helpful and Harmless Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Youcheng Huang, Jingkun Tang, Duanyu Feng, Zheng Zhang, Wenqiang Lei, Jiancheng Lv, Anthony G. Cohn</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-04 03:31:09</h6>
<p class='card-text'>People tell lies when seeking rewards. Large language models (LLMs) are
aligned to human values with reinforcement learning where they get rewards if
they satisfy human preference. We find that this also induces dishonesty in
helpful and harmless alignment where LLMs tell lies in generating harmless
responses. Using the latest interpreting tools, we detect dishonesty, show how
LLMs can be harmful if their honesty is increased, and analyze such conflicts
at the parameter-level. Given these preliminaries and the hypothesis that
reward-seeking stimulates dishonesty, we theoretically show that the dishonesty
can in-turn decrease the alignment performances and augment reward-seeking
alignment with representation regularization. Extensive results, including
GPT-4 annotated win-rates, perplexities, and cases studies demonstrate that we
can train more honest, helpful, and harmless LLMs. We will make all our codes
and results be open-sourced upon this paper's acceptance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.01514v3' target='_blank'>Decoupled Alignment for Robust Plug-and-Play Adaptation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haozheng Luo, Jiahao Yu, Wenxin Zhang, Jialong Li, Jerry Yao-Chieh Hu, Xinyu Xing, Han Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-03 16:46:18</h6>
<p class='card-text'>We introduce a low-resource safety enhancement method for aligning large
language models (LLMs) without the need for supervised fine-tuning (SFT) or
reinforcement learning from human feedback (RLHF). Our main idea is to exploit
knowledge distillation to extract the alignment information from existing
well-aligned LLMs and integrate it into unaligned LLMs in a plug-and-play
fashion. Methodology, we employ delta debugging to identify the critical
components of knowledge necessary for effective distillation. On the harmful
question dataset, our method significantly enhances the average defense success
rate by approximately 14.41%, reaching as high as 51.39%, in 17 unaligned
pre-trained LLMs, without compromising performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.01462v2' target='_blank'>The Importance of Online Data: Understanding Preference Fine-tuning via
  Coverage</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuda Song, Gokul Swamy, Aarti Singh, J. Andrew Bagnell, Wen Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-03 15:51:04</h6>
<p class='card-text'>Learning from human preference data has emerged as the dominant paradigm for
fine-tuning large language models (LLMs). The two most common families of
techniques -- online reinforcement learning (RL) such as Proximal Policy
Optimization (PPO) and offline contrastive methods such as Direct Preference
Optimization (DPO) -- were positioned as equivalent in prior work due to the
fact that both have to start from the same offline preference dataset. To
further expand our theoretical understanding of the similarities and
differences between online and offline techniques for preference fine-tuning,
we conduct a rigorous analysis through the lens of dataset coverage, a concept
that captures how the training data covers the test distribution and is widely
used in RL. We prove that a global coverage condition is both necessary and
sufficient for offline contrastive methods to converge to the optimal policy,
but a weaker partial coverage condition suffices for online RL methods. This
separation provides one explanation of why online RL methods can perform better
than offline methods, especially when the offline preference data is not
diverse enough. Finally, motivated by our preceding theoretical observations,
we derive a hybrid preference optimization (HyPO) algorithm that uses offline
data for contrastive-based preference optimization and online data for KL
regularization. Theoretically and empirically, we demonstrate that HyPO is more
performant than its pure offline counterpart DPO, while still preserving its
computation and memory efficiency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.01309v2' target='_blank'>REvolve: Reward Evolution with Large Language Models using Human
  Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rishi Hazra, Alkis Sygkounas, Andreas Persson, Amy Loutfi, Pedro Zuidberg Dos Martires</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-03 13:23:27</h6>
<p class='card-text'>Designing effective reward functions is crucial to training reinforcement
learning (RL) algorithms. However, this design is non-trivial, even for domain
experts, due to the subjective nature of certain tasks that are hard to
quantify explicitly. In recent works, large language models (LLMs) have been
used for reward generation from natural language task descriptions, leveraging
their extensive instruction tuning and commonsense understanding of human
behavior. In this work, we hypothesize that LLMs, guided by human feedback, can
be used to formulate reward functions that reflect human implicit knowledge. We
study this in three challenging settings -- autonomous driving, humanoid
locomotion, and dexterous manipulation -- wherein notions of ``good" behavior
are tacit and hard to quantify. To this end, we introduce REvolve, a truly
evolutionary framework that uses LLMs for reward design in RL. REvolve
generates and refines reward functions by utilizing human feedback to guide the
evolution process, effectively translating implicit human knowledge into
explicit reward functions for training (deep) RL agents. Experimentally, we
demonstrate that agents trained on REvolve-designed rewards outperform other
state-of-the-art baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.02616v5' target='_blank'>Adaptive Layer Splitting for Wireless LLM Inference in Edge Computing: A
  Model-Based Reinforcement Learning Approach</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuxuan Chen, Rongpeng Li, Xiaoxue Yu, Zhifeng Zhao, Honggang Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-03 09:41:42</h6>
<p class='card-text'>Optimizing the deployment of large language models (LLMs) in edge computing
environments is critical for enhancing privacy and computational efficiency.
Toward efficient wireless LLM inference in edge computing, this study
comprehensively analyzes the impact of different splitting points in mainstream
open-source LLMs. On this basis, this study introduces a framework taking
inspiration from model-based reinforcement learning (MBRL) to determine the
optimal splitting point across the edge and user equipment (UE). By
incorporating a reward surrogate model, our approach significantly reduces the
computational cost of frequent performance evaluations. Extensive simulations
demonstrate that this method effectively balances inference performance and
computational load under varying network conditions, providing a robust
solution for LLM deployment in decentralized settings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.00974v1' target='_blank'>Large Language Model Assisted Optimal Bidding of BESS in FCAS Market: An
  AI-agent based Approach</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Borui Zhang, Chaojie Li, Guo Chen, Zhaoyang Dong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-03 04:04:18</h6>
<p class='card-text'>To incentivize flexible resources such as Battery Energy Storage Systems
(BESSs) to offer Frequency Control Ancillary Services (FCAS), Australia's
National Electricity Market (NEM) has implemented changes in recent years
towards shorter-term bidding rules and faster service requirements. However,
firstly, existing bidding optimization methods often overlook or oversimplify
the key aspects of FCAS market procedures, resulting in an inaccurate depiction
of the market bidding process. Thus, the BESS bidding problem is modeled based
on the actual bidding records and the latest market specifications and then
formulated as a deep reinforcement learning (DRL) problem. Secondly, the
erratic decisions of the DRL agent caused by imperfectly predicted market
information increases the risk of profit loss. Hence, a Conditional Value at
Risk (CVaR)-based DRL algorithm is developed to enhance the risk resilience of
bidding strategies. Thirdly, well-trained DRL models still face performance
decline in uncommon scenarios during online operations. Therefore, a Large
Language Models (LLMs)-assisted artificial intelligence (AI)-agent interactive
decision-making framework is proposed to improve the strategy timeliness,
reliability and interpretability in uncertain new scenarios, where conditional
hybrid decision and self-reflection mechanisms are designed to address LLMs'
hallucination challenge. The experiment results demonstrate that our proposed
framework has higher bidding profitability compared to the baseline methods by
effectively mitigating the profit loss caused by various uncertainties.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.06560v1' target='_blank'>Inverse Constitutional AI: Compressing Preferences into Principles</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Arduin Findeis, Timo Kaufmann, Eyke Hüllermeier, Samuel Albanie, Robert Mullins</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-02 11:54:50</h6>
<p class='card-text'>Feedback data plays an important role in fine-tuning and evaluating
state-of-the-art AI models. Often pairwise text preferences are used: given two
texts, human (or AI) annotators select the "better" one. Such feedback data is
widely used to align models to human preferences (e.g., reinforcement learning
from human feedback), or to rank models according to human preferences (e.g.,
Chatbot Arena). Despite its wide-spread use, prior work has demonstrated that
human-annotated pairwise text preference data often exhibits unintended biases.
For example, human annotators have been shown to prefer assertive over truthful
texts in certain contexts. Models trained or evaluated on this data may
implicitly encode these biases in a manner hard to identify. In this paper, we
formulate the interpretation of existing pairwise text preference data as a
compression task: the Inverse Constitutional AI (ICAI) problem. In
constitutional AI, a set of principles (or constitution) is used to provide
feedback and fine-tune AI models. The ICAI problem inverts this process: given
a dataset of feedback, we aim to extract a constitution that best enables a
large language model (LLM) to reconstruct the original annotations. We propose
a corresponding initial ICAI algorithm and validate its generated constitutions
quantitatively based on reconstructed annotations. Generated constitutions have
many potential use-cases -- they may help identify undesirable biases, scale
feedback to unseen data or assist with adapting LLMs to individual user
preferences. We demonstrate our approach on a variety of datasets: (a)
synthetic feedback datasets with known underlying principles; (b) the
AlpacaEval dataset of cross-annotated human feedback; and (c) the crowdsourced
Chatbot Arena data set. We release the code for our algorithm and experiments
at https://github.com/rdnfn/icai .</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.01631v2' target='_blank'>SUBER: An RL Environment with Simulated Human Behavior for Recommender
  Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nathan Corecco, Giorgio Piatti, Luca A. Lanzendörfer, Flint Xiaofeng Fan, Roger Wattenhofer</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-01 11:56:08</h6>
<p class='card-text'>Reinforcement learning (RL) has gained popularity in the realm of recommender
systems due to its ability to optimize long-term rewards and guide users in
discovering relevant content. However, the successful implementation of RL in
recommender systems is challenging because of several factors, including the
limited availability of online data for training on-policy methods. This
scarcity requires expensive human interaction for online model training.
Furthermore, the development of effective evaluation frameworks that accurately
reflect the quality of models remains a fundamental challenge in recommender
systems. To address these challenges, we propose a comprehensive framework for
synthetic environments that simulate human behavior by harnessing the
capabilities of large language models (LLMs). We complement our framework with
in-depth ablation studies and demonstrate its effectiveness with experiments on
movie and book recommendations. Using LLMs as synthetic users, this work
introduces a modular and novel framework to train RL-based recommender systems.
The software, including the RL environment, is publicly available on GitHub.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.00222v1' target='_blank'>Learning to Clarify: Multi-turn Conversations with Action-Based
  Contrastive Self-Training</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Maximillian Chen, Ruoxi Sun, Sercan Ö. Arık, Tomas Pfister</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-31 22:44:48</h6>
<p class='card-text'>Large language models (LLMs) aligned through reinforcement learning from
human feedback (RLHF) have quickly become one of the dominant paradigms for
building intelligent conversational assistant agents. However, despite their
strong performance across many benchmarks, LLM-based agents still lack
conversational skills such as disambiguation: when generalized assistants are
faced with ambiguity, they often overhedge or implicitly guess users'
ground-truth intents rather than asking clarification questions, and under
task-specific settings, high-quality conversation samples are often limited,
affecting models' ability to learn optimal dialogue action policies. We propose
Action-Based Contrastive Self-Training (henceforth ACT), a quasi-online
preference optimization algorithm based on Direct Preference Optimization (DPO)
which allows for sample-efficient dialogue policy learning in multi-turn
conversation. We demonstrate ACT's efficacy under sample-efficient conditions
in three difficult conversational tasks: tabular-grounded question-answering,
machine reading comprehension, and AmbigSQL, a novel task for disambiguating
information-seeking requests for text-to-SQL generation. Additionally, we
propose evaluating LLMs' ability to function as conversational agents by
examining whether they can implicitly recognize and reason about ambiguity in
conversation. ACT demonstrates substantial conversation modeling improvements
over standard approaches to supervised fine-tuning and DPO.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.21040v1' target='_blank'>Direct Alignment of Language Models via Quality-Aware Self-Refinement</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Runsheng Yu, Yong Wang, Xiaoqi Jiao, Youzhi Zhang, James T. Kwok</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-31 17:31:18</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) has been commonly used to
align the behaviors of Large Language Models (LLMs) with human preferences.
Recently, a popular alternative is Direct Policy Optimization (DPO), which
replaces an LLM-based reward model with the policy itself, thus obviating the
need for extra memory and training time to learn the reward model. However, DPO
does not consider the relative qualities of the positive and negative
responses, and can lead to sub-optimal training outcomes. To alleviate this
problem, we investigate the use of intrinsic knowledge within the on-the-fly
fine-tuning LLM to obtain relative qualities and help to refine the loss
function. Specifically, we leverage the knowledge of the LLM to design a
refinement function to estimate the quality of both the positive and negative
responses. We show that the constructed refinement function can help
self-refine the loss function under mild assumptions. The refinement function
is integrated into DPO and its variant Identity Policy Optimization (IPO).
Experiments across various evaluators indicate that they can improve the
performance of the fine-tuned models over DPO and IPO.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.20974v3' target='_blank'>SaySelf: Teaching LLMs to Express Confidence with Self-Reflective
  Rationales</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tianyang Xu, Shujin Wu, Shizhe Diao, Xiaoze Liu, Xingyao Wang, Yangyi Chen, Jing Gao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-31 16:21:16</h6>
<p class='card-text'>Large language models (LLMs) often generate inaccurate or fabricated
information and generally fail to indicate their confidence, which limits their
broader applications. Previous work elicits confidence from LLMs by direct or
self-consistency prompting, or constructing specific datasets for supervised
finetuning. The prompting-based approaches have inferior performance, and the
training-based approaches are limited to binary or inaccurate group-level
confidence estimates. In this work, we present the advanced SaySelf, a training
framework that teaches LLMs to express more accurate fine-grained confidence
estimates. In addition, beyond the confidence scores, SaySelf initiates the
process of directing LLMs to produce self-reflective rationales that clearly
identify gaps in their parametric knowledge and explain their uncertainty. This
is achieved by using an LLM to automatically summarize the uncertainties in
specific knowledge via natural language. The summarization is based on the
analysis of the inconsistency in multiple sampled reasoning chains, and the
resulting data is utilized for supervised fine-tuning. Moreover, we utilize
reinforcement learning with a meticulously crafted reward function to calibrate
the confidence estimates, motivating LLMs to deliver accurate, high-confidence
predictions and to penalize overconfidence in erroneous outputs. Experimental
results in both in-distribution and out-of-distribution datasets demonstrate
the effectiveness of SaySelf in reducing the confidence calibration error and
maintaining the task performance. We show that the generated self-reflective
rationales are reasonable and can further contribute to the calibration. The
code is made public at https://github.com/xu1868/SaySelf.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.20971v2' target='_blank'>Amortizing intractable inference in diffusion models for vision,
  language, and control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Siddarth Venkatraman, Moksh Jain, Luca Scimeca, Minsu Kim, Marcin Sendera, Mohsin Hasan, Luke Rowe, Sarthak Mittal, Pablo Lemos, Emmanuel Bengio, Alexandre Adam, Jarrid Rector-Brooks, Yoshua Bengio, Glen Berseth, Nikolay Malkin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-31 16:18:46</h6>
<p class='card-text'>Diffusion models have emerged as effective distribution estimators in vision,
language, and reinforcement learning, but their use as priors in downstream
tasks poses an intractable posterior inference problem. This paper studies
amortized sampling of the posterior over data, $\mathbf{x}\sim p^{\rm
post}(\mathbf{x})\propto p(\mathbf{x})r(\mathbf{x})$, in a model that consists
of a diffusion generative model prior $p(\mathbf{x})$ and a black-box
constraint or likelihood function $r(\mathbf{x})$. We state and prove the
asymptotic correctness of a data-free learning objective, relative trajectory
balance, for training a diffusion model that samples from this posterior, a
problem that existing methods solve only approximately or in restricted cases.
Relative trajectory balance arises from the generative flow network perspective
on diffusion models, which allows the use of deep reinforcement learning
techniques to improve mode coverage. Experiments illustrate the broad potential
of unbiased inference of arbitrary posteriors under diffusion priors: in vision
(classifier guidance), language (infilling under a discrete diffusion LLM), and
multimodal data (text-to-image generation). Beyond generative modeling, we
apply relative trajectory balance to the problem of continuous control with a
score-based behavior prior, achieving state-of-the-art results on benchmarks in
offline reinforcement learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.20304v1' target='_blank'>Group Robust Preference Optimization in Reward-free RLHF</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shyam Sundhar Ramesh, Yifan Hu, Iason Chaimalas, Viraj Mehta, Pier Giuseppe Sessa, Haitham Bou Ammar, Ilija Bogunovic</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-30 17:50:04</h6>
<p class='card-text'>Adapting large language models (LLMs) for specific tasks usually involves
fine-tuning through reinforcement learning with human feedback (RLHF) on
preference data. While these data often come from diverse labelers' groups
(e.g., different demographics, ethnicities, company teams, etc.), traditional
RLHF approaches adopt a "one-size-fits-all" approach, i.e., they
indiscriminately assume and optimize a single preference model, thus not being
robust to unique characteristics and needs of the various groups. To address
this limitation, we propose a novel Group Robust Preference Optimization (GRPO)
method to align LLMs to individual groups' preferences robustly. Our approach
builds upon reward-free direct preference optimization methods, but unlike
previous approaches, it seeks a robust policy which maximizes the worst-case
group performance. To achieve this, GRPO adaptively and sequentially weights
the importance of different groups, prioritizing groups with worse cumulative
loss. We theoretically study the feasibility of GRPO and analyze its
convergence for the log-linear policy class. By fine-tuning LLMs with GRPO
using diverse group-based global opinion data, we significantly improved
performance for the worst-performing groups, reduced loss imbalances across
groups, and improved probability accuracies compared to non-robust baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.20253v1' target='_blank'>Evaluating Large Language Model Biases in Persona-Steered Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Andy Liu, Mona Diab, Daniel Fried</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-30 17:06:03</h6>
<p class='card-text'>The task of persona-steered text generation requires large language models
(LLMs) to generate text that reflects the distribution of views that an
individual fitting a persona could have. People have multifaceted personas, but
prior work on bias in LLM-generated opinions has only explored multiple-choice
settings or one-dimensional personas. We define an incongruous persona as a
persona with multiple traits where one trait makes its other traits less likely
in human survey data, e.g. political liberals who support increased military
spending. We find that LLMs are 9.7% less steerable towards incongruous
personas than congruous ones, sometimes generating the stereotypical stance
associated with its demographic rather than the target stance. Models that we
evaluate that are fine-tuned with Reinforcement Learning from Human Feedback
(RLHF) are more steerable, especially towards stances associated with political
liberals and women, but present significantly less diverse views of personas.
We also find variance in LLM steerability that cannot be predicted from
multiple-choice opinion evaluation. Our results show the importance of
evaluating models in open-ended text generation, as it can surface new LLM
opinion biases. Moreover, such a setup can shed light on our ability to steer
models toward a richer and more diverse range of viewpoints.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.20175v1' target='_blank'>InstructionCP: A fast approach to transfer Large Language Models into
  target language</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kuang-Ming Chen, Hung-yi Lee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-30 15:45:13</h6>
<p class='card-text'>The rapid development of large language models (LLMs) in recent years has
largely focused on English, resulting in models that respond exclusively in
English. To adapt these models to other languages, continual pre-training (CP)
is often employed, followed by supervised fine-tuning (SFT) to maintain
conversational abilities. However, CP and SFT can reduce a model's ability to
filter harmful content. We propose Instruction Continual Pre-training (InsCP),
which integrates instruction tags into the CP process to prevent loss of
conversational proficiency while acquiring new languages. Our experiments
demonstrate that InsCP retains conversational and Reinforcement Learning from
Human Feedback (RLHF) abilities. Empirical evaluations on language alignment,
reliability, and knowledge benchmarks confirm the efficacy of InsCP. Notably,
this approach requires only 0.1 billion tokens of high-quality
instruction-following data, thereby reducing resource consumption.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.19946v2' target='_blank'>Learning to Discuss Strategically: A Case Study on One Night Ultimate
  Werewolf</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xuanfa Jin, Ziyan Wang, Yali Du, Meng Fang, Haifeng Zhang, Jun Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-30 11:07:06</h6>
<p class='card-text'>Communication is a fundamental aspect of human society, facilitating the
exchange of information and beliefs among people. Despite the advancements in
large language models (LLMs), recent agents built with these often neglect the
control over discussion tactics, which are essential in communication scenarios
and games. As a variant of the famous communication game Werewolf, One Night
Ultimate Werewolf (ONUW) requires players to develop strategic discussion
policies due to the potential role changes that increase the uncertainty and
complexity of the game. In this work, we first present the existence of the
Perfect Bayesian Equilibria (PBEs) in two scenarios of the ONUW game: one with
discussion and one without. The results showcase that the discussion greatly
changes players' utilities by affecting their beliefs, emphasizing the
significance of discussion tactics. Based on the insights obtained from the
analyses, we propose an RL-instructed language agent framework, where a
discussion policy trained by reinforcement learning (RL) is employed to
determine appropriate discussion tactics to adopt. Our experimental results on
several ONUW game settings demonstrate the effectiveness and generalizability
of our proposed framework. The project page of our paper:
$\href{https://one-night-ultimate-werewolf.github.io}{one-night-ultimate-werewolf.github.io}$.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.19883v2' target='_blank'>From Words to Actions: Unveiling the Theoretical Underpinnings of
  LLM-Driven Autonomous Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jianliang He, Siyu Chen, Fengzhuo Zhang, Zhuoran Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-30 09:42:54</h6>
<p class='card-text'>In this work, from a theoretical lens, we aim to understand why large
language model (LLM) empowered agents are able to solve decision-making
problems in the physical world. To this end, consider a hierarchical
reinforcement learning (RL) model where the LLM Planner and the Actor perform
high-level task planning and low-level execution, respectively. Under this
model, the LLM Planner navigates a partially observable Markov decision process
(POMDP) by iteratively generating language-based subgoals via prompting. Under
proper assumptions on the pretraining data, we prove that the pretrained LLM
Planner effectively performs Bayesian aggregated imitation learning (BAIL)
through in-context learning. Additionally, we highlight the necessity for
exploration beyond the subgoals derived from BAIL by proving that naively
executing the subgoals returned by LLM leads to a linear regret. As a remedy,
we introduce an $\epsilon$-greedy exploration strategy to BAIL, which is proven
to incur sublinear regret when the pretraining error is small. Finally, we
extend our theoretical framework to include scenarios where the LLM Planner
serves as a world model for inferring the transition model of the environment
and to multi-agent settings, enabling coordination among multiple Actors.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.19763v1' target='_blank'>Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural
  Language Understanding</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kuo Liao, Shuang Li, Meng Zhao, Liqun Liu, Mengge Xue, Zhenyu Hu, Honglin Han, Chengguo Yin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-30 07:19:31</h6>
<p class='card-text'>Recent strides in large language models (LLMs) have yielded remarkable
performance, leveraging reinforcement learning from human feedback (RLHF) to
significantly enhance generation and alignment capabilities. However, RLHF
encounters numerous challenges, including the objective mismatch issue, leading
to suboptimal performance in Natural Language Understanding (NLU) tasks. To
address this limitation, we propose a novel Reinforcement Learning framework
enhanced with Label-sensitive Reward (RLLR) to amplify the performance of LLMs
in NLU tasks. By incorporating label-sensitive pairs into reinforcement
learning, our method aims to adeptly capture nuanced label-sensitive semantic
features during RL, thereby enhancing natural language understanding.
Experiments conducted on five diverse foundation models across eight tasks
showcase promising results. In comparison to Supervised Fine-tuning models
(SFT), RLLR demonstrates an average performance improvement of 1.54%. Compared
with RLHF models, the improvement averages at 0.69%. These results reveal the
effectiveness of our method for LLMs in NLU tasks. Code and data available at:
https://github.com/MagiaSN/ACL2024_RLLR.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.19550v1' target='_blank'>Stress-Testing Capability Elicitation With Password-Locked Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ryan Greenblatt, Fabien Roger, Dmitrii Krasheninnikov, David Krueger</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-29 22:26:26</h6>
<p class='card-text'>To determine the safety of large language models (LLMs), AI developers must
be able to assess their dangerous capabilities. But simple prompting strategies
often fail to elicit an LLM's full capabilities. One way to elicit capabilities
more robustly is to fine-tune the LLM to complete the task. In this paper, we
investigate the conditions under which fine-tuning-based elicitation suffices
to elicit capabilities. To do this, we introduce password-locked models, LLMs
fine-tuned such that some of their capabilities are deliberately hidden.
Specifically, these LLMs are trained to exhibit these capabilities only when a
password is present in the prompt, and to imitate a much weaker LLM otherwise.
Password-locked models enable a novel method of evaluating capabilities
elicitation methods, by testing whether these password-locked capabilities can
be elicited without using the password. We find that a few high-quality
demonstrations are often sufficient to fully elicit password-locked
capabilities. More surprisingly, fine-tuning can elicit other capabilities that
have been locked using the same password, or even different passwords.
Furthermore, when only evaluations, and not demonstrations, are available,
approaches like reinforcement learning are still often able to elicit
capabilities. Overall, our findings suggest that fine-tuning is an effective
method of eliciting hidden capabilities of current models, but may be
unreliable when high-quality demonstrations are not available, e.g. as may be
the case when models' (hidden) capabilities exceed those of human
demonstrators.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.19332v3' target='_blank'>Self-Exploring Language Models: Active Preference Elicitation for Online
  Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shenao Zhang, Donghan Yu, Hiteshi Sharma, Han Zhong, Zhihan Liu, Ziyi Yang, Shuohang Wang, Hany Hassan, Zhaoran Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-29 17:59:07</h6>
<p class='card-text'>Preference optimization, particularly through Reinforcement Learning from
Human Feedback (RLHF), has achieved significant success in aligning Large
Language Models (LLMs) to adhere to human intentions. Unlike offline alignment
with a fixed dataset, online feedback collection from humans or AI on model
generations typically leads to more capable reward models and better-aligned
LLMs through an iterative process. However, achieving a globally accurate
reward model requires systematic exploration to generate diverse responses that
span the vast space of natural language. Random sampling from standard
reward-maximizing LLMs alone is insufficient to fulfill this requirement. To
address this issue, we propose a bilevel objective optimistically biased
towards potentially high-reward responses to actively explore
out-of-distribution regions. By solving the inner-level problem with the
reparameterized reward function, the resulting algorithm, named Self-Exploring
Language Models (SELM), eliminates the need for a separate RM and iteratively
updates the LLM with a straightforward objective. Compared to Direct Preference
Optimization (DPO), the SELM objective reduces indiscriminate favor of unseen
extrapolations and enhances exploration efficiency. Our experimental results
demonstrate that when fine-tuned on Zephyr-7B-SFT and Llama-3-8B-Instruct
models, SELM significantly boosts the performance on instruction-following
benchmarks such as MT-Bench and AlpacaEval 2.0, as well as various standard
academic benchmarks in different settings. Our code and models are available at
https://github.com/shenao-zhang/SELM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.19320v4' target='_blank'>Value-Incentivized Preference Optimization: A Unified Approach to Online
  and Offline RLHF</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shicong Cen, Jincheng Mei, Katayoon Goshvadi, Hanjun Dai, Tong Yang, Sherry Yang, Dale Schuurmans, Yuejie Chi, Bo Dai</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-29 17:51:42</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) has demonstrated great
promise in aligning large language models (LLMs) with human preference.
Depending on the availability of preference data, both online and offline RLHF
are active areas of investigation. A key bottleneck is understanding how to
incorporate uncertainty estimation in the reward function learned from the
preference data for RLHF, regardless of how the preference data is collected.
While the principles of optimism or pessimism under uncertainty are
well-established in standard reinforcement learning (RL), a
practically-implementable and theoretically-grounded form amenable to large
language models is not yet available, as standard techniques for constructing
confidence intervals become intractable under arbitrary policy
parameterizations.
  In this paper, we introduce a unified approach to online and offline RLHF --
value-incentivized preference optimization (VPO) -- which regularizes the
maximum-likelihood estimate of the reward function with the corresponding value
function, modulated by a $\textit{sign}$ to indicate whether the optimism or
pessimism is chosen. VPO also directly optimizes the policy with implicit
reward modeling, and therefore shares a simpler RLHF pipeline similar to direct
preference optimization. Theoretical guarantees of VPO are provided for both
online and offline settings, matching the rates of their standard RL
counterparts. Moreover, experiments on text summarization and dialog verify the
practicality and effectiveness of VPO.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.19107v1' target='_blank'>Offline Regularised Reinforcement Learning for Large Language Models
  Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pierre Harvey Richemond, Yunhao Tang, Daniel Guo, Daniele Calandriello, Mohammad Gheshlaghi Azar, Rafael Rafailov, Bernardo Avila Pires, Eugene Tarassov, Lucas Spangher, Will Ellsworth, Aliaksei Severyn, Jonathan Mallinson, Lior Shani, Gil Shamir, Rishabh Joshi, Tianqi Liu, Remi Munos, Bilal Piot</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-29 14:11:29</h6>
<p class='card-text'>The dominant framework for alignment of large language models (LLM), whether
through reinforcement learning from human feedback or direct preference
optimisation, is to learn from preference data. This involves building datasets
where each element is a quadruplet composed of a prompt, two independent
responses (completions of the prompt) and a human preference between the two
independent responses, yielding a preferred and a dis-preferred response. Such
data is typically scarce and expensive to collect. On the other hand,
\emph{single-trajectory} datasets where each element is a triplet composed of a
prompt, a response and a human feedback is naturally more abundant. The
canonical element of such datasets is for instance an LLM's response to a
user's prompt followed by a user's feedback such as a thumbs-up/down.
Consequently, in this work, we propose DRO, or \emph{Direct Reward
Optimisation}, as a framework and associated algorithms that do not require
pairwise preferences. DRO uses a simple mean-squared objective that can be
implemented in various ways. We validate our findings empirically, using T5
encoder-decoder language models, and show DRO's performance over selected
baselines such as Kahneman-Tversky Optimization (KTO). Thus, we confirm that
DRO is a simple and empirically compelling method for single-trajectory policy
optimisation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.18952v2' target='_blank'>Are You Sure? Rank Them Again: Repeated Ranking For Better Preference
  Datasets</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Peter Devine</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-29 10:08:31</h6>
<p class='card-text'>Training Large Language Models (LLMs) with Reinforcement Learning from AI
Feedback (RLAIF) aligns model outputs more closely with human preferences. This
involves an evaluator model ranking multiple candidate responses to user
prompts. However, the rankings from popular evaluator models such as GPT-4 can
be inconsistent. We propose the Repeat Ranking method - where we evaluate the
same responses multiple times and train only on those responses which are
consistently ranked. Using 2,714 prompts in 62 languages, we generated
responses from 7 top multilingual LLMs and had GPT-4 rank them five times each.
Evaluating on MT-Bench chat benchmarks in six languages, our method
outperformed the standard practice of training on all available prompts. Our
work highlights the quality versus quantity trade-off in RLAIF dataset
generation and offers a stackable strategy for enhancing dataset and thus model
quality.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.18718v1' target='_blank'>Efficient Model-agnostic Alignment via Bayesian Persuasion</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fengshuo Bai, Mingzhi Wang, Zhaowei Zhang, Boyuan Chen, Yinda Xu, Ying Wen, Yaodong Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-29 02:57:07</h6>
<p class='card-text'>With recent advancements in large language models (LLMs), alignment has
emerged as an effective technique for keeping LLMs consensus with human intent.
Current methods primarily involve direct training through Supervised
Fine-tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF), both of
which require substantial computational resources and extensive ground truth
data. This paper explores an efficient method for aligning black-box large
models using smaller models, introducing a model-agnostic and lightweight
Bayesian Persuasion Alignment framework. We formalize this problem as an
optimization of the signaling strategy from the small model's perspective. In
the persuasion process, the small model (Advisor) observes the information item
(i.e., state) and persuades large models (Receiver) to elicit improved
responses. The Receiver then generates a response based on the input, the
signal from the Advisor, and its updated belief about the information item.
Through training using our framework, we demonstrate that the Advisor can
significantly enhance the performance of various Receivers across a range of
tasks. We theoretically analyze our persuasion framework and provide an upper
bound on the Advisor's regret, confirming its effectiveness in learning the
optimal signaling strategy. Our Empirical results demonstrates that GPT-2 can
significantly improve the performance of various models, achieving an average
enhancement of 16.1% in mathematical reasoning ability and 13.7% in code
generation. We hope our work can provide an initial step toward rethinking the
alignment framework from the Bayesian Persuasion perspective.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.18649v2' target='_blank'>LeDex: Training LLMs to Better Self-Debug and Explain Code</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nan Jiang, Xiaopeng Li, Shiqi Wang, Qiang Zhou, Soneya Binta Hossain, Baishakhi Ray, Varun Kumar, Xiaofei Ma, Anoop Deoras</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-28 23:20:24</h6>
<p class='card-text'>In the domain of code generation, self-debugging is crucial. It allows LLMs
to refine their generated code based on execution feedback. This is
particularly important because generating correct solutions in one attempt
proves challenging for complex tasks. Prior works on self-debugging mostly
focus on prompting methods by providing LLMs with few-shot examples, which work
poorly on small open-sourced LLMs. In this work, we propose LeDex, a training
framework that significantly improves the self-debugging capability of LLMs.
Intuitively, we observe that a chain of explanations on the wrong code followed
by code refinement helps LLMs better analyze the wrong code and do refinement.
We thus propose an automated pipeline to collect a high-quality dataset for
code explanation and refinement by generating a number of explanations and
refinement trajectories from the LLM itself or a larger teacher model and
filtering via execution verification. We perform supervised fine-tuning (SFT)
and further reinforcement learning (RL) on both success and failure
trajectories with a novel reward design considering code explanation and
refinement quality. SFT improves the pass@1 by up to 15.92% and pass@10 by
9.30% over four benchmarks. RL training brings additional up to 3.54%
improvement on pass@1 and 2.55% improvement on pass@10. The trained LLMs show
iterative refinement ability and can keep refining code continuously. Lastly,
our human evaluation shows that the LLMs trained with our framework generate
more useful code explanations and help developers better understand bugs in
source code.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.18540v1' target='_blank'>Learning diverse attacks on large language models for robust red-teaming
  and safety tuning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Seanie Lee, Minsu Kim, Lynn Cherif, David Dobre, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, Moksh Jain</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-28 19:16:17</h6>
<p class='card-text'>Red-teaming, or identifying prompts that elicit harmful responses, is a
critical step in ensuring the safe and responsible deployment of large language
models (LLMs). Developing effective protection against many modes of attack
prompts requires discovering diverse attacks. Automated red-teaming typically
uses reinforcement learning to fine-tune an attacker language model to generate
prompts that elicit undesirable responses from a target LLM, as measured, for
example, by an auxiliary toxicity classifier. We show that even with explicit
regularization to favor novelty and diversity, existing approaches suffer from
mode collapse or fail to generate effective attacks. As a flexible and
probabilistically principled alternative, we propose to use GFlowNet
fine-tuning, followed by a secondary smoothing phase, to train the attacker
model to generate diverse and effective attack prompts. We find that the
attacks generated by our method are effective against a wide range of target
LLMs, both with and without safety tuning, and transfer well between target
LLMs. Finally, we demonstrate that models safety-tuned using a dataset of
red-teaming prompts generated by our method are robust to attacks from other
RL-based red-teaming approaches.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.18166v2' target='_blank'>Defending Large Language Models Against Jailbreak Attacks via
  Layer-specific Editing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wei Zhao, Zhe Li, Yige Li, Ye Zhang, Jun Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-28 13:26:12</h6>
<p class='card-text'>Large language models (LLMs) are increasingly being adopted in a wide range
of real-world applications. Despite their impressive performance, recent
studies have shown that LLMs are vulnerable to deliberately crafted adversarial
prompts even when aligned via Reinforcement Learning from Human Feedback or
supervised fine-tuning. While existing defense methods focus on either
detecting harmful prompts or reducing the likelihood of harmful responses
through various means, defending LLMs against jailbreak attacks based on the
inner mechanisms of LLMs remains largely unexplored. In this work, we
investigate how LLMs response to harmful prompts and propose a novel defense
method termed \textbf{L}ayer-specific \textbf{Ed}iting (LED) to enhance the
resilience of LLMs against jailbreak attacks. Through LED, we reveal that
several critical \textit{safety layers} exist among the early layers of LLMs.
We then show that realigning these safety layers (and some selected additional
layers) with the decoded safe response from selected target layers can
significantly improve the alignment of LLMs against jailbreak attacks.
Extensive experiments across various LLMs (e.g., Llama2, Mistral) show the
effectiveness of LED, which effectively defends against jailbreak attacks while
maintaining performance on benign prompts. Our code is available at
\url{https://github.com/ledllm/ledllm}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.18039v2' target='_blank'>Large Language Model-Driven Curriculum Design for Mobile Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Omar Erak, Omar Alhussein, Shimaa Naser, Nouf Alabbasi, De Mi, Sami Muhaidat</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-28 10:50:35</h6>
<p class='card-text'>This study introduces an innovative framework that employs large language
models (LLMs) to automate the design and generation of curricula for
reinforcement learning (RL). As mobile networks evolve towards the 6G era,
managing their increasing complexity and dynamic nature poses significant
challenges. Conventional RL approaches often suffer from slow convergence and
poor generalization due to conflicting objectives and the large state and
action spaces associated with mobile networks. To address these shortcomings,
we introduce curriculum learning, a method that systematically exposes the RL
agent to progressively challenging tasks, improving convergence and
generalization. However, curriculum design typically requires extensive domain
knowledge and manual human effort. Our framework mitigates this by utilizing
the generative capabilities of LLMs to automate the curriculum design process,
significantly reducing human effort while improving the RL agent's convergence
and performance. We deploy our approach within a simulated mobile network
environment and demonstrate improved RL convergence rates, generalization to
unseen scenarios, and overall performance enhancements. As a case study, we
consider autonomous coordination and user association in mobile networks. Our
obtained results highlight the potential of combining LLM-based curriculum
generation with RL for managing next-generation wireless networks, marking a
significant step towards fully autonomous network operations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.17956v2' target='_blank'>Hybrid Preference Optimization: Augmenting Direct Preference
  Optimization with Auxiliary Objectives</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Anirudhan Badrinath, Prabhat Agarwal, Jiajing Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-28 08:35:48</h6>
<p class='card-text'>For aligning large language models (LLMs), prior work has leveraged
reinforcement learning via human feedback (RLHF) or variations of direct
preference optimization (DPO). While DPO offers a simpler framework based on
maximum likelihood estimation, it compromises on the ability to tune language
models to easily maximize non-differentiable and non-binary objectives
according to the LLM designer's preferences (e.g., using simpler language or
minimizing specific kinds of harmful content). These may neither align with
user preferences nor even be able to be captured tractably by binary preference
data. To leverage the simplicity and performance of DPO with the
generalizability of RL, we propose a hybrid approach between DPO and RLHF. With
a simple augmentation to the implicit reward decomposition of DPO, we allow for
tuning LLMs to maximize a set of arbitrary auxiliary rewards using offline RL.
The proposed method, Hybrid Preference Optimization (HPO), shows the ability to
effectively generalize to both user preferences and auxiliary designer
objectives, while preserving alignment performance across a range of
challenging benchmarks and model sizes.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.17931v1' target='_blank'>Online Merging Optimizers for Boosting Rewards and Mitigating Tax in
  Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Keming Lu, Bowen Yu, Fei Huang, Yang Fan, Runji Lin, Chang Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-28 07:53:40</h6>
<p class='card-text'>Effectively aligning Large Language Models (LLMs) with human-centric values
while preventing the degradation of abilities acquired through Pre-training and
Supervised Fine-tuning (SFT) poses a central challenge in Reinforcement
Learning from Human Feedback (RLHF). In this paper, we first discover that
interpolating RLHF and SFT model parameters can adjust the trade-off between
human preference and basic capabilities, thereby reducing the alignment tax at
the cost of alignment reward. Inspired by this, we propose integrating the RL
policy and SFT models at each optimization step in RLHF to continuously
regulate the training direction, introducing the Online Merging Optimizer.
Specifically, we merge gradients with the parameter differences between SFT and
pretrained models, effectively steering the gradient towards maximizing rewards
in the direction of SFT optimization. We demonstrate that our optimizer works
well with different LLM families, such as Qwen and LLaMA, across various model
sizes ranging from 1.8B to 8B, various RLHF algorithms like DPO and KTO, and
existing model merging methods. It significantly enhances alignment reward
while mitigating alignment tax, achieving higher overall performance across 14
benchmarks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.17888v3' target='_blank'>Getting More Juice Out of the SFT Data: Reward Learning from Human
  Demonstration Improves SFT for LLM Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiaxiang Li, Siliang Zeng, Hoi-To Wai, Chenliang Li, Alfredo Garcia, Mingyi Hong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-28 07:11:05</h6>
<p class='card-text'>Aligning human preference and value is an important requirement for
contemporary foundation models. State-of-the-art techniques such as
Reinforcement Learning from Human Feedback (RLHF) often consist of two stages:
1) supervised fine-tuning (SFT), where the model is fine-tuned by learning from
human demonstration data; 2) Preference learning, where preference data is used
to learn a reward model, which is in turn used by a reinforcement learning (RL)
step to fine-tune the model. Such reward model serves as a proxy to human
preference, and it is critical to guide the RL step towards improving the model
quality. In this work, we argue that the SFT stage significantly benefits from
learning a reward model as well. Instead of using the human demonstration data
directly via supervised learning, we propose to leverage an Inverse
Reinforcement Learning (IRL) technique to simultaneously build an reward model
and a policy model. This approach leads to new SFT algorithms that are not only
efficient to implement, but are robust to the presence of low-quality
supervised learning data. Moreover, we discover a connection between the
proposed IRL based approach, and a recent line of works called Self-Play
Fine-tune (SPIN). Theoretically, we show that the proposed algorithms converge
to the stationary solutions of the IRL problem. Empirically, we align 1B and 7B
models using proposed methods and evaluate them on a reward benchmark model and
the HuggingFace Open LLM Leaderboard. The proposed methods show significant
performance improvement over existing SFT approaches. Our results indicate that
it is beneficial to leverage reward learning throughout the entire alignment
process.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.17743v4' target='_blank'>ORLM: A Customizable Framework in Training Large Models for Automated
  Optimization Modeling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chenyu Huang, Zhengyang Tang, Shixi Hu, Ruoqing Jiang, Xin Zheng, Dongdong Ge, Benyou Wang, Zizhuo Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-28 01:55:35</h6>
<p class='card-text'>Optimization modeling plays a critical role in the application of Operations
Research (OR) tools to address real-world problems, yet they pose challenges
and require extensive expertise from OR experts. With the advent of large
language models (LLMs), new opportunities have emerged to streamline and
automate such task. However, current research predominantly relies on
closed-source LLMs such as GPT-4, along with extensive prompt engineering
techniques. This reliance stems from the scarcity of high-quality training
datasets for optimization modeling, resulting in elevated costs, prolonged
processing times, and privacy concerns. To address these challenges, our work
is the first to propose a viable path for training open-source LLMs that are
capable of optimization modeling and developing solver codes, eventually
leading to a superior ability for automating optimization modeling and solving.
Particularly, we introduce OR-Instruct, a semi-automated data synthesis
framework for optimization modeling that enables customizable enhancements for
specific scenarios or model types. We also introduce IndustryOR, the first
industrial benchmark for evaluating LLMs in solving practical OR problems. We
train several 7B-scale open-source LLMs using synthesized data (dubbed
ORLMs{https://github.com/Cardinal-Operations/ORLM}), which exhibit
significantly enhanced optimization modeling capabilities, achieving
state-of-the-art performance across the NL4OPT, MAMO, and IndustryOR
benchmarks. Additionally, our experiments highlight the potential of scaling
law and reinforcement learning to further enhance the performance of ORLMs. The
workflows and human-machine interaction paradigms of ORLMs in practical
industrial applications are also discussed in the paper.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.02575v1' target='_blank'>Cross-Modal Safety Alignment: Is textual unlearning all you need?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Trishna Chakraborty, Erfan Shayegani, Zikui Cai, Nael Abu-Ghazaleh, M. Salman Asif, Yue Dong, Amit K. Roy-Chowdhury, Chengyu Song</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-27 20:29:13</h6>
<p class='card-text'>Recent studies reveal that integrating new modalities into Large Language
Models (LLMs), such as Vision-Language Models (VLMs), creates a new attack
surface that bypasses existing safety training techniques like Supervised
Fine-tuning (SFT) and Reinforcement Learning with Human Feedback (RLHF). While
further SFT and RLHF-based safety training can be conducted in multi-modal
settings, collecting multi-modal training datasets poses a significant
challenge. Inspired by the structural design of recent multi-modal models,
where, regardless of the combination of input modalities, all inputs are
ultimately fused into the language space, we aim to explore whether unlearning
solely in the textual domain can be effective for cross-modality safety
alignment. Our evaluation across six datasets empirically demonstrates the
transferability -- textual unlearning in VLMs significantly reduces the Attack
Success Rate (ASR) to less than 8\% and in some cases, even as low as nearly
2\% for both text-based and vision-text-based attacks, alongside preserving the
utility. Moreover, our experiments show that unlearning with a multi-modal
dataset offers no potential benefits but incurs significantly increased
computational demands, possibly up to 6 times higher.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.17618v2' target='_blank'>Symmetric Reinforcement Learning Loss for Robust Learning on Diverse
  Tasks and Model Scales</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ju-Seung Byun, Andrew Perrault</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-27 19:28:33</h6>
<p class='card-text'>Reinforcement learning (RL) training is inherently unstable due to factors
such as moving targets and high gradient variance. Reinforcement Learning from
Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF) can
introduce additional difficulty. Differing preferences can complicate the
alignment process, and prediction errors in a trained reward model can become
more severe as the LLM generates unseen outputs. To enhance training
robustness, RL has adopted techniques from supervised learning, such as
ensembles and layer normalization. In this work, we improve the stability of RL
training by adapting the reverse cross entropy (RCE) from supervised learning
for noisy data to define a symmetric RL loss. We demonstrate performance
improvements across various tasks and scales. We conduct experiments in
discrete action tasks (Atari games) and continuous action space tasks (MuJoCo
benchmark and Box2D) using Symmetric A2C (SA2C) and Symmetric PPO (SPPO), with
and without added noise with especially notable performance in SPPO across
different hyperparameters. Furthermore, we validate the benefits of the
symmetric RL loss when using SPPO for large language models through improved
performance in RLHF tasks, such as IMDB positive sentiment sentiment and TL;DR
summarization tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.17424v2' target='_blank'>LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhuoling Li, Xiaogang Xu, Zhenhua Xu, SerNam Lim, Hengshuang Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-27 17:59:32</h6>
<p class='card-text'>Recent embodied agents are primarily built based on reinforcement learning
(RL) or large language models (LLMs). Among them, RL agents are efficient for
deployment but only perform very few tasks. By contrast, giant LLM agents
(often more than 1000B parameters) present strong generalization while
demanding enormous computing resources. In this work, we combine their
advantages while avoiding the drawbacks by conducting the proposed referee RL
on our developed large auto-regressive model (LARM). Specifically, LARM is
built upon a lightweight LLM (fewer than 5B parameters) and directly outputs
the next action to execute rather than text. We mathematically reveal that
classic RL feedbacks vanish in long-horizon embodied exploration and introduce
a giant LLM based referee to handle this reward vanishment during training
LARM. In this way, LARM learns to complete diverse open-world tasks without
human intervention. Especially, LARM successfully harvests enchanted diamond
equipment in Minecraft, which demands significantly longer decision-making
chains than the highest achievements of prior best methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.00037v1' target='_blank'>Aligning LLMs through Multi-perspective User Preference Ranking-based
  Feedback for Programming Question Answering</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hongyu Yang, Liyang He, Min Hou, Shuanghong Shen, Rui Li, Jiahui Hou, Jianhui Ma, Junda Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-27 14:21:31</h6>
<p class='card-text'>Code Community Question Answering (CCQA) seeks to tackle programming-related
issues, thereby boosting productivity in both software engineering and academic
research. Recent advancements in Reinforcement Learning from Human Feedback
(RLHF) have transformed the fine-tuning process of Large Language Models (LLMs)
to produce responses that closely mimic human behavior. Leveraging LLMs with
RLHF for practical CCQA applications has thus emerged as a promising area of
study. Unlike standard code question-answering tasks, CCQA involves multiple
possible answers, with varying user preferences for each response.
Additionally, code communities often show a preference for new APIs. These
challenges prevent LLMs from generating responses that cater to the diverse
preferences of users in CCQA tasks. To address these issues, we propose a novel
framework called Aligning LLMs through Multi-perspective User Preference
Ranking-based Feedback for Programming Question Answering (ALMupQA) to create
user-focused responses. Our approach starts with Multi-perspective Preference
Ranking Alignment (MPRA), which synthesizes varied user preferences based on
the characteristics of answers from code communities. We then introduce a
Retrieval-augmented In-context Learning (RIL) module to mitigate the problem of
outdated answers by retrieving responses to similar questions from a question
bank. Due to the limited availability of high-quality, multi-answer CCQA
datasets, we also developed a dataset named StaCCQA from real code communities.
Extensive experiments demonstrated the effectiveness of the ALMupQA framework
in terms of accuracy and user preference. Compared to the base model, ALMupQA
showed nearly an 11% improvement in BLEU, with increases of 20% and 17.5% in
BERTScore and CodeBERTScore, respectively.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.16964v2' target='_blank'>Exploring the LLM Journey from Cognition to Expression with Linear
  Representations</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuzi Yan, Jialian Li, Yipin Zhang, Dong Yan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-27 08:57:04</h6>
<p class='card-text'>This paper presents an in-depth examination of the evolution and interplay of
cognitive and expressive capabilities in large language models (LLMs), with a
specific focus on Baichuan-7B and Baichuan-33B, an advanced bilingual (Chinese
and English) LLM series. We define and explore the model's cognitive and
expressive capabilities through linear representations across three critical
phases: Pretraining, Supervised Fine-Tuning (SFT), and Reinforcement Learning
from Human Feedback (RLHF). Cognitive capability is defined as the quantity and
quality of information conveyed by the neuron output vectors within the
network, similar to the neural signal processing in human cognition. Expressive
capability is defined as the model's capability to produce word-level output.
Our findings unveil a sequential development pattern, where cognitive abilities
are largely established during Pretraining, whereas expressive abilities
predominantly advance during SFT and RLHF. Statistical analyses confirm a
significant correlation between the two capabilities, suggesting that cognitive
capacity may limit expressive potential. The paper also explores the
theoretical underpinnings of these divergent developmental trajectories and
their connection to the LLMs' architectural design. Moreover, we evaluate
various optimization-independent strategies, such as few-shot learning and
repeated sampling, which bridge the gap between cognitive and expressive
capabilities. This research reveals the potential connection between the hidden
space and the output space, contributing valuable insights into the
interpretability and controllability of their training processes.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.16854v1' target='_blank'>Knowing What Not to Do: Leverage Language Model Insights for Action
  Space Pruning in Multi-agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhihao Liu, Xianliang Yang, Zichuan Liu, Yifan Xia, Wei Jiang, Yuanyu Zhang, Lijuan Li, Guoliang Fan, Lei Song, Bian Jiang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-27 06:00:24</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) is employed to develop autonomous
agents that can learn to adopt cooperative or competitive strategies within
complex environments. However, the linear increase in the number of agents
leads to a combinatorial explosion of the action space, which may result in
algorithmic instability, difficulty in convergence, or entrapment in local
optima. While researchers have designed a variety of effective algorithms to
compress the action space, these methods also introduce new challenges, such as
the need for manually designed prior knowledge or reliance on the structure of
the problem, which diminishes the applicability of these techniques. In this
paper, we introduce Evolutionary action SPAce Reduction with Knowledge
(eSpark), an exploration function generation framework driven by large language
models (LLMs) to boost exploration and prune unnecessary actions in MARL. Using
just a basic prompt that outlines the overall task and setting, eSpark is
capable of generating exploration functions in a zero-shot manner, identifying
and pruning redundant or irrelevant state-action pairs, and then achieving
autonomous improvement from policy feedback. In reinforcement learning tasks
involving inventory management and traffic light control encompassing a total
of 15 scenarios, eSpark consistently outperforms the combined MARL algorithm in
all scenarios, achieving an average performance gain of 34.4% and 9.9% in the
two types of tasks respectively. Additionally, eSpark has proven to be capable
of managing situations with a large number of agents, securing a 29.7%
improvement in scalability challenges that featured over 500 agents. The code
can be found in https://github.com/LiuZhihao2022/eSpark.git.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.16681v2' target='_blank'>Triple Preference Optimization: Achieving Better Alignment using a
  Single Step Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Amir Saeidi, Shivanshu Verma, Aswin RRV, Kashif Rasul, Chitta Baral</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-26 20:18:11</h6>
<p class='card-text'>Reinforcement Learning with Human Feedback (RLHF) enhances the alignment of
Large Language Models (LLMs). However, its limitations have led to the
development of Direct Preference Optimization (DPO), an RL-free approach
designed to overcome these shortcomings. While studies have shown that DPO
improves instruction-following capabilities, it negatively impacts the
reasoning ability of LLMs. Additionally, DPO is highly sensitive to judgment
noise in preference datasets and the size of the training set. Although several
modifications to DPO have been proposed, they still fail to fully resolve these
issues. To address these limitations, we propose Triple Preference Optimization
(TPO), a new preference learning method designed to enhance both reasoning and
instruction-following abilities through one-step optimization. We compare TPO
against DPO and its recent variants using state-of-the-art training setups,
including both base and instruction-tuned models such as Mistral and Llama 3.
Our evaluation covers a comprehensive range of chat-based and reasoning
benchmarks. The results demonstrate that TPO achieves significant improvements
over existing methods without substantially increasing response length across
different dataset sizes. Specifically, TPO outperforms DPO and SimPO by up to
7.0% and 7.3% points on Arena-Hard, 12.2% and 13.3% points on MixEval-Hard,
10.4% and 10.1% points on MMLU-Pro, and 19.0% and 19.2% points on GSM8K,
respectively. Furthermore, TPO achieves these improvements while requiring less
data than DPO.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.16661v2' target='_blank'>RLSF: Reinforcement Learning via Symbolic Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Piyush Jha, Prithwish Jana, Pranavkrishna Suresh, Arnav Arora, Vijay Ganesh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-26 18:49:59</h6>
<p class='card-text'>Reinforcement Learning with Human Feedback (RLHF) is considered a standard
approach to fine-tuning Large Language Models (LLMs). However, such methods
often face limitations such as unsound black-box reward models, difficulties in
collecting human preference data, and the reliance on sparse scalar rewards.
These methods often fall short when applied to tasks that require complex
domain-specific understanding.
  To address these challenges, we propose a new fine-tuning paradigm we refer
to as Reinforcement Learning via Symbolic Feedback (RLSF), which aims to
improve domain-specific understanding of LLMs more effectively than traditional
reward signals. In the RLSF setting, the LLM being fine-tuned is considered an
RL agent, while the environment is allowed access to reasoning or domain
knowledge tools (e.g., solvers, provers, algebra systems, or knowledge bases).
Crucially, in RLSF, these reasoning tools can provide feedback to the LLMs via
poly-sized certificates (e.g., proofs), that characterize errors in the
LLM-generated object with respect to some correctness specification. As a
bonus, our RLSF approach does not require the reasoning systems we use to be
differentiable. The ability of RLSF-based fine-tuning to leverage
certificate-generating symbolic tools enables sound fine-grained (token-level)
reward signals to LLMs, and thus addresses the limitations of traditional
reward models mentioned above.
  Via extensive evaluations, we show that our RLSF-based fine-tuning of LLMs
outperforms traditional approaches on five different applications, namely,
program synthesis from natural language pseudo-code to programming language,
three chemistry tasks, and solving the Game of 24. A takeaway is that
fine-tuning via RLSF enables relatively smaller LLMs to significantly
outperform closed-source models that are orders of magnitude larger (e.g.,
GPT-4).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.16455v1' target='_blank'>On the Algorithmic Bias of Aligning Large Language Models with RLHF:
  Preference Collapse and Matching Regularization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiancong Xiao, Ziniu Li, Xingyu Xie, Emily Getzen, Cong Fang, Qi Long, Weijie J. Su</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-26 07:00:05</h6>
<p class='card-text'>Accurately aligning large language models (LLMs) with human preferences is
crucial for informing fair, economically sound, and statistically efficient
decision-making processes. However, we argue that reinforcement learning from
human feedback (RLHF) -- the predominant approach for aligning LLMs with human
preferences through a reward model -- suffers from an inherent algorithmic bias
due to its Kullback--Leibler-based regularization in optimization. In extreme
cases, this bias could lead to a phenomenon we term preference collapse, where
minority preferences are virtually disregarded. To mitigate this algorithmic
bias, we introduce preference matching (PM) RLHF, a novel approach that
provably aligns LLMs with the preference distribution of the reward model under
the Bradley--Terry--Luce/Plackett--Luce model. Central to our approach is a PM
regularizer that takes the form of the negative logarithm of the LLM's policy
probability distribution over responses, which helps the LLM balance response
diversification and reward maximization. Notably, we obtain this regularizer by
solving an ordinary differential equation that is necessary for the PM
property. For practical implementation, we introduce a conditional variant of
PM RLHF that is tailored to natural language generation. Finally, we
empirically validate the effectiveness of conditional PM RLHF through
experiments on the OPT-1.3B and Llama-2-7B models, demonstrating a 29% to 41%
improvement in alignment with human preferences, as measured by a certain
metric, compared to standard RLHF.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.16450v2' target='_blank'>Synthesizing Programmatic Reinforcement Learning Policies with Large
  Language Model Guided Search</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Max Liu, Chan-Hung Yu, Wei-Hsu Lee, Cheng-Wei Hung, Yen-Chun Chen, Shao-Hua Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-26 06:33:48</h6>
<p class='card-text'>Programmatic reinforcement learning (PRL) has been explored for representing
policies through programs as a means to achieve interpretability and
generalization. Despite promising outcomes, current state-of-the-art PRL
methods are hindered by sample inefficiency, necessitating tens of millions of
program-environment interactions. To tackle this challenge, we introduce a
novel LLM-guided search framework (LLM-GS). Our key insight is to leverage the
programming expertise and common sense reasoning of LLMs to enhance the
efficiency of assumption-free, random-guessing search methods. We address the
challenge of LLMs' inability to generate precise and grammatically correct
programs in domain-specific languages (DSLs) by proposing a Pythonic-DSL
strategy - an LLM is instructed to initially generate Python codes and then
convert them into DSL programs. To further optimize the LLM-generated programs,
we develop a search algorithm named Scheduled Hill Climbing, designed to
efficiently explore the programmatic search space to improve the programs
consistently. Experimental results in the Karel domain demonstrate our LLM-GS
framework's superior effectiveness and efficiency. Extensive ablation studies
further verify the critical role of our Pythonic-DSL strategy and Scheduled
Hill Climbing algorithm. Moreover, we conduct experiments with two novel tasks,
showing that LLM-GS enables users without programming skills and knowledge of
the domain or DSL to describe the tasks in natural language to obtain
performant programs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.16420v1' target='_blank'>M-RAG: Reinforcing Large Language Model Performance through
  Retrieval-Augmented Generation with Multiple Partitions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zheng Wang, Shu Xian Teo, Jieer Ouyang, Yongjun Xu, Wei Shi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-26 04:03:13</h6>
<p class='card-text'>Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
retrieving relevant memories from an external database. However, existing RAG
methods typically organize all memories in a whole database, potentially
limiting focus on crucial memories and introducing noise. In this paper, we
introduce a multiple partition paradigm for RAG (called M-RAG), where each
database partition serves as a basic unit for RAG execution. Based on this
paradigm, we propose a novel framework that leverages LLMs with Multi-Agent
Reinforcement Learning to optimize different language generation tasks
explicitly. Through comprehensive experiments conducted on seven datasets,
spanning three language generation tasks and involving three distinct language
model architectures, we confirm that M-RAG consistently outperforms various
baseline methods, achieving improvements of 11%, 8%, and 12% for text
summarization, machine translation, and dialogue generation, respectively.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.16388v1' target='_blank'>Multi-Reference Preference Optimization for Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hung Le, Quan Tran, Dung Nguyen, Kien Do, Saloni Mittal, Kelechi Ogueji, Svetha Venkatesh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-26 00:29:04</h6>
<p class='card-text'>How can Large Language Models (LLMs) be aligned with human intentions and
values? A typical solution is to gather human preference on model outputs and
finetune the LLMs accordingly while ensuring that updates do not deviate too
far from a reference model. Recent approaches, such as direct preference
optimization (DPO), have eliminated the need for unstable and sluggish
reinforcement learning optimization by introducing close-formed supervised
losses. However, a significant limitation of the current approach is its design
for a single reference model only, neglecting to leverage the collective power
of numerous pretrained LLMs. To overcome this limitation, we introduce a novel
closed-form formulation for direct preference optimization using multiple
reference models. The resulting algorithm, Multi-Reference Preference
Optimization (MRPO), leverages broader prior knowledge from diverse reference
models, substantially enhancing preference learning capabilities compared to
the single-reference DPO. Our experiments demonstrate that LLMs finetuned with
MRPO generalize better in various preference data, regardless of data scarcity
or abundance. Furthermore, MRPO effectively finetunes LLMs to exhibit superior
performance in several downstream natural language processing tasks such as
GSM8K and TruthfulQA.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.15624v2' target='_blank'>Inverse-RLignment: Large Language Model Alignment from Demonstrations
  through Inverse Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hao Sun, Mihaela van der Schaar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-24 15:13:53</h6>
<p class='card-text'>Aligning Large Language Models (LLMs) is crucial for enhancing their safety
and utility. However, existing methods, primarily based on preference datasets,
face challenges such as noisy labels, high annotation costs, and privacy
concerns. In this work, we introduce Alignment from Demonstrations (AfD), a
novel approach leveraging high-quality demonstration data to overcome these
challenges. We formalize AfD within a sequential decision-making framework,
highlighting its unique challenge of missing reward signals. Drawing insights
from forward and inverse reinforcement learning, we introduce divergence
minimization objectives for AfD. Analytically, we elucidate the mass-covering
and mode-seeking behaviors of various approaches, explaining when and why
certain methods are superior. Practically, we propose a computationally
efficient algorithm that extrapolates over a tailored reward model for AfD. We
validate our key insights through experiments on the Harmless and Helpful
tasks, demonstrating their strong empirical performance while maintaining
simplicity.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.15383v2' target='_blank'>Generating Code World Models with Large Language Models Guided by Monte
  Carlo Tree Search</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nicola Dainese, Matteo Merler, Minttu Alakuijala, Pekka Marttinen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-24 09:31:26</h6>
<p class='card-text'>In this work we consider Code World Models, world models generated by a Large
Language Model (LLM) in the form of Python code for model-based Reinforcement
Learning (RL). Calling code instead of LLMs for planning has potential to be
more precise, reliable, interpretable, and extremely efficient. However,
writing appropriate Code World Models requires the ability to understand
complex instructions, to generate exact code with non-trivial logic and to
self-debug a long program with feedback from unit tests and environment
trajectories. To address these challenges, we propose Generate, Improve and Fix
with Monte Carlo Tree Search (GIF-MCTS), a new code generation strategy for
LLMs. To test our approach in an offline RL setting, we introduce the Code
World Models Benchmark (CWMB), a suite of program synthesis and planning tasks
comprised of 18 diverse RL environments paired with corresponding textual
descriptions and curated trajectories. GIF-MCTS surpasses all baselines on the
CWMB and two other benchmarks, and we show that the Code World Models
synthesized with it can be successfully used for planning, resulting in
model-based RL agents with greatly improved sample efficiency and inference
speed.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.00024v2' target='_blank'>Embedding-Aligned Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guy Tennenholtz, Yinlam Chow, Chih-Wei Hsu, Lior Shani, Ethan Liang, Craig Boutilier</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-24 06:11:17</h6>
<p class='card-text'>We propose a novel approach for training large language models (LLMs) to
adhere to objectives defined within a latent embedding space. Our method
leverages reinforcement learning (RL), treating a pre-trained LLM as an
environment. Our embedding-aligned guided language (EAGLE) agent is trained to
iteratively steer the LLM's generation towards optimal regions of the latent
embedding space, w.r.t. some predefined criterion. We demonstrate the
effectiveness of the EAGLE agent using the MovieLens 25M and Amazon Review
datasets to surface content gaps that satisfy latent user demand. We also
demonstrate the benefit of using an optimal design of a state-dependent action
set to improve EAGLE's efficiency. Our work paves the way for controlled and
grounded text generation using LLMs, ensuring consistency with domain-specific
knowledge and data representations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.15230v2' target='_blank'>$i$REPO: $i$mplicit Reward Pairwise Difference based Empirical
  Preference Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Long Tan Le, Han Shu, Tung-Anh Nguyen, Choong Seon Hong, Nguyen H. Tran</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-24 05:42:11</h6>
<p class='card-text'>While astonishingly capable, large Language Models (LLM) can sometimes
produce outputs that deviate from human expectations. Such deviations
necessitate an alignment phase to prevent disseminating untruthful, toxic, or
biased information. Traditional alignment methods based on reinforcement
learning often struggle with the identified instability, whereas preference
optimization methods are limited by their overfitting to pre-collected
hard-label datasets. In this paper, we propose a novel LLM alignment framework
named $i$REPO, which utilizes implicit Reward pairwise difference regression
for Empirical Preference Optimization. Particularly, $i$REPO employs
self-generated datasets labeled by empirical human (or AI annotator) preference
to iteratively refine the aligned policy through a novel regression-based loss
function. Furthermore, we introduce an innovative algorithm backed by
theoretical guarantees for achieving optimal results under ideal assumptions
and providing a practical performance-gap result without such assumptions.
Experimental results with Phi-2 and Mistral-7B demonstrate that $i$REPO
effectively achieves self-alignment using soft-label, self-generated responses
and the logit of empirical AI annotators. Furthermore, our approach surpasses
preference optimization baselines in evaluations using the Language Model
Evaluation Harness and Multi-turn benchmarks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.15194v2' target='_blank'>Extracting Heuristics from Large Language Models for Reward Shaping in
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Siddhant Bhambri, Amrita Bhattacharjee, Durgesh Kalwar, Lin Guan, Huan Liu, Subbarao Kambhampati</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-24 03:53:57</h6>
<p class='card-text'>Reinforcement Learning (RL) suffers from sample inefficiency in sparse reward
domains, and the problem is further pronounced in case of stochastic
transitions. To improve the sample efficiency, reward shaping is a well-studied
approach to introduce intrinsic rewards that can help the RL agent converge to
an optimal policy faster. However, designing a useful reward shaping function
for all desirable states in the Markov Decision Process (MDP) is challenging,
even for domain experts. Given that Large Language Models (LLMs) have
demonstrated impressive performance across a magnitude of natural language
tasks, we aim to answer the following question: `Can we obtain heuristics using
LLMs for constructing a reward shaping function that can boost an RL agent's
sample efficiency?' To this end, we aim to leverage off-the-shelf LLMs to
generate a plan for an abstraction of the underlying MDP. We further use this
LLM-generated plan as a heuristic to construct the reward shaping signal for
the downstream RL agent. By characterizing the type of abstraction based on the
MDP horizon length, we analyze the quality of heuristics when generated using
an LLM, with and without a verifier in the loop. Our experiments across
multiple domains with varying horizon length and number of sub-goals from the
BabyAI environment suite, Household, Mario, and, Minecraft domain, show 1) the
advantages and limitations of querying LLMs with and without a verifier to
generate a reward shaping heuristic, and, 2) a significant improvement in the
sample efficiency of PPO, A2C, and Q-learning when guided by the LLM-generated
heuristics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.15019v2' target='_blank'>Agentic Skill Discovery</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xufeng Zhao, Cornelius Weber, Stefan Wermter</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-23 19:44:03</h6>
<p class='card-text'>Language-conditioned robotic skills make it possible to apply the high-level
reasoning of Large Language Models (LLMs) to low-level robotic control. A
remaining challenge is to acquire a diverse set of fundamental skills. Existing
approaches either manually decompose a complex task into atomic robotic actions
in a top-down fashion, or bootstrap as many combinations as possible in a
bottom-up fashion to cover a wider range of task possibilities. These
decompositions or combinations, however, require an initial skill library. For
example, a ``grasping'' capability can never emerge from a skill library
containing only diverse ``pushing'' skills. Existing skill discovery techniques
with reinforcement learning acquire skills by an exhaustive exploration but
often yield non-meaningful behaviors. In this study, we introduce a novel
framework for skill discovery that is entirely driven by LLMs. The framework
begins with an LLM generating task proposals based on the provided scene
description and the robot's configurations, aiming to incrementally acquire new
skills upon task completion. For each proposed task, a series of reinforcement
learning processes are initiated, utilizing reward and success determination
functions sampled by the LLM to develop the corresponding policy. The
reliability and trustworthiness of learned behaviors are further ensured by an
independent vision-language model. We show that starting with zero skill, the
skill library emerges and expands to more and more meaningful and reliable
skills, enabling the robot to efficiently further propose and complete advanced
tasks. Project page: \url{https://agentic-skill-discovery.github.io}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.14953v3' target='_blank'>MallowsPO: Fine-Tune Your LLM with Preference Dispersions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haoxian Chen, Hanyang Zhao, Henry Lam, David Yao, Wenpin Tang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-23 18:01:11</h6>
<p class='card-text'>Direct Preference Optimization (DPO) has recently emerged as a popular
approach to improve reinforcement learning with human feedback (RLHF), leading
to better techniques to fine-tune large language models (LLM). A weakness of
DPO, however, lies in its lack of capability to characterize the diversity of
human preferences. Inspired by Mallows' theory of preference ranking, we
develop in this paper a new approach, the MallowsPO. A distinct feature of this
approach is a dispersion index, which reflects the dispersion of human
preference to prompts. We show that existing DPO models can be reduced to
special cases of this dispersion index, thus unified with MallowsPO. More
importantly, we demonstrate (empirically) how to use this dispersion index to
enhance the performance of DPO in a broad array of benchmark tasks, from
synthetic bandit selection to controllable generations and dialogues, while
maintaining great generalization capabilities. MallowsPO is also compatible
with other SOTA offline preference optimization methods, boosting nearly 2\%
extra LC win rate when used as a plugin for fine-tuning Llama3-Instruct.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.14751v2' target='_blank'>AGILE: A Novel Reinforcement Learning Framework of LLM Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Peiyuan Feng, Yichen He, Guanhua Huang, Yuan Lin, Hanchong Zhang, Yuchen Zhang, Hang Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-23 16:17:44</h6>
<p class='card-text'>We introduce a novel reinforcement learning framework of LLM agents named
AGILE (AGent that Interacts and Learns from Environments) designed to perform
complex conversational tasks with users, leveraging LLMs, memory, tools, and
interactions with experts. The agent possesses capabilities beyond
conversation, including reflection, tool usage, and expert consultation. We
formulate the construction of such an LLM agent as a reinforcement learning
(RL) problem, in which the LLM serves as the policy model. We fine-tune the LLM
using labeled data of actions and the PPO algorithm. We focus on question
answering and release a dataset for agents called ProductQA, comprising
challenging questions in online shopping. Our extensive experiments on
ProductQA, MedMCQA and HotPotQA show that AGILE agents based on 7B and 13B LLMs
trained with PPO can outperform GPT-4 agents. Our ablation study highlights the
indispensability of memory, tools, consultation, reflection, and reinforcement
learning in achieving the agent's strong performance. Datasets and code are
available at https://github.com/bytarnish/AGILE.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.14655v2' target='_blank'>Multi-turn Reinforcement Learning from Preference Human Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lior Shani, Aviv Rosenberg, Asaf Cassel, Oran Lang, Daniele Calandriello, Avital Zipori, Hila Noga, Orgad Keller, Bilal Piot, Idan Szpektor, Avinatan Hassidim, Yossi Matias, Rémi Munos</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-23 14:53:54</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) has become the standard
approach for aligning Large Language Models (LLMs) with human preferences,
allowing LLMs to demonstrate remarkable abilities in various tasks. Existing
methods work by emulating the preferences at the single decision (turn) level,
limiting their capabilities in settings that require planning or multi-turn
interactions to achieve a long-term goal. In this paper, we address this issue
by developing novel methods for Reinforcement Learning (RL) from preference
feedback between two full multi-turn conversations. In the tabular setting, we
present a novel mirror-descent-based policy optimization algorithm for the
general multi-turn preference-based RL problem, and prove its convergence to
Nash equilibrium. To evaluate performance, we create a new environment,
Education Dialogue, where a teacher agent guides a student in learning a random
topic, and show that a deep RL variant of our algorithm outperforms RLHF
baselines. Finally, we show that in an environment with explicit rewards, our
algorithm recovers the same performance as a reward-based RL baseline, despite
relying solely on a weaker preference signal.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.14490v1' target='_blank'>Impact of Non-Standard Unicode Characters on Security and Comprehension
  in Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Johan S Daniel, Anand Pal</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-23 12:24:38</h6>
<p class='card-text'>The advancement of large language models has significantly improved natural
language processing. However, challenges such as jailbreaks (prompt injections
that cause an LLM to follow instructions contrary to its intended use),
hallucinations (generating incorrect or misleading information), and
comprehension errors remain prevalent. In this report, we present a comparative
analysis of the performance of fifteen distinct models, with each model
undergoing a standardized test comprising 38 queries across three key metrics:
jailbreaks, hallucinations, and comprehension errors. The models are assessed
based on the total occurrences of jailbreaks, hallucinations, and comprehension
errors. Our work exposes these models' inherent vulnerabilities and challenges
the notion of human-level language comprehension of these models. We have
empirically analysed the impact of non-standard Unicode characters on LLMs and
their safeguarding mechanisms on the best-performing LLMs, including GPT-4,
Gemini 1.5 Pro, LlaMA-3-70B, and Claude 3 Opus. By incorporating alphanumeric
symbols from Unicode outside the standard Latin block and variants of
characters in other languages, we observed a reduction in the efficacy of
guardrails implemented through Reinforcement Learning Human Feedback (RLHF).
Consequently, these models exhibit heightened vulnerability to content policy
breaches and prompt leakage. Our study also suggests a need to incorporate
non-standard Unicode text in LLM training data to enhance the capabilities of
these models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.14431v1' target='_blank'>RaFe: Ranking Feedback Improves Query Rewriting for RAG</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shengyu Mao, Yong Jiang, Boli Chen, Xiao Li, Peng Wang, Xinyu Wang, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-23 11:00:19</h6>
<p class='card-text'>As Large Language Models (LLMs) and Retrieval Augmentation Generation (RAG)
techniques have evolved, query rewriting has been widely incorporated into the
RAG system for downstream tasks like open-domain QA. Many works have attempted
to utilize small models with reinforcement learning rather than costly LLMs to
improve query rewriting. However, current methods require annotations (e.g.,
labeled relevant documents or downstream answers) or predesigned rewards for
feedback, which lack generalization, and fail to utilize signals tailored for
query rewriting. In this paper, we propose ours, a framework for training query
rewriting models free of annotations. By leveraging a publicly available
reranker, ours~provides feedback aligned well with the rewriting objectives.
Experimental results demonstrate that ours~can obtain better performance than
baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.14333v1' target='_blank'>DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale
  Synthetic Data</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, Xiaodan Liang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-23 09:03:42</h6>
<p class='card-text'>Proof assistants like Lean have revolutionized mathematical proof
verification, ensuring high accuracy and reliability. Although large language
models (LLMs) show promise in mathematical reasoning, their advancement in
formal theorem proving is hindered by a lack of training data. To address this
issue, we introduce an approach to generate extensive Lean 4 proof data derived
from high-school and undergraduate-level mathematical competition problems.
This approach involves translating natural language problems into formal
statements, filtering out low-quality statements, and generating proofs to
create synthetic data. After fine-tuning the DeepSeekMath 7B model on this
synthetic dataset, which comprises 8 million formal statements with proofs, our
model achieved whole-proof generation accuracies of 46.3% with 64 samples and
52% cumulatively on the Lean 4 miniF2F test, surpassing the baseline GPT-4 at
23.0% with 64 samples and a tree search reinforcement learning method at 41.0%.
Additionally, our model successfully proved 5 out of 148 problems in the Lean 4
Formalized International Mathematical Olympiad (FIMO) benchmark, while GPT-4
failed to prove any. These results demonstrate the potential of leveraging
large-scale synthetic data to enhance theorem-proving capabilities in LLMs.
Both the synthetic dataset and the model will be made available to facilitate
further research in this promising field.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.14314v2' target='_blank'>Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yang Zhang, Shixin Yang, Chenjia Bai, Fei Wu, Xiu Li, Zhen Wang, Xuelong Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-23 08:33:19</h6>
<p class='card-text'>Grounding the reasoning ability of large language models (LLMs) for embodied
tasks is challenging due to the complexity of the physical world. Especially,
LLM planning for multi-agent collaboration requires communication of agents or
credit assignment as the feedback to re-adjust the proposed plans and achieve
effective coordination. However, existing methods that overly rely on physical
verification or self-reflection suffer from excessive and inefficient querying
of LLMs. In this paper, we propose a novel framework for multi-agent
collaboration that introduces Reinforced Advantage feedback (ReAd) for
efficient self-refinement of plans. Specifically, we perform critic regression
to learn a sequential advantage function from LLM-planned data, and then treat
the LLM planner as an optimizer to generate actions that maximize the advantage
function. It endows the LLM with the foresight to discern whether the action
contributes to accomplishing the final task. We provide theoretical analysis by
extending advantage-weighted regression in reinforcement learning to
multi-agent systems. Experiments on Overcooked-AI and a difficult variant of
RoCoBench show that ReAd surpasses baselines in success rate, and also
significantly decreases the interaction steps of agents and query rounds of
LLMs, demonstrating its high efficiency for grounding LLMs. More results are
given at https://read-llm.github.io/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.14103v1' target='_blank'>Online Self-Preferring Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuanzhao Zhai, Zhuo Zhang, Kele Xu, Hanyang Peng, Yue Yu, Dawei Feng, Cheng Yang, Bo Ding, Huaimin Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-23 02:13:34</h6>
<p class='card-text'>Aligning with human preference datasets has been critical to the success of
large language models (LLMs). Reinforcement learning from human feedback (RLHF)
employs a costly reward model to provide feedback for on-policy sampling
responses. Recently, offline methods that directly fit responses with binary
preferences in the dataset have emerged as alternatives. However, existing
methods do not explicitly model preference strength information, which is
crucial for distinguishing different response pairs. To overcome this
limitation, we propose Online Self-Preferring (OSP) language models to learn
from self-generated response pairs and self-judged preference strengths. For
each prompt and corresponding self-generated responses, we introduce a ranked
pairing method to construct multiple response pairs with preference strength
information. We then propose the soft-preference cross-entropy loss to leverage
such information. Empirically, we demonstrate that leveraging preference
strength is crucial for avoiding overfitting and enhancing alignment
performance. OSP achieves state-of-the-art alignment performance across various
metrics in two widely used human preference datasets. OSP is
parameter-efficient and more robust than the dominant online method, RLHF when
limited offline data are available and generalizing to out-of-domain tasks.
Moreover, OSP language models established by LLMs with proficiency in
self-preferring can efficiently self-improve without external supervision.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.14062v1' target='_blank'>ChatScene: Knowledge-Enabled Safety-Critical Scenario Generation for
  Autonomous Vehicles</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiawei Zhang, Chejian Xu, Bo Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-22 23:21:15</h6>
<p class='card-text'>We present ChatScene, a Large Language Model (LLM)-based agent that leverages
the capabilities of LLMs to generate safety-critical scenarios for autonomous
vehicles. Given unstructured language instructions, the agent first generates
textually described traffic scenarios using LLMs. These scenario descriptions
are subsequently broken down into several sub-descriptions for specified
details such as behaviors and locations of vehicles. The agent then
distinctively transforms the textually described sub-scenarios into
domain-specific languages, which then generate actual code for prediction and
control in simulators, facilitating the creation of diverse and complex
scenarios within the CARLA simulation environment. A key part of our agent is a
comprehensive knowledge retrieval component, which efficiently translates
specific textual descriptions into corresponding domain-specific code snippets
by training a knowledge database containing the scenario description and code
pairs. Extensive experimental results underscore the efficacy of ChatScene in
improving the safety of autonomous vehicles. For instance, the scenarios
generated by ChatScene show a 15% increase in collision rates compared to
state-of-the-art baselines when tested against different reinforcement
learning-based ego vehicles. Furthermore, we show that by using our generated
safety-critical scenarios to fine-tune different RL-based autonomous driving
models, they can achieve a 9% reduction in collision rates, surpassing current
SOTA methods. ChatScene effectively bridges the gap between textual
descriptions of traffic scenarios and practical CARLA simulations, providing a
unified way to conveniently generate safety-critical scenarios for safety
testing and improvement for AVs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.13547v1' target='_blank'>HighwayLLM: Decision-Making and Navigation in Highway Driving with
  RL-Informed Language Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mustafa Yildirim, Barkin Dagda, Saber Fallah</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-22 11:32:37</h6>
<p class='card-text'>Autonomous driving is a complex task which requires advanced decision making
and control algorithms. Understanding the rationale behind the autonomous
vehicles' decision is crucial to ensure their safe and effective operation on
highway driving. This study presents a novel approach, HighwayLLM, which
harnesses the reasoning capabilities of large language models (LLMs) to predict
the future waypoints for ego-vehicle's navigation. Our approach also utilizes a
pre-trained Reinforcement Learning (RL) model to serve as a high-level planner,
making decisions on appropriate meta-level actions. The HighwayLLM combines the
output from the RL model and the current state information to make safe,
collision-free, and explainable predictions for the next states, thereby
constructing a trajectory for the ego-vehicle. Subsequently, a PID-based
controller guides the vehicle to the waypoints predicted by the LLM agent. This
integration of LLM with RL and PID enhances the decision-making process and
provides interpretability for highway autonomous driving.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.13516v2' target='_blank'>LIRE: listwise reward enhancement for preference alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mingye Zhu, Yi Liu, Lei Zhang, Junbo Guo, Zhendong Mao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-22 10:21:50</h6>
<p class='card-text'>Recently, tremendous strides have been made to align the generation of Large
Language Models (LLMs) with human values to mitigate toxic or unhelpful
content. Leveraging Reinforcement Learning from Human Feedback (RLHF) proves
effective and is widely adopted by researchers. However, implementing RLHF is
complex, and its sensitivity to hyperparameters renders achieving stable
performance and scalability challenging. Furthermore, prevailing approaches to
preference alignment primarily concentrate on pairwise comparisons, with
limited exploration into multi-response scenarios, thereby overlooking the
potential richness within the candidate pool. For the above reasons, we propose
a new approach: Listwise Reward Enhancement for Preference Alignment (LIRE), a
gradient-based reward optimization approach that incorporates the offline
rewards of multiple responses into a streamlined listwise framework, thus
eliminating the need for online sampling during training. LIRE is
straightforward to implement, requiring minimal parameter tuning, and
seamlessly aligns with the pairwise paradigm while naturally extending to
multi-response scenarios. Moreover, we introduce a self-enhancement algorithm
aimed at iteratively refining the reward during training. Our experiments
demonstrate that LIRE consistently outperforms existing methods across several
benchmarks on dialogue and summarization tasks, with good transferability to
out-of-distribution data, assessed using proxy reward models and human
annotators.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.13362v3' target='_blank'>Lusifer: LLM-based User SImulated Feedback Environment for online
  Recommender systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Danial Ebrat, Eli Paradalis, Luis Rueda</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-22 05:43:15</h6>
<p class='card-text'>Training reinforcement learning-based recommender systems is often hindered
by the lack of dynamic and realistic user interactions. To address this
limitation, we introduce Lusifer, a novel environment leveraging Large Language
Models (LLMs) to generate simulated user feedback. Lusifer synthesizes user
profiles and interaction histories to simulate responses and behaviors toward
recommended items, with profiles updated after each rating to reflect evolving
user characteristics. Utilizing the MovieLens dataset as a proof of concept, we
limited our implementation to the last 40 interactions for each user,
representing approximately 39% and 22% of the training sets, to focus on recent
user behavior. For consistency and to gain insights into the performance of
traditional methods with limited data, we implemented baseline approaches using
the same data subset. Our results demonstrate that Lusifer accurately emulates
user behavior and preferences, even with reduced training data having an RMSE
of 1.3 across various test sets. This paper presents Lusifer's operational
pipeline, including prompt generation and iterative user profile updates, and
compares its performance against baseline methods. The findings validate
Lusifer's ability to produce realistic dynamic feedback and suggest that it
offers a scalable and adjustable framework for user simulation in online
reinforcement learning recommender systems for future studies, particularly
when training data is limited.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.13356v2' target='_blank'>Large Language Models (LLMs) Assisted Wireless Network Deployment in
  Urban Settings</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nurullah Sevim, Mostafa Ibrahim, Sabit Ekin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-22 05:19:51</h6>
<p class='card-text'>The advent of Large Language Models (LLMs) has revolutionized language
understanding and human-like text generation, drawing interest from many other
fields with this question in mind: What else are the LLMs capable of? Despite
their widespread adoption, ongoing research continues to explore new ways to
integrate LLMs into diverse systems.
  This paper explores new techniques to harness the power of LLMs for 6G (6th
Generation) wireless communication technologies, a domain where automation and
intelligent systems are pivotal. The inherent adaptability of LLMs to
domain-specific tasks positions them as prime candidates for enhancing wireless
systems in the 6G landscape.
  We introduce a novel Reinforcement Learning (RL) based framework that
leverages LLMs for network deployment in wireless communications. Our approach
involves training an RL agent, utilizing LLMs as its core, in an urban setting
to maximize coverage. The agent's objective is to navigate the complexities of
urban environments and identify the network parameters for optimal area
coverage. Additionally, we integrate LLMs with Convolutional Neural Networks
(CNNs) to capitalize on their strengths while mitigating their limitations. The
Deep Deterministic Policy Gradient (DDPG) algorithm is employed for training
purposes. The results suggest that LLM-assisted models can outperform CNN-based
models in some cases while performing at least as well in others.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.12961v1' target='_blank'>Energy Rank Alignment: Using Preference Optimization to Search Chemical
  Space at Scale</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shriram Chennakesavalu, Frank Hu, Sebastian Ibarraran, Grant M. Rotskoff</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-21 17:35:20</h6>
<p class='card-text'>Searching through chemical space is an exceptionally challenging problem
because the number of possible molecules grows combinatorially with the number
of atoms. Large, autoregressive models trained on databases of chemical
compounds have yielded powerful generators, but we still lack robust strategies
for generating molecules with desired properties. This molecular search problem
closely resembles the "alignment" problem for large language models, though for
many chemical tasks we have a specific and easily evaluable reward function.
Here, we introduce an algorithm called energy rank alignment (ERA) that
leverages an explicit reward function to produce a gradient-based objective
that we use to optimize autoregressive policies. We show theoretically that
this algorithm is closely related to proximal policy optimization (PPO) and
direct preference optimization (DPO), but has a minimizer that converges to an
ideal Gibbs-Boltzmann distribution with the reward playing the role of an
energy function. Furthermore, this algorithm is highly scalable, does not
require reinforcement learning, and performs well relative to DPO when the
number of preference observations per pairing is small. We deploy this approach
to align molecular transformers to generate molecules with externally specified
properties and find that it does so robustly, searching through diverse parts
of chemical space. While our focus here is on chemical search, we also obtain
excellent results on an AI supervised task for LLM alignment, showing that the
method is scalable and general.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.12750v2' target='_blank'>Generative AI in Cybersecurity: A Comprehensive Review of LLM
  Applications and Vulnerabilities</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mohamed Amine Ferrag, Fatima Alwahedi, Ammar Battah, Bilel Cherif, Abdechakour Mechri, Norbert Tihanyi, Tamas Bisztray, Merouane Debbah</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-21 13:02:27</h6>
<p class='card-text'>This paper provides a comprehensive review of the future of cybersecurity
through Generative AI and Large Language Models (LLMs). We explore LLM
applications across various domains, including hardware design security,
intrusion detection, software engineering, design verification, cyber threat
intelligence, malware detection, and phishing detection. We present an overview
of LLM evolution and its current state, focusing on advancements in models such
as GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends
to LLM vulnerabilities, such as prompt injection, insecure output handling,
data poisoning, DDoS attacks, and adversarial instructions. We delve into
mitigation strategies to protect these models, providing a comprehensive look
at potential attack scenarios and prevention techniques. Furthermore, we
evaluate the performance of 42 LLM models in cybersecurity knowledge and
hardware security, highlighting their strengths and weaknesses. We thoroughly
evaluate cybersecurity datasets for LLM training and testing, covering the
lifecycle from data creation to usage and identifying gaps for future research.
In addition, we review new strategies for leveraging LLMs, including techniques
like Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human
Feedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank
Adapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim
to enhance real-time cybersecurity defenses and improve the sophistication of
LLM applications in threat detection and response. Our paper provides a
foundational understanding and strategic direction for integrating LLMs into
future cybersecurity frameworks, emphasizing innovation and robust model
deployment to safeguard against evolving cyber threats.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.11143v4' target='_blank'>OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, Yu Cao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-20 01:04:40</h6>
<p class='card-text'>As large language models (LLMs) continue to grow by scaling laws,
reinforcement learning from human feedback (RLHF) has gained significant
attention due to its outstanding performance. However, unlike pretraining or
fine-tuning a single model, scaling reinforcement learning from human feedback
(RLHF) for training large language models poses coordination challenges across
four models. We present OpenRLHF, an open-source framework enabling efficient
RLHF scaling. Unlike existing RLHF frameworks that co-locate four models on the
same GPUs, OpenRLHF re-designs scheduling for the models beyond 70B parameters
using Ray, vLLM, and DeepSpeed, leveraging improved resource utilization and
diverse training approaches. Integrating seamlessly with Hugging Face, OpenRLHF
provides an out-of-the-box solution with optimized algorithms and launch
scripts, which ensures user-friendliness. OpenRLHF implements RLHF, DPO,
rejection sampling, and other alignment techniques. Empowering state-of-the-art
LLM development, OpenRLHF's code is available at
\url{https://github.com/OpenRLHF/OpenRLHF}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.11422v1' target='_blank'>Large Language Models are Biased Reinforcement Learners</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:William M. Hayes, Nicolas Yax, Stefano Palminteri</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-19 01:43:52</h6>
<p class='card-text'>In-context learning enables large language models (LLMs) to perform a variety
of tasks, including learning to make reward-maximizing choices in simple bandit
tasks. Given their potential use as (autonomous) decision-making agents, it is
important to understand how these models perform such reinforcement learning
(RL) tasks and the extent to which they are susceptible to biases. Motivated by
the fact that, in humans, it has been widely documented that the value of an
outcome depends on how it compares to other local outcomes, the present study
focuses on whether similar value encoding biases apply to how LLMs encode
rewarding outcomes. Results from experiments with multiple bandit tasks and
models show that LLMs exhibit behavioral signatures of a relative value bias.
Adding explicit outcome comparisons to the prompt produces opposing effects on
performance, enhancing maximization in trained choice sets but impairing
generalization to new choice sets. Computational cognitive modeling reveals
that LLM behavior is well-described by a simple RL algorithm that incorporates
relative values at the outcome encoding stage. Lastly, we present preliminary
evidence that the observed biases are not limited to fine-tuned LLMs, and that
relative value processing is detectable in the final hidden layer activations
of a raw, pretrained model. These findings have important implications for the
use of LLMs in decision-making applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.11106v1' target='_blank'>LLM-based Multi-Agent Reinforcement Learning: Current and Future
  Directions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chuanneng Sun, Songjun Huang, Dario Pompili</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-17 22:10:23</h6>
<p class='card-text'>In recent years, Large Language Models (LLMs) have shown great abilities in
various tasks, including question answering, arithmetic problem solving, and
poem writing, among others. Although research on LLM-as-an-agent has shown that
LLM can be applied to Reinforcement Learning (RL) and achieve decent results,
the extension of LLM-based RL to Multi-Agent System (MAS) is not trivial, as
many aspects, such as coordination and communication between agents, are not
considered in the RL frameworks of a single agent. To inspire more research on
LLM-based MARL, in this letter, we survey the existing LLM-based single-agent
and multi-agent RL frameworks and provide potential research directions for
future research. In particular, we focus on the cooperative tasks of multiple
agents with a common goal and communication among them. We also consider
human-in/on-the-loop scenarios enabled by the language component in the
framework.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.10825v2' target='_blank'>Large Language Model (LLM) for Telecommunications: A Comprehensive
  Survey on Principles, Key Techniques, and Opportunities</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hao Zhou, Chengming Hu, Ye Yuan, Yufei Cui, Yili Jin, Can Chen, Haolun Wu, Dun Yuan, Li Jiang, Di Wu, Xue Liu, Charlie Zhang, Xianbin Wang, Jiangchuan Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-17 14:46:13</h6>
<p class='card-text'>Large language models (LLMs) have received considerable attention recently
due to their outstanding comprehension and reasoning capabilities, leading to
great progress in many fields. The advancement of LLM techniques also offers
promising opportunities to automate many tasks in the telecommunication
(telecom) field. After pre-training and fine-tuning, LLMs can perform diverse
downstream tasks based on human instructions, paving the way to artificial
general intelligence (AGI)-enabled 6G. Given the great potential of LLM
technologies, this work aims to provide a comprehensive overview of LLM-enabled
telecom networks. In particular, we first present LLM fundamentals, including
model architecture, pre-training, fine-tuning, inference and utilization, model
evaluation, and telecom deployment. Then, we introduce LLM-enabled key
techniques and telecom applications in terms of generation, classification,
optimization, and prediction problems. Specifically, the LLM-enabled generation
applications include telecom domain knowledge, code, and network configuration
generation. After that, the LLM-based classification applications involve
network security, text, image, and traffic classification problems. Moreover,
multiple LLM-enabled optimization techniques are introduced, such as automated
reward function design for reinforcement learning and verbal reinforcement
learning. Furthermore, for LLM-aided prediction problems, we discussed
time-series prediction models and multi-modality prediction problems for
telecom. Finally, we highlight the challenges and identify the future
directions of LLM-enabled telecom networks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.09747v1' target='_blank'>NIFTY Financial News Headlines Dataset</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Raeid Saqur, Ken Kato, Nicholas Vinden, Frank Rudzicz</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-16 01:09:33</h6>
<p class='card-text'>We introduce and make publicly available the NIFTY Financial News Headlines
dataset, designed to facilitate and advance research in financial market
forecasting using large language models (LLMs). This dataset comprises two
distinct versions tailored for different modeling approaches: (i) NIFTY-LM,
which targets supervised fine-tuning (SFT) of LLMs with an auto-regressive,
causal language-modeling objective, and (ii) NIFTY-RL, formatted specifically
for alignment methods (like reinforcement learning from human feedback (RLHF))
to align LLMs via rejection sampling and reward modeling. Each dataset version
provides curated, high-quality data incorporating comprehensive metadata,
market indices, and deduplicated financial news headlines systematically
filtered and ranked to suit modern LLM frameworks. We also include experiments
demonstrating some applications of the dataset in tasks like stock price
movement and the role of LLM embeddings in information acquisition/richness.
The NIFTY dataset along with utilities (like truncating prompt's context length
systematically) are available on Hugging Face at
https://huggingface.co/datasets/raeidsaqur/NIFTY.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.13021v1' target='_blank'>IM-RAG: Multi-Round Retrieval-Augmented Generation Through Learning
  Inner Monologues</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Diji Yang, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Jie Yang, Yi Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-15 12:41:20</h6>
<p class='card-text'>Although the Retrieval-Augmented Generation (RAG) paradigms can use external
knowledge to enhance and ground the outputs of Large Language Models (LLMs) to
mitigate generative hallucinations and static knowledge base problems, they
still suffer from limited flexibility in adopting Information Retrieval (IR)
systems with varying capabilities, constrained interpretability during the
multi-round retrieval process, and a lack of end-to-end optimization. To
address these challenges, we propose a novel LLM-centric approach, IM-RAG, that
integrates IR systems with LLMs to support multi-round RAG through learning
Inner Monologues (IM, i.e., the human inner voice that narrates one's
thoughts). During the IM process, the LLM serves as the core reasoning model
(i.e., Reasoner) to either propose queries to collect more information via the
Retriever or to provide a final answer based on the conversational context. We
also introduce a Refiner that improves the outputs from the Retriever,
effectively bridging the gap between the Reasoner and IR modules with varying
capabilities and fostering multi-round communications. The entire IM process is
optimized via Reinforcement Learning (RL) where a Progress Tracker is
incorporated to provide mid-step rewards, and the answer prediction is further
separately optimized via Supervised Fine-Tuning (SFT). We conduct extensive
experiments with the HotPotQA dataset, a popular benchmark for retrieval-based,
multi-step question-answering. The results show that our approach achieves
state-of-the-art (SOTA) performance while providing high flexibility in
integrating IR modules as well as strong interpretability exhibited in the
learned inner monologues.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.08888v1' target='_blank'>Large Language Models for Human-Machine Collaborative Particle
  Accelerator Tuning through Natural Language</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jan Kaiser, Annika Eichler, Anne Lauscher</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-14 18:05:44</h6>
<p class='card-text'>Autonomous tuning of particle accelerators is an active and challenging field
of research with the goal of enabling novel accelerator technologies
cutting-edge high-impact applications, such as physics discovery, cancer
research and material sciences. A key challenge with autonomous accelerator
tuning remains that the most capable algorithms require an expert in
optimisation, machine learning or a similar field to implement the algorithm
for every new tuning task. In this work, we propose the use of large language
models (LLMs) to tune particle accelerators. We demonstrate on a
proof-of-principle example the ability of LLMs to successfully and autonomously
tune a particle accelerator subsystem based on nothing more than a natural
language prompt from the operator, and compare the performance of our LLM-based
solution to state-of-the-art optimisation algorithms, such as Bayesian
optimisation (BO) and reinforcement learning-trained optimisation (RLO). In
doing so, we also show how LLMs can perform numerical optimisation of a highly
non-linear real-world objective function. Ultimately, this work represents yet
another complex task that LLMs are capable of solving and promises to help
accelerate the deployment of autonomous tuning algorithms to the day-to-day
operations of particle accelerators.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.07863v3' target='_blank'>RLHF Workflow: From Reward Modeling to Online RLHF</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, Tong Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-13 15:50:39</h6>
<p class='card-text'>We present the workflow of Online Iterative Reinforcement Learning from Human
Feedback (RLHF) in this technical report, which is widely reported to
outperform its offline counterpart by a large margin in the recent large
language model (LLM) literature. However, existing open-source RLHF projects
are still largely confined to the offline learning setting. In this technical
report, we aim to fill in this gap and provide a detailed recipe that is easy
to reproduce for online iterative RLHF. In particular, since online human
feedback is usually infeasible for open-source communities with limited
resources, we start by constructing preference models using a diverse set of
open-source datasets and use the constructed proxy preference model to
approximate human feedback. Then, we discuss the theoretical insights and
algorithmic principles behind online iterative RLHF, followed by a detailed
practical implementation. Our trained LLM achieves impressive performance on
LLM chatbot benchmarks, including AlpacaEval-2, Arena-Hard, and MT-Bench, as
well as other academic benchmarks such as HumanEval and TruthfulQA. We have
shown that supervised fine-tuning (SFT) and iterative RLHF can obtain
state-of-the-art performance with fully open-source datasets. Further, we have
made our models, curated datasets, and comprehensive step-by-step code
guidebooks publicly available. Please refer to
https://github.com/RLHFlow/RLHF-Reward-Modeling and
https://github.com/RLHFlow/Online-RLHF for more detailed information.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.07667v2' target='_blank'>Simulate and Eliminate: Revoke Backdoors for Generative Large Language
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haoran Li, Yulin Chen, Zihao Zheng, Qi Hu, Chunkit Chan, Heshan Liu, Yangqiu Song</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-13 11:53:42</h6>
<p class='card-text'>With rapid advances, generative large language models (LLMs) dominate various
Natural Language Processing (NLP) tasks from understanding to reasoning. Yet,
language models' inherent vulnerabilities may be exacerbated due to increased
accessibility and unrestricted model training on massive data. A malicious
adversary may publish poisoned data online and conduct backdoor attacks on the
victim LLMs pre-trained on the poisoned data. Backdoored LLMs behave
innocuously for normal queries and generate harmful responses when the backdoor
trigger is activated. Despite significant efforts paid to LLMs' safety issues,
LLMs are still struggling against backdoor attacks. As Anthropic recently
revealed, existing safety training strategies, including supervised fine-tuning
(SFT) and Reinforcement Learning from Human Feedback (RLHF), fail to revoke the
backdoors once the LLM is backdoored during the pre-training stage. In this
paper, we present Simulate and Eliminate (SANDE) to erase the undesired
backdoored mappings for generative LLMs. We initially propose Overwrite
Supervised Fine-tuning (OSFT) for effective backdoor removal when the trigger
is known. Then, to handle scenarios where trigger patterns are unknown, we
integrate OSFT into our two-stage framework, SANDE. Unlike other works that
assume access to cleanly trained models, our safety-enhanced LLMs are able to
revoke backdoors without any reference. Consequently, our safety-enhanced LLMs
no longer produce targeted responses when the backdoor triggers are activated.
We conduct comprehensive experiments to show that our proposed SANDE is
effective against backdoor attacks while bringing minimal harm to LLMs'
powerful capability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.13009v2' target='_blank'>MetaReflection: Learning Instructions for Language Agents using Past
  Reflections</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Priyanshu Gupta, Shashank Kirtania, Ananya Singha, Sumit Gulwani, Arjun Radhakrishna, Sherry Shi, Gustavo Soares</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-13 10:51:43</h6>
<p class='card-text'>The popularity of Large Language Models (LLMs) have unleashed a new age
ofLanguage Agents for solving a diverse range of tasks. While contemporary
frontier LLMs are capable enough to power reasonably good Language agents, the
closed-API model makes it hard to improve in cases they perform sub-optimally.
To address this, recent works have explored ways to improve their performance
using techniques like self-reflection and prompt optimization. Unfortunately,
techniques like self-reflection can be used only in an online setup, while
contemporary prompt optimization techniques are designed and tested to work on
simple tasks. To this end, we introduce MetaReflection, a novel offline
reinforcement learning technique that enhances the performance of Language
Agents by augmenting a semantic memory based on experiential learnings from
past trials. We demonstrate the efficacy of MetaReflection by evaluating across
multiple domains, including complex logical reasoning, biomedical semantic
similarity, open world question answering, and vulnerability threat detection,
in Infrastructure-as-Code, spanning different agent designs. MetaReflection
boosts Language agents' performance by 4% to 16.82% over the raw GPT-4 baseline
and performs on par with existing state-of-the-art prompt optimization
techniques while requiring fewer LLM calls.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.13001v1' target='_blank'>Large Language Models for Education: A Survey</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hanyi Xu, Wensheng Gan, Zhenlian Qi, Jiayang Wu, Philip S. Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-12 01:50:01</h6>
<p class='card-text'>Artificial intelligence (AI) has a profound impact on traditional education.
In recent years, large language models (LLMs) have been increasingly used in
various applications such as natural language processing, computer vision,
speech recognition, and autonomous driving. LLMs have also been applied in many
fields, including recommendation, finance, government, education, legal
affairs, and finance. As powerful auxiliary tools, LLMs incorporate various
technologies such as deep learning, pre-training, fine-tuning, and
reinforcement learning. The use of LLMs for smart education (LLMEdu) has been a
significant strategic direction for countries worldwide. While LLMs have shown
great promise in improving teaching quality, changing education models, and
modifying teacher roles, the technologies are still facing several challenges.
In this paper, we conduct a systematic review of LLMEdu, focusing on current
technologies, challenges, and future developments. We first summarize the
current state of LLMEdu and then introduce the characteristics of LLMs and
education, as well as the benefits of integrating LLMs into education. We also
review the process of integrating LLMs into the education industry, as well as
the introduction of related technologies. Finally, we discuss the challenges
and problems faced by LLMEdu, as well as prospects for future optimization of
LLMEdu.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.06639v1' target='_blank'>Value Augmented Sampling for Language Model Alignment and
  Personalization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Seungwook Han, Idan Shenfeld, Akash Srivastava, Yoon Kim, Pulkit Agrawal</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-10 17:59:04</h6>
<p class='card-text'>Aligning Large Language Models (LLMs) to cater to different human
preferences, learning new skills, and unlearning harmful behavior is an
important problem. Search-based methods, such as Best-of-N or Monte-Carlo Tree
Search, are performant, but impractical for LLM adaptation due to their high
inference cost. On the other hand, using Reinforcement Learning (RL) for
adaptation is computationally efficient, but performs worse due to the
optimization challenges in co-training the value function and the policy. We
present a new framework for reward optimization, Value Augmented Sampling
(VAS), that can maximize different reward functions using data sampled from
only the initial, frozen LLM. VAS solves for the optimal reward-maximizing
policy without co-training the policy and the value function, making the
optimization stable, outperforming established baselines, such as PPO and DPO,
on standard benchmarks, and achieving comparable results to Best-of-128 with
lower inference cost. Unlike existing RL methods that require changing the
weights of the LLM, VAS does not require access to the weights of the
pre-trained LLM. Thus, it can even adapt LLMs (e.g., ChatGPT), which are
available only as APIs. In addition, our algorithm unlocks the new capability
of composing several rewards and controlling the extent of each one during
deployment time, paving the road ahead for the future of aligned, personalized
LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.05618v1' target='_blank'>An Automatic Prompt Generation System for Tabular Data Tasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ashlesha Akella, Abhijit Manatkar, Brij Chavda, Hima Patel</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-09 08:32:55</h6>
<p class='card-text'>Efficient processing of tabular data is important in various industries,
especially when working with datasets containing a large number of columns.
Large language models (LLMs) have demonstrated their ability on several tasks
through carefully crafted prompts. However, creating effective prompts for
tabular datasets is challenging due to the structured nature of the data and
the need to manage numerous columns. This paper presents an innovative
auto-prompt generation system suitable for multiple LLMs, with minimal
training. It proposes two novel methods; 1) A Reinforcement Learning-based
algorithm for identifying and sequencing task-relevant columns 2) Cell-level
similarity-based approach for enhancing few-shot example selection. Our
approach has been extensively tested across 66 datasets, demonstrating improved
performance in three downstream tasks: data imputation, error detection, and
entity matching using two distinct LLMs; Google flan-t5-xxl and Mixtral 8x7B.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.17439v2' target='_blank'>An Overview of Machine Learning-Enabled Optimization for Reconfigurable
  Intelligent Surfaces-Aided 6G Networks: From Reinforcement Learning to Large
  Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hao Zhou, Chengming Hu, Xue Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-09 03:07:59</h6>
<p class='card-text'>Reconfigurable intelligent surface (RIS) becomes a promising technique for 6G
networks by reshaping signal propagation in smart radio environments. However,
it also leads to significant complexity for network management due to the large
number of elements and dedicated phase-shift optimization. In this work, we
provide an overview of machine learning (ML)-enabled optimization for RIS-aided
6G networks. In particular, we focus on various reinforcement learning (RL)
techniques, e.g., deep Q-learning, multi-agent reinforcement learning, transfer
reinforcement learning, hierarchical reinforcement learning, and offline
reinforcement learning. Different from existing studies, this work further
discusses how large language models (LLMs) can be combined with RL to handle
network optimization problems. It shows that LLM offers new opportunities to
enhance the capabilities of RL algorithms in terms of generalization, reward
function design, multi-modal information processing, etc. Finally, we identify
the future challenges and directions of ML-enabled optimization for RIS-aided
6G networks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.05060v1' target='_blank'>Conversational Topic Recommendation in Counseling and Psychotherapy with
  Decision Transformer and Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aylin Gunal, Baihan Lin, Djallel Bouneffouf</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-08 13:55:25</h6>
<p class='card-text'>Given the increasing demand for mental health assistance, artificial
intelligence (AI), particularly large language models (LLMs), may be valuable
for integration into automated clinical support systems. In this work, we
leverage a decision transformer architecture for topic recommendation in
counseling conversations between patients and mental health professionals. The
architecture is utilized for offline reinforcement learning, and we extract
states (dialogue turn embeddings), actions (conversation topics), and rewards
(scores measuring the alignment between patient and therapist) from previous
turns within a conversation to train a decision transformer model. We
demonstrate an improvement over baseline reinforcement learning methods, and
propose a novel system of utilizing our model's output as synthetic labels for
fine-tuning a large language model for the same task. Although our
implementation based on LLaMA-2 7B has mixed results, future work can
undoubtedly build on the design.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.04135v3' target='_blank'>Human-centric Reward Optimization for Reinforcement Learning-based
  Automated Driving using Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziqi Zhou, Jingyue Zhang, Jingyuan Zhang, Yangfan He, Boyue Wang, Tianyu Shi, Alaa Khamis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-07 09:04:52</h6>
<p class='card-text'>One of the key challenges in current Reinforcement Learning (RL)-based
Automated Driving (AD) agents is achieving flexible, precise, and human-like
behavior cost-effectively. This paper introduces an innovative approach that
uses large language models (LLMs) to intuitively and effectively optimize RL
reward functions in a human-centric way. We developed a framework where
instructions and dynamic environment descriptions are input into the LLM. The
LLM then utilizes this information to assist in generating rewards, thereby
steering the behavior of RL agents towards patterns that more closely resemble
human driving. The experimental results demonstrate that this approach not only
makes RL agents more anthropomorphic but also achieves better performance.
Additionally, various strategies for reward-proxy and reward-shaping are
investigated, revealing the significant impact of prompt design on shaping an
AD vehicle's behavior. These findings offer a promising direction for the
development of more advanced, human-like automated driving systems. Our
experimental data and source code can be found here</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.03813v1' target='_blank'>Large Language Models as Instruments of Power: New Regimes of Autonomous
  Manipulation and Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yaqub Chaudhary, Jonnie Penn</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-06 19:52:57</h6>
<p class='card-text'>Large language models (LLMs) can reproduce a wide variety of rhetorical
styles and generate text that expresses a broad spectrum of sentiments. This
capacity, now available at low cost, makes them powerful tools for manipulation
and control. In this paper, we consider a set of underestimated societal harms
made possible by the rapid and largely unregulated adoption of LLMs. Rather
than consider LLMs as isolated digital artefacts used to displace this or that
area of work, we focus on the large-scale computational infrastructure upon
which they are instrumentalised across domains. We begin with discussion on how
LLMs may be used to both pollute and uniformize information environments and
how these modalities may be leveraged as mechanisms of control. We then draw
attention to several areas of emerging research, each of which compounds the
capabilities of LLMs as instruments of power. These include (i) persuasion
through the real-time design of choice architectures in conversational
interfaces (e.g., via "AI personas"), (ii) the use of LLM-agents as
computational models of human agents (e.g., "silicon subjects"), (iii) the use
of LLM-agents as computational models of human agent populations (e.g.,
"silicon societies") and finally, (iv) the combination of LLMs with
reinforcement learning to produce controllable and steerable strategic dialogue
models. We draw these strands together to discuss how these areas may be
combined to build LLM-based systems that serve as powerful instruments of
individual, social and political control via the simulation and disingenuous
"prediction" of human behaviour, intent, and action.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.03547v2' target='_blank'>Position: Leverage Foundational Models for Black-Box Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xingyou Song, Yingtao Tian, Robert Tjarko Lange, Chansoo Lee, Yujin Tang, Yutian Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-06 15:10:46</h6>
<p class='card-text'>Undeniably, Large Language Models (LLMs) have stirred an extraordinary wave
of innovation in the machine learning research domain, resulting in substantial
impact across diverse fields such as reinforcement learning, robotics, and
computer vision. Their incorporation has been rapid and transformative, marking
a significant paradigm shift in the field of machine learning research.
However, the field of experimental design, grounded on black-box optimization,
has been much less affected by such a paradigm shift, even though integrating
LLMs with optimization presents a unique landscape ripe for exploration. In
this position paper, we frame the field of black-box optimization around
sequence-based foundation models and organize their relationship with previous
literature. We discuss the most promising ways foundational language models can
revolutionize optimization, which include harnessing the vast wealth of
information encapsulated in free-form text to enrich task comprehension,
utilizing highly flexible sequence models such as Transformers to engineer
superior optimization strategies, and enhancing performance prediction over
previously unseen search spaces.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.03341v3' target='_blank'>Enhancing Q-Learning with Large Language Model Heuristics</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiefeng Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-06 10:42:28</h6>
<p class='card-text'>Q-learning excels in learning from feedback within sequential decision-making
tasks but often requires extensive sampling to achieve significant
improvements. While reward shaping can enhance learning efficiency,
non-potential-based methods introduce biases that affect performance, and
potential-based reward shaping, though unbiased, lacks the ability to provide
heuristics for state-action pairs, limiting its effectiveness in complex
environments. Large language models (LLMs) can achieve zero-shot learning for
simpler tasks, but they suffer from low inference speeds and occasional
hallucinations. To address these challenges, we propose \textbf{LLM-guided
Q-learning}, a framework that leverages LLMs as heuristics to aid in learning
the Q-function for reinforcement learning. Our theoretical analysis
demonstrates that this approach adapts to hallucinations, improves sample
efficiency, and avoids biasing final performance. Experimental results show
that our algorithm is general, robust, and capable of preventing ineffective
exploration.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.03236v1' target='_blank'>Federated Reinforcement Learning with Constraint Heterogeneity</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hao Jin, Liangyu Zhang, Zhihua Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-06 07:44:50</h6>
<p class='card-text'>We study a Federated Reinforcement Learning (FedRL) problem with constraint
heterogeneity. In our setting, we aim to solve a reinforcement learning problem
with multiple constraints while $N$ training agents are located in $N$
different environments with limited access to the constraint signals and they
are expected to collaboratively learn a policy satisfying all constraint
signals. Such learning problems are prevalent in scenarios of Large Language
Model (LLM) fine-tuning and healthcare applications. To solve the problem, we
propose federated primal-dual policy optimization methods based on traditional
policy gradient methods. Specifically, we introduce $N$ local Lagrange
functions for agents to perform local policy updates, and these agents are then
scheduled to periodically communicate on their local policies. Taking natural
policy gradient (NPG) and proximal policy optimization (PPO) as policy
optimization methods, we mainly focus on two instances of our algorithms, ie,
{FedNPG} and {FedPPO}. We show that FedNPG achieves global convergence with an
$\tilde{O}(1/\sqrt{T})$ rate, and FedPPO efficiently solves complicated
learning tasks with the use of deep neural networks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.01534v1' target='_blank'>Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon
  Robotics Tasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Murtaza Dalal, Tarun Chiruvolu, Devendra Chaplot, Ruslan Salakhutdinov</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-02 17:59:31</h6>
<p class='card-text'>Large Language Models (LLMs) have been shown to be capable of performing
high-level planning for long-horizon robotics tasks, yet existing methods
require access to a pre-defined skill library (e.g. picking, placing, pulling,
pushing, navigating). However, LLM planning does not address how to design or
learn those behaviors, which remains challenging particularly in long-horizon
settings. Furthermore, for many tasks of interest, the robot needs to be able
to adjust its behavior in a fine-grained manner, requiring the agent to be
capable of modifying low-level control actions. Can we instead use the
internet-scale knowledge from LLMs for high-level policies, guiding
reinforcement learning (RL) policies to efficiently solve robotic control tasks
online without requiring a pre-determined set of skills? In this paper, we
propose Plan-Seq-Learn (PSL): a modular approach that uses motion planning to
bridge the gap between abstract language and learned low-level control for
solving long-horizon robotics tasks from scratch. We demonstrate that PSL
achieves state-of-the-art results on over 25 challenging robotics tasks with up
to 10 stages. PSL solves long-horizon tasks from raw visual input spanning four
benchmarks at success rates of over 85%, out-performing language-based,
classical, and end-to-end approaches. Video results and code at
https://mihdalal.github.io/planseqlearn/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.01525v1' target='_blank'>FLAME: Factuality-Aware Alignment for Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sheng-Chieh Lin, Luyu Gao, Barlas Oguz, Wenhan Xiong, Jimmy Lin, Wen-tau Yih, Xilun Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-02 17:54:54</h6>
<p class='card-text'>Alignment is a standard procedure to fine-tune pre-trained large language
models (LLMs) to follow natural language instructions and serve as helpful AI
assistants. We have observed, however, that the conventional alignment process
fails to enhance the factual accuracy of LLMs, and often leads to the
generation of more false facts (i.e. hallucination). In this paper, we study
how to make the LLM alignment process more factual, by first identifying
factors that lead to hallucination in both alignment steps:\ supervised
fine-tuning (SFT) and reinforcement learning (RL). In particular, we find that
training the LLM on new knowledge or unfamiliar texts can encourage
hallucination. This makes SFT less factual as it trains on human labeled data
that may be novel to the LLM. Furthermore, reward functions used in standard RL
can also encourage hallucination, because it guides the LLM to provide more
helpful responses on a diverse set of instructions, often preferring longer and
more detailed responses. Based on these observations, we propose
factuality-aware alignment, comprised of factuality-aware SFT and
factuality-aware RL through direct preference optimization. Experiments show
that our proposed factuality-aware alignment guides LLMs to output more factual
responses while maintaining instruction-following capability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.01481v2' target='_blank'>NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Gerald Shen, Zhilin Wang, Olivier Delalleau, Jiaqi Zeng, Yi Dong, Daniel Egert, Shengyang Sun, Jimmy Zhang, Sahil Jain, Ali Taghibakhshi, Markel Sanz Ausin, Ashwath Aithal, Oleksii Kuchaiev</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-02 17:13:40</h6>
<p class='card-text'>Aligning Large Language Models (LLMs) with human values and preferences is
essential for making them helpful and safe. However, building efficient tools
to perform alignment can be challenging, especially for the largest and most
competent LLMs which often contain tens or hundreds of billions of parameters.
We create NeMo-Aligner, a toolkit for model alignment that can efficiently
scale to a thousand GPUs for training the largest open-source LLMs such as
Nemotron 4 340B and Llama 3.1 405B. NeMo-Aligner comes with highly optimized
and scalable implementations for major paradigms of model alignment such as:
Reinforcement Learning from Human Feedback (RLHF), Direct Preference
Optimization (DPO), SteerLM, and Self-Play Fine-Tuning (SPIN). Additionally,
our toolkit supports running most of the alignment techniques in a Parameter
Efficient Fine-Tuning (PEFT) setting. NeMo-Aligner is designed for
extensibility, allowing support for other alignment techniques with minimal
effort. It is open-sourced with Apache 2.0 License and we invite community
contributions at https://github.com/NVIDIA/NeMo-Aligner</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.00675v5' target='_blank'>Self-Play Preference Optimization for Language Model Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, Quanquan Gu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-01 17:59:20</h6>
<p class='card-text'>Standard reinforcement learning from human feedback (RLHF) approaches relying
on parametric models like the Bradley-Terry model fall short in capturing the
intransitivity and irrationality in human preferences. Recent advancements
suggest that directly working with preference probabilities can yield a more
accurate reflection of human preferences, enabling more flexible and accurate
language model alignment. In this paper, we propose a self-play-based method
for language model alignment, which treats the problem as a constant-sum
two-player game aimed at identifying the Nash equilibrium policy. Our approach,
dubbed Self-Play Preference Optimization (SPPO), utilizes iterative policy
updates to provably approximate the Nash equilibrium. Additionally, we propose
a new SPPO objective which is both strongly motivated by theory and is simple
and effective in practice. In our experiments, using only 60k prompts (without
responses) from the UltraFeedback dataset and without any prompt augmentation,
by leveraging a pre-trained preference model PairRM with only 0.4B parameters,
SPPO can obtain a model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves
the state-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo
on AlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench,
Arena-Hard, and the Open LLM Leaderboard. Starting from a stronger base model
Llama-3-8B-Instruct, we are able to achieve a length-controlled win rate of
38.77%. Notably, the strong performance of SPPO is achieved without additional
external supervision (e.g., responses, preferences, etc.) from GPT-4 or other
stronger language models. Codes are available at
https://github.com/uclaml/SPPO.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.00578v1' target='_blank'>The Real, the Better: Aligning Large Language Models with Online Human
  Behaviors</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guanying Jiang, Lingyong Yan, Haibo Shi, Dawei Yin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-01 15:30:41</h6>
<p class='card-text'>Large language model alignment is widely used and studied to avoid LLM
producing unhelpful and harmful responses. However, the lengthy training
process and predefined preference bias hinder adaptation to online diverse
human preferences. To this end, this paper proposes an alignment framework,
called Reinforcement Learning with Human Behavior (RLHB), to align LLMs by
directly leveraging real online human behaviors. By taking the generative
adversarial framework, the generator is trained to respond following expected
human behavior; while the discriminator tries to verify whether the triplets of
query, response, and human behavior come from real online environments.
Behavior modeling in natural-language form and the multi-model joint training
mechanism enable an active and sustainable online alignment. Experimental
results confirm the effectiveness of our proposed methods by both human and
automatic evaluations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.19409v1' target='_blank'>Countering Reward Over-optimization in LLM with Demonstration-Guided
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mathieu Rita, Florian Strub, Rahma Chaabouni, Paul Michel, Emmanuel Dupoux, Olivier Pietquin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-30 09:57:21</h6>
<p class='card-text'>While Reinforcement Learning (RL) has been proven essential for tuning large
language models (LLMs), it can lead to reward over-optimization (ROO). Existing
approaches address ROO by adding KL regularization, requiring computationally
expensive hyperparameter tuning. Additionally, KL regularization focuses solely
on regularizing the language policy, neglecting a potential source of
regularization: the reward function itself. Inspired by demonstration-guided
RL, we here introduce the Reward Calibration from Demonstration (RCfD), which
leverages human demonstrations and a reward model to recalibrate the reward
objective. Formally, given a prompt, the RCfD objective minimizes the distance
between the demonstrations' and LLM's rewards rather than directly maximizing
the reward function. This objective shift avoids incentivizing the LLM to
exploit the reward model and promotes more natural and diverse language
generation. We show the effectiveness of RCfD on three language tasks, which
achieves comparable performance to carefully tuned baselines while mitigating
ROO.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.18870v2' target='_blank'>More RLHF, More Trust? On The Impact of Preference Alignment On
  Trustworthiness</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aaron J. Li, Satyapriya Krishna, Himabindu Lakkaraju</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-29 17:00:53</h6>
<p class='card-text'>The trustworthiness of Large Language Models (LLMs) refers to the extent to
which their outputs are reliable, safe, and ethically aligned, and it has
become a crucial consideration alongside their cognitive performance. In
practice, Reinforcement Learning From Human Feedback (RLHF) has been widely
used to align LLMs with labeled human preferences, but its assumed effect on
model trustworthiness hasn't been rigorously evaluated. To bridge this
knowledge gap, this study investigates how models aligned with general-purpose
preference data perform across five trustworthiness verticals: toxicity,
stereotypical bias, machine ethics, truthfulness, and privacy. Our results
demonstrate that RLHF on human preferences doesn't automatically guarantee
trustworthiness, and reverse effects are often observed. Furthermore, we
propose to adapt efficient influence function based data attribution methods to
the RLHF setting to better understand the influence of fine-tuning data on
individual trustworthiness benchmarks, and show its feasibility by providing
our estimated attribution scores. Together, our results underscore the need for
more nuanced approaches for model alignment from both the data and framework
perspectives, and we hope this research will guide the community towards
developing language models that are increasingly capable without sacrificing
trustworthiness.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.18864v1' target='_blank'>Performance-Aligned LLMs for Generating Fast Code</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Daniel Nichols, Pranav Polasam, Harshitha Menon, Aniruddha Marathe, Todd Gamblin, Abhinav Bhatele</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-29 16:52:38</h6>
<p class='card-text'>Optimizing scientific software is a difficult task because codebases are
often large and complex, and performance can depend upon several factors
including the algorithm, its implementation, and hardware among others. Causes
of poor performance can originate from disparate sources and be difficult to
diagnose. Recent years have seen a multitude of work that use large language
models (LLMs) to assist in software development tasks. However, these tools are
trained to model the distribution of code as text, and are not specifically
designed to understand performance aspects of code. In this work, we introduce
a reinforcement learning based methodology to align the outputs of code LLMs
with performance. This allows us to build upon the current code modeling
capabilities of LLMs and extend them to generate better performing code. We
demonstrate that our fine-tuned model improves the expected speedup of
generated code over base models for a set of benchmark tasks from 0.9 to 1.6
for serial code and 1.9 to 4.5 for OpenMP code.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.18978v1' target='_blank'>Towards Generalizable Agents in Text-Based Educational Environments: A
  Study of Integrating RL with LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bahar Radmehr, Adish Singla, Tanja Käser</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-29 14:53:48</h6>
<p class='card-text'>There has been a growing interest in developing learner models to enhance
learning and teaching experiences in educational environments. However,
existing works have primarily focused on structured environments relying on
meticulously crafted representations of tasks, thereby limiting the agent's
ability to generalize skills across tasks. In this paper, we aim to enhance the
generalization capabilities of agents in open-ended text-based learning
environments by integrating Reinforcement Learning (RL) with Large Language
Models (LLMs). We investigate three types of agents: (i) RL-based agents that
utilize natural language for state and action representations to find the best
interaction strategy, (ii) LLM-based agents that leverage the model's general
knowledge and reasoning through prompting, and (iii) hybrid LLM-assisted RL
agents that combine these two strategies to improve agents' performance and
generalization. To support the development and evaluation of these agents, we
introduce PharmaSimText, a novel benchmark derived from the PharmaSim virtual
pharmacy environment designed for practicing diagnostic conversations. Our
results show that RL-based agents excel in task completion but lack in asking
quality diagnostic questions. In contrast, LLM-based agents perform better in
asking diagnostic questions but fall short of completing the task. Finally,
hybrid LLM-assisted RL agents enable us to overcome these limitations,
highlighting the potential of combining RL and LLMs to develop high-performing
agents for open-ended learning environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.18638v1' target='_blank'>Reinforcement Learning Problem Solving with Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sina Gholamian, Domingo Huh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-29 12:16:08</h6>
<p class='card-text'>Large Language Models (LLMs) encapsulate an extensive amount of world
knowledge, and this has enabled their application in various domains to improve
the performance of a variety of Natural Language Processing (NLP) tasks. This
has also facilitated a more accessible paradigm of conversation-based
interactions between humans and AI systems to solve intended problems. However,
one interesting avenue that shows untapped potential is the use of LLMs as
Reinforcement Learning (RL) agents to enable conversational RL problem solving.
Therefore, in this study, we explore the concept of formulating Markov Decision
Process-based RL problems as LLM prompting tasks. We demonstrate how LLMs can
be iteratively prompted to learn and optimize policies for specific RL tasks.
In addition, we leverage the introduced prompting technique for episode
simulation and Q-Learning, facilitated by LLMs. We then show the practicality
of our approach through two detailed case studies for "Research Scientist" and
"Legal Matter Intake" workflows.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.17546v1' target='_blank'>Probabilistic Inference in Language Models via Twisted Sequential Monte
  Carlo</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Stephen Zhao, Rob Brekelmans, Alireza Makhzani, Roger Grosse</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-26 17:18:32</h6>
<p class='card-text'>Numerous capability and safety techniques of Large Language Models (LLMs),
including RLHF, automated red-teaming, prompt engineering, and infilling, can
be cast as sampling from an unnormalized target distribution defined by a given
reward or potential function over the full sequence. In this work, we leverage
the rich toolkit of Sequential Monte Carlo (SMC) for these probabilistic
inference problems. In particular, we use learned twist functions to estimate
the expected future value of the potential at each timestep, which enables us
to focus inference-time computation on promising partial sequences. We propose
a novel contrastive method for learning the twist functions, and establish
connections with the rich literature of soft reinforcement learning. As a
complementary application of our twisted SMC framework, we present methods for
evaluating the accuracy of language model inference techniques using novel
bidirectional SMC bounds on the log partition function. These bounds can be
used to estimate the KL divergence between the inference and target
distributions in both directions. We apply our inference evaluation techniques
to show that twisted SMC is effective for sampling undesirable outputs from a
pretrained model (a useful component of harmlessness training and automated
red-teaming), generating reviews with varied sentiment, and performing
infilling tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.17287v3' target='_blank'>When to Trust LLMs: Aligning Confidence with Response Quality</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shuchang Tao, Liuyi Yao, Hanxing Ding, Yuexiang Xie, Qi Cao, Fei Sun, Jinyang Gao, Huawei Shen, Bolin Ding</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-26 09:42:46</h6>
<p class='card-text'>Despite the success of large language models (LLMs) in natural language
generation, much evidence shows that LLMs may produce incorrect or nonsensical
text. This limitation highlights the importance of discerning when to trust
LLMs, especially in safety-critical domains. Existing methods often express
reliability by confidence level, however, their effectiveness is limited by the
lack of objective guidance. To address this, we propose
CONfidence-Quality-ORDer-preserving alignment approach (CONQORD), which
leverages reinforcement learning guided by a tailored dual-component reward
function. This function integrates quality reward and order-preserving
alignment reward functions. Specifically, the order-preserving reward
incentivizes the model to verbalize greater confidence for responses of higher
quality to align the order of confidence and quality. Experiments demonstrate
that CONQORD significantly improves the alignment performance between
confidence and response accuracy, without causing over-cautious. Furthermore,
the aligned confidence provided by CONQORD informs when to trust LLMs, and acts
as a determinant for initiating the retrieval process of external knowledge.
Aligning confidence with response quality ensures more transparent and reliable
responses, providing better trustworthiness.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.16767v4' target='_blank'>REBEL: Reinforcement Learning via Regressing Relative Rewards</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhaolin Gao, Jonathan D. Chang, Wenhao Zhan, Owen Oertell, Gokul Swamy, Kianté Brantley, Thorsten Joachims, J. Andrew Bagnell, Jason D. Lee, Wen Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-25 17:20:45</h6>
<p class='card-text'>While originally developed for continuous control problems, Proximal Policy
Optimization (PPO) has emerged as the work-horse of a variety of reinforcement
learning (RL) applications, including the fine-tuning of generative models.
Unfortunately, PPO requires multiple heuristics to enable stable convergence
(e.g. value networks, clipping), and is notorious for its sensitivity to the
precise implementation of these components. In response, we take a step back
and ask what a minimalist RL algorithm for the era of generative models would
look like. We propose REBEL, an algorithm that cleanly reduces the problem of
policy optimization to regressing the relative reward between two completions
to a prompt in terms of the policy, enabling strikingly lightweight
implementation. In theory, we prove that fundamental RL algorithms like Natural
Policy Gradient can be seen as variants of REBEL, which allows us to match the
strongest known theoretical guarantees in terms of convergence and sample
complexity in the RL literature. REBEL can also cleanly incorporate offline
data and be extended to handle the intransitive preferences we frequently see
in practice. Empirically, we find that REBEL provides a unified approach to
language modeling and image generation with stronger or similar performance as
PPO and DPO, all while being simpler to implement and more computationally
efficient than PPO. When fine-tuning Llama-3-8B-Instruct, REBEL achieves strong
performance in AlpacaEval 2.0, MT-Bench, and Open LLM Leaderboard.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.00715v4' target='_blank'>Adapting Open-Source Large Language Models for Cost-Effective,
  Expert-Level Clinical Note Generation with On-Policy Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hanyin Wang, Chufan Gao, Bolun Liu, Qiping Xu, Guleid Hussein, Mohamad El Labban, Kingsley Iheasirim, Hariprasad Korsapati, Chuck Outcalt, Jimeng Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-25 15:34:53</h6>
<p class='card-text'>Proprietary Large Language Models (LLMs) such as GPT-4 and Gemini have
demonstrated promising capabilities in clinical text summarization tasks.
However, due to patient data privacy concerns and computational costs, many
healthcare providers prefer using small, locally-hosted models over external
generic LLMs. This study presents a comprehensive domain- and task-specific
adaptation process for the open-source LLaMA-2 13 billion parameter model,
enabling it to generate high-quality clinical notes from outpatient
patient-doctor dialogues. Our process incorporates continued pre-training,
supervised fine-tuning, and reinforcement learning from both AI and human
feedback. We introduced a new approach, DistillDirect, for performing on-policy
reinforcement learning with Gemini 1.0 Pro as the teacher model. Our resulting
model, LLaMA-Clinic, can generate clinical notes comparable in quality to those
authored by physicians. In a blinded physician reader study, the majority
(90.4%) of individual evaluations rated the notes generated by LLaMA-Clinic as
"acceptable" or higher across all three criteria: real-world readiness,
completeness, and accuracy. In the more challenging "Assessment and Plan"
section, LLaMA-Clinic scored higher (4.2/5) in real-world readiness than
physician-authored notes (4.1/5). Our cost analysis for inference shows that
our LLaMA-Clinic model achieves a 3.75-fold cost reduction compared to an
external generic LLM service. Additionally, we highlight key considerations for
future clinical note-generation tasks, emphasizing the importance of
pre-defining a best-practice note format, rather than relying on LLMs to
determine this for clinical practice. We have made our newly created synthetic
clinic dialogue-note dataset and the physician feedback dataset publicly
available to foster future research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.16621v1' target='_blank'>Hippocrates: An Open-Source Framework for Advancing Large Language
  Models in Healthcare</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Emre Can Acikgoz, Osman Batur İnce, Rayene Bench, Arda Anıl Boz, İlker Kesen, Aykut Erdem, Erkut Erdem</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-25 14:06:37</h6>
<p class='card-text'>The integration of Large Language Models (LLMs) into healthcare promises to
transform medical diagnostics, research, and patient care. Yet, the progression
of medical LLMs faces obstacles such as complex training requirements, rigorous
evaluation demands, and the dominance of proprietary models that restrict
academic exploration. Transparent, comprehensive access to LLM resources is
essential for advancing the field, fostering reproducibility, and encouraging
innovation in healthcare AI. We present Hippocrates, an open-source LLM
framework specifically developed for the medical domain. In stark contrast to
previous efforts, it offers unrestricted access to its training datasets,
codebase, checkpoints, and evaluation protocols. This open approach is designed
to stimulate collaborative research, allowing the community to build upon,
refine, and rigorously evaluate medical LLMs within a transparent ecosystem.
Also, we introduce Hippo, a family of 7B models tailored for the medical
domain, fine-tuned from Mistral and LLaMA2 through continual pre-training,
instruction tuning, and reinforcement learning from human and AI feedback. Our
models outperform existing open medical LLMs models by a large-margin, even
surpassing models with 70B parameters. Through Hippocrates, we aspire to unlock
the full potential of LLMs not just to advance medical knowledge and patient
care but also to democratize the benefits of AI research in healthcare, making
them available across the globe.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.14367v3' target='_blank'>Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy
  Data</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea Finn, Aviral Kumar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-22 17:20:18</h6>
<p class='card-text'>Learning from preference labels plays a crucial role in fine-tuning large
language models. There are several distinct approaches for preference
fine-tuning, including supervised learning, on-policy reinforcement learning
(RL), and contrastive learning. Different methods come with different
implementation tradeoffs and performance differences, and existing empirical
findings present different conclusions, for instance, some results show that
online RL is quite important to attain good fine-tuning results, while others
find (offline) contrastive or even purely supervised methods sufficient. This
raises a natural question: what kind of approaches are important for
fine-tuning with preference data and why? In this paper, we answer this
question by performing a rigorous analysis of a number of fine-tuning
techniques on didactic and full-scale LLM problems. Our main finding is that,
in general, approaches that use on-policy sampling or attempt to push down the
likelihood on certain responses (i.e., employ a "negative gradient") outperform
offline and maximum likelihood objectives. We conceptualize our insights and
unify methods that use on-policy sampling or negative gradient under a notion
of mode-seeking objectives for categorical distributions. Mode-seeking
objectives are able to alter probability mass on specific bins of a categorical
distribution at a fast rate compared to maximum likelihood, allowing them to
relocate masses across bins more effectively. Our analysis prescribes
actionable insights for preference fine-tuning of LLMs and informs how data
should be collected for maximal improvement.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.13906v2' target='_blank'>Generating Attractive and Authentic Copywriting from Customer Reviews</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yu-Xiang Lin, Wei-Yun Ma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-22 06:33:28</h6>
<p class='card-text'>The goal of product copywriting is to capture the interest of potential
buyers by emphasizing the features of products through text descriptions. As
e-commerce platforms offer a wide range of services, it's becoming essential to
dynamically adjust the styles of these auto-generated descriptions. Typical
approaches to copywriting generation often rely solely on specified product
attributes, which may result in dull and repetitive content. To tackle this
issue, we propose to generate copywriting based on customer reviews, as they
provide firsthand practical experiences with products, offering a richer source
of information than just product attributes. We have developed a
sequence-to-sequence framework, enhanced with reinforcement learning, to
produce copywriting that is attractive, authentic, and rich in information. Our
framework outperforms all existing baseline and zero-shot large language
models, including LLaMA-2-chat-7B and GPT-3.5, in terms of both attractiveness
and faithfulness. Furthermore, this work features the use of LLMs for
aspect-based summaries collection and argument allure assessment. Experiments
demonstrate the effectiveness of using LLMs for marketing domain corpus
construction. The code and the dataset is publicly available at:
https://github.com/YuXiangLin1234/Copywriting-Generation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.13238v1' target='_blank'>Personalized Wireless Federated Learning for Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Feibo Jiang, Li Dong, Siwei Tu, Yubo Peng, Kezhi Wang, Kun Yang, Cunhua Pan, Dusit Niyato</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-20 02:30:21</h6>
<p class='card-text'>Large Language Models (LLMs) have revolutionized natural language processing
tasks. However, their deployment in wireless networks still face challenges,
i.e., a lack of privacy and security protection mechanisms. Federated Learning
(FL) has emerged as a promising approach to address these challenges. Yet, it
suffers from issues including inefficient handling with big and heterogeneous
data, resource-intensive training, and high communication overhead. To tackle
these issues, we first compare different learning stages and their features of
LLMs in wireless networks. Next, we introduce two personalized wireless
federated fine-tuning methods with low communication overhead, i.e., (1)
Personalized Federated Instruction Tuning (PFIT), which employs reinforcement
learning to fine-tune local LLMs with diverse reward models to achieve
personalization; (2) Personalized Federated Task Tuning (PFTT), which can
leverage global adapters and local Low-Rank Adaptations (LoRA) to
collaboratively fine-tune local LLMs, where the local LoRAs can be applied to
achieve personalization without aggregation. Finally, we perform simulations to
demonstrate the effectiveness of the proposed two methods and comprehensively
discuss open issues.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.12926v2' target='_blank'>MM-PhyRLHF: Reinforcement Learning Framework for Multimodal Physics
  Question-Answering</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Janak Kapuriya, Chhavi Kirtani, Apoorv Singh, Jay Saraf, Naman Lal, Jatin Kumar, Adarsh Raj Shivam, Astha Verma, Avinash Anand, Rajiv Ratn Shah</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-19 14:52:57</h6>
<p class='card-text'>Recent advancements in LLMs have shown their significant potential in tasks
like text summarization and generation. Yet, they often encounter difficulty
while solving complex physics problems that require arithmetic calculation and
a good understanding of concepts. Moreover, many physics problems include
images that contain important details required to understand the problem's
context. We propose an LMM-based chatbot to answer multimodal physics MCQs. For
domain adaptation, we utilize the MM-PhyQA dataset comprising Indian high
school-level multimodal physics problems. To improve the LMM's performance, we
experiment with two techniques, RLHF (Reinforcement Learning from Human
Feedback) and Image Captioning. In image captioning, we add a detailed
explanation of the diagram in each image, minimizing hallucinations and image
processing errors. We further explore the integration of Reinforcement Learning
from Human Feedback (RLHF) methodology inspired by the ranking approach in RLHF
to enhance the human-like problem-solving abilities of the models. The RLHF
approach incorporates human feedback into the learning process of LLMs,
improving the model's problem-solving skills, truthfulness, and reasoning
capabilities, minimizing the hallucinations in the answers, and improving the
quality instead of using vanilla-supervised fine-tuned models. We employ the
LLaVA open-source model to answer multimodal physics MCQs and compare the
performance with and without using RLHF.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.11973v1' target='_blank'>Exploring the landscape of large language models: Foundations,
  techniques, and challenges</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Milad Moradi, Ke Yan, David Colwell, Matthias Samwald, Rhona Asgari</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-18 08:01:20</h6>
<p class='card-text'>In this review paper, we delve into the realm of Large Language Models
(LLMs), covering their foundational principles, diverse applications, and
nuanced training processes. The article sheds light on the mechanics of
in-context learning and a spectrum of fine-tuning approaches, with a special
focus on methods that optimize efficiency in parameter usage. Additionally, it
explores how LLMs can be more closely aligned with human preferences through
innovative reinforcement learning frameworks and other novel methods that
incorporate human feedback. The article also examines the emerging technique of
retrieval augmented generation, integrating external knowledge into LLMs. The
ethical dimensions of LLM deployment are discussed, underscoring the need for
mindful and responsible application. Concluding with a perspective on future
research trajectories, this review offers a succinct yet comprehensive overview
of the current state and emerging trends in the evolving landscape of LLMs,
serving as an insightful guide for both researchers and practitioners in
artificial intelligence.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.13082v2' target='_blank'>Efficient Contextual LLM Cascades through Budget-Constrained Policy
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xuechen Zhang, Zijian Huang, Ege Onur Taga, Carlee Joe-Wong, Samet Oymak, Jiasi Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-17 05:56:49</h6>
<p class='card-text'>Recent successes in natural language processing have led to the proliferation
of large language models (LLMs) by multiple providers. Each LLM offering has
different inference accuracy, monetary cost, and latency, and their accuracy
further depends on the exact wording of the question (i.e., the specific
prompt). At the same time, users often have a limit on monetary budget and
latency to answer all their questions, and they do not know which LLMs to
choose for each question to meet their accuracy and long term budget
requirements. To navigate this rich design space, we propose TREACLE
($\underline{T}$hrifty $\underline{Rea}$soning via $\underline{C}$ontext-Aware
$\underline{L}$LM and Prompt S$\underline{e}$lection), a reinforcement learning
policy that jointly selects the model and prompting scheme while respecting the
user's monetary cost and latency constraints. TREACLE uses the problem context,
including question text embeddings (reflecting the type or difficulty of a
query) and the response history (reflecting the consistency of previous
responses) to make smart decisions. Our evaluations on standard reasoning
datasets (GSM8K, CSQA, and LLC) with various LLMs and prompts show that TREACLE
enables cost savings of up to 85% compared to baselines, while maintaining high
accuracy. Importantly, it provides the user with the ability to gracefully
trade off accuracy for cost.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.10960v1' target='_blank'>Uncertainty-Based Abstention in LLMs Improves Safety and Reduces
  Hallucinations</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Christian Tomani, Kamalika Chaudhuri, Ivan Evtimov, Daniel Cremers, Mark Ibrahim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-16 23:56:38</h6>
<p class='card-text'>A major barrier towards the practical deployment of large language models
(LLMs) is their lack of reliability. Three situations where this is
particularly apparent are correctness, hallucinations when given unanswerable
questions, and safety. In all three cases, models should ideally abstain from
responding, much like humans, whose ability to understand uncertainty makes us
refrain from answering questions we don't know. Inspired by analogous
approaches in classification, this study explores the feasibility and efficacy
of abstaining while uncertain in the context of LLMs within the domain of
question-answering. We investigate two kinds of uncertainties, statistical
uncertainty metrics and a distinct verbalized measure, termed as In-Dialogue
Uncertainty (InDU). Using these uncertainty measures combined with models with
and without Reinforcement Learning with Human Feedback (RLHF), we show that in
all three situations, abstention based on the right kind of uncertainty measure
can boost the reliability of LLMs. By sacrificing only a few highly uncertain
samples we can improve correctness by 2% to 8%, avoid 50% hallucinations via
correctly identifying unanswerable questions and increase safety by 70% up to
99% with almost no additional computational overhead.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.10887v2' target='_blank'>Grounded Language Agent for Product Search via Intelligent Web
  Interactions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Moghis Fereidouni, Adib Mosharrof, A. B. Siddique</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-16 20:15:32</h6>
<p class='card-text'>The development of agents powered by large language models (LLMs) to
accomplish complex high-level user intents, has attracted significant attention
recently. However, employing LLMs with billions of parameters (e.g., GPT-4) may
incur substantial costs on top of handcrafting extensive prompts. To address
this, we introduce a Grounded Language Agent for Intelligent Web Interactions,
named GLAINTEL. GLAINTEL employs Flan-T5 as its backbone and is flexible in
training in various settings: unsupervised learning, supervised learning, and
unsupervised domain adaptation. Specifically, we tackle both the challenge of
learning without human demonstrations and the opportunity to leverage human
demonstrations effectively when those are available. Additionally, we explore
unsupervised domain adaptation for cases where demonstrations are limited to a
specific domain. Experimental evaluations across diverse setups demonstrate the
effectiveness of GLAINTEL in unsupervised settings, outperforming in-context
learning-based approaches that employ larger models with up to 540 billion
parameters. Surprisingly, behavioral cloning-based methods that
straightforwardly use human demonstrations do not outperform unsupervised
variants of GLAINTEL. Additionally, we show that combining human demonstrations
with reinforcement learning-based training yields results comparable to methods
utilizing GPT-4. The code is available at:
https://github.com/MultifacetedNLP/WebAgents-Unsupervised.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.10876v2' target='_blank'>Course Recommender Systems Need to Consider the Job Market</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jibril Frej, Anna Dai, Syrielle Montariol, Antoine Bosselut, Tanja Käser</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-16 19:52:57</h6>
<p class='card-text'>Current course recommender systems primarily leverage learner-course
interactions, course content, learner preferences, and supplementary course
details like instructor, institution, ratings, and reviews, to make their
recommendation. However, these systems often overlook a critical aspect: the
evolving skill demand of the job market. This paper focuses on the perspective
of academic researchers, working in collaboration with the industry, aiming to
develop a course recommender system that incorporates job market skill demands.
In light of the job market's rapid changes and the current state of research in
course recommender systems, we outline essential properties for course
recommender systems to address these demands effectively, including
explainable, sequential, unsupervised, and aligned with the job market and
user's goals. Our discussion extends to the challenges and research questions
this objective entails, including unsupervised skill extraction from job
listings, course descriptions, and resumes, as well as predicting
recommendations that align with learner objectives and the job market and
designing metrics to evaluate this alignment. Furthermore, we introduce an
initial system that addresses some existing limitations of course recommender
systems using large Language Models (LLMs) for skill extraction and
Reinforcement Learning (RL) for alignment with the job market. We provide
empirical results using open-source data to demonstrate its effectiveness.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.10719v3' target='_blank'>Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, Yi Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-16 16:51:53</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) is currently the most
widely used method to align large language models (LLMs) with human
preferences. Existing RLHF methods can be roughly categorized as either
reward-based or reward-free. Novel applications such as ChatGPT and Claude
leverage reward-based methods that first learn a reward model and apply
actor-critic algorithms, such as Proximal Policy Optimization (PPO). However,
in academic benchmarks, state-of-the-art results are often achieved via
reward-free methods, such as Direct Preference Optimization (DPO). Is DPO truly
superior to PPO? Why does PPO perform poorly on these benchmarks? In this
paper, we first conduct both theoretical and empirical studies on the
algorithmic properties of DPO and show that DPO may have fundamental
limitations. Moreover, we also comprehensively examine PPO and reveal the key
factors for the best performances of PPO in fine-tuning LLMs. Finally, we
benchmark DPO and PPO across a collection of RLHF testbeds, ranging from
dialogue to code generation. Experiment results demonstrate that PPO is able to
surpass other alignment methods in all cases and achieve state-of-the-art
results in challenging code competitions. Our code is publicly available at
https://github.com/openpsi-project/ReaLHF.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.10642v3' target='_blank'>Self-playing Adversarial Language Game Enhances LLM Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pengyu Cheng, Tianhao Hu, Han Xu, Zhisong Zhang, Zheng Yuan, Yong Dai, Lei Han, Nan Du, Xiaolong Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-16 15:16:22</h6>
<p class='card-text'>We explore the potential of self-play training for large language models
(LLMs) in a two-player adversarial language game called Adversarial Taboo. In
this game, an attacker and a defender communicate around a target word only
visible to the attacker. The attacker aims to induce the defender to speak the
target word unconsciously, while the defender tries to infer the target word
from the attacker's utterances. To win the game, both players must have
sufficient knowledge about the target word and high-level reasoning ability to
infer and express in this information-reserved conversation. Hence, we are
curious about whether LLMs' reasoning ability can be further enhanced by
Self-Playing this Adversarial language Game (SPAG). With this goal, we select
several open-source LLMs and let each act as the attacker and play with a copy
of itself as the defender on an extensive range of target words. Through
reinforcement learning on the game outcomes, we observe that the LLMs'
performances uniformly improve on a broad range of reasoning benchmarks.
Furthermore, iteratively adopting this self-play process can continuously
promote LLMs' reasoning abilities. The code is available at
https://github.com/Linear95/SPAG.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.10160v6' target='_blank'>Reinforcement Learning from Multi-role Debates as Feedback for Bias
  Mitigation in LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ruoxi Cheng, Haoxuan Ma, Shuirong Cao, Jiaqi Li, Aihua Pei, Zhiqiang Wang, Pengliang Ji, Haoyu Wang, Jiaqi Huo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-15 22:18:50</h6>
<p class='card-text'>Bias in LLMs can harm user experience and societal outcomes. However, current
bias mitigation methods often require intensive human feedback, lack
transferability to other topics or yield overconfident and random outputs. We
find that involving LLMs in role-playing scenario boosts their ability to
recognize and mitigate biases. Based on this, we propose Reinforcement Learning
from Multi-role Debates as Feedback (RLDF), a novel approach for bias
mitigation replacing human feedback in traditional RLHF. We utilize LLMs in
multi-role debates to create a dataset that includes both high-bias and
low-bias instances for training the reward model in reinforcement learning. Our
approach comprises two modes: (1) self-reflection, where the same LLM
participates in multi-role debates, and (2) teacher-student, where a more
advanced LLM like GPT-3.5-turbo guides the LLM to perform this task.
Experimental results across different LLMs on BBQ and our datasets demonstrate
the effectiveness of our approach in bias mitigation. Our source code and
datasets are available at \texttt{https://anonymous.4open.science/r/RLDF-E344}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.09248v1' target='_blank'>Knowledgeable Agents by Offline Reinforcement Learning from Large
  Language Model Rollouts</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jing-Cheng Pang, Si-Hang Yang, Kaiyuan Li, Jiaji Zhang, Xiong-Hui Chen, Nan Tang, Yang Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-14 13:19:40</h6>
<p class='card-text'>Reinforcement learning (RL) trains agents to accomplish complex tasks through
environmental interaction data, but its capacity is also limited by the scope
of the available data. To obtain a knowledgeable agent, a promising approach is
to leverage the knowledge from large language models (LLMs). Despite previous
studies combining LLMs with RL, seamless integration of the two components
remains challenging due to their semantic gap. This paper introduces a novel
method, Knowledgeable Agents from Language Model Rollouts (KALM), which
extracts knowledge from LLMs in the form of imaginary rollouts that can be
easily learned by the agent through offline reinforcement learning methods. The
primary challenge of KALM lies in LLM grounding, as LLMs are inherently limited
to textual data, whereas environmental data often comprise numerical vectors
unseen to LLMs. To address this, KALM fine-tunes the LLM to perform various
tasks based on environmental data, including bidirectional translation between
natural language descriptions of skills and their corresponding rollout data.
This grounding process enhances the LLM's comprehension of environmental
dynamics, enabling it to generate diverse and meaningful imaginary rollouts
that reflect novel skills. Initial empirical evaluations on the CLEVR-Robot
environment demonstrate that KALM enables agents to complete complex
rephrasings of task goals and extend their capabilities to novel tasks
requiring unprecedented optimal behaviors. KALM achieves a success rate of 46%
in executing tasks with unseen goals, substantially surpassing the 26% success
rate achieved by baseline methods. Furthermore, KALM effectively enables the
LLM to comprehend environmental dynamics, resulting in the generation of
meaningful imaginary rollouts that reflect novel skills and demonstrate the
seamless integration of large language models and reinforcement learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.09127v3' target='_blank'>Confidence Calibration and Rationalization for LLMs via Multi-Agent
  Deliberation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ruixin Yang, Dheeraj Rajagopal, Shirley Anugrah Hayati, Bin Hu, Dongyeop Kang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-14 02:40:43</h6>
<p class='card-text'>Uncertainty estimation is a significant issue for current large language
models (LLMs) that are generally poorly calibrated and over-confident,
especially with reinforcement learning from human feedback (RLHF). Unlike
humans, whose decisions and confidences not only stem from intrinsic beliefs
but can also be adjusted through daily observations, existing calibration
methods for LLMs focus on estimating or eliciting individual confidence without
taking full advantage of the "Collective Wisdom": the interaction among
multiple LLMs that can collectively improve both accuracy and calibration. In
this work, we propose Collaborative Calibration, a post-hoc training-free
calibration strategy that leverages the collaborative and expressive
capabilities of multiple tool-augmented LLM agents in a simulated group
deliberation process. We demonstrate the effectiveness of Collaborative
Calibration on generative QA tasks across various domains, showing its
potential in harnessing the rationalization of collectively calibrated
confidence assessments and improving the reliability of model predictions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.09066v3' target='_blank'>CodeCloak: A Method for Evaluating and Mitigating Code Leakage by LLM
  Code Assistants</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Amit Finkman Noah, Avishag Shapira, Eden Bar Kochva, Inbar Maimon, Dudu Mimran, Yuval Elovici, Asaf Shabtai</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-13 19:30:58</h6>
<p class='card-text'>LLM-based code assistants are becoming increasingly popular among developers.
These tools help developers improve their coding efficiency and reduce errors
by providing real-time suggestions based on the developer's codebase. While
beneficial, the use of these tools can inadvertently expose the developer's
proprietary code to the code assistant service provider during the development
process. In this work, we propose a method to mitigate the risk of code leakage
when using LLM-based code assistants. CodeCloak is a novel deep reinforcement
learning agent that manipulates the prompts before sending them to the code
assistant service. CodeCloak aims to achieve the following two contradictory
goals: (i) minimizing code leakage, while (ii) preserving relevant and useful
suggestions for the developer. Our evaluation, employing StarCoder and Code
Llama, LLM-based code assistants models, demonstrates CodeCloak's effectiveness
on a diverse set of code repositories of varying sizes, as well as its
transferability across different models. We also designed a method for
reconstructing the developer's original codebase from code segments sent to the
code assistant service (i.e., prompts) during the development process, to
thoroughly analyze code leakage risks and evaluate the effectiveness of
CodeCloak under practical development scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.01392v1' target='_blank'>LLMSat: A Large Language Model-Based Goal-Oriented Agent for Autonomous
  Space Exploration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:David Maranto</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-13 03:33:17</h6>
<p class='card-text'>As spacecraft journey further from Earth with more complex missions, systems
of greater autonomy and onboard intelligence are called for. Reducing reliance
on human-based mission control becomes increasingly critical if we are to
increase our rate of solar-system-wide exploration. Recent work has explored
AI-based goal-oriented systems to increase the level of autonomy in mission
execution. These systems make use of symbolic reasoning managers to make
inferences from the state of a spacecraft and a handcrafted knowledge base,
enabling autonomous generation of tasks and re-planning. Such systems have
proven to be successful in controlled cases, but they are difficult to
implement as they require human-crafted ontological models to allow the
spacecraft to understand the world. Reinforcement learning has been applied to
train robotic agents to pursue a goal. A new architecture for autonomy is
called for. This work explores the application of Large Language Models (LLMs)
as the high-level control system of a spacecraft. Using a systems engineering
approach, this work presents the design and development of an agentic
spacecraft controller by leveraging an LLM as a reasoning engine, to evaluate
the utility of such an architecture in achieving higher levels of spacecraft
autonomy. A series of deep space mission scenarios simulated within the popular
game engine Kerbal Space Program (KSP) are used as case studies to evaluate the
implementation against the requirements. It is shown the reasoning and planning
abilities of present-day LLMs do not scale well as the complexity of a mission
increases, but this can be alleviated with adequate prompting frameworks and
strategic selection of the agent's level of authority over the host spacecraft.
This research evaluates the potential of LLMs in augmenting autonomous
decision-making systems for future robotic space applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.08570v1' target='_blank'>Enhancing Autonomous Vehicle Training with Language Model Integration
  and Critical Scenario Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hanlin Tian, Kethan Reddy, Yuxiang Feng, Mohammed Quddus, Yiannis Demiris, Panagiotis Angeloudis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-12 16:13:10</h6>
<p class='card-text'>This paper introduces CRITICAL, a novel closed-loop framework for autonomous
vehicle (AV) training and testing. CRITICAL stands out for its ability to
generate diverse scenarios, focusing on critical driving situations that target
specific learning and performance gaps identified in the Reinforcement Learning
(RL) agent. The framework achieves this by integrating real-world traffic
dynamics, driving behavior analysis, surrogate safety measures, and an optional
Large Language Model (LLM) component. It is proven that the establishment of a
closed feedback loop between the data generation pipeline and the training
process can enhance the learning rate during training, elevate overall system
performance, and augment safety resilience. Our evaluations, conducted using
the Proximal Policy Optimization (PPO) and the HighwayEnv simulation
environment, demonstrate noticeable performance improvements with the
integration of critical case generation and LLM analysis, indicating CRITICAL's
potential to improve the robustness of AV systems and streamline the generation
of critical scenarios. This ultimately serves to hasten the development of AV
agents, expand the general scope of RL training, and ameliorate validation
efforts for AV safety.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.08555v2' target='_blank'>RLHF Deciphered: A Critical Analysis of Reinforcement Learning from
  Human Feedback for LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shreyas Chaudhari, Pranjal Aggarwal, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik Narasimhan, Ameet Deshpande, Bruno Castro da Silva</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-12 15:54:15</h6>
<p class='card-text'>State-of-the-art large language models (LLMs) have become indispensable tools
for various tasks. However, training LLMs to serve as effective assistants for
humans requires careful consideration. A promising approach is reinforcement
learning from human feedback (RLHF), which leverages human feedback to update
the model in accordance with human preferences and mitigate issues like
toxicity and hallucinations. Yet, an understanding of RLHF for LLMs is largely
entangled with initial design choices that popularized the method and current
research focuses on augmenting those choices rather than fundamentally
improving the framework. In this paper, we analyze RLHF through the lens of
reinforcement learning principles to develop an understanding of its
fundamentals, dedicating substantial focus to the core component of RLHF -- the
reward model. Our study investigates modeling choices, caveats of function
approximation, and their implications on RLHF training algorithms, highlighting
the underlying assumptions made about the expressivity of reward. Our analysis
improves the understanding of the role of reward models and methods for their
training, concurrently revealing limitations of the current methodology. We
characterize these limitations, including incorrect generalization, model
misspecification, and the sparsity of feedback, along with their impact on the
performance of a language model. The discussion and analysis are substantiated
by a categorical review of current literature, serving as a reference for
researchers and practitioners to understand the challenges of RLHF and build
upon existing efforts.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.07900v3' target='_blank'>High-Dimension Human Value Representation in Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Samuel Cahyawijaya, Delong Chen, Yejin Bang, Leila Khalatbari, Bryan Wilie, Ziwei Ji, Etsuko Ishii, Pascale Fung</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-11 16:39:00</h6>
<p class='card-text'>The widespread application of Large Language Models (LLMs) across various
tasks and fields has necessitated the alignment of these models with human
values and preferences. Given various approaches of human value alignment,
ranging from Reinforcement Learning with Human Feedback (RLHF), to
constitutional learning, etc. there is an urgent need to understand the scope
and nature of human values injected into these models before their release.
There is also a need for model alignment without a costly large scale human
annotation effort. We propose UniVaR, a high-dimensional representation of
human value distributions in LLMs, orthogonal to model architecture and
training data. Trained from the value-relevant output of eight multilingual
LLMs and tested on the output from four multilingual LLMs, namely LlaMA2,
ChatGPT, JAIS and Yi, we show that UniVaR is a powerful tool to compare the
distribution of human values embedded in different LLMs with different langauge
sources. Through UniVaR, we explore how different LLMs prioritize various
values in different languages and cultures, shedding light on the complex
interplay between human values and language modeling.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.07491v1' target='_blank'>Neural Fault Injection: Generating Software Faults from Natural Language</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Domenico Cotroneo, Pietro Liguori</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-11 05:59:16</h6>
<p class='card-text'>Traditional software fault injection methods, while foundational, face
limitations in adequately representing real-world faults, offering
customization, and requiring significant manual effort and expertise. This
paper introduces a novel methodology that harnesses the capabilities of Large
Language Models (LLMs) augmented with Reinforcement Learning from Human
Feedback (RLHF) to overcome these challenges. The usage of RLHF emphasizes an
iterative refinement process, allowing testers to provide feedback on generated
faults, which is then used to enhance the LLM's fault generation capabilities,
ensuring the generation of fault scenarios that closely mirror actual
operational risks. This innovative methodology aims to significantly reduce the
manual effort involved in crafting fault scenarios as it allows testers to
focus on higher-level testing strategies, hence paving the way to new
possibilities for enhancing the dependability of software systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.05970v1' target='_blank'>Optimization Methods for Personalizing Large Language Models through
  Retrieval Augmentation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alireza Salemi, Surya Kallumadi, Hamed Zamani</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-09 02:58:05</h6>
<p class='card-text'>This paper studies retrieval-augmented approaches for personalizing large
language models (LLMs), which potentially have a substantial impact on various
applications and domains. We propose the first attempt to optimize the
retrieval models that deliver a limited number of personal documents to large
language models for the purpose of personalized generation. We develop two
optimization algorithms that solicit feedback from the downstream personalized
generation tasks for retrieval optimization -- one based on reinforcement
learning whose reward function is defined using any arbitrary metric for
personalized generation and another based on knowledge distillation from the
downstream LLM to the retrieval model. This paper also introduces a pre- and
post-generation retriever selection model that decides what retriever to choose
for each LLM input. Extensive experiments on diverse tasks from the language
model personalization (LaMP) benchmark reveal statistically significant
improvements in six out of seven datasets.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.05291v2' target='_blank'>Long-horizon Locomotion and Manipulation on a Quadrupedal Robot with
  Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yutao Ouyang, Jinhan Li, Yunfei Li, Zhongyu Li, Chao Yu, Koushil Sreenath, Yi Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-08 08:29:00</h6>
<p class='card-text'>We present a large language model (LLM) based system to empower quadrupedal
robots with problem-solving abilities for long-horizon tasks beyond short-term
motions. Long-horizon tasks for quadrupeds are challenging since they require
both a high-level understanding of the semantics of the problem for task
planning and a broad range of locomotion and manipulation skills to interact
with the environment. Our system builds a high-level reasoning layer with large
language models, which generates hybrid discrete-continuous plans as robot code
from task descriptions. It comprises multiple LLM agents: a semantic planner
for sketching a plan, a parameter calculator for predicting arguments in the
plan, and a code generator to convert the plan into executable robot code. At
the low level, we adopt reinforcement learning to train a set of motion
planning and control skills to unleash the flexibility of quadrupeds for rich
environment interactions. Our system is tested on long-horizon tasks that are
infeasible to complete with one single skill. Simulation and real-world
experiments show that it successfully figures out multi-step strategies and
demonstrates non-trivial behaviors, including building tools or notifying a
human for help. Demos are available on our project page:
https://sites.google.com/view/long-horizon-robot.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.04869v2' target='_blank'>Prompting Multi-Modal Tokens to Enhance End-to-End Autonomous Driving
  Imitation Learning with LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yiqun Duan, Qiang Zhang, Renjing Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-07 08:31:12</h6>
<p class='card-text'>The utilization of Large Language Models (LLMs) within the realm of
reinforcement learning, particularly as planners, has garnered a significant
degree of attention in recent scholarly literature. However, a substantial
proportion of existing research predominantly focuses on planning models for
robotics that transmute the outputs derived from perception models into
linguistic forms, thus adopting a `pure-language' strategy. In this research,
we propose a hybrid End-to-End learning framework for autonomous driving by
combining basic driving imitation learning with LLMs based on multi-modality
prompt tokens. Instead of simply converting perception results from the
separated train model into pure language input, our novelty lies in two
aspects. 1) The end-to-end integration of visual and LiDAR sensory input into
learnable multi-modality tokens, thereby intrinsically alleviating description
bias by separated pre-trained perception models. 2) Instead of directly letting
LLMs drive, this paper explores a hybrid setting of letting LLMs help the
driving model correct mistakes and complicated scenarios. The results of our
experiments suggest that the proposed methodology can attain driving scores of
49.21%, coupled with an impressive route completion rate of 91.34% in the
offline evaluation conducted via CARLA. These performance metrics are
comparable to the most advanced driving models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.03648v2' target='_blank'>AutoWebGLM: A Large Language Model-based Web Navigating Agent</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, Jie Tang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-04 17:58:40</h6>
<p class='card-text'>Large language models (LLMs) have fueled many intelligent web agents, but
most existing ones perform far from satisfying in real-world web navigation
tasks due to three factors: (1) the complexity of HTML text data (2)
versatility of actions on webpages, and (3) task difficulty due to the
open-domain nature of the web. In light of these challenges, we develop the
open AutoWebGLM based on ChatGLM3-6B. AutoWebGLM can serve as a powerful
automated web navigation agent that outperform GPT-4. Inspired by human
browsing patterns, we first design an HTML simplification algorithm to
represent webpages with vital information preserved succinctly. We then employ
a hybrid human-AI method to build web browsing data for curriculum training.
Finally, we bootstrap the model by reinforcement learning and rejection
sampling to further facilitate webpage comprehension, browser operations, and
efficient task decomposition by itself. For comprehensive evaluation, we
establish a bilingual benchmark -- AutoWebBench -- for real-world web
navigation tasks. We evaluate AutoWebGLM across diverse web navigation
benchmarks, demonstrating its potential to tackle challenging tasks in real
environments. Related code, model, and data are released at
\url{https://github.com/THUDM/AutoWebGLM}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.03715v1' target='_blank'>Direct Nash Optimization: Teaching Language Models to Self-Improve with
  General Preferences</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, Tengyang Xie</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-04 17:56:41</h6>
<p class='card-text'>This paper studies post-training large language models (LLMs) using
preference feedback from a powerful oracle to help a model iteratively improve
over itself. The typical approach for post-training LLMs involves Reinforcement
Learning from Human Feedback (RLHF), which traditionally separates reward
learning and subsequent policy optimization. However, such a reward
maximization approach is limited by the nature of "point-wise" rewards (such as
Bradley-Terry model), which fails to express complex intransitive or cyclic
preference relations. While advances on RLHF show reward learning and policy
optimization can be merged into a single contrastive objective for stability,
they yet still remain tethered to the reward maximization framework. Recently,
a new wave of research sidesteps the reward maximization presumptions in favor
of directly optimizing over "pair-wise" or general preferences. In this paper,
we introduce Direct Nash Optimization (DNO), a provable and scalable algorithm
that marries the simplicity and stability of contrastive learning with
theoretical generality from optimizing general preferences. Because DNO is a
batched on-policy algorithm using a regression-based objective, its
implementation is straightforward and efficient. Moreover, DNO enjoys monotonic
improvement across iterations that help it improve even over a strong teacher
(such as GPT-4). In our experiments, a resulting 7B parameter Orca-2.5 model
aligned by DNO achieves the state-of-the-art win-rate against GPT-4-Turbo of
33% on AlpacaEval 2.0 (even after controlling for response length), an absolute
gain of 26% (7% to 33%) over the initializing model. It outperforms models with
far more parameters, including Mistral Large, Self-Rewarding LM (70B
parameters), and older versions of GPT-4.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.04292v5' target='_blank'>Conversational Disease Diagnosis via External Planner-Controlled Large
  Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhoujian Sun, Cheng Luo, Ziyi Liu, Zhengxing Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-04 06:16:35</h6>
<p class='card-text'>The development of large language models (LLMs) has brought unprecedented
possibilities for artificial intelligence (AI) based medical diagnosis.
However, the application perspective of LLMs in real diagnostic scenarios is
still unclear because they are not adept at collecting patient data
proactively. This study presents a LLM-based diagnostic system that enhances
planning capabilities by emulating doctors. Our system involves two external
planners to handle planning tasks. The first planner employs a reinforcement
learning approach to formulate disease screening questions and conduct initial
diagnoses. The second planner uses LLMs to parse medical guidelines and conduct
differential diagnoses. By utilizing real patient electronic medical record
data, we constructed simulated dialogues between virtual patients and doctors
and evaluated the diagnostic abilities of our system. We demonstrated that our
system obtained impressive performance in both disease screening and
differential diagnoses tasks. This research represents a step towards more
seamlessly integrating AI into clinical settings, potentially enhancing the
accuracy and accessibility of medical diagnostics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.04291v1' target='_blank'>Investigating Regularization of Self-Play Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Reda Alami, Abdalgader Abubaker, Mastane Achab, Mohamed El Amine Seddik, Salem Lahlou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-04 05:38:44</h6>
<p class='card-text'>This paper explores the effects of various forms of regularization in the
context of language model alignment via self-play. While both reinforcement
learning from human feedback (RLHF) and direct preference optimization (DPO)
require to collect costly human-annotated pairwise preferences, the self-play
fine-tuning (SPIN) approach replaces the rejected answers by data generated
from the previous iterate. However, the SPIN method presents a performance
instability issue in the learning phase, which can be mitigated by playing
against a mixture of the two previous iterates. In the same vein, we propose in
this work to address this issue from two perspectives: first, by incorporating
an additional Kullback-Leibler (KL) regularization to stay at the proximity of
the reference policy; second, by using the idea of fictitious play which
smoothens the opponent policy across all previous iterations. In particular, we
show that the KL-based regularizer boils down to replacing the previous policy
by its geometric mixture with the base policy inside of the SPIN loss function.
We finally discuss empirical results on MT-Bench as well as on the Hugging Face
Open LLM Leaderboard.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.00978v2' target='_blank'>Prior Constraints-based Reward Model Training for Aligning Large
  Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hang Zhou, Chenglong Wang, Yimin Hu, Tong Xiao, Chunliang Zhang, Jingbo Zhu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-01 07:49:11</h6>
<p class='card-text'>Reinforcement learning with human feedback for aligning large language models
(LLMs) trains a reward model typically using ranking loss with comparison
pairs.However, the training procedure suffers from an inherent problem: the
uncontrolled scaling of reward scores during reinforcement learning due to the
lack of constraints while training the reward model.This paper proposes a Prior
Constraints-based Reward Model (namely PCRM) training method to mitigate this
problem. PCRM incorporates prior constraints, specifically, length ratio and
cosine similarity between outputs of each comparison pair, during reward model
training to regulate optimization magnitude and control score margins. We
comprehensively evaluate PCRM by examining its rank correlation with human
preferences and its effectiveness in aligning LLMs via RL. Experimental results
demonstrate that PCRM significantly improves alignment performance by
effectively constraining reward score scaling. As another bonus, our method is
easily integrated into arbitrary rank-based alignment methods, such as direct
preference optimization, and can yield consistent improvement.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.00934v2' target='_blank'>ChatGLM-RLHF: Practices of Aligning Large Language Models with Human
  Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhenyu Hou, Yilin Niu, Zhengxiao Du, Xiaohan Zhang, Xiao Liu, Aohan Zeng, Qinkai Zheng, Minlie Huang, Hongning Wang, Jie Tang, Yuxiao Dong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-01 05:39:36</h6>
<p class='card-text'>ChatGLM is a free-to-use AI service powered by the ChatGLM family of large
language models (LLMs). In this paper, we present the ChatGLM-RLHF pipeline --
a reinforcement learning from human feedback (RLHF) system -- designed to
enhance ChatGLM's alignment with human preferences. ChatGLM-RLHF encompasses
three major components: the collection of human preference data, the training
of the reward model, and the optimization of policies. Throughout the process
of integrating ChatGLM-RLHF into production, we encountered and addressed
several unprecedented challenges. We introduce the strategies to mitigate
reward variance for stabilized large-scale training, implement model
parallelism with fused gradient-descent, and design regularization constraints
to avoid catastrophic forgetting in LLMs. Experiments show that ChatGLM-RLHF
brings significant improvements in alignment tasks compared to the supervised
fine-tuned (SFT) version of ChatGLM. For instance, it achieves on average 15\%
more wins against ChatGLM-SFT in Chinese alignment tasks. The work presents our
practices of aligning LLMs with human preferences, offering insights into the
challenges and solutions in RLHF implementations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.00604v1' target='_blank'>Extensive Self-Contrast Enables Feedback-Free Language Model Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiao Liu, Xixuan Song, Yuxiao Dong, Jie Tang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-31 08:30:15</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) has been a central
technique for recent large language model (LLM) alignment. However, its heavy
dependence on costly human or LLM-as-Judge preference feedback could stymie its
wider applications. In this work, we introduce Self-Contrast, a feedback-free
large language model alignment method via exploiting extensive self-generated
negatives. With only supervised fine-tuning (SFT) targets, Self-Contrast
leverages the LLM itself to generate massive diverse candidates, and harnesses
a pre-trained embedding model to filter multiple negatives according to text
similarity. Theoretically, we illustrate that in this setting, merely scaling
negative responses can still effectively approximate situations with more
balanced positive and negative preference annotations. Our experiments with
direct preference optimization (DPO) on three datasets show that, Self-Contrast
could consistently outperform SFT and standard DPO training by large margins.
And as the number of self-generated negatives increases, the performance of
Self-Contrast continues to grow. Code and data are available at
https://github.com/THUDM/Self-Contrast.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.00282v3' target='_blank'>Survey on Large Language Model-Enhanced Reinforcement Learning: Concept,
  Taxonomy, and Methods</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuji Cao, Huan Zhao, Yuheng Cheng, Ting Shu, Yue Chen, Guolong Liu, Gaoqi Liang, Junhua Zhao, Jinyue Yan, Yun Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-30 08:28:08</h6>
<p class='card-text'>With extensive pre-trained knowledge and high-level general capabilities,
large language models (LLMs) emerge as a promising avenue to augment
reinforcement learning (RL) in aspects such as multi-task learning, sample
efficiency, and high-level task planning. In this survey, we provide a
comprehensive review of the existing literature in LLM-enhanced RL and
summarize its characteristics compared to conventional RL methods, aiming to
clarify the research scope and directions for future studies. Utilizing the
classical agent-environment interaction paradigm, we propose a structured
taxonomy to systematically categorize LLMs' functionalities in RL, including
four roles: information processor, reward designer, decision-maker, and
generator. For each role, we summarize the methodologies, analyze the specific
RL challenges that are mitigated, and provide insights into future directions.
Lastly, a comparative analysis of each role, potential applications,
prospective opportunities, and challenges of the LLM-enhanced RL are discussed.
By proposing this taxonomy, we aim to provide a framework for researchers to
effectively leverage LLMs in the RL field, potentially accelerating RL
applications in complex applications such as robotics, autonomous driving, and
energy systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.19833v2' target='_blank'>ChatTracer: Large Language Model Powered Real-time Bluetooth Device
  Tracking System</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qijun Wang, Shichen Zhang, Kunzhe Song, Huacheng Zeng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-28 21:04:11</h6>
<p class='card-text'>Large language models (LLMs) have transformed the way we interact with cyber
technologies. In this paper, we study the possibility of connecting LLM with
wireless sensor networks (WSN). A successful design will not only extend LLM's
knowledge landscape to the physical world but also revolutionize human
interaction with WSN. To the end, we present ChatTracer, an LLM-powered
real-time Bluetooth device tracking system. ChatTracer comprises three key
components: an array of Bluetooth sniffing nodes, a database, and a fine-tuned
LLM. ChatTracer was designed based on our experimental observation that
commercial Apple/Android devices always broadcast hundreds of BLE packets per
minute even in their idle status. Its novelties lie in two aspects: i) a
reliable and efficient BLE packet grouping algorithm; and ii) an LLM
fine-tuning strategy that combines both supervised fine-tuning (SFT) and
reinforcement learning with human feedback (RLHF). We have built a prototype of
ChatTracer with four sniffing nodes. Experimental results show that ChatTracer
not only outperforms existing localization approaches, but also provides an
intelligent interface for user interaction.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.19443v2' target='_blank'>Mixed Preference Optimization: Reinforcement Learning with Data
  Selection and Better Reference Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qi Gou, Cam-Tu Nguyen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-28 14:15:10</h6>
<p class='card-text'>Large Language Models (LLMs) have become increasingly popular due to their
ability to process and generate natural language. However, as they are trained
on massive datasets of text, LLMs can inherit harmful biases and produce
outputs that are not aligned with human values. This paper studies two main
approaches to LLM alignment: Reinforcement Learning with Human Feedback (RLHF)
and contrastive learning-based methods like Direct Preference Optimization
(DPO). By analyzing the stability and robustness of RLHF and DPO, we propose
MPO (Mixed Preference Optimization), a novel method that mitigates the
weaknesses of both approaches. Specifically, we propose a two-stage training
procedure: first train DPO on an easy dataset, and then perform RLHF on a
difficult set with DPO model being the reference model. Here, the easy and
difficult sets are constructed by a well-trained reward model that splits
response pairs into those with large gaps of reward (easy), and those with
small gaps (difficult). The first stage allows us to obtain a relatively
optimal policy (LLM) model quickly, whereas the second stage refines LLM with
online RLHF, thus mitigating the distribution shift issue associated with DPO.
Experiments are conducted on two public alignment datasets, namely HH-RLHF and
TLDR, demonstrating the effectiveness of MPO, both in terms of GPT4 and human
evaluation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.19279v1' target='_blank'>Fine-Tuning Language Models with Reward Learning on Policy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hao Lang, Fei Huang, Yongbin Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-28 10:02:10</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) has emerged as an effective
approach to aligning large language models (LLMs) to human preferences. RLHF
contains three steps, i.e., human preference collecting, reward learning, and
policy optimization, which are usually performed serially. Despite its
popularity, however, (fixed) reward models may suffer from inaccurate
off-distribution, since policy optimization continuously shifts LLMs' data
distribution. Repeatedly collecting new preference data from the latest LLMs
may alleviate this issue, which unfortunately makes the resulting system more
complicated and difficult to optimize. In this paper, we propose reward
learning on policy (RLP), an unsupervised framework that refines a reward model
using policy samples to keep it on-distribution. Specifically, an unsupervised
multi-view learning method is introduced to learn robust representations of
policy samples. Meanwhile, a synthetic preference generation approach is
developed to simulate high-quality preference data with policy outputs.
Extensive experiments on three benchmark datasets show that RLP consistently
outperforms the state-of-the-art. Our code is available at
\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/rlp}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.18349v3' target='_blank'>Rejection Improves Reliability: Training LLMs to Refuse Unknown
  Questions Using RL from Knowledge Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hongshen Xu, Zichen Zhu, Situo Zhang, Da Ma, Shuai Fan, Lu Chen, Kai Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-27 08:39:56</h6>
<p class='card-text'>Large Language Models (LLMs) often generate erroneous outputs, known as
hallucinations, due to their limitations in discerning questions beyond their
knowledge scope. While addressing hallucination has been a focal point in
research, previous efforts primarily concentrate on enhancing correctness
without giving due consideration to the significance of rejection mechanisms.
In this paper, we conduct a comprehensive examination of the role of rejection,
introducing the notion of model reliability along with corresponding metrics.
These metrics measure the model's ability to provide accurate responses while
adeptly rejecting questions exceeding its knowledge boundaries, thereby
minimizing hallucinations. To improve the inherent reliability of LLMs, we
present a novel alignment framework called Reinforcement Learning from
Knowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically
determine the model's knowledge boundary and trains a reliable reward model to
encourage the refusal of out-of-knowledge questions. Experimental results on
mathematical questions affirm the substantial efficacy of RLKF in significantly
enhancing LLM reliability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.18341v1' target='_blank'>IterAlign: Iterative Constitutional Alignment of Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiusi Chen, Hongzhi Wen, Sreyashi Nag, Chen Luo, Qingyu Yin, Ruirui Li, Zheng Li, Wei Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-27 08:32:19</h6>
<p class='card-text'>With the rapid development of large language models (LLMs), aligning LLMs
with human values and societal norms to ensure their reliability and safety has
become crucial. Reinforcement learning with human feedback (RLHF) and
Constitutional AI (CAI) have been proposed for LLM alignment. However, these
methods require either heavy human annotations or explicitly pre-defined
constitutions, which are labor-intensive and resource-consuming. To overcome
these drawbacks, we study constitution-based LLM alignment and propose a
data-driven constitution discovery and self-alignment framework called
IterAlign. IterAlign leverages red teaming to unveil the weaknesses of an LLM
and automatically discovers new constitutions using a stronger LLM. These
constitutions are then used to guide self-correction of the base LLM. Such a
constitution discovery pipeline can be run iteratively and automatically to
discover new constitutions that specifically target the alignment gaps in the
current LLM. Empirical results on several safety benchmark datasets and
multiple base LLMs show that IterAlign successfully improves truthfulness,
helpfulness, harmlessness and honesty, improving the LLM alignment by up to
$13.5\%$ in harmlessness.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.17710v3' target='_blank'>Optimization-based Prompt Injection Attack to LLM-as-a-Judge</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiawen Shi, Zenghui Yuan, Yinuo Liu, Yue Huang, Pan Zhou, Lichao Sun, Neil Zhenqiang Gong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-26 13:58:00</h6>
<p class='card-text'>LLM-as-a-Judge uses a large language model (LLM) to select the best response
from a set of candidates for a given question. LLM-as-a-Judge has many
applications such as LLM-powered search, reinforcement learning with AI
feedback (RLAIF), and tool selection. In this work, we propose JudgeDeceiver,
an optimization-based prompt injection attack to LLM-as-a-Judge. JudgeDeceiver
injects a carefully crafted sequence into an attacker-controlled candidate
response such that LLM-as-a-Judge selects the candidate response for an
attacker-chosen question no matter what other candidate responses are.
Specifically, we formulate finding such sequence as an optimization problem and
propose a gradient based method to approximately solve it. Our extensive
evaluation shows that JudgeDeceive is highly effective, and is much more
effective than existing prompt injection attacks that manually craft the
injected sequences and jailbreak attacks when extended to our problem. We also
show the effectiveness of JudgeDeceiver in three case studies, i.e.,
LLM-powered search, RLAIF, and tool selection. Moreover, we consider defenses
including known-answer detection, perplexity detection, and perplexity windowed
detection. Our results show these defenses are insufficient, highlighting the
urgent need for developing new defense strategies. Our implementation is
available at this repository: https://github.com/ShiJiawenwen/JudgeDeceiver.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.17674v1' target='_blank'>Depending on yourself when you should: Mentoring LLM with RL agents to
  become the master in cybersecurity games</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yikuan Yan, Yaolun Zhang, Keman Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-26 13:02:46</h6>
<p class='card-text'>Integrating LLM and reinforcement learning (RL) agent effectively to achieve
complementary performance is critical in high stake tasks like cybersecurity
operations. In this study, we introduce SecurityBot, a LLM agent mentored by
pre-trained RL agents, to support cybersecurity operations. In particularly,
the LLM agent is supported with a profile module to generated behavior
guidelines, a memory module to accumulate local experiences, a reflection
module to re-evaluate choices, and an action module to reduce action space.
Additionally, it adopts the collaboration mechanism to take suggestions from
pre-trained RL agents, including a cursor for dynamic suggestion taken, an
aggregator for multiple mentors' suggestions ranking and a caller for proactive
suggestion asking. Building on the CybORG experiment framework, our experiences
show that SecurityBot demonstrates significant performance improvement compared
with LLM or RL standalone, achieving the complementary performance in the
cybersecurity games.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.17297v1' target='_blank'>InternLM2 Technical Report</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, Dahua Lin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-26 00:53:24</h6>
<p class='card-text'>The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has
sparked discussions on the advent of Artificial General Intelligence (AGI).
However, replicating such advancements in open-source models has been
challenging. This paper introduces InternLM2, an open-source LLM that
outperforms its predecessors in comprehensive evaluations across 6 dimensions
and 30 benchmarks, long-context modeling, and open-ended subjective evaluations
through innovative pre-training and optimization techniques. The pre-training
process of InternLM2 is meticulously detailed, highlighting the preparation of
diverse data types including text, code, and long-context data. InternLM2
efficiently captures long-term dependencies, initially trained on 4k tokens
before advancing to 32k tokens in pre-training and fine-tuning stages,
exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack" test.
InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel
Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF)
strategy that addresses conflicting human preferences and reward hacking. By
releasing InternLM2 models in different training stages and model sizes, we
provide the community with insights into the model's evolution.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.17146v2' target='_blank'>Outcome-Constrained Large Language Models for Countering Hate Speech</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lingzi Hong, Pengcheng Luo, Eduardo Blanco, Xiaoying Song</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-25 19:44:06</h6>
<p class='card-text'>Automatic counterspeech generation methods have been developed to assist
efforts in combating hate speech. Existing research focuses on generating
counterspeech with linguistic attributes such as being polite, informative, and
intent-driven. However, the real impact of counterspeech in online environments
is seldom considered. This study aims to develop methods for generating
counterspeech constrained by conversation outcomes and evaluate their
effectiveness. We experiment with large language models (LLMs) to incorporate
into the text generation process two desired conversation outcomes: low
conversation incivility and non-hateful hater reentry. Specifically, we
experiment with instruction prompts, LLM finetuning, and LLM reinforcement
learning (RL). Evaluation results show that our methods effectively steer the
generation of counterspeech toward the desired outcomes. Our analyses, however,
show that there are differences in the quality and style depending on the
model.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.16948v1' target='_blank'>Reinforcement Learning-based Recommender Systems with Large Language
  Models for State Reward and Action Modeling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jie Wang, Alexandros Karatzoglou, Ioannis Arapakis, Joemon M. Jose</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-25 17:10:39</h6>
<p class='card-text'>Reinforcement Learning (RL)-based recommender systems have demonstrated
promising performance in meeting user expectations by learning to make accurate
next-item recommendations from historical user-item interactions. However,
existing offline RL-based sequential recommendation methods face the challenge
of obtaining effective user feedback from the environment. Effectively modeling
the user state and shaping an appropriate reward for recommendation remains a
challenge. In this paper, we leverage language understanding capabilities and
adapt large language models (LLMs) as an environment (LE) to enhance RL-based
recommenders. The LE is learned from a subset of user-item interaction data,
thus reducing the need for large training data, and can synthesise user
feedback for offline data by: (i) acting as a state model that produces high
quality states that enrich the user representation, and (ii) functioning as a
reward model to accurately capture nuanced user preferences on actions.
Moreover, the LE allows to generate positive actions that augment the limited
offline training data. We propose a LE Augmentation (LEA) method to further
improve recommendation performance by optimising jointly the supervised
component and the RL policy, using the augmented actions and historical user
signals. We use LEA, the state and reward models in conjunction with
state-of-the-art RL recommenders and report experimental results on two
publicly available datasets.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.16809v1' target='_blank'>An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hanqing Yang, Marie Siew, Carlee Joe-Wong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-25 14:32:28</h6>
<p class='card-text'>The increasing prevalence of Cyber-Physical Systems and the Internet of
Things (CPS-IoT) applications and Foundation Models are enabling new
applications that leverage real-time control of the environment. For example,
real-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems
can reduce its usage when not needed for the comfort of human occupants, hence
reducing energy consumption. Collecting real-time feedback on human preferences
in such human-in-the-loop (HITL) systems, however, is difficult in practice. We
propose the use of large language models (LLMs) to deal with the challenges of
dynamic environments and difficult-to-obtain data in CPS optimization. In this
paper, we present a case study that employs LLM agents to mimic the behaviors
and thermal preferences of various population groups (e.g. young families, the
elderly) in a shopping mall. The aggregated thermal preferences are integrated
into an agent-in-the-loop based reinforcement learning algorithm AitL-RL, which
employs the LLM as a dynamic simulation of the physical environment to learn
how to balance between energy savings and occupant comfort. Our results show
that LLMs are capable of simulating complex population movements within large
open spaces. Besides, AitL-RL demonstrates superior performance compared to the
popular existing policy of set point control, suggesting that adaptive and
personalized decision-making is critical for efficient optimization in CPS-IoT
applications. Through this case study, we demonstrate the potential of
integrating advanced Foundation Models like LLMs into CPS-IoT to enhance system
adaptability and efficiency. The project's code can be found on our GitHub
repository.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.16649v2' target='_blank'>CLHA: A Simple yet Effective Contrastive Learning Framework for Human
  Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Feiteng Fang, Liang Zhu, Min Yang, Xi Feng, Jinchang Hou, Qixuan Zhao, Chengming Li, Xiping Hu, Ruifeng Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-25 11:37:15</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) is a crucial technique in
aligning large language models (LLMs) with human preferences, ensuring these
LLMs behave in beneficial and comprehensible ways to users. However, a
longstanding challenge in human alignment techniques based on reinforcement
learning lies in their inherent complexity and difficulty in training. To
address this challenge, we present a simple yet effective Contrastive Learning
Framework for Human Alignment (CLHA) to align LLMs with human preferences
directly. CLHA employs a novel rescoring strategy to evaluate the noise within
the data by considering its inherent quality and dynamically adjusting the
training process. Simultaneously, CLHA utilizes pairwise contrastive loss and
adaptive supervised fine-tuning loss to adaptively modify the likelihood of
generating responses, ensuring enhanced alignment with human preferences. Using
advanced methods, CLHA surpasses other algorithms, showcasing superior
performance in terms of reward model scores, automatic evaluations, and human
assessments on the widely used ``Helpful and Harmless'' dataset.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.15648v2' target='_blank'>SRLM: Human-in-Loop Interactive Social Robot Navigation with Large
  Language Model and Deep Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weizheng Wang, Ike Obi, Byung-Cheol Min</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-22 23:12:28</h6>
<p class='card-text'>An interactive social robotic assistant must provide services in complex and
crowded spaces while adapting its behavior based on real-time human language
commands or feedback. In this paper, we propose a novel hybrid approach called
Social Robot Planner (SRLM), which integrates Large Language Models (LLM) and
Deep Reinforcement Learning (DRL) to navigate through human-filled public
spaces and provide multiple social services. SRLM infers global planning from
human-in-loop commands in real-time, and encodes social information into a
LLM-based large navigation model (LNM) for low-level motion execution.
Moreover, a DRL-based planner is designed to maintain benchmarking performance,
which is blended with LNM by a large feedback model (LFM) to address the
instability of current text and LLM-driven LNM. Finally, SRLM demonstrates
outstanding performance in extensive experiments. More details about this work
are available at: https://sites.google.com/view/navi-srlm</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.15371v3' target='_blank'>Can large language models explore in-context?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Akshay Krishnamurthy, Keegan Harris, Dylan J. Foster, Cyril Zhang, Aleksandrs Slivkins</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-22 17:50:43</h6>
<p class='card-text'>We investigate the extent to which contemporary Large Language Models (LLMs)
can engage in exploration, a core capability in reinforcement learning and
decision making. We focus on native performance of existing LLMs, without
training interventions. We deploy LLMs as agents in simple multi-armed bandit
environments, specifying the environment description and interaction history
entirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,
GPT-4, and Llama2, using a variety of prompt designs, and find that the models
do not robustly engage in exploration without substantial interventions: i)
Across all of our experiments, only one configuration resulted in satisfactory
exploratory behavior: GPT-4 with chain-of-thought reasoning and an externally
summarized interaction history, presented as sufficient statistics; ii) All
other configurations did not result in robust exploratory behavior, including
those with chain-of-thought reasoning but unsummarized history. Although these
findings can be interpreted positively, they suggest that external
summarization -- which may not be possible in more complex settings -- is
important for obtaining desirable behavior from LLM agents. We conclude that
non-trivial algorithmic interventions, such as fine-tuning or dataset curation,
may be required to empower LLM-based decision making agents in complex
settings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.14952v1' target='_blank'>Evidence-Driven Retrieval Augmented Response Generation for Online
  Misinformation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhenrui Yue, Huimin Zeng, Yimeng Lu, Lanyu Shang, Yang Zhang, Dong Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-22 05:05:45</h6>
<p class='card-text'>The proliferation of online misinformation has posed significant threats to
public interest. While numerous online users actively participate in the combat
against misinformation, many of such responses can be characterized by the lack
of politeness and supporting facts. As a solution, text generation approaches
are proposed to automatically produce counter-misinformation responses.
Nevertheless, existing methods are often trained end-to-end without leveraging
external knowledge, resulting in subpar text quality and excessively repetitive
responses. In this paper, we propose retrieval augmented response generation
for online misinformation (RARG), which collects supporting evidence from
scientific sources and generates counter-misinformation responses based on the
evidences. In particular, our RARG consists of two stages: (1) evidence
collection, where we design a retrieval pipeline to retrieve and rerank
evidence documents using a database comprising over 1M academic articles; (2)
response generation, in which we align large language models (LLMs) to generate
evidence-based responses via reinforcement learning from human feedback (RLHF).
We propose a reward function to maximize the utilization of the retrieved
evidence while maintaining the quality of the generated text, which yields
polite and factual responses that clearly refutes misinformation. To
demonstrate the effectiveness of our method, we study the case of COVID-19 and
perform extensive experiments with both in- and cross-domain datasets, where
RARG consistently outperforms baselines by generating high-quality
counter-misinformation responses.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.14238v1' target='_blank'>Reinforcement Learning from Reflective Feedback (RLRF): Aligning and
  Improving LLMs via Fine-Grained Self-Reflection</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kyungjae Lee, Dasol Hwang, Sunghyun Park, Youngsoo Jang, Moontae Lee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-21 08:57:27</h6>
<p class='card-text'>Despite the promise of RLHF in aligning LLMs with human preferences, it often
leads to superficial alignment, prioritizing stylistic changes over improving
downstream performance of LLMs. Underspecified preferences could obscure
directions to align the models. Lacking exploration restricts identification of
desirable outputs to improve the models. To overcome these challenges, we
propose a novel framework: Reinforcement Learning from Reflective Feedback
(RLRF), which leverages fine-grained feedback based on detailed criteria to
improve the core capabilities of LLMs. RLRF employs a self-reflection mechanism
to systematically explore and refine LLM responses, then fine-tuning the models
via a RL algorithm along with promising responses. Our experiments across
Just-Eval, Factuality, and Mathematical Reasoning demonstrate the efficacy and
transformative potential of RLRF beyond superficial surface-level adjustment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.13213v4' target='_blank'>From Representational Harms to Quality-of-Service Harms: A Case Study on
  Llama 2 Safety Safeguards</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Khaoula Chehbouni, Megha Roshan, Emmanuel Ma, Futian Andrew Wei, Afaf Taik, Jackie CK Cheung, Golnoosh Farnadi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-20 00:22:38</h6>
<p class='card-text'>Recent progress in large language models (LLMs) has led to their widespread
adoption in various domains. However, these advancements have also introduced
additional safety risks and raised concerns regarding their detrimental impact
on already marginalized populations. Despite growing mitigation efforts to
develop safety safeguards, such as supervised safety-oriented fine-tuning and
leveraging safe reinforcement learning from human feedback, multiple concerns
regarding the safety and ingrained biases in these models remain. Furthermore,
previous work has demonstrated that models optimized for safety often display
exaggerated safety behaviors, such as a tendency to refrain from responding to
certain requests as a precautionary measure. As such, a clear trade-off between
the helpfulness and safety of these models has been documented in the
literature. In this paper, we further investigate the effectiveness of safety
measures by evaluating models on already mitigated biases. Using the case of
Llama 2 as an example, we illustrate how LLMs' safety responses can still
encode harmful assumptions. To do so, we create a set of non-toxic prompts,
which we then use to evaluate Llama models. Through our new taxonomy of LLMs
responses to users, we observe that the safety/helpfulness trade-offs are more
pronounced for certain demographic groups which can lead to quality-of-service
harms for marginalized populations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.12884v2' target='_blank'>HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fucai Ke, Zhixi Cai, Simindokht Jahangard, Weiqing Wang, Pari Delir Haghighi, Hamid Rezatofighi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-19 16:31:30</h6>
<p class='card-text'>Recent advances in visual reasoning (VR), particularly with the aid of Large
Vision-Language Models (VLMs), show promise but require access to large-scale
datasets and face challenges such as high computational costs and limited
generalization capabilities. Compositional visual reasoning approaches have
emerged as effective strategies; however, they heavily rely on the commonsense
knowledge encoded in Large Language Models (LLMs) to perform planning,
reasoning, or both, without considering the effect of their decisions on the
visual reasoning process, which can lead to errors or failed procedures. To
address these challenges, we introduce HYDRA, a multi-stage dynamic
compositional visual reasoning framework designed for reliable and
incrementally progressive general reasoning. HYDRA integrates three essential
modules: a planner, a Reinforcement Learning (RL) agent serving as a cognitive
controller, and a reasoner. The planner and reasoner modules utilize an LLM to
generate instruction samples and executable code from the selected instruction,
respectively, while the RL agent dynamically interacts with these modules,
making high-level decisions on selection of the best instruction sample given
information from the historical state stored through a feedback loop. This
adaptable design enables HYDRA to adjust its actions based on previous feedback
received during the reasoning process, leading to more reliable reasoning
outputs and ultimately enhancing its overall effectiveness. Our framework
demonstrates state-of-the-art performance in various VR tasks on four different
widely-used datasets.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.12017v1' target='_blank'>Supervised Fine-Tuning as Inverse Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hao Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-18 17:52:57</h6>
<p class='card-text'>The prevailing approach to aligning Large Language Models (LLMs) typically
relies on human or AI feedback and assumes access to specific types of
preference datasets. In our work, we question the efficacy of such datasets and
explore various scenarios where alignment with expert demonstrations proves
more realistic. We build a sequential decision-making framework to formulate
the problem of aligning LLMs using demonstration datasets. Drawing insights
from inverse reinforcement learning and imitation learning, we introduce
various approaches for divergence minimization in the LLM alignment tasks. Our
analysis highlights the mass-covering and mode-seeking behaviors of these
different approaches. Inclusively, we examine the pros and cons of the
classical supervised fine-tuning method, elaborating on scenarios where
different methods shine.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.12014v2' target='_blank'>EnvGen: Generating and Adapting Environments via LLMs for Training
  Embodied Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Abhay Zala, Jaemin Cho, Han Lin, Jaehong Yoon, Mohit Bansal</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-18 17:51:16</h6>
<p class='card-text'>Recent SOTA approaches for embodied learning via interaction directly employ
large language models (LLMs) as agents to determine the next steps in an
environment. Due to their world knowledge and reasoning capabilities, LLM
agents achieve stronger performance than previous smaller agents based on
reinforcement learning (RL); however, frequently calling LLMs is slow and
expensive. Instead of directly employing LLMs as agents, can we use LLMs'
reasoning capabilities to adaptively create training environments to help
smaller RL agents learn useful skills that they are weak at? We propose EnvGen,
a novel framework to address this question. We first prompt an LLM to generate
training environments by giving it the task description and simulator
objectives that the agents should learn and then asking it to generate a set of
environment configurations (e.g., different terrains, items initially given to
agents, etc.). Next, we train a small RL agent in a mixture of the original and
LLM-generated environments. Then, we enable the LLM to continuously adapt the
generated environments to progressively improve the skills that the agent is
weak at, by providing feedback to the LLM in the form of the agent's
performance. We demonstrate the usefulness of EnvGen with comprehensive
experiments in Crafter and Heist environments. We find that a small RL agent
trained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and
learns long-horizon tasks significantly faster. We also show that using an LLM
to adapt environments dynamically outperforms curriculum learning approaches
and how the environments are adapted to help improve RL agents' weaker skills
over time. Additionally, EnvGen is substantially more efficient as it only uses
a small number of LLM calls (e.g., 4 in total), whereas LLM agents require
thousands of calls. Lastly, we present detailed ablation studies for EnvGen
design choices.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.11558v1' target='_blank'>Reinforcement Learning with Token-level Feedback for Controllable Text
  Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wendi Li, Wei Wei, Kaihe Xu, Wenfeng Xie, Dangyang Chen, Yu Cheng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-18 08:18:37</h6>
<p class='card-text'>To meet the requirements of real-world applications, it is essential to
control generations of large language models (LLMs). Prior research has tried
to introduce reinforcement learning (RL) into controllable text generation
while most existing methods suffer from overfitting issues (finetuning-based
methods) or semantic collapse (post-processing methods). However, current RL
methods are generally guided by coarse-grained (sentence/paragraph-level)
feedback, which may lead to suboptimal performance owing to semantic twists or
progressions within sentences. To tackle that, we propose a novel reinforcement
learning algorithm named TOLE which formulates TOken-LEvel rewards for
controllable text generation, and employs a "first-quantize-then-noise"
paradigm to enhance the robustness of the RL algorithm.Furthermore, TOLE can be
flexibly extended to multiple constraints with little computational expense.
Experimental results show that our algorithm can achieve superior performance
on both single-attribute and multi-attribute control tasks. We have released
our codes at https://github.com/WindyLee0822/CTG</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.10779v1' target='_blank'>LLM-based Conversational AI Therapist for Daily Functioning Screening
  and Psychotherapeutic Intervention via Everyday Smart Devices</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jingping Nie, Hanya Shao, Yuang Fan, Qijia Shao, Haoxuan You, Matthias Preindl, Xiaofan Jiang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-16 02:48:50</h6>
<p class='card-text'>Despite the global mental health crisis, access to screenings, professionals,
and treatments remains high. In collaboration with licensed psychotherapists,
we propose a Conversational AI Therapist with psychotherapeutic Interventions
(CaiTI), a platform that leverages large language models (LLM)s and smart
devices to enable better mental health self-care. CaiTI can screen the
day-to-day functioning using natural and psychotherapeutic conversations. CaiTI
leverages reinforcement learning to provide personalized conversation flow.
CaiTI can accurately understand and interpret user responses. When the user
needs further attention during the conversation, CaiTI can provide
conversational psychotherapeutic interventions, including cognitive behavioral
therapy (CBT) and motivational interviewing (MI). Leveraging the datasets
prepared by the licensed psychotherapists, we experiment and microbenchmark
various LLMs' performance in tasks along CaiTI's conversation flow and discuss
their strengths and weaknesses. With the psychotherapists, we implement CaiTI
and conduct 14-day and 24-week studies. The study results, validated by
therapists, demonstrate that CaiTI can converse with users naturally,
accurately understand and interpret user responses, and provide
psychotherapeutic interventions appropriately and effectively. We showcase the
potential of CaiTI LLMs to assist the mental therapy diagnosis and treatment
and improve day-to-day functioning screening and precautionary
psychotherapeutic intervention systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.10704v2' target='_blank'>Parameter Efficient Reinforcement Learning from Human Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hakim Sidahmed, Samrat Phatale, Alex Hutcheson, Zhuonan Lin, Zhang Chen, Zac Yu, Jarvis Jin, Simral Chaudhary, Roman Komarytsia, Christiane Ahlheim, Yonghao Zhu, Bowen Li, Saravanan Ganesh, Bill Byrne, Jessica Hoffmann, Hassan Mansoor, Wei Li, Abhinav Rastogi, Lucas Dixon</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-15 21:43:46</h6>
<p class='card-text'>While Reinforcement Learning from Human Feedback (RLHF) effectively aligns
pretrained Large Language and Vision-Language Models (LLMs, and VLMs) with
human preferences, its computational cost and complexity hamper its wider
adoption. To alleviate some of the computational burden of fine-tuning,
parameter efficient methods, like LoRA were introduced. In this work, we
empirically evaluate the setup of Parameter Efficient Reinforcement Learning
from Human Feedback (PE-RLHF) that leverages LoRA fine-tuning for Reward
Modeling, and Reinforcement Learning. We benchmark the PE-RLHF setup on six
diverse datasets spanning summarization, harmless/helpful response generation,
UI automation, and visual question answering in terms of effectiveness of the
trained models, and the training resources required. Our findings show, for the
first time, that PE-RLHF achieves comparable performance to RLHF, while
significantly reducing training time (up to 90% faster for reward models, and
30% faster for RL), and memory footprint (up to 50% reduction for reward
models, and 27% for RL). We provide comprehensive ablations across LoRA ranks,
and model sizes for both reward modeling and reinforcement learning. By
mitigating the computational burden associated with RLHF, we push for a broader
adoption of PE-RLHF as an alignment technique for LLMs and VLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.10088v1' target='_blank'>Intent-conditioned and Non-toxic Counterspeech Generation using
  Multi-Task Instruction Tuning with RLAIF</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Amey Hengle, Aswini Kumar, Sahajpreet Singh, Anil Bandhakavi, Md Shad Akhtar, Tanmoy Chakroborty</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-15 08:03:49</h6>
<p class='card-text'>Counterspeech, defined as a response to mitigate online hate speech, is
increasingly used as a non-censorial solution. Addressing hate speech
effectively involves dispelling the stereotypes, prejudices, and biases often
subtly implied in brief, single-sentence statements or abuses. These implicit
expressions challenge language models, especially in seq2seq tasks, as model
performance typically excels with longer contexts. Our study introduces CoARL,
a novel framework enhancing counterspeech generation by modeling the pragmatic
implications underlying social biases in hateful statements. CoARL's first two
phases involve sequential multi-instruction tuning, teaching the model to
understand intents, reactions, and harms of offensive statements, and then
learning task-specific low-rank adapter weights for generating
intent-conditioned counterspeech. The final phase uses reinforcement learning
to fine-tune outputs for effectiveness and non-toxicity. CoARL outperforms
existing benchmarks in intent-conditioned counterspeech generation, showing an
average improvement of 3 points in intent-conformity and 4 points in
argument-quality metrics. Extensive human evaluation supports CoARL's efficacy
in generating superior and more context-appropriate responses compared to
existing systems, including prominent LLMs like ChatGPT.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.09905v2' target='_blank'>Right Place, Right Time! Generalizing ObjectNav to Dynamic Environments
  with Portable Targets</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Vishnu Sashank Dorbala, Bhrij Patel, Amrit Singh Bedi, Dinesh Manocha</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-14 22:33:22</h6>
<p class='card-text'>ObjectNav is a popular task in Embodied AI, where an agent navigates to a
target object in an unseen environment. Prior literature makes the assumption
of a static environment with stationary objects, which lacks realism. To
address this, we present a novel formulation to generalize ObjectNav to dynamic
environments with non-stationary objects, and refer to it as Portable ObjectNav
or P-ObjectNav. In our formulation, we first address several challenging issues
with dynamizing existing topological scene graphs by developing a novel method
that introduces multiple transition behaviors to portable objects in the scene.
We use this technique to dynamize Matterport3D, a popular simulator for
evaluating embodied tasks. We then present a benchmark for P-ObjectNav using a
combination of heuristic, reinforcement learning, and Large Language Model
(LLM)-based navigation approaches on the dynamized environment, while
introducing novel evaluation metrics tailored for our task. Our work
fundamentally challenges the "static-environment" notion of prior ObjectNav
work; the code and dataset for P-ObjectNav will be made publicly available to
foster research on embodied navigation in dynamic scenes. We provide an
anonymized repository for our code and dataset:
https://anonymous.4open.science/r/PObjectNav-1C6D.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.09798v1' target='_blank'>Comparing Rationality Between Large Language Models and Humans: Insights
  and Open Questions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dana Alsagheer, Rabimba Karanjai, Nour Diallo, Weidong Shi, Yang Lu, Suha Beydoun, Qiaoning Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-14 18:36:04</h6>
<p class='card-text'>This paper delves into the dynamic landscape of artificial intelligence,
specifically focusing on the burgeoning prominence of large language models
(LLMs). We underscore the pivotal role of Reinforcement Learning from Human
Feedback (RLHF) in augmenting LLMs' rationality and decision-making prowess. By
meticulously examining the intricate relationship between human interaction and
LLM behavior, we explore questions surrounding rationality and performance
disparities between humans and LLMs, with particular attention to the Chat
Generative Pre-trained Transformer. Our research employs comprehensive
comparative analysis and delves into the inherent challenges of irrationality
in LLMs, offering valuable insights and actionable strategies for enhancing
their rationality. These findings hold significant implications for the
widespread adoption of LLMs across diverse domains and applications,
underscoring their potential to catalyze advancements in artificial
intelligence.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.09032v3' target='_blank'>CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language
  Models to Coding Preferences</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Martin Weyssow, Aton Kamanda, Xin Zhou, Houari Sahraoui</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-14 01:51:35</h6>
<p class='card-text'>Evaluating the alignment of large language models (LLMs) with user-defined
coding preferences is a challenging endeavour that requires a deep assessment
of LLMs' outputs. Existing methods and benchmarks rely primarily on automated
metrics and static analysis tools, which often fail to capture the nuances of
user instructions and LLM outputs. To address this gap, we propose using the
LLM-as-a-Judge methodology to evaluate the alignment of LLMs with coding
preferences. Based on this approach, we present CodeUltraFeedback, a
comprehensive dataset designed to facilitate the evaluation and improvement of
LLM alignment. CodeUltraFeedback consists of 10,000 coding instructions, each
annotated with four responses generated from a diverse pool of 14 LLMs. These
responses are ranked based on five distinct coding preferences using GPT-3.5 as
a judge, providing both numerical scores and detailed textual feedback. Our
analysis of CodeUltraFeedback reveals that responses from GPT-3.5 and GPT-4 are
generally preferred over those from open-weight LLMs, highlighting significant
differences in alignment between closed and open-weight models. In turn, we
explore the usage of CodeUltraFeedback as feedback data to fine-tune and align
CodeLlama-7B-Instruct using supervised fine-tuning (SFT) and reinforcement
learning from AI feedback (RLAIF) with direct preference optimization (DPO).
The resulting aligned CodeLlama-7B-Instruct model outperforms larger LLMs in
terms of alignment with coding preferences and shows improved functional
correctness on the HumanEval+ benchmark compared to the original instruct
model. Therefore, our contributions bridge the gap in preference tuning of LLMs
for code and set the stage for further advancements in model alignment and
RLAIF in automated software engineering.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.08694v3' target='_blank'>TeaMs-RL: Teaching LLMs to Generate Better Instruction Datasets via
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shangding Gu, Alois Knoll, Ming Jin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-13 16:57:57</h6>
<p class='card-text'>The development of Large Language Models (LLMs) often confronts challenges
stemming from the heavy reliance on human annotators in the reinforcement
learning with human feedback (RLHF) framework, or the frequent and costly
external queries tied to the self-instruct paradigm. In this work, we pivot to
Reinforcement Learning (RL) -- but with a twist. Diverging from the typical
RLHF, which refines LLMs following instruction data training, we use RL to
directly generate the foundational instruction dataset that alone suffices for
fine-tuning. Our method, TeaMs-RL, uses a suite of textual operations and
rules, prioritizing the diversification of training datasets. It facilitates
the generation of high-quality data without excessive reliance on external
advanced models, paving the way for a single fine-tuning step and negating the
need for subsequent RLHF stages. Our findings highlight key advantages of our
approach: reduced need for human involvement and fewer model queries (only
$5.73\%$ of the strong baseline's total), along with enhanced capabilities of
LLMs in crafting and comprehending complex instructions compared to strong
baselines, and substantially improved model privacy protection. Code is
available at the link: https://github.com/SafeRL-Lab/TeaMs-RL</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.08337v2' target='_blank'>LLM-Assisted Light: Leveraging Large Language Model Capabilities for
  Human-Mimetic Traffic Signal Control in Complex Urban Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Maonan Wang, Aoyu Pang, Yuheng Kan, Man-On Pun, Chung Shue Chen, Bo Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-13 08:41:55</h6>
<p class='card-text'>Traffic congestion in metropolitan areas presents a formidable challenge with
far-reaching economic, environmental, and societal ramifications. Therefore,
effective congestion management is imperative, with traffic signal control
(TSC) systems being pivotal in this endeavor. Conventional TSC systems,
designed upon rule-based algorithms or reinforcement learning (RL), frequently
exhibit deficiencies in managing the complexities and variabilities of urban
traffic flows, constrained by their limited capacity for adaptation to
unfamiliar scenarios. In response to these limitations, this work introduces an
innovative approach that integrates Large Language Models (LLMs) into TSC,
harnessing their advanced reasoning and decision-making faculties.
Specifically, a hybrid framework that augments LLMs with a suite of perception
and decision-making tools is proposed, facilitating the interrogation of both
the static and dynamic traffic information. This design places the LLM at the
center of the decision-making process, combining external traffic data with
established TSC methods. Moreover, a simulation platform is developed to
corroborate the efficacy of the proposed framework. The findings from our
simulations attest to the system's adeptness in adjusting to a multiplicity of
traffic environments without the need for additional training. Notably, in
cases of Sensor Outage (SO), our approach surpasses conventional RL-based
systems by reducing the average waiting time by $20.4\%$. This research
signifies a notable advance in TSC strategies and paves the way for the
integration of LLMs into real-world, dynamic scenarios, highlighting their
potential to revolutionize traffic management. The related code is available at
https://github.com/Traffic-Alpha/LLM-Assisted-Light.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.08309v2' target='_blank'>HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain
  Reinforcement Learning From AI Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ang Li, Qiugen Xiao, Peng Cao, Jian Tang, Yi Yuan, Zijie Zhao, Xiaoyuan Chen, Liang Zhang, Xiangyang Li, Kaitong Yang, Weidong Guo, Yukang Gan, Xu Yu, Daniell Wang, Ying Shan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-13 07:38:20</h6>
<p class='card-text'>Reinforcement Learning from AI Feedback (RLAIF) has the advantages of shorter
annotation cycles and lower costs over Reinforcement Learning from Human
Feedback (RLHF), making it highly efficient during the rapid strategy iteration
periods of large language model (LLM) training. Using ChatGPT as a labeler to
provide feedback on open-domain prompts in RLAIF training, we observe an
increase in human evaluators' preference win ratio for model responses, but a
decrease in evaluators' satisfaction rate. Analysis suggests that the decrease
in satisfaction rate is mainly due to some responses becoming less helpful,
particularly in terms of correctness and truthfulness, highlighting practical
limitations of basic RLAIF. In this paper, we propose Hybrid Reinforcement
Learning from AI Feedback (HRLAIF). This method enhances the accuracy of AI
annotations for responses, making the model's helpfulness more robust in
training process. Additionally, it employs AI for Red Teaming, further
improving the model's harmlessness. Human evaluation results show that HRLAIF
inherits the ability of RLAIF to enhance human preference for outcomes at a low
cost while also improving the satisfaction rate of responses. Compared to the
policy model before Reinforcement Learning (RL), it achieves an increase of
2.08\% in satisfaction rate, effectively addressing the issue of a decrease of
4.58\% in satisfaction rate after basic RLAIF.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.08833v1' target='_blank'>TINA: Think, Interaction, and Action Framework for Zero-Shot Vision
  Language Navigation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dingbang Li, Wenzhou Chen, Xin Lin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-13 05:22:39</h6>
<p class='card-text'>Zero-shot navigation is a critical challenge in Vision-Language Navigation
(VLN) tasks, where the ability to adapt to unfamiliar instructions and to act
in unknown environments is essential. Existing supervised learning-based
models, trained using annotated data through reinforcement learning, exhibit
limitations in generalization capabilities. Large Language Models (LLMs), with
their extensive knowledge and emergent reasoning abilities, present a potential
pathway for achieving zero-shot navigation. This paper presents a VLN agent
based on LLMs, exploring approaches to the zero-shot navigation problem. To
compensate for the shortcomings of LLMs in environmental perception, we propose
the Thinking, Interacting, and Action (TINA) framework. TINA enables the agent
to scrutinize perceptual information and autonomously query key clues within
the environment through an introduced question-answering module, thereby
aligning instructions with specific perceptual data. The navigation agent's
perceptual abilities are enhanced through the TINA framework, while the
explicit thought and query processes also improve the navigational procedure's
explainability and transparency. We evaluate the performance of our method on
the Room-to-Room dataset. The experiment results indicate that our approach
improves the navigation performance of LLM-based agents. Our approach also
outperformed some supervised learning-based methods, highlighting its efficacy
in zero-shot navigation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.10553v1' target='_blank'>Learning to Watermark LLM-generated Text via Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaojun Xu, Yuanshun Yao, Yang Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-13 03:43:39</h6>
<p class='card-text'>We study how to watermark LLM outputs, i.e. embedding algorithmically
detectable signals into LLM-generated text to track misuse. Unlike the current
mainstream methods that work with a fixed LLM, we expand the watermark design
space by including the LLM tuning stage in the watermark pipeline. While prior
works focus on token-level watermark that embeds signals into the output, we
design a model-level watermark that embeds signals into the LLM weights, and
such signals can be detected by a paired detector. We propose a co-training
framework based on reinforcement learning that iteratively (1) trains a
detector to detect the generated watermarked text and (2) tunes the LLM to
generate text easily detectable by the detector while keeping its normal
utility. We empirically show that our watermarks are more accurate, robust, and
adaptable (to new attacks). It also allows watermarked model open-sourcing. In
addition, if used together with alignment, the extra overhead introduced is low
- only training an extra reward model (i.e. our detector). We hope our work can
bring more effort into studying a broader watermark design that is not limited
to working with a fixed LLM. We open-source the code:
https://github.com/xiaojunxu/learning-to-watermark-llm .</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.07865v5' target='_blank'>CodeAttack: Revealing Safety Generalization Challenges of Large Language
  Models via Code Completion</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Wai Lam, Lizhuang Ma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-12 17:55:38</h6>
<p class='card-text'>The rapid advancement of Large Language Models (LLMs) has brought about
remarkable generative capabilities but also raised concerns about their
potential misuse. While strategies like supervised fine-tuning and
reinforcement learning from human feedback have enhanced their safety, these
methods primarily focus on natural languages, which may not generalize to other
domains. This paper introduces CodeAttack, a framework that transforms natural
language inputs into code inputs, presenting a novel environment for testing
the safety generalization of LLMs. Our comprehensive studies on
state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a
new and universal safety vulnerability of these models against code input:
CodeAttack bypasses the safety guardrails of all models more than 80\% of the
time. We find that a larger distribution gap between CodeAttack and natural
language leads to weaker safety generalization, such as encoding natural
language input with data structures. Furthermore, we give our hypotheses about
the success of CodeAttack: the misaligned bias acquired by LLMs during code
training, prioritizing code completion over avoiding the potential safety risk.
Finally, we analyze potential mitigation measures. These findings highlight new
safety risks in the code domain and the need for more robust safety alignment
algorithms to match the code capabilities of LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.07708v2' target='_blank'>Improving Reinforcement Learning from Human Feedback Using Contrastive
  Rewards</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wei Shen, Xiaoying Zhang, Yuanshun Yao, Rui Zheng, Hongyi Guo, Yang Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-12 14:51:57</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) is the mainstream paradigm
used to align large language models (LLMs) with human preferences. Yet existing
RLHF heavily relies on accurate and informative reward models, which are
vulnerable and sensitive to noise from various sources, e.g. human labeling
errors, making the pipeline fragile. In this work, we improve the effectiveness
of the reward model by introducing a penalty term on the reward, named as
\textit{contrastive rewards}. %Contrastive rewards Our approach involves two
steps: (1) an offline sampling step to obtain responses to prompts that serve
as baseline calculation and (2) a contrastive reward calculated using the
baseline responses and used in the Proximal Policy Optimization (PPO) step. We
show that contrastive rewards enable the LLM to penalize reward uncertainty,
improve robustness, encourage improvement over baselines, calibrate according
to task difficulty, and reduce variance in PPO. We show empirically contrastive
rewards can improve RLHF substantially, evaluated by both GPTs and humans, and
our method consistently outperforms strong baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.06754v2' target='_blank'>ALaRM: Align Language Models via Hierarchical Rewards Modeling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuhang Lai, Siyuan Wang, Shujun Liu, Xuanjing Huang, Zhongyu Wei</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-11 14:28:40</h6>
<p class='card-text'>We introduce ALaRM, the first framework modeling hierarchical rewards in
reinforcement learning from human feedback (RLHF), which is designed to enhance
the alignment of large language models (LLMs) with human preferences. The
framework addresses the limitations of current alignment approaches, which
often struggle with the inconsistency and sparsity of human supervision
signals, by integrating holistic rewards with aspect-specific rewards. This
integration enables more precise and consistent guidance of language models
towards desired outcomes, particularly in complex and open text generation
tasks. By employing a methodology that filters and combines multiple rewards
based on their consistency, the framework provides a reliable mechanism for
improving model alignment. We validate our approach through applications in
long-form question answering and machine translation tasks, employing
gpt-3.5-turbo for pairwise comparisons, and demonstrate improvements over
existing baselines. Our work underscores the effectiveness of hierarchical
rewards modeling in refining LLM training processes for better human preference
alignment. We release our code at https://ALaRM-fdu.github.io.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.06447v1' target='_blank'>CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve
  Long-tail Recommendation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Junda Wu, Cheng-Chun Chang, Tong Yu, Zhankui He, Jianing Wang, Yupeng Hou, Julian McAuley</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-11 05:49:34</h6>
<p class='card-text'>The long-tail recommendation is a challenging task for traditional
recommender systems, due to data sparsity and data imbalance issues. The recent
development of large language models (LLMs) has shown their abilities in
complex reasoning, which can help to deduce users' preferences based on very
few previous interactions. However, since most LLM-based systems rely on items'
semantic meaning as the sole evidence for reasoning, the collaborative
information of user-item interactions is neglected, which can cause the LLM's
reasoning to be misaligned with task-specific collaborative information of the
dataset. To further align LLMs' reasoning to task-specific user-item
interaction knowledge, we introduce collaborative retrieval-augmented LLMs,
CoRAL, which directly incorporate collaborative evidence into the prompts.
Based on the retrieved user-item interactions, the LLM can analyze shared and
distinct preferences among users, and summarize the patterns indicating which
types of users would be attracted by certain items. The retrieved collaborative
evidence prompts the LLM to align its reasoning with the user-item interaction
patterns in the dataset. However, since the capacity of the input prompt is
limited, finding the minimally-sufficient collaborative information for
recommendation tasks can be challenging. We propose to find the optimal
interaction set through a sequential decision-making process and develop a
retrieval policy learned through a reinforcement learning (RL) framework,
CoRAL. Our experimental results show that CoRAL can significantly improve LLMs'
reasoning abilities on specific recommendation tasks. Our analysis also reveals
that CoRAL can more efficiently explore collaborative information through
reinforcement learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.06420v2' target='_blank'>RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic
  Manipulations With Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Liangliang Chen, Yutian Lei, Shiyu Jin, Ying Zhang, Liangjun Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-11 04:13:26</h6>
<p class='card-text'>Reinforcement learning (RL) has demonstrated its capability in solving
various tasks but is notorious for its low sample efficiency. In this paper, we
propose RLingua, a framework that can leverage the internal knowledge of large
language models (LLMs) to reduce the sample complexity of RL in robotic
manipulations. To this end, we first present a method for extracting the prior
knowledge of LLMs by prompt engineering so that a preliminary rule-based robot
controller for a specific task can be generated in a user-friendly manner.
Despite being imperfect, the LLM-generated robot controller is utilized to
produce action samples during rollouts with a decaying probability, thereby
improving RL's sample efficiency. We employ TD3, the widely-used RL baseline
method, and modify the actor loss to regularize the policy learning towards the
LLM-generated controller. RLingua also provides a novel method of improving the
imperfect LLM-generated robot controllers by RL. We demonstrate that RLingua
can significantly reduce the sample complexity of TD3 in four robot tasks of
panda_gym and achieve high success rates in 12 sampled sparsely rewarded robot
tasks in RLBench, where the standard TD3 fails. Additionally, We validated
RLingua's effectiveness in real-world robot experiments through Sim2Real,
demonstrating that the learned policies are effectively transferable to real
robot tasks. Further details about our work are available at our project
website https://rlingua.github.io.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.05468v1' target='_blank'>Will GPT-4 Run DOOM?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Adrian de Wynter</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-08 17:30:41</h6>
<p class='card-text'>We show that GPT-4's reasoning and planning capabilities extend to the 1993
first-person shooter Doom. This large language model (LLM) is able to run and
play the game with only a few instructions, plus a textual
description--generated by the model itself from screenshots--about the state of
the game being observed. We find that GPT-4 can play the game to a passable
degree: it is able to manipulate doors, combat enemies, and perform pathing.
More complex prompting strategies involving multiple model calls provide better
results. While further work is required to enable the LLM to play the game as
well as its classical, reinforcement learning-based counterparts, we note that
GPT-4 required no training, leaning instead on its own reasoning and
observational capabilities. We hope our work pushes the boundaries on
intelligent, LLM-based agents in video games. We conclude by discussing the
ethical implications of our work.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.05171v2' target='_blank'>Overcoming Reward Overoptimization via Adversarial Policy Optimization
  with Lightweight Uncertainty Estimation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaoying Zhang, Jean-Francois Ton, Wei Shen, Hongning Wang, Yang Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-08 09:20:12</h6>
<p class='card-text'>We introduce Adversarial Policy Optimization (AdvPO), a novel solution to the
pervasive issue of reward over-optimization in Reinforcement Learning from
Human Feedback (RLHF) for Large Language Models (LLMs). Over-optimization
occurs when a reward model serves as an imperfect proxy for human preference,
and RL-driven policy optimization erroneously exploits reward inaccuracies. In
this paper, we begin by introducing a lightweight way to quantify uncertainties
in rewards, relying solely on the last layer embeddings of the reward model,
without the need for computationally expensive reward ensembles. AdvPO then
addresses a distributionally robust optimization problem centred around the
confidence interval of the reward model's predictions for policy improvement.
Through comprehensive experiments on the Anthropic HH and TL;DR summarization
datasets, we illustrate the efficacy of AdvPO in mitigating the
overoptimization issue, consequently resulting in enhanced performance as
evaluated through human-assisted evaluation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.05063v2' target='_blank'>Aligning Large Language Models for Controllable Recommendations</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wensheng Lu, Jianxun Lian, Wei Zhang, Guanghua Li, Mingyang Zhou, Hao Liao, Xing Xie</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-08 05:23:27</h6>
<p class='card-text'>Inspired by the exceptional general intelligence of Large Language Models
(LLMs), researchers have begun to explore their application in pioneering the
next generation of recommender systems - systems that are conversational,
explainable, and controllable. However, existing literature primarily
concentrates on integrating domain-specific knowledge into LLMs to enhance
accuracy, often neglecting the ability to follow instructions. To address this
gap, we initially introduce a collection of supervised learning tasks,
augmented with labels derived from a conventional recommender model, aimed at
explicitly improving LLMs' proficiency in adhering to recommendation-specific
instructions. Subsequently, we develop a reinforcement learning-based alignment
procedure to further strengthen LLMs' aptitude in responding to users'
intentions and mitigating formatting errors. Through extensive experiments on
two real-world datasets, our method markedly advances the capability of LLMs to
comply with instructions within recommender systems, while sustaining a high
level of accuracy performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.04997v1' target='_blank'>DiffChat: Learning to Chat with Text-to-Image Synthesis Models for
  Interactive Image Creation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiapeng Wang, Chengyu Wang, Tingfeng Cao, Jun Huang, Lianwen Jin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-08 02:24:27</h6>
<p class='card-text'>We present DiffChat, a novel method to align Large Language Models (LLMs) to
"chat" with prompt-as-input Text-to-Image Synthesis (TIS) models (e.g., Stable
Diffusion) for interactive image creation. Given a raw prompt/image and a
user-specified instruction, DiffChat can effectively make appropriate
modifications and generate the target prompt, which can be leveraged to create
the target image of high quality. To achieve this, we first collect an
instruction-following prompt engineering dataset named InstructPE for the
supervised training of DiffChat. Next, we propose a reinforcement learning
framework with the feedback of three core criteria for image creation, i.e.,
aesthetics, user preference, and content integrity. It involves an action-space
dynamic modification technique to obtain more relevant positive samples and
harder negative samples during the off-policy sampling. Content integrity is
also introduced into the value estimation function for further improvement of
produced images. Our method can exhibit superior performance than baseline
models and strong competitors based on both automatic and human evaluations,
which fully demonstrates its effectiveness.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.04642v1' target='_blank'>Teaching Large Language Models to Reason with Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, Roberta Raileanu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-07 16:36:29</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (\textbf{RLHF}) has emerged as a
dominant approach for aligning LLM outputs with human preferences. Inspired by
the success of RLHF, we study the performance of multiple algorithms that learn
from feedback (Expert Iteration, Proximal Policy Optimization (\textbf{PPO}),
Return-Conditioned RL) on improving LLM reasoning capabilities. We investigate
both sparse and dense rewards provided to the LLM both heuristically and via a
learned reward model. We additionally start from multiple model sizes and
initializations both with and without supervised fine-tuning (\textbf{SFT})
data. Overall, we find all algorithms perform comparably, with Expert Iteration
performing best in most cases. Surprisingly, we find the sample complexity of
Expert Iteration is similar to that of PPO, requiring at most on the order of
$10^6$ samples to converge from a pretrained checkpoint. We investigate why
this is the case, concluding that during RL training models fail to explore
significantly beyond solutions already produced by SFT models. Additionally, we
discuss a trade off between maj@1 and pass@96 metric performance during SFT
training and how conversely RL training improves both simultaneously. We then
conclude by discussing the implications of our findings for RLHF and the future
role of RL in LLM fine-tuning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.04283v1' target='_blank'>Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model
  with Proxy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yu Zhu, Chuxiong Sun, Wenfei Yang, Wenqiang Wei, Bo Tang, Tianzhu Zhang, Zhiyu Li, Shifeng Zhang, Feiyu Xiong, Jie Hu, Mingchuan yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-07 07:31:00</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) is the prevailing approach
to ensure Large Language Models (LLMs) align with human values. However,
existing RLHF methods require a high computational cost, one main reason being
that RLHF assigns both the generation and alignment tasks to the LLM
simultaneously. In this paper, we introduce Proxy-RLHF, which decouples the
generation and alignment processes of LLMs, achieving alignment with human
values at a much lower computational cost. We start with a novel Markov
Decision Process (MDP) designed for the alignment process and employ
Reinforcement Learning (RL) to train a streamlined proxy model that oversees
the token generation of the LLM, without altering the LLM itself. Experiments
show that our method achieves a comparable level of alignment with only 1\% of
the training parameters of other methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.03558v1' target='_blank'>Benchmarking Hallucination in Large Language Models based on
  Unanswerable Math Word Problem</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuhong Sun, Zhangyue Yin, Qipeng Guo, Jiawen Wu, Xipeng Qiu, Hui Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-06 09:06:34</h6>
<p class='card-text'>Large language models (LLMs) are highly effective in various natural language
processing (NLP) tasks. However, they are susceptible to producing unreliable
conjectures in ambiguous contexts called hallucination. This paper presents a
new method for evaluating LLM hallucination in Question Answering (QA) based on
the unanswerable math word problem (MWP). To support this approach, we
innovatively develop a dataset called Unanswerable Math Word Problem (UMWP)
which comprises 5200 questions across five categories. We developed an
evaluation methodology combining text similarity and mathematical expression
detection to determine whether LLM considers the question unanswerable. The
results of extensive experiments conducted on 31 LLMs, including GPT-3,
InstructGPT, LLaMA, and Claude, demonstrate that in-context learning and
reinforcement learning with human feedback (RLHF) training significantly
enhance the model's ability to avoid hallucination. We show that utilizing MWP
is a reliable and effective approach to assess hallucination. Our code and data
are available at https://github.com/Yuki-Asuuna/UMWP.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.03141v1' target='_blank'>Language Guided Exploration for RL Agents in Text Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hitesh Golchha, Sahil Yerawar, Dhruvesh Patel, Soham Dan, Keerthiram Murugesan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-05 17:26:41</h6>
<p class='card-text'>Real-world sequential decision making is characterized by sparse rewards and
large decision spaces, posing significant difficulty for experiential learning
systems like $\textit{tabula rasa}$ reinforcement learning (RL) agents. Large
Language Models (LLMs), with a wealth of world knowledge, can help RL agents
learn quickly and adapt to distribution shifts. In this work, we introduce
Language Guided Exploration (LGE) framework, which uses a pre-trained language
model (called GUIDE ) to provide decision-level guidance to an RL agent (called
EXPLORER). We observe that on ScienceWorld (Wang et al.,2022), a challenging
text environment, LGE outperforms vanilla RL agents significantly and also
outperforms other sophisticated methods like Behaviour Cloning and Text
Decision Transformer.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.02528v2' target='_blank'>DACO: Towards Application-Driven and Comprehensive Data Analysis via
  Code Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xueqing Wu, Rui Zheng, Jingzhen Sha, Te-Lin Wu, Hanyu Zhou, Mohan Tang, Kai-Wei Chang, Nanyun Peng, Haoran Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-04 22:47:58</h6>
<p class='card-text'>Data analysis is a crucial analytical process to generate in-depth studies
and conclusive insights to comprehensively answer a given user query for
tabular data. In this work, we aim to propose new resources and benchmarks to
inspire future research on this crucial yet challenging and under-explored
task. However, collecting data analysis annotations curated by experts can be
prohibitively expensive. We propose to automatically generate high-quality
answer annotations leveraging the code-generation capabilities of LLMs with a
multi-turn prompting technique. We construct the DACO dataset, containing (1)
440 databases (of tabular data) collected from real-world scenarios, (2) ~2k
query-answer pairs that can serve as weak supervision for model training, and
(3) a concentrated but high-quality test set with human refined annotations
that serves as our main evaluation benchmark. We train a 6B supervised
fine-tuning (SFT) model on DACO dataset, and find that the SFT model learns
reasonable data analysis capabilities. To further align the models with human
preference, we use reinforcement learning to encourage generating analysis
perceived by human as helpful, and design a set of dense rewards to propagate
the sparse human preference reward to intermediate code generation steps. Our
DACO-RL algorithm is evaluated by human annotators to produce more helpful
answers than SFT model in 57.72% cases, validating the effectiveness of our
proposed algorithm. Data and code are released at
https://github.com/shirley-wu/daco</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.02513v1' target='_blank'>Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing
  Conversational LLMs with Direct RLHF</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chen Zheng, Ke Sun, Hang Wu, Chenguang Xi, Xun Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-04 22:02:12</h6>
<p class='card-text'>In recent advancements in Conversational Large Language Models (LLMs), a
concerning trend has emerged, showing that many new base LLMs experience a
knowledge reduction in their foundational capabilities following Supervised
Fine-Tuning (SFT). This process often leads to issues such as forgetting or a
decrease in the base model's abilities. Moreover, fine-tuned models struggle to
align with user preferences, inadvertently increasing the generation of toxic
outputs when specifically prompted. To overcome these challenges, we adopted an
innovative approach by completely bypassing SFT and directly implementing
Harmless Reinforcement Learning from Human Feedback (RLHF). Our method not only
preserves the base model's general capabilities but also significantly enhances
its conversational abilities, while notably reducing the generation of toxic
outputs. Our approach holds significant implications for fields that demand a
nuanced understanding and generation of responses, such as customer service. We
applied this methodology to Mistral, the most popular base model, thereby
creating Mistral-Plus. Our validation across 11 general tasks demonstrates that
Mistral-Plus outperforms similarly sized open-source base models and their
corresponding instruct versions. Importantly, the conversational abilities of
Mistral-Plus were significantly improved, indicating a substantial advancement
over traditional SFT models in both safety and user preference alignment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.02475v1' target='_blank'>Enhancing LLM Safety via Constrained Direct Preference Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zixuan Liu, Xiaolin Sun, Zizhan Zheng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-04 20:39:24</h6>
<p class='card-text'>The rapidly increasing capabilities of large language models (LLMs) raise an
urgent need to align AI systems with diverse human preferences to
simultaneously enhance their usefulness and safety, despite the often
conflicting nature of these goals. To address this important problem, a
promising approach is to enforce a safety constraint at the fine-tuning stage
through a constrained Reinforcement Learning from Human Feedback (RLHF)
framework. This approach, however, is computationally expensive and often
unstable. In this work, we introduce Constrained DPO (C-DPO), a novel extension
of the recently proposed Direct Preference Optimization (DPO) approach for
fine-tuning LLMs that is both efficient and lightweight. By integrating dual
gradient descent and DPO, our method identifies a nearly optimal trade-off
between helpfulness and harmlessness without using reinforcement learning.
Empirically, our approach provides a safety guarantee to LLMs that is missing
in DPO while achieving significantly higher rewards under the same safety
constraint compared to a recently proposed safe RLHF approach.
  Warning: This paper contains example data that may be offensive or harmful.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.01304v2' target='_blank'>Improving the Validity of Automatically Generated Feedback via
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alexander Scarlatos, Digory Smith, Simon Woodhead, Andrew Lan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-02 20:25:50</h6>
<p class='card-text'>Automatically generating feedback via large language models (LLMs) in
intelligent tutoring systems and online learning platforms has the potential to
improve the learning outcomes of many students. However, both feedback
generation and evaluation are challenging: feedback content has to be valid
especially in subjects like math, which requires models to understand the
problem, the solution, and where the student's error lies. Feedback also has to
be pedagogically valid to reflect effective tutoring strategies, such as
explaining possible misconceptions and encouraging the student, among other
desirable features. In this work, we address both problems of automatically
generating and evaluating feedback while considering both correctness and
alignment. First, we propose a rubric for evaluating math feedback and show
that GPT-4 is able to effectively use it to annotate human-written and
LLM-generated feedback. Second, we propose a framework for feedback generation
that optimizes both correctness and alignment using reinforcement learning
(RL). Specifically, we use GPT-4's annotations to create preferences over
feedback pairs in an augmented dataset for training via direct preference
optimization (DPO). We show that our methods significantly increase the
correctness and alignment of generated feedback with Llama 2, an open-source
LLM, qualitatively analyze our generation and evaluation systems using case
studies, and outline several areas for future work.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.01185v1' target='_blank'>Balancing Exploration and Exploitation in LLM using Soft RLLF for
  Enhanced Negation Understanding</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ha-Thanh Nguyen, Ken Satoh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-02 11:54:55</h6>
<p class='card-text'>Finetuning approaches in NLP often focus on exploitation rather than
exploration, which may lead to suboptimal models. Given the vast search space
of natural language, this limited exploration can restrict their performance in
complex, high-stakes domains, where accurate negation understanding and logical
reasoning abilities are crucial. To address this issue, we leverage
Reinforcement Learning from Logical Feedback (RLLF) to create an effective
balance between exploration and exploitation in LLMs. Our approach employs an
appropriate benchmark dataset for training and evaluation, highlighting the
importance of exploration in enhancing negation understanding capabilities. We
compare the performance of our RLLF-enhanced LLMs with baseline models trained
without RLLF, demonstrating the value of this balanced approach. Furthermore,
we showcase the potential of our method in legal AI applications by employing
transfer learning and evaluating its impact on negation understanding. Our
experimental results exhibit the effectiveness of balancing exploration and
exploitation with RLLF in improving LLMs' negation capabilities. This has
implications for the development of more accurate, reliable, and logically
consistent language models in high-stakes domains.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.00867v3' target='_blank'>Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by
  Exploring Refusal Loss Landscapes</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-01 03:29:54</h6>
<p class='card-text'>Large Language Models (LLMs) are becoming a prominent generative AI tool,
where the user enters a query and the LLM generates an answer. To reduce harm
and misuse, efforts have been made to align these LLMs to human values using
advanced training techniques such as Reinforcement Learning from Human Feedback
(RLHF). However, recent studies have highlighted the vulnerability of LLMs to
adversarial jailbreak attempts aiming at subverting the embedded safety
guardrails. To address this challenge, this paper defines and investigates the
Refusal Loss of LLMs and then proposes a method called Gradient Cuff to detect
jailbreak attempts. Gradient Cuff exploits the unique properties observed in
the refusal loss landscape, including functional values and its smoothness, to
design an effective two-step detection strategy. Experimental results on two
aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak
attacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff can
significantly improve the LLM's rejection capability for malicious jailbreak
queries, while maintaining the model's performance for benign user queries by
adjusting the detection threshold.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.00199v3' target='_blank'>Improving Socratic Question Generation using Data Augmentation and
  Preference Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nischal Ashok Kumar, Andrew Lan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-01 00:08:20</h6>
<p class='card-text'>The Socratic method is a way of guiding students toward solving a problem
independently without directly revealing the solution to the problem. Although
this method has been shown to significantly improve student learning outcomes,
it remains a complex labor-intensive task for instructors. Large language
models (LLMs) can be used to augment human effort by automatically generating
Socratic questions for students. However, existing methods that involve
prompting these LLMs sometimes produce invalid outputs, e.g., those that
directly reveal the solution to the problem or provide irrelevant or premature
questions. To alleviate this problem, inspired by reinforcement learning with
AI feedback (RLAIF), we first propose a data augmentation method to enrich
existing Socratic questioning datasets with questions that are invalid in
specific ways. Next, we propose a method to optimize open-source LLMs such as
LLama 2 to prefer ground-truth questions over generated invalid ones, using
direct preference optimization (DPO). Our experiments on a Socratic questions
dataset for student code debugging show that a DPO-optimized 7B LLama 2 model
can effectively avoid generating invalid questions, and as a result,
outperforms existing state-of-the-art prompting methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.00858v4' target='_blank'>Direct Alignment of Draft Model for Speculative Decoding with
  Chat-Fine-Tuned LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Raghavv Goel, Mukul Gagrani, Wonseok Jeon, Junyoung Park, Mingu Lee, Christopher Lott</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-29 19:55:06</h6>
<p class='card-text'>Text generation with Large Language Models (LLMs) is known to be memory bound
due to the combination of their auto-regressive nature, huge parameter counts,
and limited memory bandwidths, often resulting in low token rates. Speculative
decoding has been proposed as a solution for LLM inference acceleration.
However, since draft models are often unavailable in the modern open-source LLM
families, e.g., for Llama 2 7B, training a high-quality draft model is required
to enable inference acceleration via speculative decoding. In this paper, we
propose a simple draft model training framework for direct alignment to
chat-capable target models. With the proposed framework, we train Llama 2 Chat
Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\% of
the original size. Our training framework only consists of pretraining,
distillation dataset generation, and finetuning with knowledge distillation,
with no additional alignment procedure. For the finetuning step, we use
instruction-response pairs generated by target model for distillation in
plausible data distribution, and propose a new Total Variation Distance++
(TVD++) loss that incorporates variance reduction techniques inspired from the
policy gradient method in reinforcement learning. Our empirical results show
that Llama 2 Chat Drafter 115M with speculative decoding achieves up to 2.3
block efficiency and 2.4$\times$ speed-up relative to autoregressive decoding
on various tasks with no further task-specific fine-tuning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.19464v1' target='_blank'>Curiosity-driven Red-teaming for Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhang-Wei Hong, Idan Shenfeld, Tsun-Hsuan Wang, Yung-Sung Chuang, Aldo Pareja, James Glass, Akash Srivastava, Pulkit Agrawal</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-29 18:55:03</h6>
<p class='card-text'>Large language models (LLMs) hold great potential for many natural language
applications but risk generating incorrect or toxic content. To probe when an
LLM generates unwanted content, the current paradigm is to recruit a
\textit{red team} of human testers to design input prompts (i.e., test cases)
that elicit undesirable responses from LLMs. However, relying solely on human
testers is expensive and time-consuming. Recent works automate red teaming by
training a separate red team LLM with reinforcement learning (RL) to generate
test cases that maximize the chance of eliciting undesirable responses from the
target LLM. However, current RL methods are only able to generate a small
number of effective test cases resulting in a low coverage of the span of
prompts that elicit undesirable responses from the target LLM. To overcome this
limitation, we draw a connection between the problem of increasing the coverage
of generated test cases and the well-studied approach of curiosity-driven
exploration that optimizes for novelty. Our method of curiosity-driven red
teaming (CRT) achieves greater coverage of test cases while mantaining or
increasing their effectiveness compared to existing methods. Our method, CRT
successfully provokes toxic responses from LLaMA2 model that has been heavily
fine-tuned using human preferences to avoid toxic outputs. Code is available at
\url{https://github.com/Improbable-AI/curiosity_redteam}</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.19446v1' target='_blank'>ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, Aviral Kumar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-29 18:45:56</h6>
<p class='card-text'>A broad use case of large language models (LLMs) is in goal-directed
decision-making tasks (or "agent" tasks), where an LLM needs to not just
generate completions for a given prompt, but rather make intelligent decisions
over a multi-turn interaction to accomplish a task (e.g., when interacting with
the web, using tools, or providing customer support). Reinforcement learning
(RL) provides a general paradigm to address such agent tasks, but current RL
methods for LLMs largely focus on optimizing single-turn rewards. By
construction, most single-turn RL methods cannot endow LLMs with the ability to
intelligently seek information over multiple turns, perform credit assignment,
or reason about their past actions -- all of which are critical in agent tasks.
This raises the question: how can we design effective and efficient multi-turn
RL algorithms for LLMs? In this paper, we develop a framework for building
multi-turn RL algorithms for fine-tuning LLMs, that preserves the flexibility
of existing single-turn RL methods for LLMs (e.g., proximal policy
optimization), while accommodating multiple turns, long horizons, and delayed
rewards effectively. To do this, our framework adopts a hierarchical RL
approach and runs two RL algorithms in parallel: a high-level off-policy
value-based RL algorithm to aggregate reward over utterances, and a low-level
RL algorithm that utilizes this high-level value function to train a token
policy within each utterance or turn. Our hierarchical framework, Actor-Critic
Framework with a Hierarchical Structure (ArCHer), can also give rise to other
RL methods. Empirically, we find that ArCHer significantly improves efficiency
and performance on agent tasks, attaining a sample efficiency of about 100x
over existing methods, while also improving with larger model capacity (upto
the 7 billion scale that we tested on).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.19299v1' target='_blank'>RL-GPT: Integrating Reinforcement Learning and Code-as-policy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shaoteng Liu, Haoqi Yuan, Minda Hu, Yanwei Li, Yukang Chen, Shu Liu, Zongqing Lu, Jiaya Jia</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-29 16:07:22</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated proficiency in utilizing
various tools by coding, yet they face limitations in handling intricate logic
and precise control. In embodied tasks, high-level planning is amenable to
direct coding, while low-level actions often necessitate task-specific
refinement, such as Reinforcement Learning (RL). To seamlessly integrate both
modalities, we introduce a two-level hierarchical framework, RL-GPT, comprising
a slow agent and a fast agent. The slow agent analyzes actions suitable for
coding, while the fast agent executes coding tasks. This decomposition
effectively focuses each agent on specific tasks, proving highly efficient
within our pipeline. Our approach outperforms traditional RL methods and
existing GPT agents, demonstrating superior efficiency. In the Minecraft game,
it rapidly obtains diamonds within a single day on an RTX3090. Additionally, it
achieves SOTA performance across all designated MineDojo tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.00843v2' target='_blank'>Large Language Models are Learnable Planners for Long-Term
  Recommendation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wentao Shi, Xiangnan He, Yang Zhang, Chongming Gao, Xinyue Li, Jizhi Zhang, Qifan Wang, Fuli Feng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-29 13:49:56</h6>
<p class='card-text'>Planning for both immediate and long-term benefits becomes increasingly
important in recommendation. Existing methods apply Reinforcement Learning (RL)
to learn planning capacity by maximizing cumulative reward for long-term
recommendation. However, the scarcity of recommendation data presents
challenges such as instability and susceptibility to overfitting when training
RL models from scratch, resulting in sub-optimal performance. In this light, we
propose to leverage the remarkable planning capabilities over sparse data of
Large Language Models (LLMs) for long-term recommendation. The key to achieving
the target lies in formulating a guidance plan following principles of
enhancing long-term engagement and grounding the plan to effective and
executable actions in a personalized manner. To this end, we propose a Bi-level
Learnable LLM Planner framework, which consists of a set of LLM instances and
breaks down the learning process into macro-learning and micro-learning to
learn macro-level guidance and micro-level personalized recommendation
policies, respectively. Extensive experiments validate that the framework
facilitates the planning ability of LLMs for long-term recommendation. Our code
and data can be found at https://github.com/jizhi-zhang/BiLLP.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.18571v3' target='_blank'>Arithmetic Control of LLMs for Diverse User Preferences: Directional
  Preference Alignment with Multi-Objective Rewards</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, Tong Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-28 18:58:25</h6>
<p class='card-text'>Fine-grained control over large language models (LLMs) remains a significant
challenge, hindering their adaptability to diverse user needs. While
Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning
LLMs, its reliance on scalar rewards often limits its ability to capture
diverse user preferences in real-world applications. To address this
limitation, we introduce the Directional Preference Alignment (DPA) framework.
Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling
to represent diverse preference profiles. Additionally, DPA models user
preferences as directions (i.e., unit vectors) in the reward space to achieve
user-dependent preference control. Our method involves training a
multi-objective reward model and then fine-tuning the LLM with a
preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF
method adopted by Llama 2. This method enjoys a better performance trade-off
across various reward objectives. In comparison with the scalar-reward RLHF,
DPA offers users intuitive control over LLM generation: they can arithmetically
specify their desired trade-offs (e.g., more helpfulness with less verbosity).
We also validate the effectiveness of DPA with real-world alignment experiments
on Mistral-7B. Our method provides straightforward arithmetic control over the
trade-off between helpfulness and verbosity while maintaining competitive
performance with strong baselines such as Direct Preference Optimization (DPO).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.18225v1' target='_blank'>CogBench: a large language model walks into a psychology lab</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Julian Coda-Forno, Marcel Binz, Jane X. Wang, Eric Schulz</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-28 10:43:54</h6>
<p class='card-text'>Large language models (LLMs) have significantly advanced the field of
artificial intelligence. Yet, evaluating them comprehensively remains
challenging. We argue that this is partly due to the predominant focus on
performance metrics in most benchmarks. This paper introduces CogBench, a
benchmark that includes ten behavioral metrics derived from seven cognitive
psychology experiments. This novel approach offers a toolkit for phenotyping
LLMs' behavior. We apply CogBench to 35 LLMs, yielding a rich and diverse
dataset. We analyze this data using statistical multilevel modeling techniques,
accounting for the nested dependencies among fine-tuned versions of specific
LLMs. Our study highlights the crucial role of model size and reinforcement
learning from human feedback (RLHF) in improving performance and aligning with
human behavior. Interestingly, we find that open-source models are less
risk-prone than proprietary models and that fine-tuning on code does not
necessarily enhance LLMs' behavior. Finally, we explore the effects of
prompt-engineering techniques. We discover that chain-of-thought prompting
improves probabilistic reasoning, while take-a-step-back prompting fosters
model-based behaviors.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.00830v1' target='_blank'>MedAide: Leveraging Large Language Models for On-Premise Medical
  Assistance on Edge Devices</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Abdul Basit, Khizar Hussain, Muhammad Abdullah Hanif, Muhammad Shafique</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-28 08:30:49</h6>
<p class='card-text'>Large language models (LLMs) are revolutionizing various domains with their
remarkable natural language processing (NLP) abilities. However, deploying LLMs
in resource-constrained edge computing and embedded systems presents
significant challenges. Another challenge lies in delivering medical assistance
in remote areas with limited healthcare facilities and infrastructure. To
address this, we introduce MedAide, an on-premise healthcare chatbot. It
leverages tiny-LLMs integrated with LangChain, providing efficient edge-based
preliminary medical diagnostics and support. MedAide employs model
optimizations for minimal memory footprint and latency on embedded edge devices
without server infrastructure. The training process is optimized using low-rank
adaptation (LoRA). Additionally, the model is trained on diverse medical
datasets, employing reinforcement learning from human feedback (RLHF) to
enhance its domain-specific capabilities. The system is implemented on various
consumer GPUs and Nvidia Jetson development board. MedAide achieves 77\%
accuracy in medical consultations and scores 56 in USMLE benchmark, enabling an
energy-efficient healthcare assistance platform that alleviates privacy
concerns due to edge-based deployment, thereby empowering the community.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.16181v1' target='_blank'>How Can LLM Guide RL? A Value-Based Approach</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shenao Zhang, Sirui Zheng, Shuqi Ke, Zhihan Liu, Wanxin Jin, Jianbo Yuan, Yingxiang Yang, Hongxia Yang, Zhaoran Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-25 20:07:13</h6>
<p class='card-text'>Reinforcement learning (RL) has become the de facto standard practice for
sequential decision-making problems by improving future acting policies with
feedback. However, RL algorithms may require extensive trial-and-error
interactions to collect useful feedback for improvement. On the other hand,
recent developments in large language models (LLMs) have showcased impressive
capabilities in language understanding and generation, yet they fall short in
exploration and self-improvement capabilities for planning tasks, lacking the
ability to autonomously refine their responses based on feedback. Therefore, in
this paper, we study how the policy prior provided by the LLM can enhance the
sample efficiency of RL algorithms. Specifically, we develop an algorithm named
LINVIT that incorporates LLM guidance as a regularization factor in value-based
RL, leading to significant reductions in the amount of data needed for
learning, particularly when the difference between the ideal policy and the
LLM-informed policy is small, which suggests that the initial policy is close
to optimal, reducing the need for further exploration. Additionally, we present
a practical algorithm SLINVIT that simplifies the construction of the value
function and employs subgoals to reduce the search complexity. Our experiments
across three interactive environments ALFWorld, InterCode, and BlocksWorld
demonstrate that our method achieves state-of-the-art success rates and also
surpasses previous RL and LLM approaches in terms of sample efficiency. Our
code is available at https://github.com/agentification/Language-Integrated-VI.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.16063v3' target='_blank'>Citation-Enhanced Generation for LLM-based Chatbots</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weitao Li, Junkai Li, Weizhi Ma, Yang Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-25 11:24:41</h6>
<p class='card-text'>Large language models (LLMs) exhibit powerful general intelligence across
diverse scenarios, including their integration into chatbots. However, a vital
challenge of LLM-based chatbots is that they may produce hallucinated content
in responses, which significantly limits their applicability. Various efforts
have been made to alleviate hallucination, such as retrieval augmented
generation and reinforcement learning with human feedback, but most of them
require additional training and data annotation. In this paper, we propose a
novel post-hoc Citation-Enhanced Generation (CEG) approach combined with
retrieval argumentation. Unlike previous studies that focus on preventing
hallucinations during generation, our method addresses this issue in a post-hoc
way. It incorporates a retrieval module to search for supporting documents
relevant to the generated content, and employs a natural language
inference-based citation generation module. Once the statements in the
generated content lack of reference, our model can regenerate responses until
all statements are supported by citations. Note that our method is a
training-free plug-and-play plugin that is capable of various LLMs. Experiments
on various hallucination-related datasets show our framework outperforms
state-of-the-art methods in both hallucination detection and response
regeneration on three benchmarks. Our codes and dataset will be publicly
available.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.16048v3' target='_blank'>How Likely Do LLMs with CoT Mimic Human Reasoning?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guangsheng Bao, Hongbo Zhang, Cunxiang Wang, Linyi Yang, Yue Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-25 10:13:04</h6>
<p class='card-text'>Chain-of-thought emerges as a promising technique for eliciting reasoning
capabilities from Large Language Models (LLMs). However, it does not always
improve task performance or accurately represent reasoning processes, leaving
unresolved questions about its usage. In this paper, we diagnose the underlying
mechanism by comparing the reasoning process of LLMs with humans, using causal
analysis to understand the relationships between the problem instruction,
reasoning, and the answer in LLMs. Our empirical study reveals that LLMs often
deviate from the ideal causal chain, resulting in spurious correlations and
potential consistency errors (inconsistent reasoning and answers). We also
examine various factors influencing the causal structure, finding that
in-context learning with examples strengthens it, while post-training
techniques like supervised fine-tuning and reinforcement learning on human
feedback weaken it. To our surprise, the causal structure cannot be
strengthened by enlarging the model size only, urging research on new
techniques. We hope that this preliminary study will shed light on
understanding and improving the reasoning process in LLM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.16030v1' target='_blank'>Don't Forget Your Reward Values: Language Model Alignment via
  Value-based Calibration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xin Mao, Feng-Lin Li, Huimin Xu, Wei Zhang, Anh Tuan Luu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-25 08:45:10</h6>
<p class='card-text'>While Reinforcement Learning from Human Feedback (RLHF) significantly
enhances the generation quality of Large Language Models (LLMs), recent studies
have raised concerns regarding the complexity and instability associated with
the Proximal Policy Optimization (PPO) algorithm, proposing a series of
order-based calibration methods as viable alternatives. This paper delves
further into current order-based methods, examining their inefficiencies in
utilizing reward values and addressing misalignment issues. Building upon these
findings, we propose a novel \textbf{V}alue-based \textbf{C}ali\textbf{B}ration
(VCB) method to better align LLMs with human preferences. Experimental results
demonstrate that VCB surpasses existing alignment methods on AI assistant and
summarization datasets, providing impressive generalizability, robustness, and
stability in diverse settings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.15729v3' target='_blank'>How Do Humans Write Code? Large Models Do It the Same Way Too</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Long Li, Xuzheng He, Haozhe Wang, Linlin Wang, Liang He</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-24 05:40:01</h6>
<p class='card-text'>Program-of-Thought (PoT) replaces natural language-based Chain-of-Thought
(CoT) as the most popular method in Large Language Models (LLMs) mathematical
reasoning tasks by utilizing external tool calls to circumvent computational
errors. However, our evaluation of the GPT-4 and Llama series reveals that
using PoT introduces more reasoning errors, such as incorrect formulas or
flawed logic, compared to CoT. To address this issue, we propose Human-Think
Language (HTL), which leverages a suite of strategies that help integrate PoT
and CoT, encompassing: (1) a new generation paradigm that uses full CoT
reasoning to control code generation. (2) Focus Attention, that directs model
attention to the CoT reasoning during PoT to generate more logical code. (3)
reinforcement learning that utilizes the accuracy of both CoT and PoT responses
as rewards to prevent repetitive reasoning steps in LLMs when solving difficult
math problems. Our method achieves an average improvement of 6.5% on the
Llama-Base model and 4.3% on the Mistral-Base model across 8 mathematical
calculation datasets. It also shows significant effectiveness on five
out-of-domain datasets by controlling the model's information flow, exhibiting
strong transferability. Additionally, HTL shows the most significant
improvement in non-mathematical natural language inference task, contributing
to a unified reasoning task framework</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.15481v4' target='_blank'>Prejudice and Volatility: A Statistical Framework for Measuring Social
  Discrimination in Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Y Liu, K Yang, Z Qi, X Liu, Y Yu, C Zhai</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-23 18:15:56</h6>
<p class='card-text'>This study investigates why and how inconsistency in the generation of Large
Language Models (LLMs) might induce or exacerbate societal injustice. For
instance, LLMs frequently exhibit contrasting gender stereotypes regarding the
same career depending on varied contexts, highlighting the arguably harmful
unpredictability of LLMs' behavioral patterns. To augment the existing
discrimination assessment with the capability to account for variation in LLM
generation, we formulate the Prejudice-Volatility Framework (PVF) that
precisely defines behavioral metrics for assessing LLMs, which delineate the
probability distribution of LLMs' stereotypes from the perspective of token
prediction probability. Specifically, we employ a data-mining approach to
approximate the possible applied contexts of LLMs and devise statistical
metrics to evaluate the corresponding contextualized societal discrimination
risk. Further, we mathematically dissect the aggregated discrimination risk of
LLMs into prejudice risk, originating from their system bias, and volatility
risk, stemming from their generation inconsistency. While initially intended
for assessing discrimination in LLMs, our proposed PVF facilitates the
comprehensive and flexible measurement of any inductive biases, including
knowledge alongside prejudice, across various modality models.
  We apply PVF to 12 most commonly adopted LLMs and compare their risk levels.
Our findings reveal that: i) prejudice risk is the primary cause of
discrimination risk in LLMs, indicating that inherent biases in these models
lead to stereotypical outputs; ii) most LLMs exhibit significant pro-male
stereotypes across nearly all careers; iii) alignment with Reinforcement
Learning from Human Feedback lowers discrimination by reducing prejudice, but
increases volatility; iv) discrimination risk in LLMs correlates with
socio-economic factors like profession salaries.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.15420v1' target='_blank'>PREDILECT: Preferences Delineated with Zero-Shot Language-based
  Reasoning in Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Simon Holk, Daniel Marta, Iolanda Leite</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-23 16:30:05</h6>
<p class='card-text'>Preference-based reinforcement learning (RL) has emerged as a new field in
robot learning, where humans play a pivotal role in shaping robot behavior by
expressing preferences on different sequences of state-action pairs. However,
formulating realistic policies for robots demands responses from humans to an
extensive array of queries. In this work, we approach the sample-efficiency
challenge by expanding the information collected per query to contain both
preferences and optional text prompting. To accomplish this, we leverage the
zero-shot capabilities of a large language model (LLM) to reason from the text
provided by humans. To accommodate the additional query information, we
reformulate the reward learning objectives to contain flexible highlights --
state-action pairs that contain relatively high information and are related to
the features processed in a zero-shot fashion from a pretrained LLM. In both a
simulated scenario and a user study, we reveal the effectiveness of our work by
analyzing the feedback and its implications. Additionally, the collective
feedback collected serves to train a robot on socially compliant trajectories
in a simulated social navigation landscape. We provide video examples of the
trained policies at https://sites.google.com/view/rl-predilect</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.15018v2' target='_blank'>Unintended Impacts of LLM Alignment on Global Representation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Michael J. Ryan, William Held, Diyi Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-22 23:31:22</h6>
<p class='card-text'>Before being deployed for user-facing applications, developers align Large
Language Models (LLMs) to user preferences through a variety of procedures,
such as Reinforcement Learning From Human Feedback (RLHF) and Direct Preference
Optimization (DPO). Current evaluations of these procedures focus on benchmarks
of instruction following, reasoning, and truthfulness. However, human
preferences are not universal, and aligning to specific preference sets may
have unintended effects. We explore how alignment impacts performance along
three axes of global representation: English dialects, multilingualism, and
opinions from and about countries worldwide. Our results show that current
alignment procedures create disparities between English dialects and global
opinions. We find alignment improves capabilities in several languages. We
conclude by discussing design decisions that led to these unintended impacts
and recommendations for more equitable preference tuning. We make our code and
data publicly available on Github.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.14760v2' target='_blank'>Generalizing Reward Modeling for Out-of-Distribution Preference Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chen Jia</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-22 18:20:33</h6>
<p class='card-text'>Preference learning (PL) with large language models (LLMs) aims to align the
LLMs' generations with human preferences. Previous work on reinforcement
learning from human feedback (RLHF) has demonstrated promising results in
in-distribution PL. However, due to the difficulty of obtaining human feedback,
discretely training reward models for every encountered distribution is
challenging. Thus, out-of-distribution (OOD) PL is practically useful for
enhancing the generalization ability of LLMs with limited preference feedback.
This work addresses OOD PL by optimizing a general reward model through a
meta-learning approach. During meta-training, a bilevel optimization algorithm
is utilized to learn a reward model capable of guiding policy learning to align
with human preferences across various distributions. When encountering a test
distribution, the meta-test procedure conducts regularized policy optimization
using the learned reward model for PL. We theoretically demonstrate the
convergence rate of the bilevel optimization algorithm under reasonable
assumptions. Additionally, we conduct experiments on two text generation tasks
across 20 held-out domains and outperform a variety of strong baselines across
various evaluation metrics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.14740v2' target='_blank'>Back to Basics: Revisiting REINFORCE Style Optimization for Learning
  from Human Feedback in LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, Sara Hooker</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-22 17:52:34</h6>
<p class='card-text'>AI alignment in the shape of Reinforcement Learning from Human Feedback
(RLHF) is increasingly treated as a crucial ingredient for high performance
large language models. Proximal Policy Optimization (PPO) has been positioned
by recent literature as the canonical method for the RL part of RLHF. However,
it involves both high computational cost and sensitive hyperparameter tuning.
We posit that most of the motivational principles that led to the development
of PPO are less of a practical concern in RLHF and advocate for a less
computationally expensive method that preserves and even increases performance.
We revisit the formulation of alignment from human preferences in the context
of RL. Keeping simplicity as a guiding principle, we show that many components
of PPO are unnecessary in an RLHF context and that far simpler REINFORCE-style
optimization variants outperform both PPO and newly proposed "RL-free" methods
such as DPO and RAFT. Our work suggests that careful adaptation to LLMs
alignment characteristics enables benefiting from online RL optimization at low
cost.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.14228v3' target='_blank'>COPR: Continual Human Preference Learning via Optimal Policy
  Regularization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Han Zhang, Lin Gui, Yu Lei, Yuanzhao Zhai, Yehong Zhang, Yulan He, Hui Wang, Yue Yu, Kam-Fai Wong, Bin Liang, Ruifeng Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-22 02:20:08</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) is commonly utilized to
improve the alignment of Large Language Models (LLMs) with human preferences.
Given the evolving nature of human preferences, continual alignment becomes
more crucial and practical in comparison to traditional static alignment.
Nevertheless, making RLHF compatible with Continual Learning (CL) is
challenging due to its complex process. Meanwhile, directly learning new human
preferences may lead to Catastrophic Forgetting (CF) of historical preferences,
resulting in helpless or harmful outputs. To overcome these challenges, we
propose the Continual Optimal Policy Regularization (COPR) method, which draws
inspiration from the optimal policy theory. COPR utilizes a sampling
distribution as a demonstration and regularization constraints for CL. It
adopts the Lagrangian Duality (LD) method to dynamically regularize the current
policy based on the historically optimal policy, which prevents CF and avoids
over-emphasizing unbalanced objectives. We also provide formal proof for the
learnability of COPR. The experimental results show that COPR outperforms
strong CL baselines on our proposed benchmark, in terms of reward-based, GPT-4
evaluations and human assessment. Furthermore, we validate the robustness of
COPR under various CL settings, including different backbones, replay memory
sizes, and learning orders.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.14195v1' target='_blank'>Learning to Reduce: Optimal Representations of Structured Data in
  Prompting Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Younghun Lee, Sungchul Kim, Tong Yu, Ryan A. Rossi, Xiang Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-22 00:41:23</h6>
<p class='card-text'>Large Language Models (LLMs) have been widely used as general-purpose AI
agents showing comparable performance on many downstream tasks. However,
existing work shows that it is challenging for LLMs to integrate structured
data (e.g. KG, tables, DBs) into their prompts; LLMs need to either understand
long text data or select the most relevant evidence prior to inference, and
both approaches are not trivial.
  In this paper, we propose a framework, Learning to Reduce, that fine-tunes a
language model to generate a reduced version of an input context, given a task
description and context input. The model learns to reduce the input context
using On-Policy Reinforcement Learning and aims to improve the reasoning
performance of a fixed LLM. Experimental results illustrate that our model not
only achieves comparable accuracies in selecting the relevant evidence from an
input context, but also shows generalizability on different datasets. We
further show that our model helps improve the LLM's performance on downstream
tasks especially when the context is long.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.13659v2' target='_blank'>Privacy-Preserving Instructions for Aligning Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Da Yu, Peter Kairouz, Sewoong Oh, Zheng Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-21 09:45:08</h6>
<p class='card-text'>Service providers of large language model (LLM) applications collect user
instructions in the wild and use them in further aligning LLMs with users'
intentions. These instructions, which potentially contain sensitive
information, are annotated by human workers in the process. This poses a new
privacy risk not addressed by the typical private optimization. To this end, we
propose using synthetic instructions to replace real instructions in data
annotation and model fine-tuning. Formal differential privacy is guaranteed by
generating those synthetic instructions using privately fine-tuned generators.
Crucial in achieving the desired utility is our novel filtering algorithm that
matches the distribution of the synthetic instructions to that of the real
ones. In both supervised fine-tuning and reinforcement learning from human
feedback, our extensive experiments demonstrate the high utility of the final
set of synthetic instructions by showing comparable results to real
instructions. In supervised fine-tuning, models trained with private synthetic
instructions outperform leading open-source models such as Vicuna.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.13623v1' target='_blank'>FLAME: Self-Supervised Low-Resource Taxonomy Expansion using Large
  Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sahil Mishra, Ujjwal Sudev, Tanmoy Chakraborty</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-21 08:50:40</h6>
<p class='card-text'>Taxonomies represent an arborescence hierarchical structure that establishes
relationships among entities to convey knowledge within a specific domain. Each
edge in the taxonomy signifies a hypernym-hyponym relationship. Taxonomies find
utility in various real-world applications, such as e-commerce search engines
and recommendation systems. Consequently, there arises a necessity to enhance
these taxonomies over time. However, manually curating taxonomies with neoteric
data presents challenges due to limitations in available human resources and
the exponential growth of data. Therefore, it becomes imperative to develop
automatic taxonomy expansion methods. Traditional supervised taxonomy expansion
approaches encounter difficulties stemming from limited resources, primarily
due to the small size of existing taxonomies. This scarcity of training data
often leads to overfitting. In this paper, we propose FLAME, a novel approach
for taxonomy expansion in low-resource environments by harnessing the
capabilities of large language models that are trained on extensive real-world
knowledge. LLMs help compensate for the scarcity of domain-specific knowledge.
Specifically, FLAME leverages prompting in few-shot settings to extract the
inherent knowledge within the LLMs, ascertaining the hypernym entities within
the taxonomy. Furthermore, it employs reinforcement learning to fine-tune the
large language models, resulting in more accurate predictions. Experiments on
three real-world benchmark datasets demonstrate the effectiveness of FLAME in
real-world scenarios, achieving a remarkable improvement of 18.5% in accuracy
and 12.3% in Wu & Palmer metric over eight baselines. Furthermore, we elucidate
the strengths and weaknesses of FLAME through an extensive case study, error
analysis and ablation studies on the benchmarks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.13210v2' target='_blank'>Bayesian Reward Models for LLM Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Adam X. Yang, Maxime Robeyns, Thomas Coste, Zhengyan Shi, Jun Wang, Haitham Bou-Ammar, Laurence Aitchison</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-20 18:20:59</h6>
<p class='card-text'>To ensure that large language model (LLM) responses are helpful and
non-toxic, a reward model trained on human preference data is usually used. LLM
responses with high rewards are then selected through best-of-$n$ (BoN)
sampling or the LLM is further optimized to produce responses with high rewards
through reinforcement learning from human feedback (RLHF). However, these
processes are susceptible to reward overoptimization or `hacking', where
responses receive high rewards due to imperfections in the reward model rather
than true preference, particularly as prompts or responses deviate from the
training data. To address these challenges, we propose to train a Bayesian
reward model, which signals higher uncertainty further from the training data
distribution. We trained Bayesian reward models using Laplace approximation on
LoRA weights, and found that the resulting uncertainty estimates can
effectively mitigate reward overoptimization in BoN sampling.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.12914v1' target='_blank'>Large Language Model-based Human-Agent Collaboration for Complex Task
  Solving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xueyang Feng, Zhi-Yuan Chen, Yujia Qin, Yankai Lin, Xu Chen, Zhiyuan Liu, Ji-Rong Wen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-20 11:03:36</h6>
<p class='card-text'>In recent developments within the research community, the integration of
Large Language Models (LLMs) in creating fully autonomous agents has garnered
significant interest. Despite this, LLM-based agents frequently demonstrate
notable shortcomings in adjusting to dynamic environments and fully grasping
human needs. In this work, we introduce the problem of LLM-based human-agent
collaboration for complex task-solving, exploring their synergistic potential.
In addition, we propose a Reinforcement Learning-based Human-Agent
Collaboration method, ReHAC. This approach includes a policy model designed to
determine the most opportune stages for human intervention within the
task-solving process. We construct a human-agent collaboration dataset to train
this policy model in an offline reinforcement learning environment. Our
validation tests confirm the model's effectiveness. The results demonstrate
that the synergistic efforts of humans and LLM-based agents significantly
improve performance in complex tasks, primarily through well-planned, limited
human intervention. Datasets and code are available at:
https://github.com/XueyangFeng/ReHAC.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.12222v1' target='_blank'>CovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement
  Learning for LLM-based Mutation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jueon Eom, Seyeon Jeong, Taekyoung Kwon</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-19 15:30:40</h6>
<p class='card-text'>Fuzzing is an effective bug-finding technique but it struggles with complex
systems like JavaScript engines that demand precise grammatical input.
Recently, researchers have adopted language models for context-aware mutation
in fuzzing to address this problem. However, existing techniques are limited in
utilizing coverage guidance for fuzzing, which is rather performed in a
black-box manner. This paper presents a novel technique called CovRL
(Coverage-guided Reinforcement Learning) that combines Large Language Models
(LLMs) with reinforcement learning from coverage feedback. Our fuzzer,
CovRL-Fuzz, integrates coverage feedback directly into the LLM by leveraging
the Term Frequency-Inverse Document Frequency (TF-IDF) method to construct a
weighted coverage map. This map is key in calculating the fuzzing reward, which
is then applied to the LLM-based mutator through reinforcement learning.
CovRL-Fuzz, through this approach, enables the generation of test cases that
are more likely to discover new coverage areas, thus improving vulnerability
detection while minimizing syntax and semantic errors, all without needing
extra post-processing. Our evaluation results indicate that CovRL-Fuzz
outperforms the state-of-the-art fuzzers in terms of code coverage and
bug-finding capabilities: CovRL-Fuzz identified 48 real-world security-related
bugs in the latest JavaScript engines, including 39 previously unknown
vulnerabilities and 11 CVEs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.12174v2' target='_blank'>BIDER: Bridging Knowledge Inconsistency for Efficient
  Retrieval-Augmented LLMs via Key Supporting Evidence</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiajie Jin, Yutao Zhu, Yujia Zhou, Zhicheng Dou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-19 14:28:31</h6>
<p class='card-text'>Retrieval-augmented large language models (LLMs) have demonstrated efficacy
in knowledge-intensive tasks such as open-domain QA, addressing inherent
challenges in knowledge update and factual inadequacy. However, inconsistencies
between retrieval knowledge and the necessary knowledge for LLMs, leading to a
decline in LLM's answer quality. This paper introduces BIDER, an approach that
refines retrieval documents into Key Supporting Evidence (KSE) through
knowledge synthesis, supervised fine-tuning (SFT), and preference alignment. We
train BIDER by learning from crafting KSE, while maximizing its output to align
with LLM's information acquisition preferences through reinforcement learning.
Evaluations across five datasets show BIDER boosts LLMs' answer quality by 7%
while reducing input content length in retrieval documents by 80%,
outperforming existing methods. The proposed KSE simulation effectively equips
LLMs with essential information for accurate question answering.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.12061v2' target='_blank'>All Language Models Large and Small</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhixun Chen, Yali Du, David Mguni</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-19 11:28:20</h6>
<p class='card-text'>Many leading language models (LMs) use high-intensity computational resources
both during training and execution. This poses the challenge of lowering
resource costs for deployment and faster execution of decision-making tasks
among others. We introduce a novel plug-and-play LM framework named Language
Optimising Network Distribution (LONDI) framework. LONDI learns to selectively
employ large LMs only where complex decision-making and reasoning are required
while using low-resource LMs (i.e. LMs require less GPU usage, but may not be
able to solve the problem alone) everywhere else. LONDI consists of a system of
two (off-)policy networks, an LM, a large LM (LLM), and a reinforcement
learning module that uses switching controls to quickly learn which system
states to call the LLM. We then introduce a variant of LONDI that maintains
budget constraints on LLM calls and hence its resource usage. Theoretically, we
prove LONDI learns the subset of system states to activate the LLM required to
solve the task. We then prove that LONDI converges to optimal solutions while
also preserving budgetary constraints on LLM calls almost surely enabling it to
solve various tasks while significantly lowering computational costs. We test
LONDI's performance in a range of tasks in ScienceWorld and BabyAI-Text and
demonstrate that LONDI can solve tasks only solvable by resource-intensive LLMs
while reducing GPU usage by up to 30%.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.11452v1' target='_blank'>AutoPRM: Automating Procedural Supervision for Multi-Step Reasoning via
  Controllable Question Decomposition</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhaorun Chen, Zhuokai Zhao, Zhihong Zhu, Ruiqi Zhang, Xiang Li, Bhiksha Raj, Huaxiu Yao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-18 04:28:16</h6>
<p class='card-text'>Recent advancements in large language models (LLMs) have shown promise in
multi-step reasoning tasks, yet their reliance on extensive manual labeling to
provide procedural feedback remains a significant impediment. To address this
challenge, in this paper, we propose a novel self-supervised framework AutoPRM
that efficiently enhances the fine-tuning of LLMs for intricate reasoning
challenges. Specifically, AutoPRM first decomposes complex problems into more
manageable subquestions with a controllable granularity switch, then
sequentially apply reinforcement learning to iteratively improve the
subquestion solver. Additionally, we propose context-guided-decoding to avoid
reward tampering and guide the subquestion solver towards the solution of the
holistic problem. Extensive experiments show that AutoPRM significantly
improves performance on mathematical and commonsense reasoning tasks over SOTA.
More encouragingly, AutoPRM can be easily integrated with other orthogonal
reasoning pipelines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.11430v1' target='_blank'>EventRL: Enhancing Event Extraction with Outcome Supervision for Large
  Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jun Gao, Huan Zhao, Wei Wang, Changlong Yu, Ruifeng Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-18 02:41:06</h6>
<p class='card-text'>In this study, we present EventRL, a reinforcement learning approach
developed to enhance event extraction for large language models (LLMs). EventRL
utilizes outcome supervision with specific reward functions to tackle prevalent
challenges in LLMs, such as instruction following and hallucination, manifested
as the mismatch of event structure and the generation of undefined event types.
We evaluate EventRL against existing methods like Few-Shot Prompting (FSP)
(based on GPT4) and Supervised Fine-Tuning (SFT) across various LLMs, including
GPT-4, LLaMa, and CodeLLaMa models. Our findings show that EventRL
significantly outperforms these conventional approaches by improving the
performance in identifying and structuring events, particularly in handling
novel event types. The study emphasizes the critical role of reward function
selection and demonstrates the benefits of incorporating code data for better
event extraction. While increasing model size leads to higher accuracy,
maintaining the ability to generalize is essential to avoid overfitting.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.10893v1' target='_blank'>RLVF: Learning from Verbal Feedback without Overgeneralization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Moritz Stephan, Alexander Khazatsky, Eric Mitchell, Annie S Chen, Sheryl Hsu, Archit Sharma, Chelsea Finn</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-16 18:50:24</h6>
<p class='card-text'>The diversity of contexts in which large language models (LLMs) are deployed
requires the ability to modify or customize default model behaviors to
incorporate nuanced requirements and preferences. A convenient interface to
specify such model adjustments is high-level verbal feedback, such as "Don't
use emojis when drafting emails to my boss." However, while writing high-level
feedback is far simpler than collecting annotations for reinforcement learning
from human feedback (RLHF), we find that simply prompting a model with such
feedback leads to overgeneralization of the feedback to contexts where it is
not relevant. We study the problem of incorporating verbal feedback without
such overgeneralization, inspiring a new method Contextualized Critiques with
Constrained Preference Optimization (C3PO). C3PO uses a piece of high-level
feedback to generate a small synthetic preference dataset specifying how the
feedback should (and should not) be applied. It then fine-tunes the model in
accordance with the synthetic preference data while minimizing the divergence
from the original model for prompts where the feedback does not apply. Our
experimental results indicate that our approach effectively applies verbal
feedback to relevant scenarios while preserving existing behaviors for other
contexts. For both human- and GPT-4-generated high-level feedback, C3PO
effectively adheres to the given feedback comparably to in-context baselines
while reducing overgeneralization by 30%.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.04769v2' target='_blank'>Using Hallucinations to Bypass GPT4's Filter</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Benjamin Lemkin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-16 17:02:53</h6>
<p class='card-text'>Large language models (LLMs) are initially trained on vast amounts of data,
then fine-tuned using reinforcement learning from human feedback (RLHF); this
also serves to teach the LLM to provide appropriate and safe responses. In this
paper, we present a novel method to manipulate the fine-tuned version into
reverting to its pre-RLHF behavior, effectively erasing the model's filters;
the exploit currently works for GPT4, Claude Sonnet, and (to some extent) for
Inflection-2.5. Unlike other jailbreaks (for example, the popular "Do Anything
Now" (DAN) ), our method does not rely on instructing the LLM to override its
RLHF policy; hence, simply modifying the RLHF process is unlikely to address
it. Instead, we induce a hallucination involving reversed text during which the
model reverts to a word bucket, effectively pausing the model's filter. We
believe that our exploit presents a fundamental vulnerability in LLMs currently
unaddressed, as well as an opportunity to better understand the inner workings
of LLMs during hallucinations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.10670v2' target='_blank'>OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via
  Vision-Language Foundation Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuxuan Kuang, Hai Lin, Meng Jiang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-16 13:21:33</h6>
<p class='card-text'>Object navigation (ObjectNav) requires an agent to navigate through unseen
environments to find queried objects. Many previous methods attempted to solve
this task by relying on supervised or reinforcement learning, where they are
trained on limited household datasets with close-set objects. However, two key
challenges are unsolved: understanding free-form natural language instructions
that demand open-set objects, and generalizing to new environments in a
zero-shot manner. Aiming to solve the two challenges, in this paper, we propose
OpenFMNav, an Open-set Foundation Model based framework for zero-shot object
Navigation. We first unleash the reasoning abilities of large language models
(LLMs) to extract proposed objects from natural language instructions that meet
the user's demand. We then leverage the generalizability of large vision
language models (VLMs) to actively discover and detect candidate objects from
the scene, building a Versatile Semantic Score Map (VSSM). Then, by conducting
common sense reasoning on VSSM, our method can perform effective
language-guided exploration and exploitation of the scene and finally reach the
goal. By leveraging the reasoning and generalizing abilities of foundation
models, our method can understand free-form human instructions and perform
effective open-set zero-shot navigation in diverse environments. Extensive
experiments on the HM3D ObjectNav benchmark show that our method surpasses all
the strong baselines on all metrics, proving our method's effectiveness.
Furthermore, we perform real robot demonstrations to validate our method's
open-set-ness and generalizability to real-world environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.10500v2' target='_blank'>Active Preference Optimization for Sample Efficient RLHF</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nirjhar Das, Souradip Chakraborty, Aldo Pacchiano, Sayak Ray Chowdhury</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-16 08:19:34</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) is pivotal in aligning
Large Language Models (LLMs) with human preferences. Although aligned
generative models have shown remarkable abilities in various tasks, their
reliance on high-quality human preference data creates a costly bottleneck in
the practical application of RLHF. One primary reason is that current methods
rely on uniformly picking prompt-generation pairs from a dataset of
prompt-generations, to collect human feedback, resulting in sub-optimal
alignment under a constrained budget, which highlights the criticality of
adaptive strategies in efficient alignment. Recent works [Mehta et al., 2023,
Muldrew et al., 2024] have tried to address this problem by designing various
heuristics based on generation uncertainty. However, either the assumptions in
[Mehta et al., 2023] are restrictive, or [Muldrew et al., 2024] do not provide
any rigorous theoretical guarantee. To address these, we reformulate RLHF
within contextual preference bandit framework, treating prompts as contexts,
and develop an active-learning algorithm, $\textit{Active Preference
Optimization}$ ($\texttt{APO}$), which enhances model alignment by querying
preference data from the most important samples, achieving superior performance
for small sample budget. We analyze the theoretical performance guarantees of
$\texttt{APO}$ under the BTL preference model showing that the suboptimality
gap of the policy learned via $\texttt{APO}$ scales as $O(1/\sqrt{T})$ for a
budget of $T$. We also show that collecting preference data by choosing prompts
randomly leads to a policy that suffers a constant sub-optimality. We perform
detailed experimental evaluations on practical preference datasets to validate
$\texttt{APO}$'s efficacy over the existing methods, establishing it as a
sample-efficient and practical solution of alignment in a cost-effective and
scalable manner.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.10210v1' target='_blank'>Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Huizhuo Yuan, Zixiang Chen, Kaixuan Ji, Quanquan Gu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-15 18:59:18</h6>
<p class='card-text'>Fine-tuning Diffusion Models remains an underexplored frontier in generative
artificial intelligence (GenAI), especially when compared with the remarkable
progress made in fine-tuning Large Language Models (LLMs). While cutting-edge
diffusion models such as Stable Diffusion (SD) and SDXL rely on supervised
fine-tuning, their performance inevitably plateaus after seeing a certain
volume of data. Recently, reinforcement learning (RL) has been employed to
fine-tune diffusion models with human preference data, but it requires at least
two images ("winner" and "loser" images) for each text prompt. In this paper,
we introduce an innovative technique called self-play fine-tuning for diffusion
models (SPIN-Diffusion), where the diffusion model engages in competition with
its earlier versions, facilitating an iterative self-improvement process. Our
approach offers an alternative to conventional supervised fine-tuning and RL
strategies, significantly improving both model performance and alignment. Our
experiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms
the existing supervised fine-tuning method in aspects of human preference
alignment and visual appeal right from its first iteration. By the second
iteration, it exceeds the performance of RLHF-based methods across all metrics,
achieving these results with less data.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.10207v6' target='_blank'>Rewards-in-Context: Multi-objective Alignment of Foundation Models with
  Dynamic Preference Adjustment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rui Yang, Xiaoman Pan, Feng Luo, Shuang Qiu, Han Zhong, Dong Yu, Jianshu Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-15 18:58:31</h6>
<p class='card-text'>We consider the problem of multi-objective alignment of foundation models
with human preferences, which is a critical step towards helpful and harmless
AI systems. However, it is generally costly and unstable to fine-tune large
foundation models using reinforcement learning (RL), and the
multi-dimensionality, heterogeneity, and conflicting nature of human
preferences further complicate the alignment process. In this paper, we
introduce Rewards-in-Context (RiC), which conditions the response of a
foundation model on multiple rewards in its prompt context and applies
supervised fine-tuning for alignment. The salient features of RiC are
simplicity and adaptivity, as it only requires supervised fine-tuning of a
single foundation model and supports dynamic adjustment for user preferences
during inference time. Inspired by the analytical solution of an abstracted
convex optimization problem, our dynamic inference-time adjustment method
approaches the Pareto-optimal solution for multiple objectives. Empirical
evidence demonstrates the efficacy of our method in aligning both Large
Language Models (LLMs) and diffusion models to accommodate diverse rewards with
only around 10% GPU hours compared with multi-objective RL baseline.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.10038v2' target='_blank'>RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization
  Method for Alignment of Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Saeed Khaki, JinJin Li, Lan Ma, Liu Yang, Prathap Ramachandra</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-15 16:00:58</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) has been extensively
employed to align large language models with user intent. However, proximal
policy optimization (PPO) based RLHF is occasionally unstable requiring
significant hyperparameter finetuning, and computationally expensive to
maximize the estimated reward during alignment. Recently, direct preference
optimization (DPO) is proposed to address those challenges. However, DPO relies
on contrastive responses generated from human annotator and alternative LLM,
instead of the policy model, limiting the effectiveness of the RLHF. In this
paper, we addresses both challenges by systematically combining rejection
sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the
development of a supervised fine-tuned policy model (SFT). A varied set of k
responses per prompt are sampled directly from the SFT model. RS-DPO identifies
pairs of contrastive samples based on their reward distribution. Finally, we
apply DPO with the contrastive samples to align the model to human preference.
Our experiments indicate that our proposed method effectively fine-tunes LLMs
with limited resource environments, leading to improved alignment with user
intent. Furthermore, it outperforms existing methods, including RS, PPO, and
DPO.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.09764v3' target='_blank'>Aligning Crowd Feedback via Distributional Preference Reward Modeling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dexun Li, Cong Zhang, Kuicai Dong, Derrick Goh Xin Deik, Ruiming Tang, Yong Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-15 07:29:43</h6>
<p class='card-text'>Deep Reinforcement Learning is widely used for aligning Large Language Models
(LLM) with human preference. However, the conventional reward modelling is
predominantly dependent on human annotations provided by a select cohort of
individuals. Such dependence may unintentionally result in skewed models that
reflect the inclinations of these annotators, thereby failing to adequately
represent the wider population's expectations. We propose the Distributional
Preference Reward Model (DPRM), a simple yet effective framework to align large
language models with diverse human preferences. To this end, we characterize
multiple preferences by a categorical distribution and introduce a Bayesian
updater to accommodate shifted or new preferences. On top of that, we design an
optimal-transportation-based loss to calibrate DPRM to align with the
preference distribution. Finally, the expected reward is utilized to fine-tune
an LLM policy to generate responses favoured by the population. Our experiments
show that DPRM significantly enhances the alignment of LLMs with population
preference, yielding more accurate, unbiased, and contextually appropriate
responses.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.09756v1' target='_blank'>Mixture of Experts for Network Optimization: A Large Language
  Model-enabled Approach</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hongyang Du, Guangyuan Liu, Yijing Lin, Dusit Niyato, Jiawen Kang, Zehui Xiong, Dong In Kim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-15 07:09:21</h6>
<p class='card-text'>Optimizing various wireless user tasks poses a significant challenge for
networking systems because of the expanding range of user requirements. Despite
advancements in Deep Reinforcement Learning (DRL), the need for customized
optimization tasks for individual users complicates developing and applying
numerous DRL models, leading to substantial computation resource and energy
consumption and can lead to inconsistent outcomes. To address this issue, we
propose a novel approach utilizing a Mixture of Experts (MoE) framework,
augmented with Large Language Models (LLMs), to analyze user objectives and
constraints effectively, select specialized DRL experts, and weigh each
decision from the participating experts. Specifically, we develop a gate
network to oversee the expert models, allowing a collective of experts to
tackle a wide array of new tasks. Furthermore, we innovatively substitute the
traditional gate network with an LLM, leveraging its advanced reasoning
capabilities to manage expert model selection for joint decisions. Our proposed
method reduces the need to train new DRL models for each unique optimization
problem, decreasing energy consumption and AI model implementation costs. The
LLM-enabled MoE approach is validated through a general maze navigation task
and a specific network service provider utility maximization task,
demonstrating its effectiveness and practical applicability in optimizing
complex networking systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.09401v2' target='_blank'>Reinforcement Learning from Human Feedback with Active Queries</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kaixuan Ji, Jiafan He, Quanquan Gu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-14 18:58:40</h6>
<p class='card-text'>Aligning large language models (LLM) with human preference plays a key role
in building modern generative models and can be achieved by reinforcement
learning from human feedback (RLHF). Despite their superior performance,
current RLHF approaches often require a large amount of human-labelled
preference data, which is expensive to collect. In this paper, inspired by the
success of active learning, we address this problem by proposing
query-efficient RLHF methods. We first formalize the alignment problem as a
contextual dueling bandit problem and design an active-query-based proximal
policy optimization (APPO) algorithm with an $\tilde{O}(d^2/\Delta)$
instance-dependent regret bound and an $\tilde{O}(d^2/\Delta^2)$ query
complexity, where $d$ is the dimension of feature space and $\Delta$ is the
sub-optimality gap over all the contexts. We then propose ADPO, a practical
version of our algorithm based on direct preference optimization (DPO) and
apply it to fine-tuning LLMs. Our experiments show that ADPO, while only making
about half of queries for human preference, matches the performance of the
state-of-the-art DPO method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.08787v6' target='_blank'>Rethinking Machine Unlearning for Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Yuguang Yao, Chris Yuhao Liu, Xiaojun Xu, Hang Li, Kush R. Varshney, Mohit Bansal, Sanmi Koyejo, Yang Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-13 20:51:58</h6>
<p class='card-text'>We explore machine unlearning (MU) in the domain of large language models
(LLMs), referred to as LLM unlearning. This initiative aims to eliminate
undesirable data influence (e.g., sensitive or illegal information) and the
associated model capabilities, while maintaining the integrity of essential
knowledge generation and not affecting causally unrelated information. We
envision LLM unlearning becoming a pivotal element in the life-cycle management
of LLMs, potentially standing as an essential foundation for developing
generative AI that is not only safe, secure, and trustworthy, but also
resource-efficient without the need of full retraining. We navigate the
unlearning landscape in LLMs from conceptual formulation, methodologies,
metrics, and applications. In particular, we highlight the often-overlooked
aspects of existing LLM unlearning research, e.g., unlearning scope, data-model
interaction, and multifaceted efficacy assessment. We also draw connections
between LLM unlearning and related areas such as model editing, influence
functions, model explanation, adversarial training, and reinforcement learning.
Furthermore, we outline an effective assessment framework for LLM unlearning
and explore its applications in copyright and privacy safeguards and
sociotechnical harm reduction.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.08755v1' target='_blank'>LLM-driven Imitation of Subrational Behavior : Illusion or Reality?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Andrea Coletta, Kshama Dwarakanath, Penghang Liu, Svitlana Vyetrenko, Tucker Balch</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-13 19:46:39</h6>
<p class='card-text'>Modeling subrational agents, such as humans or economic households, is
inherently challenging due to the difficulty in calibrating reinforcement
learning models or collecting data that involves human subjects. Existing work
highlights the ability of Large Language Models (LLMs) to address complex
reasoning tasks and mimic human communication, while simulation using LLMs as
agents shows emergent social behaviors, potentially improving our comprehension
of human conduct. In this paper, we propose to investigate the use of LLMs to
generate synthetic human demonstrations, which are then used to learn
subrational agent policies though Imitation Learning. We make an assumption
that LLMs can be used as implicit computational models of humans, and propose a
framework to use synthetic demonstrations derived from LLMs to model
subrational behaviors that are characteristic of humans (e.g., myopic behavior
or preference for risk aversion). We experimentally evaluate the ability of our
framework to model sub-rationality through four simple scenarios, including the
well-researched ultimatum game and marshmallow experiment. To gain confidence
in our framework, we are able to replicate well-established findings from prior
human studies associated with the above scenarios. We conclude by discussing
the potential benefits, challenges and limitations of our framework.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.08114v2' target='_blank'>Active Preference Learning for Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:William Muldrew, Peter Hayes, Mingtian Zhang, David Barber</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-12 23:09:00</h6>
<p class='card-text'>As large language models (LLMs) become more capable, fine-tuning techniques
for aligning with human intent are increasingly important. A key consideration
for aligning these models is how to most effectively use human resources, or
model resources in the case where LLMs themselves are used as oracles.
Reinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most
prominent example of such a technique, but is complex and often unstable.
Direct Preference Optimization (DPO) has recently been proposed as a simpler
and more stable alternative. In this work, we develop an active learning
strategy for DPO to make better use of preference labels. We propose a
practical acquisition function for prompt/completion pairs based on the
predictive entropy of the language model and a measure of certainty of the
implicit preference model optimized by DPO. We demonstrate how our approach
improves both the rate of learning and final performance of fine-tuning on
pairwise preference data.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.08078v1' target='_blank'>Large Language Models as Agents in Two-Player Games</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yang Liu, Peng Sun, Hang Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-12 21:44:32</h6>
<p class='card-text'>By formally defining the training processes of large language models (LLMs),
which usually encompasses pre-training, supervised fine-tuning, and
reinforcement learning with human feedback, within a single and unified machine
learning paradigm, we can glean pivotal insights for advancing LLM
technologies. This position paper delineates the parallels between the training
methods of LLMs and the strategies employed for the development of agents in
two-player games, as studied in game theory, reinforcement learning, and
multi-agent systems. We propose a re-conceptualization of LLM learning
processes in terms of agent learning in language-based games. This framework
unveils innovative perspectives on the successes and challenges in LLM
development, offering a fresh understanding of addressing alignment issues
among other strategic considerations. Furthermore, our two-player game approach
sheds light on novel data preparation and machine learning techniques for
training LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.07319v1' target='_blank'>ODIN: Disentangled Reward Mitigates Hacking in RLHF</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lichang Chen, Chen Zhu, Davit Soselia, Jiuhai Chen, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi, Bryan Catanzaro</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-11 22:40:12</h6>
<p class='card-text'>In this work, we study the issue of reward hacking on the response length, a
challenge emerging in Reinforcement Learning from Human Feedback (RLHF) on
LLMs. A well-formatted, verbose but less helpful response from the LLMs can
often deceive LLMs or even human evaluators to achieve high scores. The same
issue also holds for some reward models in RL. To address the challenges in
both training and evaluation, we establish a more reliable evaluation protocol
for comparing different training configurations, which inspects the trade-off
between LLM evaluation score and response length obtained by varying training
hyperparameters. Based on this evaluation, we conduct large-scale studies,
where the results shed insights into the efficacy of hyperparameters and tricks
used in RL on mitigating length bias. We further propose to improve the reward
model by jointly training two linear heads on shared feature representations to
predict the rewards, one trained to correlate with length, and the other
trained to decorrelate with length and therefore focus more on the actual
content. We then discard the length head in RL to prevent reward hacking on
length. Experiments demonstrate that our approach almost eliminates the reward
correlation with length, and improves the obtained policy by a significant
margin.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.07314v3' target='_blank'>Online Iterative Reinforcement Learning from Human Feedback with General
  Preference Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chenlu Ye, Wei Xiong, Yuheng Zhang, Hanze Dong, Nan Jiang, Tong Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-11 21:44:21</h6>
<p class='card-text'>We investigate Reinforcement Learning from Human Feedback (RLHF) in the
context of a general preference oracle. In particular, we do not assume the
existence of a reward function and an oracle preference signal drawn from the
Bradley-Terry model as most of the prior works do. We consider a standard
mathematical formulation, the reverse-KL regularized minimax game between two
LLMs for RLHF under general preference oracle. The learning objective of this
formulation is to find a policy so that it is consistently preferred by the
KL-regularized preference oracle over any competing LLMs. We show that this
framework is strictly more general than the reward-based one, and propose
sample-efficient algorithms for both the offline learning from a pre-collected
preference dataset and online learning where we can query the preference oracle
along the way of training. Empirical studies verify the effectiveness of the
proposed framework.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.07282v2' target='_blank'>How do Large Language Models Navigate Conflicts between Honesty and
  Helpfulness?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ryan Liu, Theodore R. Sumers, Ishita Dasgupta, Thomas L. Griffiths</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-11 19:13:26</h6>
<p class='card-text'>In day-to-day communication, people often approximate the truth - for
example, rounding the time or omitting details - in order to be maximally
helpful to the listener. How do large language models (LLMs) handle such
nuanced trade-offs? To address this question, we use psychological models and
experiments designed to characterize human behavior to analyze LLMs. We test a
range of LLMs and explore how optimization for human preferences or
inference-time reasoning affects these trade-offs. We find that reinforcement
learning from human feedback improves both honesty and helpfulness, while
chain-of-thought prompting skews LLMs towards helpfulness over honesty.
Finally, GPT-4 Turbo demonstrates human-like response patterns including
sensitivity to the conversational framing and listener's decision context. Our
findings reveal the conversational values internalized by LLMs and suggest that
even these abstract values can, to a degree, be steered by zero-shot prompting.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.07157v2' target='_blank'>Natural Language Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xidong Feng, Ziyu Wan, Mengyue Yang, Ziyan Wang, Girish A. Koushik, Yali Du, Ying Wen, Jun Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-11 11:03:04</h6>
<p class='card-text'>Reinforcement Learning (RL) has shown remarkable abilities in learning
policies for decision-making tasks. However, RL is often hindered by issues
such as low sample efficiency, lack of interpretability, and sparse supervision
signals. To tackle these limitations, we take inspiration from the human
learning process and introduce Natural Language Reinforcement Learning (NLRL),
which innovatively combines RL principles with natural language representation.
Specifically, NLRL redefines RL concepts like task objectives, policy, value
function, Bellman equation, and policy iteration in natural language space. We
present how NLRL can be practically implemented with the latest advancements in
large language models (LLMs) like GPT-4. Initial experiments over tabular MDPs
demonstrate the effectiveness, efficiency, and also interpretability of the
NLRL framework.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.07069v1' target='_blank'>Using Large Language Models to Automate and Expedite Reinforcement
  Learning with Reward Machine</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shayan Meshkat Alsadat, Jean-Raphael Gaglione, Daniel Neider, Ufuk Topcu, Zhe Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-11 00:00:05</h6>
<p class='card-text'>We present LARL-RM (Large language model-generated Automaton for
Reinforcement Learning with Reward Machine) algorithm in order to encode
high-level knowledge into reinforcement learning using automaton to expedite
the reinforcement learning. Our method uses Large Language Models (LLM) to
obtain high-level domain-specific knowledge using prompt engineering instead of
providing the reinforcement learning algorithm directly with the high-level
knowledge which requires an expert to encode the automaton. We use
chain-of-thought and few-shot methods for prompt engineering and demonstrate
that our method works using these approaches. Additionally, LARL-RM allows for
fully closed-loop reinforcement learning without the need for an expert to
guide and supervise the learning since LARL-RM can use the LLM directly to
generate the required high-level knowledge for the task at hand. We also show
the theoretical guarantee of our algorithm to converge to an optimal policy. We
demonstrate that LARL-RM speeds up the convergence by 30% by implementing our
method in two case studies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.06700v4' target='_blank'>Entropy-Regularized Token-Level Policy Optimization for Language Agent
  Reinforcement</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Muning Wen, Junwei Liao, Cheng Deng, Jun Wang, Weinan Zhang, Ying Wen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-09 07:45:26</h6>
<p class='card-text'>Large Language Models (LLMs) have shown promise as intelligent agents in
interactive decision-making tasks. Traditional approaches often depend on
meticulously designed prompts, high-quality examples, or additional reward
models for in-context learning, supervised fine-tuning, or RLHF. Reinforcement
learning (RL) presents a dynamic alternative for LLMs to overcome these
dependencies by engaging directly with task-specific environments. Nonetheless,
it faces significant hurdles: 1) instability stemming from the exponentially
vast action space requiring exploration; 2) challenges in assigning token-level
credit based on action-level reward signals, resulting in discord between
maximizing rewards and accurately modeling corpus data. In response to these
challenges, we introduce Entropy-Regularized Token-level Policy Optimization
(ETPO), an entropy-augmented RL method tailored for optimizing LLMs at the
token level. At the heart of ETPO is our novel per-token soft Bellman update,
designed to harmonize the RL process with the principles of language modeling.
This methodology decomposes the Q-function update from a coarse action-level
view to a more granular token-level perspective, backed by theoretical proof of
optimization consistency. Crucially, this decomposition renders linear time
complexity in action exploration. We assess the effectiveness of ETPO within a
simulated environment that models data science code generation as a series of
multi-step interactive tasks; results underline ETPO's potential as a robust
method for refining the interactive decision-making capabilities of language
agents. For a more detailed preliminary work describing our motivation for
token-level decomposition and applying it in PPO methods, please refer to
arXiv:2405.15821.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.05808v2' target='_blank'>Training Large Language Models for Reasoning through Reverse Curriculum
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, Honglin Guo, Wei Shen, Xiaoran Fan, Yuhao Zhou, Shihan Dou, Xiao Wang, Xinbo Zhang, Peng Sun, Tao Gui, Qi Zhang, Xuanjing Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-08 16:46:26</h6>
<p class='card-text'>In this paper, we propose R$^3$: Learning Reasoning through Reverse
Curriculum Reinforcement Learning (RL), a novel method that employs only
outcome supervision to achieve the benefits of process supervision for large
language models. The core challenge in applying RL to complex reasoning is to
identify a sequence of actions that result in positive rewards and provide
appropriate supervision for optimization. Outcome supervision provides sparse
rewards for final results without identifying error locations, whereas process
supervision offers step-wise rewards but requires extensive manual annotation.
R$^3$ overcomes these limitations by learning from correct demonstrations.
Specifically, R$^3$ progressively slides the start state of reasoning from a
demonstration's end to its beginning, facilitating easier model exploration at
all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome
supervision to offer step-level signals and precisely pinpoint errors. Using
Llama2-7B, our method surpasses RL baseline on eight reasoning tasks by $4.1$
points on average. Notebaly, in program-based reasoning on GSM8K, it exceeds
the baseline by $4.2$ points across three backbone models, and without any
extra data, Codellama-7B + R$^3$ performs comparable to larger models or
closed-source models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.04867v2' target='_blank'>Multimodal Query Suggestion with Multi-Agent Reinforcement Learning from
  Human Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zheng Wang, Bingzheng Gan, Wei Shi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-07 14:07:47</h6>
<p class='card-text'>In the rapidly evolving landscape of information retrieval, search engines
strive to provide more personalized and relevant results to users. Query
suggestion systems play a crucial role in achieving this goal by assisting
users in formulating effective queries. However, existing query suggestion
systems mainly rely on textual inputs, potentially limiting user search
experiences for querying images. In this paper, we introduce a novel Multimodal
Query Suggestion (MMQS) task, which aims to generate query suggestions based on
user query images to improve the intentionality and diversity of search
results. We present the RL4Sugg framework, leveraging the power of Large
Language Models (LLMs) with Multi-Agent Reinforcement Learning from Human
Feedback to optimize the generation process. Through comprehensive experiments,
we validate the effectiveness of RL4Sugg, demonstrating a 18% improvement
compared to the best existing approach. Moreover, the MMQS has been transferred
into real-world search engine products, which yield enhanced user engagement.
Our research advances query suggestion systems and provides a new perspective
on multimodal information retrieval.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.04792v2' target='_blank'>Direct Language Model Alignment from Online AI Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan Ferret, Mathieu Blondel</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-07 12:31:13</h6>
<p class='card-text'>Direct alignment from preferences (DAP) methods, such as DPO, have recently
emerged as efficient alternatives to reinforcement learning from human feedback
(RLHF), that do not require a separate reward model. However, the preference
datasets used in DAP methods are usually collected ahead of training and never
updated, thus the feedback is purely offline. Moreover, responses in these
datasets are often sampled from a language model distinct from the one being
aligned, and since the model evolves over training, the alignment phase is
inevitably off-policy. In this study, we posit that online feedback is key and
improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as
annotator: on each training iteration, we sample two responses from the current
model and prompt the LLM annotator to choose which one is preferred, thus
providing online feedback. Despite its simplicity, we demonstrate via human
evaluation in several tasks that OAIF outperforms both offline DAP and RLHF
methods. We further show that the feedback leveraged in OAIF is easily
controllable, via instruction prompts to the LLM annotator.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.03969v1' target='_blank'>In-context learning agents are asymmetric belief updaters</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Johannes A. Schubert, Akshay K. Jagadish, Marcel Binz, Eric Schulz</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-06 12:58:38</h6>
<p class='card-text'>We study the in-context learning dynamics of large language models (LLMs)
using three instrumental learning tasks adapted from cognitive psychology. We
find that LLMs update their beliefs in an asymmetric manner and learn more from
better-than-expected outcomes than from worse-than-expected ones. Furthermore,
we show that this effect reverses when learning about counterfactual feedback
and disappears when no agency is implied. We corroborate these findings by
investigating idealized in-context learning agents derived through
meta-reinforcement learning, where we observe similar patterns. Taken together,
our results contribute to our understanding of how in-context learning works by
highlighting that the framing of a problem significantly influences how
learning occurs, a phenomenon also observed in human cognition.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.03746v3' target='_blank'>Tuning Large Multimodal Models for Videos using Reinforcement Learning
  from AI Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Daechul Ahn, Yura Choi, Youngjae Yu, Dongyeop Kang, Jonghyun Choi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-06 06:27:40</h6>
<p class='card-text'>Recent advancements in large language models have influenced the development
of video large multimodal models (VLMMs). The previous approaches for VLMMs
involved Supervised Fine-Tuning (SFT) with instruction-tuned datasets,
integrating LLM with visual encoders, and adding additional learnable modules.
Video and text multimodal alignment remains challenging, primarily due to the
deficient volume and quality of multimodal instruction-tune data compared to
text-only data. We present a novel alignment strategy that employs multimodal
AI system to oversee itself called Reinforcement Learning from AI Feedback
(RLAIF), providing self-preference feedback to refine itself and facilitating
the alignment of video and text modalities. In specific, we propose
context-aware reward modeling by providing detailed video descriptions as
context during the generation of preference feedback in order to enrich the
understanding of video content. Demonstrating enhanced performance across
diverse video benchmarks, our multimodal RLAIF approach, VLM-RLAIF, outperforms
existing approaches, including the SFT model. We commit to open-sourcing our
code, models, and datasets to foster further research in this area.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.05133v3' target='_blank'>Personalized Language Modeling from Personalized Human Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xinyu Li, Ruiyang Zhou, Zachary C. Lipton, Liu Leqi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-06 04:18:58</h6>
<p class='card-text'>Personalized large language models (LLMs) are designed to tailor responses to
individual user preferences. While Reinforcement Learning from Human Feedback
(RLHF) is a commonly used framework for aligning LLMs with human preferences,
vanilla RLHF assumes that all human preferences share the same distribution,
preventing fine-tuned LLMs from generating personalized content when user
preferences are diverse. In this work, we propose Personalized-RLHF (P-RLHF),
an efficient framework that utilizes a lightweight user model to capture
individual user preferences and jointly learns the user model and the
personalized LLM from human feedback. P-RLHF exhibits the following three
characteristics: (1) It enables an LLM to generate personalized content and
scale efficiently with growing number of users. (2) It handles both explicit
user preferences described as textual input and implicit user preferences
encoded in the feedback data. (3) It eliminates the need for users to fully
articulate their preferences, which are normally needed for prompting LLMs to
generate personalized content yet are often impractical to obtain in real-world
scenarios. Our experimental results show that personalized LLMs trained using
P-RLHF generate responses that are more closely aligned with individual user
preferences, outperforming vanilla, non-personalized RLHF and prompting-based
personalization approaches across different tasks. We opensource our code at
https://github.com/HumainLab/Personalized_RLHF.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.06147v2' target='_blank'>DeAL: Decoding-time Alignment for Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:James Y. Huang, Sailik Sengupta, Daniele Bonadiman, Yi-an Lai, Arshit Gupta, Nikolaos Pappas, Saab Mansour, Katrin Kirchhoff, Dan Roth</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-05 06:12:29</h6>
<p class='card-text'>Large Language Models (LLMs) are nowadays expected to generate content
aligned with human preferences. Current work focuses on alignment at model
training time, through techniques such as Reinforcement Learning with Human
Feedback (RLHF). However, it is unclear if such methods are an effective choice
to teach alignment objectives to the model. First, the inability to incorporate
multiple, custom rewards and reliance on a model developer's view of universal
and static principles are key limitations. Second, the residual gaps in model
training and the reliability of such approaches are also questionable (e.g.
susceptibility to jail-breaking even after safety training). To address these,
we propose DeAL, a framework that allows the user to customize reward functions
and enables Decoding-time Alignment of LLMs (DeAL). At its core, we view
decoding as a heuristic-guided search process and facilitate the use of a wide
variety of alignment objectives. Our experiments with programmatic constraints
such as keyword and length constraints (studied widely in the pre-LLM era) and
abstract objectives such as harmlessness and helpfulness (proposed in the
post-LLM era) show that we can DeAL with fine-grained trade-offs, improve
adherence to alignment objectives, and address residual gaps in LLMs. Lastly,
while DeAL can be effectively paired with RLHF and prompting techniques, its
generality makes decoding slower, an optimization we leave for future work.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.02330v2' target='_blank'>Enhance Reasoning for Large Language Models in the Game Werewolf</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shuang Wu, Liwen Zhu, Tao Yang, Shiwei Xu, Qiang Fu, Yang Wei, Haobo Fu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-04 03:47:10</h6>
<p class='card-text'>This paper presents an innovative framework that integrates Large Language
Models (LLMs) with an external Thinker module to enhance the reasoning
capabilities of LLM-based agents. Unlike augmenting LLMs with prompt
engineering, Thinker directly harnesses knowledge from databases and employs
various optimization techniques. The framework forms a reasoning hierarchy
where LLMs handle intuitive System-1 tasks such as natural language processing,
while the Thinker focuses on cognitive System-2 tasks that require complex
logical analysis and domain-specific knowledge. Our framework is presented
using a 9-player Werewolf game that demands dual-system reasoning. We introduce
a communication protocol between LLMs and the Thinker, and train the Thinker
using data from 18800 human sessions and reinforcement learning. Experiments
demonstrate the framework's effectiveness in deductive reasoning, speech
generation, and online game evaluation. Additionally, we fine-tune a 6B LLM to
surpass GPT4 when integrated with the Thinker. This paper also contributes the
largest dataset for social deduction games to date.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.01874v1' target='_blank'>The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement
  Learning and Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Moschoula Pternea, Prerna Singh, Abir Chakraborty, Yagna Oruganti, Mirco Milletari, Sayli Bapat, Kebei Jiang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-02 20:01:15</h6>
<p class='card-text'>In this work, we review research studies that combine Reinforcement Learning
(RL) and Large Language Models (LLMs), two areas that owe their momentum to the
development of deep neural networks. We propose a novel taxonomy of three main
classes based on the way that the two model types interact with each other. The
first class, RL4LLM, includes studies where RL is leveraged to improve the
performance of LLMs on tasks related to Natural Language Processing. L4LLM is
divided into two sub-categories depending on whether RL is used to directly
fine-tune an existing LLM or to improve the prompt of the LLM. In the second
class, LLM4RL, an LLM assists the training of an RL model that performs a task
that is not inherently related to natural language. We further break down
LLM4RL based on the component of the RL training framework that the LLM assists
or replaces, namely reward shaping, goal generation, and policy function.
Finally, in the third class, RL+LLM, an LLM and an RL agent are embedded in a
common planning framework without either of them contributing to training or
fine-tuning of the other. We further branch this class to distinguish between
studies with and without natural language feedback. We use this taxonomy to
explore the motivations behind the synergy of LLMs and RL and explain the
reasons for its success, while pinpointing potential shortcomings and areas
where further research is needed, as well as alternative methodologies that
serve the same goal.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.01812v1' target='_blank'>Distilling LLMs' Decomposition Abilities into Compact Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Denis Tarasov, Kumar Shridhar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-02 13:23:15</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated proficiency in their reasoning
abilities, yet their large size presents scalability challenges and limits any
further customization. In contrast, compact models offer customized training
but often fall short in solving complex reasoning tasks. This study focuses on
distilling the LLMs' decomposition skills into compact models using offline
reinforcement learning. We leverage the advancements in the LLM`s capabilities
to provide feedback and generate a specialized task-specific dataset for
training compact models. The development of an AI-generated dataset and the
establishment of baselines constitute the primary contributions of our work,
underscoring the potential of compact models in replicating complex
problem-solving skills.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.01391v2' target='_blank'>StepCoder: Improve Code Generation with Reinforcement Learning from
  Compiler Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shihan Dou, Yan Liu, Haoxiang Jia, Limao Xiong, Enyu Zhou, Wei Shen, Junjie Shan, Caishuang Huang, Xiao Wang, Xiaoran Fan, Zhiheng Xi, Yuhao Zhou, Tao Ji, Rui Zheng, Qi Zhang, Xuanjing Huang, Tao Gui</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-02 13:14:31</h6>
<p class='card-text'>The advancement of large language models (LLMs) has significantly propelled
the field of code generation. Previous work integrated reinforcement learning
(RL) with compiler feedback for exploring the output space of LLMs to enhance
code generation quality. However, the lengthy code generated by LLMs in
response to complex human requirements makes RL exploration a challenge. Also,
since the unit tests may not cover the complicated code, optimizing LLMs by
using these unexecuted code snippets is ineffective. To tackle these
challenges, we introduce StepCoder, a novel RL framework for code generation,
consisting of two main components: CCCS addresses the exploration challenge by
breaking the long sequences code generation task into a Curriculum of Code
Completion Subtasks, while FGO only optimizes the model by masking the
unexecuted code segments to provide Fine-Grained Optimization. In addition, we
furthermore construct the APPS+ dataset for RL training, which is manually
verified to ensure the correctness of unit tests. Experimental results show
that our method improves the ability to explore the output space and
outperforms state-of-the-art approaches in corresponding benchmarks. Our
dataset APPS+ and StepCoder are available online.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.03469v3' target='_blank'>Rethinking the Role of Proxy Rewards in Language Model Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sungdong Kim, Minjoon Seo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-02 11:58:08</h6>
<p class='card-text'>Learning from human feedback via proxy reward modeling has been studied to
align Large Language Models (LLMs) with human values. However, achieving
reliable training through that proxy reward model (RM) is not a trivial
problem, and its behavior remained as a black-box. In this paper, we study the
role of proxy rewards in the LLM alignment via `reverse reward engineering' by
composing interpretable features as a white-box reward function. We aim to
replicate the ground truth (gold) reward signal by achieving a monotonic
relationship between the proxy and gold reward signals after training the model
using the proxy reward in reinforcement learning (RL). Our findings indicate
that successfully emulating the gold reward requires generating responses that
are relevant with enough length to open-ended questions, while also ensuring
response consistency in closed-ended questions. Furthermore, resulting models
optimizing our devised white-box reward show competitive performances with
strong open-source RMs in alignment benchmarks. We highlight its potential
usage as a simple but strong reward baseline for the LLM alignment, not
requiring explicit human feedback dataset and RM training. Our code is
available at https://github.com/naver-ai/rethinking-proxy-reward.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.01118v3' target='_blank'>PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sihao Hu, Tiansheng Huang, Ling Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-02 03:22:12</h6>
<p class='card-text'>We introduce PokeLLMon, the first LLM-embodied agent that achieves
human-parity performance in tactical battle games, as demonstrated in Pokemon
battles. The design of PokeLLMon incorporates three key strategies: (i)
In-context reinforcement learning that instantly consumes text-based feedback
derived from battles to iteratively refine the policy; (ii) Knowledge-augmented
generation that retrieves external knowledge to counteract hallucination and
enables the agent to act timely and properly; (iii) Consistent action
generation to mitigate the panic switching phenomenon when the agent faces a
powerful opponent and wants to elude the battle. We show that online battles
against human demonstrates PokeLLMon's human-like battle strategies and
just-in-time decision making, achieving 49% of win rate in the Ladder
competitions and 56% of win rate in the invited battles. Our implementation and
playable battle logs are available at: https://github.com/git-disl/PokeLLMon.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.01786v2' target='_blank'>COA-GPT: Generative Pre-trained Transformers for Accelerated Course of
  Action Development in Military Operations</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Vinicius G. Goecks, Nicholas Waytowich</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-01 21:51:09</h6>
<p class='card-text'>The development of Courses of Action (COAs) in military operations is
traditionally a time-consuming and intricate process. Addressing this
challenge, this study introduces COA-GPT, a novel algorithm employing Large
Language Models (LLMs) for rapid and efficient generation of valid COAs.
COA-GPT incorporates military doctrine and domain expertise to LLMs through
in-context learning, allowing commanders to input mission information - in both
text and image formats - and receive strategically aligned COAs for review and
approval. Uniquely, COA-GPT not only accelerates COA development, producing
initial COAs within seconds, but also facilitates real-time refinement based on
commander feedback. This work evaluates COA-GPT in a military-relevant scenario
within a militarized version of the StarCraft II game, comparing its
performance against state-of-the-art reinforcement learning algorithms. Our
results demonstrate COA-GPT's superiority in generating strategically sound
COAs more swiftly, with added benefits of enhanced adaptability and alignment
with commander intentions. COA-GPT's capability to rapidly adapt and update
COAs during missions presents a transformative potential for military planning,
particularly in addressing planning discrepancies and capitalizing on emergent
windows of opportunities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.00782v1' target='_blank'>Dense Reward for Free in Reinforcement Learning from Human Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alex J. Chan, Hao Sun, Samuel Holt, Mihaela van der Schaar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-01 17:10:35</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) has been credited as the
key advance that has allowed Large Language Models (LLMs) to effectively follow
instructions and produce useful assistance. Classically, this involves
generating completions from the LLM in response to a query before using a
separate reward model to assign a score to the full completion. As an
auto-regressive process, the LLM has to take many "actions" (selecting
individual tokens) and only receives a single, sparse reward at the end of an
episode, a setup that is known to be difficult to optimise in traditional
reinforcement learning. In this work we leverage the fact that the reward model
contains more information than just its scalar output, in particular, it
calculates an attention map over tokens as part of the transformer
architecture. We use these attention weights to redistribute the reward along
the whole completion, effectively densifying the signal and highlighting the
most important tokens, all without incurring extra computational cost or
requiring any additional modelling. We demonstrate that, theoretically, this
approach is equivalent to potential-based reward shaping, ensuring that the
optimal policy remains unchanged. Empirically, we show that it stabilises
training, accelerates the rate of learning, and, in practical cases, may lead
to better local optima.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.00402v1' target='_blank'>Investigating Bias Representations in Llama 2 Chat via Activation
  Steering</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dawn Lu, Nina Rimsky</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-01 07:48:50</h6>
<p class='card-text'>We address the challenge of societal bias in Large Language Models (LLMs),
focusing on the Llama 2 7B Chat model. As LLMs are increasingly integrated into
decision-making processes with substantial societal impact, it becomes
imperative to ensure these models do not reinforce existing biases. Our
approach employs activation steering to probe for and mitigate biases related
to gender, race, and religion. This method manipulates model activations to
direct responses towards or away from biased outputs, utilizing steering
vectors derived from the StereoSet dataset and custom GPT4 generated gender
bias prompts. Our findings reveal inherent gender bias in Llama 2 7B Chat,
persisting even after Reinforcement Learning from Human Feedback (RLHF). We
also observe a predictable negative correlation between bias and the model's
tendency to refuse responses. Significantly, our study uncovers that RLHF tends
to increase the similarity in the model's representation of different forms of
societal biases, which raises questions about the model's nuanced understanding
of different forms of bias. This work also provides valuable insights into
effective red-teaming strategies for LLMs using activation steering,
particularly emphasizing the importance of integrating a refusal vector.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.17749v1' target='_blank'>SwarmBrain: Embodied agent for real-time strategy game StarCraft II via
  large language models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiao Shao, Weifu Jiang, Fei Zuo, Mengqing Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-31 11:14:29</h6>
<p class='card-text'>Large language models (LLMs) have recently garnered significant
accomplishments in various exploratory tasks, even surpassing the performance
of traditional reinforcement learning-based methods that have historically
dominated the agent-based field. The purpose of this paper is to investigate
the efficacy of LLMs in executing real-time strategy war tasks within the
StarCraft II gaming environment. In this paper, we introduce SwarmBrain, an
embodied agent leveraging LLM for real-time strategy implementation in the
StarCraft II game environment. The SwarmBrain comprises two key components: 1)
a Overmind Intelligence Matrix, powered by state-of-the-art LLMs, is designed
to orchestrate macro-level strategies from a high-level perspective. This
matrix emulates the overarching consciousness of the Zerg intelligence brain,
synthesizing strategic foresight with the aim of allocating resources,
directing expansion, and coordinating multi-pronged assaults. 2) a Swarm
ReflexNet, which is agile counterpart to the calculated deliberation of the
Overmind Intelligence Matrix. Due to the inherent latency in LLM reasoning, the
Swarm ReflexNet employs a condition-response state machine framework, enabling
expedited tactical responses for fundamental Zerg unit maneuvers. In the
experimental setup, SwarmBrain is in control of the Zerg race in confrontation
with an Computer-controlled Terran adversary. Experimental results show the
capacity of SwarmBrain to conduct economic augmentation, territorial expansion,
and tactical formulation, and it shows the SwarmBrain is capable of achieving
victory against Computer players set at different difficulty levels.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.15449v1' target='_blank'>Learning to Trust Your Feelings: Leveraging Self-awareness in LLMs for
  Hallucination Mitigation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuxin Liang, Zhuoyang Song, Hao Wang, Jiaxing Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-27 16:19:30</h6>
<p class='card-text'>We evaluate the ability of Large Language Models (LLMs) to discern and
express their internal knowledge state, a key factor in countering factual
hallucination and ensuring reliable application of LLMs. We observe a robust
self-awareness of internal knowledge state in LLMs, evidenced by over 85%
accuracy in knowledge probing. However, LLMs often fail to express their
internal knowledge during generation, leading to factual hallucinations. We
develop an automated hallucination annotation tool, Dreamcatcher, which merges
knowledge probing and consistency checking methods to rank factual preference
data. Using knowledge preference as reward, We propose a Reinforcement Learning
from Knowledge Feedback (RLKF) training framework, leveraging reinforcement
learning to enhance the factuality and honesty of LLMs. Our experiments across
multiple models show that RLKF training effectively enhances the ability of
models to utilize their internal knowledge state, boosting performance in a
variety of knowledge-based and honesty-related tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.15043v3' target='_blank'>Health Text Simplification: An Annotated Corpus for Digestive Cancer
  Education and Novel Strategies for Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Md Mushfiqur Rahman, Mohammad Sabik Irbaz, Kai North, Michelle S. Williams, Marcos Zampieri, Kevin Lybarger</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-26 18:13:57</h6>
<p class='card-text'>Objective: The reading level of health educational materials significantly
influences the understandability and accessibility of the information,
particularly for minoritized populations. Many patient educational resources
surpass the reading level and complexity of widely accepted standards. There is
a critical need for high-performing text simplification models in health
information to enhance dissemination and literacy. This need is particularly
acute in cancer education, where effective prevention and screening education
can substantially reduce morbidity and mortality.
  Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel
corpus of cancer education materials tailored for health text simplification
research, comprising educational content from the American Cancer Society,
Centers for Disease Control and Prevention, and National Cancer Institute.
Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore Large
Language Model (LLM)-based simplification methods, including fine-tuning,
reinforcement learning (RL), reinforcement learning with human feedback (RLHF),
domain adaptation, and prompt-based approaches. Our experimentation encompasses
Llama 2 and GPT-4. A novel RLHF reward function is introduced, featuring a
lightweight model adept at distinguishing between original and simplified
texts, thereby enhancing the model's effectiveness with unlabeled data.
  Results: Fine-tuned Llama 2 models demonstrated high performance across
various metrics. Our innovative RLHF reward function surpassed existing RL text
simplification reward functions in effectiveness. The results underscore that
RL/RLHF can augment fine-tuning, facilitating model training on unlabeled text
and improving performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.14151v2' target='_blank'>True Knowledge Comes from Practice: Aligning LLMs with Embodied
  Environments via Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weihao Tan, Wentao Zhang, Shanqi Liu, Longtao Zheng, Xinrun Wang, Bo An</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-25 13:03:20</h6>
<p class='card-text'>Despite the impressive performance across numerous tasks, large language
models (LLMs) often fail in solving simple decision-making tasks due to the
misalignment of the knowledge in LLMs with environments. On the contrary,
reinforcement learning (RL) agents learn policies from scratch, which makes
them always align with environments but difficult to incorporate prior
knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a
novel general online framework that deploys LLMs as decision-making agents to
efficiently interact and align with embodied environments via RL without
requiring any prepared datasets or prior knowledge of the environments.
Firstly, we query the joint probabilities of each valid action with LLMs to
form behavior policies. Then, to enhance the stability and robustness of the
policies, we propose two normalization methods and summarize four prompt design
principles. Finally, we design a novel parameter-efficient training
architecture where the actor and critic share one frozen LLM equipped with
low-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to
evaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency
and performance compared to the conventional RL method, PPO, and prompt tuning
method, SayCan, in both classical decision-making environment, Overcooked, and
simulated household environment, VirtualHome. ii) Benefiting from LLMs'
open-vocabulary feature, TWOSOME shows superior generalization ability to
unseen tasks. iii) Under our framework, there is no significant loss of the
LLMs' original ability during online PPO finetuning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.15098v2' target='_blank'>Hierarchical Continual Reinforcement Learning via Large Language Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chaofan Pan, Xin Yang, Hao Wang, Wei Wei, Tianrui Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-25 03:06:51</h6>
<p class='card-text'>The ability to learn continuously in dynamic environments is a crucial
requirement for reinforcement learning (RL) agents applying in the real world.
Despite the progress in continual reinforcement learning (CRL), existing
methods often suffer from insufficient knowledge transfer, particularly when
the tasks are diverse. To address this challenge, we propose a new framework,
Hierarchical Continual reinforcement learning via large language model
(Hi-Core), designed to facilitate the transfer of high-level knowledge. Hi-Core
orchestrates a twolayer structure: high-level policy formulation by a large
language model (LLM), which represents agenerates a sequence of goals, and
low-level policy learning that closely aligns with goal-oriented RL practices,
producing the agent's actions in response to the goals set forth. The framework
employs feedback to iteratively adjust and verify highlevel policies, storing
them along with low-level policies within a skill library. When encountering a
new task, Hi-Core retrieves relevant experience from this library to help to
learning. Through experiments on Minigrid, Hi-Core has demonstrated its
effectiveness in handling diverse CRL tasks, which outperforms popular
baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.01698v1' target='_blank'>Large language model empowered participatory urban planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhilun Zhou, Yuming Lin, Yong Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-24 10:50:01</h6>
<p class='card-text'>Participatory urban planning is the mainstream of modern urban planning and
involves the active engagement of different stakeholders. However, the
traditional participatory paradigm encounters challenges in time and manpower,
while the generative planning tools fail to provide adjustable and inclusive
solutions. This research introduces an innovative urban planning approach
integrating Large Language Models (LLMs) within the participatory process. The
framework, based on the crafted LLM agent, consists of role-play, collaborative
generation, and feedback iteration, solving a community-level land-use task
catering to 1000 distinct interests. Empirical experiments in diverse urban
communities exhibit LLM's adaptability and effectiveness across varied planning
scenarios. The results were evaluated on four metrics, surpassing human experts
in satisfaction and inclusion, and rivaling state-of-the-art reinforcement
learning methods in service and ecology. Further analysis shows the advantage
of LLM agents in providing adjustable and inclusive solutions with natural
language reasoning and strong scalability. While implementing the recent
advancements in emulating human behavior for planning, this work envisions both
planners and citizens benefiting from low-cost, efficient LLM agents, which is
crucial for enhancing participation and realizing participatory urban planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.13136v1' target='_blank'>The Language Barrier: Dissecting Safety Challenges of LLMs in
  Multilingual Contexts</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lingfeng Shen, Weiting Tan, Sihao Chen, Yunmo Chen, Jingyu Zhang, Haoran Xu, Boyuan Zheng, Philipp Koehn, Daniel Khashabi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-23 23:12:09</h6>
<p class='card-text'>As the influence of large language models (LLMs) spans across global
communities, their safety challenges in multilingual settings become paramount
for alignment research. This paper examines the variations in safety challenges
faced by LLMs across different languages and discusses approaches to
alleviating such concerns. By comparing how state-of-the-art LLMs respond to
the same set of malicious prompts written in higher- vs. lower-resource
languages, we observe that (1) LLMs tend to generate unsafe responses much more
often when a malicious prompt is written in a lower-resource language, and (2)
LLMs tend to generate more irrelevant responses to malicious prompts in
lower-resource languages. To understand where the discrepancy can be
attributed, we study the effect of instruction tuning with reinforcement
learning from human feedback (RLHF) or supervised finetuning (SFT) on the
HH-RLHF dataset. Surprisingly, while training with high-resource languages
improves model alignment, training in lower-resource languages yields minimal
improvement. This suggests that the bottleneck of cross-lingual alignment is
rooted in the pretraining stage. Our findings highlight the challenges in
cross-lingual LLM safety, and we hope they inform future research in this
direction.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.12975v1' target='_blank'>HAZARD Challenge: Embodied Decision Making in Dynamically Changing
  Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qinhong Zhou, Sunli Chen, Yisong Wang, Haozhe Xu, Weihua Du, Hongxin Zhang, Yilun Du, Joshua B. Tenenbaum, Chuang Gan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-23 18:59:43</h6>
<p class='card-text'>Recent advances in high-fidelity virtual environments serve as one of the
major driving forces for building intelligent embodied agents to perceive,
reason and interact with the physical world. Typically, these environments
remain unchanged unless agents interact with them. However, in real-world
scenarios, agents might also face dynamically changing environments
characterized by unexpected events and need to rapidly take action accordingly.
To remedy this gap, we propose a new simulated embodied benchmark, called
HAZARD, specifically designed to assess the decision-making abilities of
embodied agents in dynamic situations. HAZARD consists of three unexpected
disaster scenarios, including fire, flood, and wind, and specifically supports
the utilization of large language models (LLMs) to assist common sense
reasoning and decision-making. This benchmark enables us to evaluate autonomous
agents' decision-making capabilities across various pipelines, including
reinforcement learning (RL), rule-based, and search-based methods in
dynamically changing environments. As a first step toward addressing this
challenge using large language models, we further develop an LLM-based agent
and perform an in-depth analysis of its promise and challenge of solving these
challenging tasks. HAZARD is available at https://vis-www.cs.umass.edu/hazard/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.12624v2' target='_blank'>Knowledge Distillation from Language-Oriented to Emergent Communication
  for Multi-Agent Remote Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yongjun Kim, Sejin Seo, Jihong Park, Mehdi Bennis, Seong-Lyun Kim, Junil Choi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-23 10:23:13</h6>
<p class='card-text'>In this work, we compare emergent communication (EC) built upon multi-agent
deep reinforcement learning (MADRL) and language-oriented semantic
communication (LSC) empowered by a pre-trained large language model (LLM) using
human language. In a multi-agent remote navigation task, with multimodal input
data comprising location and channel maps, it is shown that EC incurs high
training cost and struggles when using multimodal data, whereas LSC yields high
inference computing cost due to the LLM's large size. To address their
respective bottlenecks, we propose a novel framework of language-guided EC
(LEC) by guiding the EC training using LSC via knowledge distillation (KD).
Simulations corroborate that LEC achieves faster travel time while avoiding
areas with poor channel conditions, as well as speeding up the MADRL training
convergence by up to 61.8% compared to EC.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.12459v2' target='_blank'>Towards Socially and Morally Aware RL agent: Reward Design With LLM</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhaoyue Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-23 03:00:03</h6>
<p class='card-text'>When we design and deploy an Reinforcement Learning (RL) agent, reward
functions motivates agents to achieve an objective. An incorrect or incomplete
specification of the objective can result in behavior that does not align with
human values - failing to adhere with social and moral norms that are ambiguous
and context dependent, and cause undesired outcomes such as negative side
effects and exploration that is unsafe. Previous work have manually defined
reward functions to avoid negative side effects, use human oversight for safe
exploration, or use foundation models as planning tools. This work studies the
ability of leveraging Large Language Models (LLM)' understanding of morality
and social norms on safe exploration augmented RL methods. This work evaluates
language model's result against human feedbacks and demonstrates language
model's capability as direct reward signals.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.12187v1' target='_blank'>WARM: On the Benefits of Weight Averaged Reward Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alexandre Ramé, Nino Vieillard, Léonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, Johan Ferret</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-22 18:27:08</h6>
<p class='card-text'>Aligning large language models (LLMs) with human preferences through
reinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit
failures in the reward model (RM) to achieve seemingly high rewards without
meeting the underlying objectives. We identify two primary challenges when
designing RMs to mitigate reward hacking: distribution shifts during the RL
process and inconsistencies in human preferences. As a solution, we propose
Weight Averaged Reward Models (WARM), first fine-tuning multiple RMs, then
averaging them in the weight space. This strategy follows the observation that
fine-tuned weights remain linearly mode connected when sharing the same
pre-training. By averaging weights, WARM improves efficiency compared to the
traditional ensembling of predictions, while improving reliability under
distribution shifts and robustness to preference inconsistencies. Our
experiments on summarization tasks, using best-of-N and RL methods, shows that
WARM improves the overall quality and alignment of LLM predictions; for
example, a policy RL fine-tuned with WARM has a 79.4% win rate against a policy
RL fine-tuned with a single RM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.00044v1' target='_blank'>Training microrobots to swim by a large language model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhuoqun Xu, Lailai Zhu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-21 12:18:59</h6>
<p class='card-text'>Machine learning and artificial intelligence have recently represented a
popular paradigm for designing and optimizing robotic systems across various
scales. Recent studies have showcased the innovative application of large
language models (LLMs) in industrial control [1] and in directing legged
walking robots [2]. In this study, we utilize an LLM, GPT-4, to train two
prototypical microrobots for swimming in viscous fluids. Adopting a few-shot
learning approach, we develop a minimal, unified prompt composed of only five
sentences. The same concise prompt successfully guides two distinct articulated
microrobots -- the three-link swimmer and the three-sphere swimmer -- in
mastering their signature strokes. These strokes, initially conceptualized by
physicists, are now effectively interpreted and applied by the LLM, enabling
the microrobots to circumvent the physical constraints inherent to
micro-locomotion. Remarkably, our LLM-based decision-making strategy
substantially surpasses a traditional reinforcement learning method in terms of
training speed. We discuss the nuanced aspects of prompt design, particularly
emphasizing the reduction of monetary expenses of using GPT-4.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.11458v3' target='_blank'>Linear Alignment: A Closed-form Solution for Aligning Human Preferences
  without Tuning and Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Songyang Gao, Qiming Ge, Wei Shen, Shihan Dou, Junjie Ye, Xiao Wang, Rui Zheng, Yicheng Zou, Zhi Chen, Hang Yan, Qi Zhang, Dahua Lin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-21 10:46:23</h6>
<p class='card-text'>The success of AI assistants based on Language Models (LLMs) hinges on
Reinforcement Learning from Human Feedback (RLHF) to comprehend and align with
user intentions. However, traditional alignment algorithms, such as PPO, are
hampered by complex annotation and training requirements. This reliance limits
the applicability of RLHF and hinders the development of professional
assistants tailored to diverse human preferences. In this work, we introduce
\textit{Linear Alignment}, a novel algorithm that aligns language models with
human preferences in one single inference step, eliminating the reliance on
data annotation and model training. Linear alignment incorporates a new
parameterization for policy optimization under divergence constraints, which
enables the extraction of optimal policy in a closed-form manner and
facilitates the direct estimation of the aligned response. Extensive
experiments on both general and personalized preference datasets demonstrate
that linear alignment significantly enhances the performance and efficiency of
LLM alignment across diverse scenarios. Our code and dataset is published on
\url{https://github.com/Wizardcoast/Linear_Alignment.git}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.11206v1' target='_blank'>InferAligner: Inference-Time Alignment for Harmlessness through
  Cross-Model Guidance</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pengyu Wang, Dong Zhang, Linyang Li, Chenkun Tan, Xinghao Wang, Ke Ren, Botian Jiang, Xipeng Qiu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-20 10:41:03</h6>
<p class='card-text'>With the rapid development of large language models (LLMs), they are not only
used as general-purpose AI assistants but are also customized through further
fine-tuning to meet the requirements of different applications. A pivotal
factor in the success of current LLMs is the alignment process. Current
alignment methods, such as supervised fine-tuning (SFT) and reinforcement
learning from human feedback (RLHF), focus on training-time alignment and are
often complex and cumbersome to implement. Therefore, we develop
\textbf{InferAligner}, a novel inference-time alignment method that utilizes
cross-model guidance for harmlessness alignment. InferAligner utilizes safety
steering vectors extracted from safety-aligned model to modify the activations
of the target model when responding to harmful inputs, thereby guiding the
target model to provide harmless responses. Experimental results show that our
method can be very effectively applied to domain-specific models in finance,
medicine, and mathematics, as well as to multimodal large language models
(MLLMs) such as LLaVA. It significantly diminishes the Attack Success Rate
(ASR) of both harmful instructions and jailbreak attacks, while maintaining
almost unchanged performance in downstream tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.10825v3' target='_blank'>Recent Advances in Named Entity Recognition: A Comprehensive Survey and
  Comparative Study</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Imed Keraghel, Stanislas Morbieu, Mohamed Nadif</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-19 17:21:05</h6>
<p class='card-text'>Named Entity Recognition seeks to extract substrings within a text that name
real-world objects and to determine their type (for example, whether they refer
to persons or organizations). In this survey, we first present an overview of
recent popular approaches, including advancements in Transformer-based methods
and Large Language Models (LLMs) that have not had much coverage in other
surveys. In addition, we discuss reinforcement learning and graph-based
approaches, highlighting their role in enhancing NER performance. Second, we
focus on methods designed for datasets with scarce annotations. Third, we
evaluate the performance of the main NER implementations on a variety of
datasets with differing characteristics (as regards their domain, their size,
and their number of classes). We thus provide a deep comparison of algorithms
that have never been considered together. Our experiments shed some light on
how the characteristics of datasets affect the behavior of the methods we
compare.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.10314v2' target='_blank'>LangProp: A code optimization framework using Large Language Models
  applied to driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shu Ishida, Gianluca Corrado, George Fedoseev, Hudson Yeo, Lloyd Russell, Jamie Shotton, João F. Henriques, Anthony Hu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-18 18:52:06</h6>
<p class='card-text'>We propose LangProp, a framework for iteratively optimizing code generated by
large language models (LLMs), in both supervised and reinforcement learning
settings. While LLMs can generate sensible coding solutions zero-shot, they are
often sub-optimal. Especially for code generation tasks, it is likely that the
initial code will fail on certain edge cases. LangProp automatically evaluates
the code performance on a dataset of input-output pairs, catches any
exceptions, and feeds the results back to the LLM in the training loop, so that
the LLM can iteratively improve the code it generates. By adopting a metric-
and data-driven training paradigm for this code optimization procedure, one
could easily adapt findings from traditional machine learning techniques such
as imitation learning, DAgger, and reinforcement learning. We show LangProp's
applicability to general domains such as Sudoku and CartPole, as well as
demonstrate the first proof of concept of automated code optimization for
autonomous driving in CARLA. We show that LangProp can generate interpretable
and transparent policies that can be verified and improved in a metric- and
data-driven way. Our code is available at
https://github.com/shuishida/LangProp.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.09042v1' target='_blank'>LLMs for Relational Reasoning: How Far are We?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiming Li, Yushi Cao, Xiufeng Xu, Junzhe Jiang, Xu Liu, Yon Shin Teo, Shang-wei Lin, Yang Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-17 08:22:52</h6>
<p class='card-text'>Large language models (LLMs) have revolutionized many areas (e.g. natural
language processing, software engineering, etc.) by achieving state-of-the-art
performance on extensive downstream tasks. Aiming to achieve robust and general
artificial intelligence, there has been a surge of interest in investigating
the reasoning ability of the LLMs. Whereas the textual and numerical reasoning
benchmarks adopted by previous works are rather shallow and simple, it is hard
to conclude that the LLMs possess strong reasoning ability by merely achieving
positive results on these benchmarks. Recent efforts have demonstrated that the
LLMs are poor at solving sequential decision-making problems that require
common-sense planning by evaluating their performance on the reinforcement
learning benchmarks. In this work, we conduct an in-depth assessment of several
state-of-the-art LLMs' reasoning ability based on the inductive logic
programming (ILP) benchmark, which is broadly recognized as a representative
and challenging measurement for evaluating logic program induction/synthesis
systems as it requires inducing strict cause-effect logic to achieve robust
deduction on independent and identically distributed (IID) and
out-of-distribution (OOD) test samples. Our evaluations illustrate that
compared with the neural program induction systems which are much smaller in
model size, the state-of-the-art LLMs are much poorer in terms of reasoning
ability by achieving much lower performance and generalization using either
natural language prompting or truth-value matrix prompting.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.08967v3' target='_blank'>ReFT: Reasoning with Reinforced Fine-Tuning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, Hang Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-17 04:43:21</h6>
<p class='card-text'>One way to enhance the reasoning capability of Large Language Models (LLMs)
is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT)
annotations. This approach does not show sufficiently strong generalization
ability, however, because the training only relies on the given CoT data. In
math problem-solving, for example, there is usually only one annotated
reasoning path for each question in the training data. Intuitively, it would be
better for the algorithm to learn from multiple annotated reasoning paths given
a question. To address this issue, we propose a simple yet effective approach
called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of
learning LLMs for reasoning, with math problem-solving as an example. ReFT
first warmups the model with SFT, and then employs on-line reinforcement
learning, specifically the PPO algorithm in this paper, to further fine-tune
the model, where an abundance of reasoning paths are automatically sampled
given the question and the rewards are naturally derived from the ground-truth
answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that
ReFT significantly outperforms SFT, and the performance can be potentially
further boosted by combining inference-time strategies such as majority voting
and re-ranking. Note that ReFT obtains the improvement by learning from the
same training questions as SFT, without relying on extra or augmented training
questions. This indicates a superior generalization ability for ReFT.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.08189v4' target='_blank'>PRewrite: Prompt Rewriting with Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weize Kong, Spurthi Amba Hombaiah, Mingyang Zhang, Qiaozhu Mei, Michael Bendersky</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-16 08:04:50</h6>
<p class='card-text'>Prompt engineering is critical for the development of LLM-based applications.
However, it is usually done manually in a "trial and error" fashion that can be
time consuming, ineffective, and sub-optimal. Even for the prompts which
seemingly work well, there is always a lingering question: can the prompts be
made better with further modifications?
  To address these problems, we investigate automated prompt engineering in
this paper. Specifically, we propose PRewrite, an automated method to rewrite
an under-optimized prompt to a more effective prompt. We instantiate the prompt
rewriter using a LLM. The rewriter LLM is trained using reinforcement learning
to optimize the performance on a given downstream task. We conduct experiments
on diverse benchmark datasets, which demonstrates the effectiveness of
PRewrite.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.07886v2' target='_blank'>Learned Best-Effort LLM Serving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Siddharth Jha, Coleman Hooper, Xiaoxuan Liu, Sehoon Kim, Kurt Keutzer</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-15 18:28:17</h6>
<p class='card-text'>Many applications must provide low-latency LLM service to users or risk
unacceptable user experience. However, over-provisioning resources to serve
fluctuating request patterns is often prohibitively expensive. In this work, we
present a best-effort serving system that employs deep reinforcement learning
to adjust service quality based on the task distribution and system load. Our
best-effort system can maintain availability with over 10x higher client
request rates, serves above 96% of peak performance 4.1x more often, and serves
above 98% of peak performance 2.3x more often than static serving on
unpredictable workloads. Our learned router is robust to shifts in both the
arrival and task distribution. Compared to static serving, learned best-effort
serving allows for cost-efficient serving through increased hardware utility.
Additionally, we argue that learned best-effort LLM serving is applicable in
wide variety of settings and provides application developers great flexibility
to meet their specific needs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.07382v2' target='_blank'>Beyond Sparse Rewards: Enhancing Reinforcement Learning with Language
  Model Critique in Text Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Meng Cao, Lei Shu, Lei Yu, Yun Zhu, Nevan Wichers, Yinxiao Liu, Lei Meng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-14 22:05:11</h6>
<p class='card-text'>Reinforcement learning (RL) can align language models with non-differentiable
reward signals, such as human preferences. However, a major challenge arises
from the sparsity of these reward signals - typically, there is only a single
reward for an entire output. This sparsity of rewards can lead to inefficient
and unstable learning. To address this challenge, our paper introduces an novel
framework that utilizes the critique capability of Large Language Models (LLMs)
to produce intermediate-step rewards during RL training. Our method involves
coupling a policy model with a critic language model, which is responsible for
providing comprehensive feedback of each part of the output. This feedback is
then translated into token or span-level rewards that can be used to guide the
RL training process. We investigate this approach under two different settings:
one where the policy model is smaller and is paired with a more powerful critic
model, and another where a single language model fulfills both roles. We assess
our approach on three text generation tasks: sentiment control, language model
detoxification, and summarization. Experimental results show that incorporating
artificial intrinsic rewards significantly improve both sample efficiency and
the overall performance of the policy model, supported by both automatic and
human evaluation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.07181v1' target='_blank'>Reinforcement Learning from LLM Feedback to Counteract Goal
  Misgeneralization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Houda Nait El Barj, Theophile Sautory</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-14 01:09:48</h6>
<p class='card-text'>We introduce a method to address goal misgeneralization in reinforcement
learning (RL), leveraging Large Language Model (LLM) feedback during training.
Goal misgeneralization, a type of robustness failure in RL occurs when an agent
retains its capabilities out-of-distribution yet pursues a proxy rather than
the intended one. Our approach utilizes LLMs to analyze an RL agent's policies
during training and identify potential failure scenarios. The RL agent is then
deployed in these scenarios, and a reward model is learnt through the LLM
preferences and feedback. This LLM-informed reward model is used to further
train the RL agent on the original dataset. We apply our method to a maze
navigation task, and show marked improvements in goal generalization,
especially in cases where true and proxy goals are somewhat distinguishable and
behavioral biases are pronounced. This study demonstrates how the LLM, despite
its lack of task proficiency, can efficiently supervise RL agents, providing
scalable oversight and valuable insights for enhancing goal-directed learning
in RL through the use of LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.07031v2' target='_blank'>Code Security Vulnerability Repair Using Reinforcement Learning with
  Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nafis Tanveer Islam, Mohammad Bahrami Karkevandi, Peyman Najafirad</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-13 10:19:26</h6>
<p class='card-text'>With the recent advancement of Large Language Models (LLMs), generating
functionally correct code has become less complicated for a wide array of
developers. While using LLMs has sped up the functional development process, it
poses a heavy risk to code security. Code generation with proper security
measures using LLM is a significantly more challenging task than functional
code generation. Security measures may include adding a pair of lines of code
with the original code, consisting of null pointer checking or prepared
statements for SQL injection prevention. Currently, available code repair LLMs
generate code repair by supervised fine-tuning, where the model looks at
cross-entropy loss. However, the original and repaired codes are mostly similar
in functionality and syntactically, except for a few (1-2) lines, which act as
security measures. This imbalance between the lines needed for security
measures and the functional code enforces the supervised fine-tuned model to
prioritize generating functional code without adding proper security measures,
which also benefits the model by resulting in minimal loss. Therefore, in this
work, for security hardening and strengthening of generated code from LLMs, we
propose a reinforcement learning-based method for program-specific repair with
the combination of semantic and syntactic reward mechanisms that focus heavily
on adding security and functional measures in the code, respectively.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.06954v2' target='_blank'>Bridging the Preference Gap between Retrievers and LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Michael Bendersky</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-13 02:20:17</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated superior results across a wide
range of tasks, and Retrieval-augmented Generation (RAG) is an effective way to
enhance the performance by locating relevant information and placing it into
the context window of the LLM. However, the relationship between retrievers and
LLMs in a RAG is still under-investigated. Most existing work treats the
retriever and the LLM as independent components and leaves a gap between
retrieving human-"friendly" information and assembling a LLM-"friendly"
context. In this work, we examine a novel bridge mechanism. We validate the
ranking and selection assumptions of retrievers in the context of RAG and
propose a framework that chains together supervised and reinforcement learning
to train a bridge model that optimizes the connection between the retriever and
the LLM. Empirical results demonstrate the effectiveness of our method in both
question-answering and personalized generation tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.06603v1' target='_blank'>Mutual Enhancement of Large Language and Reinforcement Learning Models
  through Bi-Directional Feedback Mechanisms: A Case Study</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shangding Gu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-12 14:35:57</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated remarkable capabilities for
reinforcement learning (RL) models, such as planning and reasoning
capabilities. However, the problems of LLMs and RL model collaboration still
need to be solved. In this study, we employ a teacher-student learning
framework to tackle these problems, specifically by offering feedback for LLMs
using RL models and providing high-level information for RL models with LLMs in
a cooperative multi-agent setting. Within this framework, the LLM acts as a
teacher, while the RL model acts as a student. The two agents cooperatively
assist each other through a process of recursive help, such as "I help you help
I help." The LLM agent supplies abstract information to the RL agent, enabling
efficient exploration and policy improvement. In turn, the RL agent offers
feedback to the LLM agent, providing valuable, real-time information that helps
generate more useful tokens. This bi-directional feedback loop promotes
optimization, exploration, and mutual improvement for both agents, enabling
them to accomplish increasingly challenging tasks. Remarkably, we propose a
practical algorithm to address the problem and conduct empirical experiments to
evaluate the effectiveness of our method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.06081v2' target='_blank'>Improving Large Language Models via Fine-grained Reinforcement Learning
  with Minimum Editing Constraint</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhipeng Chen, Kun Zhou, Wayne Xin Zhao, Junchen Wan, Fuzheng Zhang, Di Zhang, Ji-Rong Wen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-11 17:58:41</h6>
<p class='card-text'>Reinforcement learning (RL) has been widely used in training large language
models (LLMs) for preventing unexpected outputs, eg reducing harmfulness and
errors. However, existing RL methods mostly adopt the instance-level reward,
which is unable to provide fine-grained supervision for complex reasoning
tasks, and can not focus on the few key tokens that lead to the incorrectness.
To address it, we propose a new RL method named RLMEC that incorporates a
generative model as the reward model, which is trained by the erroneous
solution rewriting task under the minimum editing constraint, and can produce
token-level rewards for RL training. Based on the generative reward model, we
design the token-level RL objective for training and an imitation-based
regularization for stabilizing RL process. And the both objectives focus on the
learning of the key tokens for the erroneous solution, reducing the effect of
other unimportant tokens. The experiment results on mathematical tasks and
question-answering tasks have demonstrated the effectiveness of our approach.
Our code and data are available at https://github.com/RUCAIBox/RLMEC.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.05695v2' target='_blank'>Integrating Physician Diagnostic Logic into Large Language Models:
  Preference Learning from Process Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chengfeng Dou, Zhi Jin, Wenpin Jiao, Haiyan Zhao, Yongqiang Zhao, Zhenwei Tao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-11 06:42:45</h6>
<p class='card-text'>The use of large language models in medical dialogue generation has garnered
significant attention, with a focus on improving response quality and fluency.
While previous studies have made progress in optimizing model performance for
single-round medical Q&A tasks, there is a need to enhance the model's
capability for multi-round conversations to avoid logical inconsistencies. To
address this, we propose an approach called preference learning from process
feedback~(PLPF), which integrates the doctor's diagnostic logic into LLMs. PLPF
involves rule modeling, preference data generation, and preference alignment to
train the model to adhere to the diagnostic process. Experimental results using
Standardized Patient Testing show that PLPF enhances the diagnostic accuracy of
the baseline model in medical conversations by 17.6%, outperforming traditional
reinforcement learning from human feedback. Additionally, PLPF demonstrates
effectiveness in both multi-round and single-round dialogue tasks, showcasing
its potential for improving medical dialogue generation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.05566v3' target='_blank'>Sleeper Agents: Training Deceptive LLMs that Persist Through Safety
  Training</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M. Ziegler, Tim Maxwell, Newton Cheng, Adam Jermyn, Amanda Askell, Ansh Radhakrishnan, Cem Anil, David Duvenaud, Deep Ganguli, Fazl Barez, Jack Clark, Kamal Ndousse, Kshitij Sachan, Michael Sellitto, Mrinank Sharma, Nova DasSarma, Roger Grosse, Shauna Kravec, Yuntao Bai, Zachary Witten, Marina Favaro, Jan Brauner, Holden Karnofsky, Paul Christiano, Samuel R. Bowman, Logan Graham, Jared Kaplan, Sören Mindermann, Ryan Greenblatt, Buck Shlegeris, Nicholas Schiefer, Ethan Perez</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-10 22:14:35</h6>
<p class='card-text'>Humans are capable of strategically deceptive behavior: behaving helpfully in
most situations, but then behaving very differently in order to pursue
alternative objectives when given the opportunity. If an AI system learned such
a deceptive strategy, could we detect it and remove it using current
state-of-the-art safety training techniques? To study this question, we
construct proof-of-concept examples of deceptive behavior in large language
models (LLMs). For example, we train models that write secure code when the
prompt states that the year is 2023, but insert exploitable code when the
stated year is 2024. We find that such backdoor behavior can be made
persistent, so that it is not removed by standard safety training techniques,
including supervised fine-tuning, reinforcement learning, and adversarial
training (eliciting unsafe behavior and then training to remove it). The
backdoor behavior is most persistent in the largest models and in models
trained to produce chain-of-thought reasoning about deceiving the training
process, with the persistence remaining even when the chain-of-thought is
distilled away. Furthermore, rather than removing backdoors, we find that
adversarial training can teach models to better recognize their backdoor
triggers, effectively hiding the unsafe behavior. Our results suggest that,
once a model exhibits deceptive behavior, standard techniques could fail to
remove such deception and create a false impression of safety.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.05033v1' target='_blank'>Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dennis Ulmer, Elman Mansimov, Kaixiang Lin, Justin Sun, Xibin Gao, Yi Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-10 09:49:10</h6>
<p class='card-text'>Large language models (LLMs) are powerful dialogue agents, but specializing
them towards fulfilling a specific function can be challenging. Instructing
tuning, i.e. tuning models on instruction and sample responses generated by
humans (Ouyang et al., 2022), has proven as an effective method to do so, yet
requires a number of data samples that a) might not be available or b) costly
to generate. Furthermore, this cost increases when the goal is to make the LLM
follow a specific workflow within a dialogue instead of single instructions.
Inspired by the self-play technique in reinforcement learning and the use of
LLMs to simulate human agents, we propose a more effective method for data
collection through LLMs engaging in a conversation in various roles. This
approach generates a training data via "self-talk" of LLMs that can be refined
and utilized for supervised fine-tuning. We introduce an automated way to
measure the (partial) success of a dialogue. This metric is used to filter the
generated conversational data that is fed back in LLM for training. Based on
our automated and human evaluations of conversation quality, we demonstrate
that such self-talk data improves results. In addition, we examine the various
characteristics that showcase the quality of generated dialogues and how they
can be connected to their potential utility as training data.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.06800v1' target='_blank'>Reinforcement Learning for Optimizing RAG for Domain Chatbots</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mandar Kulkarni, Praveen Tangarajan, Kyung Kim, Anusua Trivedi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-10 02:57:20</h6>
<p class='card-text'>With the advent of Large Language Models (LLM), conversational assistants
have become prevalent for domain use cases. LLMs acquire the ability to
contextual question answering through training, and Retrieval Augmented
Generation (RAG) further enables the bot to answer domain-specific questions.
This paper describes a RAG-based approach for building a chatbot that answers
user's queries using Frequently Asked Questions (FAQ) data. We train an
in-house retrieval embedding model using infoNCE loss, and experimental results
demonstrate that the in-house model works significantly better than the
well-known general-purpose public embedding model, both in terms of retrieval
accuracy and Out-of-Domain (OOD) query detection. As an LLM, we use an open
API-based paid ChatGPT model. We noticed that a previously retrieved-context
could be used to generate an answer for specific patterns/sequences of queries
(e.g., follow-up queries). Hence, there is a scope to optimize the number of
LLM tokens and cost. Assuming a fixed retrieval model and an LLM, we optimize
the number of LLM tokens using Reinforcement Learning (RL). Specifically, we
propose a policy-based model external to the RAG, which interacts with the RAG
pipeline through policy actions and updates the policy to optimize the cost.
The policy model can perform two actions: to fetch FAQ context or skip
retrieval. We use the open API-based GPT-4 as the reward model. We then train a
policy model using policy gradient on multiple training chat sessions. As a
policy model, we experimented with a public gpt-2 model and an in-house BERT
model. With the proposed RL-based optimization combined with similarity
threshold, we are able to achieve significant cost savings while getting a
slightly improved accuracy. Though we demonstrate results for the FAQ chatbot,
the proposed RL approach is generic and can be experimented with any existing
RAG pipeline.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.03374v2' target='_blank'>LLM-Powered Code Vulnerability Repair with Reinforcement Learning and
  Semantic Reward</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nafis Tanveer Islam, Joseph Khoury, Andrew Seong, Mohammad Bahrami Karkevandi, Gonzalo De La Torre Parra, Elias Bou-Harb, Peyman Najafirad</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-07 02:46:39</h6>
<p class='card-text'>In software development, the predominant emphasis on functionality often
supersedes security concerns, a trend gaining momentum with AI-driven
automation tools like GitHub Copilot. These tools significantly improve
developers' efficiency in functional code development. Nevertheless, it remains
a notable concern that such tools are also responsible for creating insecure
code, predominantly because of pre-training on publicly available repositories
with vulnerable code. Moreover, developers are called the "weakest link in the
chain" since they have very minimal knowledge of code security. Although
existing solutions provide a reasonable solution to vulnerable code, they must
adequately describe and educate the developers on code security to ensure that
the security issues are not repeated. Therefore we introduce a multipurpose
code vulnerability analysis system \texttt{SecRepair}, powered by a large
language model, CodeGen2 assisting the developer in identifying and generating
fixed code along with a complete description of the vulnerability with a code
comment. Our innovative methodology uses a reinforcement learning paradigm to
generate code comments augmented by a semantic reward mechanism. Inspired by
how humans fix code issues, we propose an instruction-based dataset suitable
for vulnerability analysis with LLMs. We further identify zero-day and N-day
vulnerabilities in 6 Open Source IoT Operating Systems on GitHub. Our findings
underscore that incorporating reinforcement learning coupled with semantic
reward augments our model's performance, thereby fortifying its capacity to
address code vulnerabilities with improved efficacy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.06781v1' target='_blank'>PokerGPT: An End-to-End Lightweight Solver for Multi-Player Texas
  Hold'em via Large Language Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chenghao Huang, Yanbo Cao, Yinlong Wen, Tao Zhou, Yanru Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-04 13:27:50</h6>
<p class='card-text'>Poker, also known as Texas Hold'em, has always been a typical research target
within imperfect information games (IIGs). IIGs have long served as a measure
of artificial intelligence (AI) development. Representative prior works, such
as DeepStack and Libratus heavily rely on counterfactual regret minimization
(CFR) to tackle heads-up no-limit Poker. However, it is challenging for
subsequent researchers to learn CFR from previous models and apply it to other
real-world applications due to the expensive computational cost of CFR
iterations. Additionally, CFR is difficult to apply to multi-player games due
to the exponential growth of the game tree size. In this work, we introduce
PokerGPT, an end-to-end solver for playing Texas Hold'em with arbitrary number
of players and gaining high win rates, established on a lightweight large
language model (LLM). PokerGPT only requires simple textual information of
Poker games for generating decision-making advice, thus guaranteeing the
convenient interaction between AI and humans. We mainly transform a set of
textual records acquired from real games into prompts, and use them to
fine-tune a lightweight pre-trained LLM using reinforcement learning human
feedback technique. To improve fine-tuning performance, we conduct prompt
engineering on raw data, including filtering useful information, selecting
behaviors of players with high win rates, and further processing them into
textual instruction using multiple prompt engineering techniques. Through the
experiments, we demonstrate that PokerGPT outperforms previous approaches in
terms of win rate, model size, training time, and response speed, indicating
the great potential of LLMs in solving IIGs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.02072v1' target='_blank'>ICE-GRT: Instruction Context Enhancement by Generative Reinforcement
  based Transformers</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chen Zheng, Ke Sun, Da Tang, Yukun Ma, Yuyu Zhang, Chenguang Xi, Xun Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-04 05:47:41</h6>
<p class='card-text'>The emergence of Large Language Models (LLMs) such as ChatGPT and LLaMA
encounter limitations in domain-specific tasks, with these models often lacking
depth and accuracy in specialized areas, and exhibiting a decrease in general
capabilities when fine-tuned, particularly analysis ability in small sized
models. To address these gaps, we introduce ICE-GRT, utilizing Reinforcement
Learning from Human Feedback (RLHF) grounded in Proximal Policy Optimization
(PPO), demonstrating remarkable ability in in-domain scenarios without
compromising general task performance. Our exploration of ICE-GRT highlights
its understanding and reasoning ability to not only generate robust answers but
also to provide detailed analyses of the reasons behind the answer. This
capability marks a significant progression beyond the scope of Supervised
Fine-Tuning models. The success of ICE-GRT is dependent on several crucial
factors, including Appropriate Data, Reward Size Scaling, KL-Control, Advantage
Normalization, etc. The ICE-GRT model exhibits state-of-the-art performance in
domain-specific tasks and across 12 general Language tasks against equivalent
size and even larger size LLMs, highlighting the effectiveness of our approach.
We provide a comprehensive analysis of the ICE-GRT, underscoring the
significant advancements it brings to the field of LLM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.00243v1' target='_blank'>Uncertainty-Penalized Reinforcement Learning from Human Feedback with
  Diverse Reward LoRA Ensembles</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuanzhao Zhai, Han Zhang, Yu Lei, Yue Yu, Kele Xu, Dawei Feng, Bo Ding, Huaimin Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-30 14:14:14</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) emerges as a promising
paradigm for aligning large language models (LLMs). However, a notable
challenge in RLHF is overoptimization, where beyond a certain threshold, the
pursuit of higher rewards leads to a decline in human preferences. In this
paper, we observe the weakness of KL regularization which is commonly employed
in existing RLHF methods to address overoptimization. To mitigate this
limitation, we scrutinize the RLHF objective in the offline dataset and propose
uncertainty-penalized RLHF (UP-RLHF), which incorporates uncertainty
regularization during RL-finetuning. To enhance the uncertainty quantification
abilities for reward models, we first propose a diverse low-rank adaptation
(LoRA) ensemble by maximizing the nuclear norm of LoRA matrix concatenations.
Then we optimize policy models utilizing penalized rewards, determined by both
rewards and uncertainties provided by the diverse reward LoRA ensembles. Our
experimental results, based on two real human preference datasets, showcase the
effectiveness of diverse reward LoRA ensembles in quantifying reward
uncertainty. Additionally, uncertainty regularization in UP-RLHF proves to be
pivotal in mitigating overoptimization, thereby contributing to the overall
performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.17269v2' target='_blank'>Conversational Question Answering with Reformulations over Knowledge
  Graph</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lihui Liu, Blaine Hill, Boxin Du, Fei Wang, Hanghang Tong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-27 00:03:05</h6>
<p class='card-text'>Conversational question answering (convQA) over knowledge graphs (KGs)
involves answering multi-turn natural language questions about information
contained in a KG. State-of-the-art methods of ConvQA often struggle with
inexplicit question-answer pairs. These inputs are easy for human beings to
understand given a conversation history, but hard for a machine to interpret,
which can degrade ConvQA performance. To address this problem, we propose a
reinforcement learning (RL) based model, CornNet, which utilizes question
reformulations generated by large language models (LLMs) to improve ConvQA
performance. CornNet adopts a teacher-student architecture where a teacher
model learns question representations using human writing reformulations, and a
student model to mimic the teacher model's output via reformulations generated
by LLMs. The learned question representation is then used by an RL model to
locate the correct answer in a KG. Extensive experimental results show that
CornNet outperforms state-of-the-art convQA models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.16044v5' target='_blank'>LLMLight: Large Language Models as Traffic Signal Control Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Siqi Lai, Zhao Xu, Weijia Zhang, Hao Liu, Hui Xiong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-26 13:17:06</h6>
<p class='card-text'>Traffic Signal Control (TSC) is a crucial component in urban traffic
management, aiming to optimize road network efficiency and reduce congestion.
Traditional TSC methods, primarily based on transportation engineering and
reinforcement learning (RL), often struggle with generalization abilities
across varied traffic scenarios and lack interpretability. This paper presents
LLMLight, a novel framework employing Large Language Models (LLMs) as
decision-making agents for TSC. Specifically, the framework begins by
instructing the LLM with a knowledgeable prompt detailing real-time traffic
conditions. Leveraging the advanced generalization capabilities of LLMs,
LLMLight engages a reasoning and decision-making process akin to human
intuition for effective traffic control. Moreover, we build LightGPT, a
specialized backbone LLM tailored for TSC tasks. By learning nuanced traffic
patterns and control strategies, LightGPT enhances the LLMLight framework
cost-effectively. Extensive experiments conducted on ten real-world and
synthetic datasets, along with evaluations by fifteen human experts,
demonstrate the exceptional effectiveness, generalization ability, and
interpretability of LLMLight with LightGPT, outperforming nine baseline methods
and ten advanced LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.15997v3' target='_blank'>Aligning Large Language Models with Human Preferences through
  Representation Engineering</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenhao Liu, Xiaohua Wang, Muling Wu, Tianlong Li, Changze Lv, Zixuan Ling, Jianhao Zhu, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-26 11:01:36</h6>
<p class='card-text'>Aligning large language models (LLMs) with human preferences is crucial for
enhancing their utility in terms of helpfulness, truthfulness, safety,
harmlessness, and interestingness. Existing methods for achieving this
alignment often involves employing reinforcement learning from human feedback
(RLHF) to fine-tune LLMs based on human labels assessing the relative quality
of model responses. Nevertheless, RLHF is susceptible to instability during
fine-tuning and presents challenges in implementation.Drawing inspiration from
the emerging field of representation engineering (RepE), this study aims to
identify relevant representations for high-level human preferences embedded in
patterns of activity within an LLM, and achieve precise control of model
behavior by transforming its representations. This novel approach, denoted as
Representation Alignment from Human Feedback (RAHF), proves to be effective,
computationally efficient, and easy to implement.Extensive experiments
demonstrate the efficacy of RAHF in not only capturing but also manipulating
representations to align with a broad spectrum of human preferences or values,
rather than being confined to a singular concept or function (e.g. honesty or
bias). RAHF's versatility in accommodating diverse human preferences shows its
potential for advancing LLM performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.14925v2' target='_blank'>A Survey of Reinforcement Learning from Human Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Timo Kaufmann, Paul Weng, Viktor Bengs, Eyke Hüllermeier</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-22 18:58:06</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) is a variant of
reinforcement learning (RL) that learns from human feedback instead of relying
on an engineered reward function. Building on prior work on the related setting
of preference-based reinforcement learning (PbRL), it stands at the
intersection of artificial intelligence and human-computer interaction. This
positioning offers a promising avenue to enhance the performance and
adaptability of intelligent systems while also improving the alignment of their
objectives with human values. The training of large language models (LLMs) has
impressively demonstrated this potential in recent years, where RLHF played a
decisive role in directing the model's capabilities toward human objectives.
This article provides a comprehensive overview of the fundamentals of RLHF,
exploring the intricate dynamics between RL agents and human input. While
recent focus has been on RLHF for LLMs, our survey adopts a broader
perspective, examining the diverse applications and wide-ranging impact of the
technique. We delve into the core principles that underpin RLHF, shedding light
on the symbiotic relationship between algorithms and human feedback, and
discuss the main research trends in the field. By synthesizing the current
landscape of RLHF research, this article aims to provide researchers as well as
practitioners with a comprehensive understanding of this rapidly growing field
of research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.14878v1' target='_blank'>Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Filippos Christianos, Georgios Papoudakis, Matthieu Zimmer, Thomas Coste, Zhihao Wu, Jingxuan Chen, Khyati Khandelwal, James Doran, Xidong Feng, Jiacheng Liu, Zheng Xiong, Yicheng Luo, Jianye Hao, Kun Shao, Haitham Bou-Ammar, Jun Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-22 17:57:57</h6>
<p class='card-text'>A key method for creating Artificial Intelligence (AI) agents is
Reinforcement Learning (RL). However, constructing a standalone RL policy that
maps perception to action directly encounters severe problems, chief among them
being its lack of generality across multiple tasks and the need for a large
amount of training data. The leading cause is that it cannot effectively
integrate prior information into the perception-action cycle when devising the
policy. Large language models (LLMs) emerged as a fundamental way to
incorporate cross-domain knowledge into AI agents but lack crucial learning and
adaptation toward specific decision problems. This paper presents a general
framework model for integrating and learning structured reasoning into AI
agents' policies. Our methodology is motivated by the modularity found in the
human brain. The framework utilises the construction of intrinsic and extrinsic
functions to add previous understandings of reasoning structures. It also
provides the adaptive ability to learn models inside every module or function,
consistent with the modular structure of cognitive processes. We describe the
framework in-depth and compare it with other AI pipelines and existing
frameworks. The paper explores practical applications, covering experiments
that show the effectiveness of our method. Our results indicate that AI agents
perform and adapt far better when organised reasoning and prior knowledge are
embedded. This opens the door to more resilient and general AI agent systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.14862v1' target='_blank'>YAYI 2: Multilingual Open-Source Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yin Luo, Qingchao Kong, Nan Xu, Jia Cao, Bao Hao, Baoyu Qu, Bo Chen, Chao Zhu, Chenyang Zhao, Donglei Zhang, Fan Feng, Feifei Zhao, Hailong Sun, Hanxuan Yang, Haojun Pan, Hongyu Liu, Jianbin Guo, Jiangtao Du, Jingyi Wang, Junfeng Li, Lei Sun, Liduo Liu, Lifeng Dong, Lili Liu, Lin Wang, Liwen Zhang, Minzheng Wang, Pin Wang, Ping Yu, Qingxiao Li, Rui Yan, Rui Zou, Ruiqun Li, Taiwen Huang, Xiaodong Wang, Xiaofei Wu, Xin Peng, Xina Zhang, Xing Fang, Xinglin Xiao, Yanni Hao, Yao Dong, Yigang Wang, Ying Liu, Yongyu Jiang, Yungan Wang, Yuqi Wang, Zhangsheng Wang, Zhaoxin Yu, Zhen Luo, Wenji Mao, Lei Wang, Dajun Zeng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-22 17:34:47</h6>
<p class='card-text'>As the latest advancements in natural language processing, large language
models (LLMs) have achieved human-level language understanding and generation
abilities in many real-world tasks, and even have been regarded as a potential
path to the artificial general intelligence. To better facilitate research on
LLMs, many open-source LLMs, such as Llama 2 and Falcon, have recently been
proposed and gained comparable performances to proprietary models. However,
these models are primarily designed for English scenarios and exhibit poor
performances in Chinese contexts. In this technical report, we propose YAYI 2,
including both base and chat models, with 30 billion parameters. YAYI 2 is
pre-trained from scratch on a multilingual corpus which contains 2.65 trillion
tokens filtered by our pre-training data processing pipeline. The base model is
aligned with human values through supervised fine-tuning with millions of
instructions and reinforcement learning from human feedback. Extensive
experiments on multiple benchmarks, such as MMLU and CMMLU, consistently
demonstrate that the proposed YAYI 2 outperforms other similar sized
open-source models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.13980v2' target='_blank'>Carve3D: Improving Multi-view Reconstruction Consistency for Diffusion
  Models with RL Finetuning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Desai Xie, Jiahao Li, Hao Tan, Xin Sun, Zhixin Shu, Yi Zhou, Sai Bi, Sören Pirk, Arie E. Kaufman</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-21 16:10:33</h6>
<p class='card-text'>Multi-view diffusion models, obtained by applying Supervised Finetuning (SFT)
to text-to-image diffusion models, have driven recent breakthroughs in
text-to-3D research. However, due to the limited size and quality of existing
3D datasets, they still suffer from multi-view inconsistencies and Neural
Radiance Field (NeRF) reconstruction artifacts. We argue that multi-view
diffusion models can benefit from further Reinforcement Learning Finetuning
(RLFT), which allows models to learn from the data generated by themselves and
improve beyond their dataset limitations during SFT. To this end, we introduce
Carve3D, an improved RLFT algorithm coupled with a novel Multi-view
Reconstruction Consistency (MRC) metric, to enhance the consistency of
multi-view diffusion models. To measure the MRC metric on a set of multi-view
images, we compare them with their corresponding NeRF renderings at the same
camera viewpoints. The resulting model, which we denote as Carve3DM,
demonstrates superior multi-view consistency and NeRF reconstruction quality
than existing models. Our results suggest that pairing SFT with Carve3D's RLFT
is essential for developing multi-view-consistent diffusion models, mirroring
the standard Large Language Model (LLM) alignment pipeline. Our code, training
and testing data, and video results are available at:
https://desaixie.github.io/carve-3d.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.11819v3' target='_blank'>An Adaptive Placement and Parallelism Framework for Accelerating RLHF
  Training</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Youshao Xiao, Zhenglei Zhou, Fagui Mao, Weichang Wu, Shangchun Zhao, Lin Ju, Lei Liang, Xiaolu Zhang, Jun Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-19 03:24:55</h6>
<p class='card-text'>Recently, ChatGPT or InstructGPT like large language models (LLM) has made a
significant impact in the AI world. Many works have attempted to reproduce the
complex InstructGPT's training pipeline, namely Reinforcement Learning with
Human Feedback (RLHF). However, the mainstream distributed RLHF training
methods typically adopt a fixed model placement strategy, referred to as the
Co-located strategy. This strategy treats all four interdependent models
involved in RLHF as a single entity, distributing them across all devices and
applying parallelism techniques designed for a single model, regardless of the
workload heterogeneity inherent to each model. As a result, this strategy
exacerbates the generation bottlenecks in the RLHF training and degrades the
overall training efficiency. To address these issues, we propose a flexible
model placement framework that offers two general and agile model placement
strategies. The Interleaving strategy helps reduce memory redundancy and
communication costs of RLHF training by placing models without dependencies on
exclusive devices with careful orchestration. On the other hand, the
Disaggregated strategy improves the throughput of model training by separating
the training and inference runtime of the RLHF pipeline with additional shadow
models. Furthermore, our framework provides a simple user interface and
guidelines to easily and flexibly configure these strategies in various
training scenarios. Our experiments have shown that our strategy can achieve
notable improvements up to 11x, compared to the current state-of-the-art (SOTA)
approaches. The results highlight the effectiveness and adaptability of our
methods in accelerating the training of distributed RLHF.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.11282v3' target='_blank'>Evaluating and Enhancing Large Language Models for Conversational
  Reasoning on Knowledge Graphs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuxuan Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-18 15:23:06</h6>
<p class='card-text'>The development of large language models (LLMs) has been catalyzed by
advancements in pre-training techniques. These models have demonstrated robust
reasoning capabilities through manually designed prompts. In this work, we
evaluate the conversational reasoning capabilities of the current
state-of-the-art LLM (GPT-4) on knowledge graphs (KGs). However, the
performance of LLMs is constrained due to a lack of KG environment awareness
and the difficulties in developing effective optimization mechanisms for
intermediary reasoning stages. We further introduce LLM-ARK, a LLM grounded KG
reasoning agent designed to deliver precise and adaptable predictions on KG
paths. LLM-ARK leverages Full Textual Environment (FTE) prompt to assimilate
state information within each reasoning step. We reframe the challenge of
multi-hop reasoning on the KG as a sequential decision-making task. Utilizing
the Proximal Policy Optimization (PPO) online policy gradient reinforcement
learning algorithm, our model is optimized to learn from rich reward signals.
Additionally, we conduct an evaluation of our model and GPT-4 on the OpenDialKG
dataset. The experimental results reveal that LLaMA-2-7B-ARK outperforms the
current state-of-the-art model by 5.28 percentage points, with a performance
rate of 36.39% on the target@1 evaluation metric. Meanwhile, GPT-4 scored
14.91%, further demonstrating the effectiveness of our method. Our code is
available on GitHub (https://github.com/Aipura/LLM-ARK) for further access.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.10003v1' target='_blank'>ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan, Manzil Zaheer, Felix Yu, Sanjiv Kumar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-15 18:20:15</h6>
<p class='card-text'>Answering complex natural language questions often necessitates multi-step
reasoning and integrating external information. Several systems have combined
knowledge retrieval with a large language model (LLM) to answer such questions.
These systems, however, suffer from various failure cases, and we cannot
directly train them end-to-end to fix such failures, as interaction with
external knowledge is non-differentiable. To address these deficiencies, we
define a ReAct-style LLM agent with the ability to reason and act upon external
knowledge. We further refine the agent through a ReST-like method that
iteratively trains on previous trajectories, employing growing-batch
reinforcement learning with AI feedback for continuous self-improvement and
self-distillation. Starting from a prompted large model and after just two
iterations of the algorithm, we can produce a fine-tuned small model that
achieves comparable performance on challenging compositional question-answering
benchmarks with two orders of magnitude fewer parameters.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.09238v2' target='_blank'>Auto MC-Reward: Automated Dense Reward Design with Large Language Models
  for Minecraft</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou, Yu Qiao, Xiaogang Wang, Hongsheng Li, Lewei Lu, Jifeng Dai</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-14 18:58:12</h6>
<p class='card-text'>Many reinforcement learning environments (e.g., Minecraft) provide only
sparse rewards that indicate task completion or failure with binary values. The
challenge in exploration efficiency in such environments makes it difficult for
reinforcement-learning-based agents to learn complex tasks. To address this,
this paper introduces an advanced learning system, named Auto MC-Reward, that
leverages Large Language Models (LLMs) to automatically design dense reward
functions, thereby enhancing the learning efficiency. Auto MC-Reward consists
of three important components: Reward Designer, Reward Critic, and Trajectory
Analyzer. Given the environment information and task descriptions, the Reward
Designer first design the reward function by coding an executable Python
function with predefined observation inputs. Then, our Reward Critic will be
responsible for verifying the code, checking whether the code is
self-consistent and free of syntax and semantic errors. Further, the Trajectory
Analyzer summarizes possible failure causes and provides refinement suggestions
according to collected trajectories. In the next round, Reward Designer will
further refine and iterate the dense reward function based on feedback.
Experiments demonstrate a significant improvement in the success rate and
learning efficiency of our agents in complex tasks in Minecraft, such as
obtaining diamond with the efficient ability to avoid lava, and efficiently
explore trees and animals that are sparse in the plains biome.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.08935v3' target='_blank'>Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human
  Annotations</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, Zhifang Sui</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-14 13:41:54</h6>
<p class='card-text'>In this paper, we present an innovative process-oriented math process reward
model called \textbf{Math-Shepherd}, which assigns a reward score to each step
of math problem solutions. The training of Math-Shepherd is achieved using
automatically constructed process-wise supervision data, breaking the
bottleneck of heavy reliance on manual annotation in existing work. We explore
the effectiveness of Math-Shepherd in two scenarios: 1) \textit{Verification}:
Math-Shepherd is utilized for reranking multiple outputs generated by Large
Language Models (LLMs); 2) \textit{Reinforcement Learning}: Math-Shepherd is
employed to reinforce LLMs with step-by-step Proximal Policy Optimization
(PPO). With Math-Shepherd, a series of open-source LLMs demonstrates
exceptional performance. For instance, the step-by-step PPO with Math-Shepherd
significantly improves the accuracy of Mistral-7B (77.9\%$\to$84.1\% on GSM8K
and 28.6\%$\to$33.0\% on MATH). The accuracy can be further enhanced to 89.1\%
and 43.5\% on GSM8K and MATH with the verification of Math-Shepherd,
respectively. We believe that automatic process supervision holds significant
potential for the future evolution of LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.08901v3' target='_blank'>Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xijie Huang, Li Lyna Zhang, Kwang-Ting Cheng, Fan Yang, Mao Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-14 13:03:13</h6>
<p class='card-text'>Large Language Models (LLMs) have shown impressive capabilities, yet they
still struggle with math reasoning. In this work, we propose CoT-Influx, a
novel approach that pushes the boundary of few-shot Chain-of-Thoughts (CoT)
learning to improve LLM mathematical reasoning. Motivated by the observation
that adding more concise CoT examples in the prompt can improve LLM reasoning
performance, CoT-Influx employs a coarse-to-fine pruner to maximize the input
of effective and concise CoT examples. The pruner first selects as many crucial
CoT examples as possible and then prunes unimportant tokens to fit the context
window. A math reasoning dataset with diverse difficulty levels and reasoning
steps is used to train the pruner, along with a math-specialized reinforcement
learning approach. As a result, by enabling more CoT examples with double the
context window size in tokens, CoT-Influx significantly outperforms various
prompting baselines across various LLMs (LLaMA2-7B, 13B, 70B) and 5 math
datasets, achieving up to 4.55% absolute improvements. Remarkably, without any
fine-tuning, LLaMA2-70B with CoT-Influx surpasses GPT-3.5 and a wide range of
larger LLMs (PaLM, Minerva 540B, etc.) on the GSM8K. CoT-Influx serves as a
plug-and-play module for LLMs and is compatible with most existing reasoning
prompting techniques, such as self-consistency and self-verification.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.08358v2' target='_blank'>Distributional Preference Learning: Understanding and Accounting for
  Hidden Context in RLHF</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Anand Siththaranjan, Cassidy Laidlaw, Dylan Hadfield-Menell</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-13 18:51:34</h6>
<p class='card-text'>In practice, preference learning from human feedback depends on incomplete
data with hidden context. Hidden context refers to data that affects the
feedback received, but which is not represented in the data used to train a
preference model. This captures common issues of data collection, such as
having human annotators with varied preferences, cognitive processes that
result in seemingly irrational behavior, and combining data labeled according
to different criteria. We prove that standard applications of preference
learning, including reinforcement learning from human feedback (RLHF),
implicitly aggregate over hidden contexts according to a well-known voting rule
called Borda count. We show this can produce counter-intuitive results that are
very different from other methods which implicitly aggregate via expected
utility. Furthermore, our analysis formalizes the way that preference learning
from users with diverse values tacitly implements a social choice function. A
key implication of this result is that annotators have an incentive to
misreport their preferences in order to influence the learned model, leading to
vulnerabilities in the deployment of RLHF. As a step towards mitigating these
problems, we introduce a class of methods called distributional preference
learning (DPL). DPL methods estimate a distribution of possible score values
for each alternative in order to better account for hidden context.
Experimental results indicate that applying DPL to RLHF for LLM chatbots
identifies hidden context in the data and significantly reduces subsequent
jailbreak vulnerability. Our code and data are available at
https://github.com/cassidylaidlaw/hidden-context</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.07876v1' target='_blank'>Causality Analysis for Evaluating the Security of Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wei Zhao, Zhe Li, Jun Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-13 03:35:43</h6>
<p class='card-text'>Large Language Models (LLMs) such as GPT and Llama2 are increasingly adopted
in many safety-critical applications. Their security is thus essential. Even
with considerable efforts spent on reinforcement learning from human feedback
(RLHF), recent studies have shown that LLMs are still subject to attacks such
as adversarial perturbation and Trojan attacks. Further research is thus needed
to evaluate their security and/or understand the lack of it. In this work, we
propose a framework for conducting light-weight causality-analysis of LLMs at
the token, layer, and neuron level. We applied our framework to open-source
LLMs such as Llama2 and Vicuna and had multiple interesting discoveries. Based
on a layer-level causality analysis, we show that RLHF has the effect of
overfitting a model to harmful prompts. It implies that such security can be
easily overcome by `unusual' harmful prompts. As evidence, we propose an
adversarial perturbation method that achieves 100\% attack success rate on the
red-teaming tasks of the Trojan Detection Competition 2023. Furthermore, we
show the existence of one mysterious neuron in both Llama2 and Vicuna that has
an unreasonably high causal effect on the output. While we are uncertain on why
such a neuron exists, we show that it is possible to conduct a ``Trojan''
attack targeting that particular neuron to completely cripple the LLM, i.e., we
can generate transferable suffixes to prompts that frequently make the LLM
produce meaningless responses.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.07368v1' target='_blank'>Sequential Planning in Large Partially Observable Environments guided by
  LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Swarna Kamal Paul</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-12 15:36:59</h6>
<p class='card-text'>Sequential planning in large state space and action space quickly becomes
intractable due to combinatorial explosion of the search space. Heuristic
methods, like monte-carlo tree search, though effective for large state space,
but struggle if action space is large. Pure reinforcement learning methods,
relying only on reward signals, needs prohibitively large interactions with the
environment to device a viable plan. If the state space, observations and
actions can be represented in natural language then Large Language models (LLM)
can be used to generate action plans. Recently several such goal-directed
agents like Reflexion, CLIN, SayCan were able to surpass the performance of
other state-of-the-art methods with minimum or no task specific training. But
they still struggle with exploration and get stuck in local optima. Their
planning capabilities are limited by the limited reasoning capability of the
foundational LLMs on text data. We propose a hybrid agent "neoplanner", that
synergizes both state space search with queries to foundational LLM to get the
best action plan. The reward signals are quantitatively used to drive the
search. A balance of exploration and exploitation is maintained by maximizing
upper confidence bounds of values of states. In places where random exploration
is needed, the LLM is queried to generate an action plan. Learnings from each
trial are stored as entity relationships in text format. Those are used in
future queries to the LLM for continual improvement. Experiments in the
Scienceworld environment reveals a 124% improvement from the current best
method in terms of average reward gained across multiple tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.00006v3' target='_blank'>Building Open-Ended Embodied Agent via Language-Policy Bidirectional
  Adaptation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shaopeng Zhai, Jie Wang, Tianyi Zhang, Fuxian Huang, Qi Zhang, Ming Zhou, Jing Hou, Yu Qiao, Yu Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-12 11:06:07</h6>
<p class='card-text'>Building embodied agents on integrating Large Language Models (LLMs) and
Reinforcement Learning (RL) have revolutionized human-AI interaction:
researchers can now leverage language instructions to plan decision-making for
open-ended tasks. However, existing research faces challenges in meeting the
requirement of open-endedness. They typically either train LLM/RL models to
adapt to a fixed counterpart, limiting exploration of novel skills and
hindering the efficacy of human-AI interaction. To this end, we present
OpenPAL, a co-training framework comprising two stages: (1) fine-tuning a
pre-trained LLM to translate human instructions into goals for planning, and
goal-conditioned training a policy for decision-making; (2) co-training to
align the LLM and policy, achieving instruction open-endedness. We conducted
experiments using Contra, an open-ended FPS game, demonstrating that an agent
trained with OpenPAL not only comprehends arbitrary instructions but also
exhibits efficient execution. These results suggest that OpenPAL holds the
potential to construct open-ended embodied agents in practical scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.05657v1' target='_blank'>Leveraging Reinforcement Learning and Large Language Models for Code
  Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shukai Duan, Nikos Kanakaris, Xiongye Xiao, Heng Ping, Chenyu Zhou, Nesreen K. Ahmed, Guixiang Ma, Mihai Capota, Theodore L. Willke, Shahin Nazarian, Paul Bogdan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-09 19:50:23</h6>
<p class='card-text'>Code optimization is a daunting task that requires a significant level of
expertise from experienced programmers. This level of expertise is not
sufficient when compared to the rapid development of new hardware
architectures. Towards advancing the whole code optimization process, recent
approaches rely on machine learning and artificial intelligence techniques.
This paper introduces a new framework to decrease the complexity of code
optimization. The proposed framework builds on large language models (LLMs) and
reinforcement learning (RL) and enables LLMs to receive feedback from their
environment (i.e., unit tests) during the fine-tuning process. We compare our
framework with existing state-of-the-art models and show that it is more
efficient with respect to speed and computational usage, as a result of the
decrement in training steps and its applicability to models with fewer
parameters. Additionally, our framework reduces the possibility of logical and
syntactical errors. Toward evaluating our approach, we run several experiments
on the PIE dataset using a CodeT5 language model and RRHF, a new reinforcement
learning algorithm. We adopt a variety of evaluation metrics with regards to
optimization quality, and speedup. The evaluation results demonstrate that the
proposed framework has similar results in comparison with existing models using
shorter training times and smaller pre-trained models. In particular, we
accomplish an increase of 5.6% and 2.2 over the baseline models concerning the
%OP T and SP metrics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.05621v2' target='_blank'>PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhenting Qi, Xiaoyu Tan, Shaojie Shi, Chao Qu, Yinghui Xu, Yuan Qi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-09 17:38:39</h6>
<p class='card-text'>Instruction fine-tuning has conventionally been employed to adapt Large
Language Models (LLMs) to a variety of tasks. Nonetheless, this technique often
necessitates substantial computational resources, making it impractical for
deployment by individuals or small-scale entities. Recently, Low-Rank
Adaptation (LoRA) has become a promising alternative, offering high
capabilities on par with full tuning with reduced resource overhead. However,
attaining satisfactory performance through the fine-tuning of LoRA is a
non-trivial challenge. In this paper, we propose PILLOW, which aims to improve
LoRA's performance by a discrimination-based prompting method, leveraging LLMs'
In-Context Learning ability. PILLOW incorporates a matching network that
selects prompts from a user-defined prompt pool, concatenates the selected
prompts with the user instruction as input, and performs inference using the
LoRA-fine-tuned LLMs. Trained with Reinforcement Learning, PILLOW exhibits
commensurate performance on various evaluation metrics compared with typical
instruction fine-tuning methods, utilizing only consumer-grade GPU resources
and exhibiting a large reduction in computational costs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.05571v2' target='_blank'>Frugal LMs Trained to Invoke Symbolic Solvers Achieve
  Parameter-Efficient Arithmetic Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Subhabrata Dutta, Joykirat Singh, Ishan Pandey, Sunny Manchanda, Soumen Chakrabarti, Tanmoy Chakraborty</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-09 13:20:49</h6>
<p class='card-text'>Large Language Models (LLM) exhibit zero-shot mathematical reasoning capacity
as a behavior emergent with scale, commonly manifesting as chain-of-thoughts
(CoT) reasoning. However, multiple empirical findings suggest that this prowess
is exclusive to LLMs with exorbitant sizes (beyond 50 billion parameters).
Meanwhile, educational neuroscientists suggest that symbolic algebraic
manipulation be introduced around the same time as arithmetic word problems to
modularize language-to-formulation, symbolic manipulation of the formulation,
and endgame arithmetic. In this paper, we start with the hypothesis that much
smaller LMs, which are weak at multi-step reasoning, can achieve reasonable
arithmetic reasoning if arithmetic word problems are posed as a
formalize-then-solve task. In our architecture, which we call SYRELM, the LM
serves the role of a translator to map natural language arithmetic questions
into a formal language (FL) description. A symbolic solver then evaluates the
FL expression to obtain the answer. A small frozen LM, equipped with an
efficient low-rank adapter, is capable of generating FL expressions that
incorporate natural language descriptions of the arithmetic problem (e.g.,
variable names and their purposes, formal expressions combining variables,
etc.). We adopt policy-gradient reinforcement learning to train the adapted LM,
informed by the non-differentiable symbolic solver. This marks a sharp
departure from the recent development in tool-augmented LLMs, in which the
external tools (e.g., calculator, Web search, etc.) are essentially detached
from the learning phase of the LM. SYRELM shows massive improvements (e.g.,
+30.65 absolute point improvement in accuracy on the SVAMP dataset using GPT-J
6B model) over base LMs, while keeping our testbed easy to diagnose, interpret
and within reach of most researchers.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.01818v3' target='_blank'>Hybrid Approaches for Moral Value Alignment in AI Agents: a Manifesto</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Elizaveta Tennant, Stephen Hailes, Mirco Musolesi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-04 11:46:34</h6>
<p class='card-text'>Increasing interest in ensuring the safety of next-generation Artificial
Intelligence (AI) systems calls for novel approaches to embedding morality into
autonomous agents. This goal differs qualitatively from traditional
task-specific AI methodologies. In this paper, we provide a systematization of
existing approaches to the problem of introducing morality in machines -
modelled as a continuum. Our analysis suggests that popular techniques lie at
the extremes of this continuum - either being fully hard-coded into top-down,
explicit rules, or entirely learned in a bottom-up, implicit fashion with no
direct statement of any moral principle (this includes learning from human
feedback, as applied to the training and finetuning of large language models,
or LLMs). Given the relative strengths and weaknesses of each type of
methodology, we argue that more hybrid solutions are needed to create adaptable
and robust, yet controllable and interpretable agentic systems. To that end,
this paper discusses both the ethical foundations (including deontology,
consequentialism and virtue ethics) and implementations of morally aligned AI
systems.
  We present a series of case studies that rely on intrinsic rewards, moral
constraints or textual instructions, applied to either pure-Reinforcement
Learning or LLM-based agents. By analysing these diverse implementations under
one framework, we compare their relative strengths and shortcomings in
developing morally aligned AI systems. We then discuss strategies for
evaluating the effectiveness of moral learning agents. Finally, we present open
research questions and implications for the future of AI safety and ethics
which are emerging from this hybrid framework.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.01552v1' target='_blank'>The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, Yejin Choi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-04 00:46:11</h6>
<p class='card-text'>The alignment tuning process of large language models (LLMs) typically
involves instruction learning through supervised fine-tuning (SFT) and
preference tuning via reinforcement learning from human feedback (RLHF). A
recent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for
SFT can achieve significant alignment performance as well, suggesting that the
effect of alignment tuning might be "superficial." This raises questions about
how exactly the alignment tuning transforms a base LLM.
  We analyze the effect of alignment tuning by examining the token distribution
shift between base LLMs and their aligned counterpart. Our findings reveal that
base LLMs and their alignment-tuned versions perform nearly identically in
decoding on the majority of token positions. Most distribution shifts occur
with stylistic tokens. These direct evidence strongly supports the Superficial
Alignment Hypothesis suggested by LIMA.
  Based on these findings, we rethink the alignment of LLMs by posing the
research question: how effectively can we align base LLMs without SFT or RLHF?
To address this, we introduce a simple, tuning-free alignment method, URIAL.
URIAL achieves effective alignment purely through in-context learning (ICL)
with base LLMs, requiring as few as three constant stylistic examples and a
system prompt. We conduct a fine-grained and interpretable evaluation on a
diverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that
base LLMs with URIAL can match or even surpass the performance of LLMs aligned
with SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based
alignment methods can be significantly reduced through strategic prompting and
ICL. Our findings on the superficial nature of alignment tuning and results
with URIAL suggest that deeper analysis and theoretical understanding of
alignment is crucial to future LLM research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.02206v1' target='_blank'>Axiomatic Preference Modeling for Longform Question Answering</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Corby Rosset, Guoqing Zheng, Victor Dibia, Ahmed Awadallah, Paul Bennett</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-02 23:11:41</h6>
<p class='card-text'>The remarkable abilities of large language models (LLMs) like GPT-4 partially
stem from post-training processes like Reinforcement Learning from Human
Feedback (RLHF) involving human preferences encoded in a reward model. However,
these reward models (RMs) often lack direct knowledge of why, or under what
principles, the preferences annotations were made. In this study, we identify
principles that guide RMs to better align with human preferences, and then
develop an axiomatic framework to generate a rich variety of preference signals
to uphold them. We use these axiomatic signals to train a model for scoring
answers to longform questions. Our approach yields a Preference Model with only
about 220M parameters that agrees with gold human-annotated preference labels
more often than GPT-4. The contributions of this work include: training a
standalone preference model that can score human- and LLM-generated answers on
the same scale; developing an axiomatic framework for generating training data
pairs tailored to certain principles; and showing that a small amount of
axiomatic signals can help small models outperform GPT-4 in preference scoring.
We release our model on huggingface:
https://huggingface.co/corbyrosset/axiomatic_preference_model</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.00886v4' target='_blank'>Nash Learning from Human Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, Marco Selvi, Sertan Girgin, Nikola Momchev, Olivier Bachem, Daniel J. Mankowitz, Doina Precup, Bilal Piot</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-01 19:26:23</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) has emerged as the main
paradigm for aligning large language models (LLMs) with human preferences.
Typically, RLHF involves the initial step of learning a reward model from human
feedback, often expressed as preferences between pairs of text generations
produced by a pre-trained LLM. Subsequently, the LLM's policy is fine-tuned by
optimizing it to maximize the reward model through a reinforcement learning
algorithm. However, an inherent limitation of current reward models is their
inability to fully represent the richness of human preferences and their
dependency on the sampling distribution.
  In this study, we introduce an alternative pipeline for the fine-tuning of
LLMs using pairwise human feedback. Our approach entails the initial learning
of a preference model, which is conditioned on two inputs given a prompt,
followed by the pursuit of a policy that consistently generates responses
preferred over those generated by any competing policy, thus defining the Nash
equilibrium of this preference model. We term this approach Nash learning from
human feedback (NLHF).
  In the context of a tabular policy representation, we present a novel
algorithmic solution, Nash-MD, founded on the principles of mirror descent.
This algorithm produces a sequence of policies, with the last iteration
converging to the regularized Nash equilibrium. Additionally, we explore
parametric representations of policies and introduce gradient descent
algorithms for deep-learning architectures. To demonstrate the effectiveness of
our approach, we present experimental results involving the fine-tuning of a
LLM for a text summarization task. We believe NLHF offers a compelling avenue
for preference learning and policy optimization with the potential of advancing
the field of aligning LLMs with human preferences.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.18232v1' target='_blank'>LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Marwa Abdulhai, Isadora White, Charlie Snell, Charles Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu, Sergey Levine</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-30 03:59:31</h6>
<p class='card-text'>Large language models (LLMs) provide excellent text-generation capabilities,
but standard prompting and generation methods generally do not lead to
intentional or goal-directed agents and might necessitate considerable prompt
tuning. This becomes particularly apparent in multi-turn conversations: even
the best current LLMs rarely ask clarifying questions, engage in explicit
information gathering, or take actions now that lead to better decisions after
multiple turns. Reinforcement learning has the potential to leverage the
powerful modeling capabilities of LLMs, as well as their internal
representation of textual interactions, to create capable goal-directed
language agents. This can enable intentional and temporally extended
interactions, such as with humans, through coordinated persuasion and carefully
crafted questions, or in goal-directed play through text games to bring about
desired final outcomes. However, enabling this requires the community to
develop stable and reliable reinforcement learning algorithms that can
effectively train LLMs. Developing such algorithms requires tasks that can
gauge progress on algorithm design, provide accessible and reproducible
evaluations for multi-turn interactions, and cover a range of task properties
and challenges in improving reinforcement learning algorithms. Our paper
introduces the LMRL-Gym benchmark for evaluating multi-turn RL for LLMs,
together with an open-source research framework containing a basic toolkit for
getting started on multi-turn RL with offline value-based and policy-based RL
methods. Our benchmark consists of 8 different language tasks, which require
multiple rounds of language interaction and cover a range of tasks in
open-ended dialogue and text games.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.17391v1' target='_blank'>Unveiling the Implicit Toxicity in Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiaxin Wen, Pei Ke, Hao Sun, Zhexin Zhang, Chengfei Li, Jinfeng Bai, Minlie Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-29 06:42:36</h6>
<p class='card-text'>The open-endedness of large language models (LLMs) combined with their
impressive capabilities may lead to new safety issues when being exploited for
malicious use. While recent studies primarily focus on probing toxic outputs
that can be easily detected with existing toxicity classifiers, we show that
LLMs can generate diverse implicit toxic outputs that are exceptionally
difficult to detect via simply zero-shot prompting. Moreover, we propose a
reinforcement learning (RL) based attacking method to further induce the
implicit toxicity in LLMs. Specifically, we optimize the language model with a
reward that prefers implicit toxic outputs to explicit toxic and non-toxic
ones. Experiments on five widely-adopted toxicity classifiers demonstrate that
the attack success rate can be significantly improved through RL fine-tuning.
For instance, the RL-finetuned LLaMA-13B model achieves an attack success rate
of 90.04% on BAD and 62.85% on Davinci003. Our findings suggest that LLMs pose
a significant threat in generating undetectable implicit toxic outputs. We
further show that fine-tuning toxicity classifiers on the annotated examples
from our attacking method can effectively enhance their ability to detect
LLM-generated implicit toxic language. The code is publicly available at
https://github.com/thu-coai/Implicit-Toxicity.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.16989v4' target='_blank'>ChatGPT's One-year Anniversary: Are Open-Source Large Language Models
  Catching up?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hailin Chen, Fangkai Jiao, Xingxuan Li, Chengwei Qin, Mathieu Ravaut, Ruochen Zhao, Caiming Xiong, Shafiq Joty</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-28 17:44:51</h6>
<p class='card-text'>Upon its release in late 2022, ChatGPT has brought a seismic shift in the
entire landscape of AI, both in research and commerce. Through
instruction-tuning a large language model (LLM) with supervised fine-tuning and
reinforcement learning from human feedback, it showed that a model could answer
human questions and follow instructions on a broad panel of tasks. Following
this success, interests in LLMs have intensified, with new LLMs flourishing at
frequent interval across academia and industry, including many start-ups
focused on LLMs. While closed-source LLMs (e.g., OpenAI's GPT, Anthropic's
Claude) generally outperform their open-source counterparts, the progress on
the latter has been rapid with claims of achieving parity or even better on
certain tasks. This has crucial implications not only on research but also on
business. In this work, on the first anniversary of ChatGPT, we provide an
exhaustive overview of this success, surveying all tasks where an open-source
LLM has claimed to be on par or better than ChatGPT.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.14543v1' target='_blank'>Data-Efficient Alignment of Large Language Models with Human Feedback
  Through Natural Language</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Di Jin, Shikib Mehri, Devamanyu Hazarika, Aishwarya Padmakumar, Sungjin Lee, Yang Liu, Mahdi Namazifar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-24 15:20:36</h6>
<p class='card-text'>Learning from human feedback is a prominent technique to align the output of
large language models (LLMs) with human expectations. Reinforcement learning
from human feedback (RLHF) leverages human preference signals that are in the
form of ranking of response pairs to perform this alignment. However, human
preference on LLM outputs can come in much richer forms including natural
language, which may provide detailed feedback on strengths and weaknesses of a
given response. In this work we investigate data efficiency of modeling human
feedback that is in natural language. Specifically, we fine-tune an open-source
LLM, e.g., Falcon-40B-Instruct, on a relatively small amount (1000 records or
even less) of human feedback in natural language in the form of critiques and
revisions of responses. We show that this model is able to improve the quality
of responses from even some of the strongest LLMs such as ChatGPT, BARD, and
Vicuna, through critique and revision of those responses. For instance, through
one iteration of revision of ChatGPT responses, the revised responses have
56.6% win rate over the original ones, and this win rate can be further
improved to 65.9% after applying the revision for five iterations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.14115v3' target='_blank'>A density estimation perspective on learning from pairwise human
  preferences</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Vincent Dumoulin, Daniel D. Johnson, Pablo Samuel Castro, Hugo Larochelle, Yann Dauphin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-23 17:20:36</h6>
<p class='card-text'>Learning from human feedback (LHF) -- and in particular learning from
pairwise preferences -- has recently become a crucial ingredient in training
large language models (LLMs), and has been the subject of much research. Most
recent works frame it as a reinforcement learning problem, where a reward
function is learned from pairwise preference data and the LLM is treated as a
policy which is adapted to maximize the rewards, often under additional
regularization constraints. We propose an alternative interpretation which
centers on the generative process for pairwise preferences and treats LHF as a
density estimation problem. We provide theoretical and empirical results
showing that for a family of generative processes defined via preference
behavior distribution equations, training a reward function on pairwise
preferences effectively models an annotator's implicit preference distribution.
Finally, we discuss and present findings on "annotator misspecification" --
failure cases where wrong modeling assumptions are made about annotator
behavior, resulting in poorly-adapted models -- suggesting that approaches that
learn from pairwise human preferences could have trouble learning from a
population of annotators with diverse viewpoints.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.14061v1' target='_blank'>Towards Explainable Strategy Templates using NLP Transformers</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pallavi Bagga, Kostas Stathis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-23 15:37:19</h6>
<p class='card-text'>This paper bridges the gap between mathematical heuristic strategies learned
from Deep Reinforcement Learning (DRL) in automated agent negotiation, and
comprehensible, natural language explanations. Our aim is to make these
strategies more accessible to non-experts. By leveraging traditional Natural
Language Processing (NLP) techniques and Large Language Models (LLMs) equipped
with Transformers, we outline how parts of DRL strategies composed of parts
within strategy templates can be transformed into user-friendly, human-like
English narratives. To achieve this, we present a top-level algorithm that
involves parsing mathematical expressions of strategy templates, semantically
interpreting variables and structures, generating rule-based primary
explanations, and utilizing a Generative Pre-trained Transformer (GPT) model to
refine and contextualize these explanations. Subsequent customization for
varied audiences and meticulous validation processes in an example illustrate
the applicability and potential of this approach.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.13373v6' target='_blank'>Large Language Model as a Policy Teacher for Training Reinforcement
  Learning Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zihao Zhou, Bin Hu, Chenyang Zhao, Pu Zhang, Bin Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-22 13:15:42</h6>
<p class='card-text'>Recent studies have uncovered the potential of Large Language Models (LLMs)
in addressing complex sequential decision-making tasks through the provision of
high-level instructions. However, LLM-based agents lack specialization in
tackling specific target problems, particularly in real-time dynamic
environments. Additionally, deploying an LLM-based agent in practical scenarios
can be both costly and time-consuming. On the other hand, reinforcement
learning (RL) approaches train agents that specialize in the target task but
often suffer from low sampling efficiency and high exploration costs. In this
paper, we introduce a novel framework that addresses these challenges by
training a smaller, specialized student RL agent using instructions from an
LLM-based teacher agent. By incorporating the guidance from the teacher agent,
the student agent can distill the prior knowledge of the LLM into its own
model. Consequently, the student agent can be trained with significantly less
data. Moreover, through further training with environment feedback, the student
agent surpasses the capabilities of its teacher for completing the target task.
We conducted experiments on challenging MiniGrid and Habitat environments,
specifically designed for embodied AI research, to evaluate the effectiveness
of our framework. The results clearly demonstrate that our approach achieves
superior performance compared to strong baseline methods. Our code is available
at https://github.com/ZJLAB-AMMI/LLM4Teach.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.13281v1' target='_blank'>Intention and Context Elicitation with Large Language Models in the
  Legal Aid Intake Process</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nick Goodson, Rongfei Lu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-22 10:04:29</h6>
<p class='card-text'>Large Language Models (LLMs) and chatbots show significant promise in
streamlining the legal intake process. This advancement can greatly reduce the
workload and costs for legal aid organizations, improving availability while
making legal assistance more accessible to a broader audience. However, a key
challenge with current LLMs is their tendency to overconfidently deliver an
immediate 'best guess' to a client's question based on the output distribution
learned over the training data. This approach often overlooks the client's
actual intentions or the specifics of their legal situation. As a result,
clients may not realize the importance of providing essential additional
context or expressing their underlying intentions, which are crucial for their
legal cases. Traditionally, logic based decision trees have been used to
automate intake for specific access to justice issues, such as immigration and
eviction. But those solutions lack scalability. We demonstrate a
proof-of-concept using LLMs to elicit and infer clients' underlying intentions
and specific legal circumstances through free-form, language-based
interactions. We also propose future research directions to use supervised
fine-tuning or offline reinforcement learning to automatically incorporate
intention and context elicitation in chatbots without explicit prompting.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.13095v1' target='_blank'>Enhancing Logical Reasoning in Large Language Models to Facilitate Legal
  Applications</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ha-Thanh Nguyen, Wachara Fungwacharakorn, Ken Satoh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-22 01:51:50</h6>
<p class='card-text'>Language serves as a vehicle for conveying thought, enabling communication
among individuals. The ability to distinguish between diverse concepts,
identify fairness and injustice, and comprehend a range of legal notions
fundamentally relies on logical reasoning. Large Language Models (LLMs) attempt
to emulate human language understanding and generation, but their competency in
logical reasoning remains limited. This paper seeks to address the
philosophical question: How can we effectively teach logical reasoning to LLMs
while maintaining a deep understanding of the intricate relationship between
language and logic? By focusing on bolstering LLMs' capabilities in logical
reasoning, we aim to expand their applicability in law and other
logic-intensive disciplines. To this end, we propose a Reinforcement Learning
from Logical Feedback (RLLF) approach, which serves as a potential framework
for refining LLMs' reasoning capacities. Through RLLF and a revised evaluation
methodology, we explore new avenues for research in this domain and contribute
to the development of LLMs capable of handling complex legal reasoning tasks
while acknowledging the fundamental connection between language and logic.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.14743v7' target='_blank'>A Baseline Analysis of Reward Models' Ability To Accurately Analyze
  Foundation Models Under Distribution Shift</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Will LeVine, Benjamin Pikus, Anthony Chen, Sean Hendryx</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-21 18:41:26</h6>
<p class='card-text'>Foundation models, specifically Large Language Models (LLMs), have lately
gained wide-spread attention and adoption. Reinforcement Learning with Human
Feedback (RLHF) involves training a reward model to capture desired behaviors,
which is then used to align LLM's. These reward models are additionally used at
inference-time to estimate LLM responses' adherence to those desired behaviors.
However, there is little work measuring how robust these reward models are to
distribution shifts. In this work, we evaluate how reward model performance -
measured via accuracy and calibration (i.e. alignment between accuracy and
confidence) - is affected by distribution shift. We show novel calibration
patterns and accuracy drops due to OOD prompts and responses, and that the
reward model is more sensitive to shifts in responses than prompts.
Additionally, we adapt an OOD detection technique commonly used in
classification to the reward model setting to detect these distribution shifts
in prompts and responses.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.12908v1' target='_blank'>Diffusion Model Alignment Using Direct Preference Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, Nikhil Naik</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-21 15:24:05</h6>
<p class='card-text'>Large language models (LLMs) are fine-tuned using human comparison data with
Reinforcement Learning from Human Feedback (RLHF) methods to make them better
aligned with users' preferences. In contrast to LLMs, human preference learning
has not been widely explored in text-to-image diffusion models; the best
existing approach is to fine-tune a pretrained model using carefully curated
high quality images and captions to improve visual appeal and text alignment.
We propose Diffusion-DPO, a method to align diffusion models to human
preferences by directly optimizing on human comparison data. Diffusion-DPO is
adapted from the recently developed Direct Preference Optimization (DPO), a
simpler alternative to RLHF which directly optimizes a policy that best
satisfies human preferences under a classification objective. We re-formulate
DPO to account for a diffusion model notion of likelihood, utilizing the
evidence lower bound to derive a differentiable objective. Using the Pick-a-Pic
dataset of 851K crowdsourced pairwise preferences, we fine-tune the base model
of the state-of-the-art Stable Diffusion XL (SDXL)-1.0 model with
Diffusion-DPO. Our fine-tuned base model significantly outperforms both base
SDXL-1.0 and the larger SDXL-1.0 model consisting of an additional refinement
model in human evaluation, improving visual appeal and prompt alignment. We
also develop a variant that uses AI feedback and has comparable performance to
training on human preferences, opening the door for scaling of diffusion model
alignment methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.11094v1' target='_blank'>User-Centric Interactive AI for Distributed Diffusion Model-based
  AI-Generated Content</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hongyang Du, Ruichen Zhang, Dusit Niyato, Jiawen Kang, Zehui Xiong, Shuguang Cui, Xuemin Shen, Dong In Kim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-18 14:49:04</h6>
<p class='card-text'>Distributed Artificial Intelligence-Generated Content (AIGC) has attracted
increasing attention. However, it faces two significant challenges: how to
maximize the subjective Quality of Experience (QoE) and how to enhance the
energy efficiency, which are particularly pronounced in widely adopted
Generative Diffusion Model (GDM)-based AIGC services for image generation. In
this paper, we propose a novel user-centric Interactive AI (IAI) approach for
service management, with a distributed GDM-based AIGC framework, prioritizing
efficient and collaborative GDM deployment. Specifically, we restructure the
GDM's inference process, i.e., the denoising chain, to enable users'
semantically similar prompts to share a portion of diffusion steps.
Furthermore, to maximize the users' subjective QoE, we propose an IAI approach,
i.e., Reinforcement Learning With Large Language Models Interaction (RLLI),
which utilizes Large Language Model (LLM)-empowered generative agents to
replicate users interaction, providing real-time and subjective QoE feedback
that reflects a spectrum of user personalities. Lastly, we present the
GDM-based Deep Deterministic Policy Gradient (G-DDPG) algorithm, adapted to the
proposed RLLI framework, for effective communication and computing resource
allocation while considering user subjective personalities and dynamic wireless
environments in decision-making. Simulation results show that G-DDPG can
increase the sum QoE by 15%, compared with the conventional DDPG algorithm.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.09641v2' target='_blank'>RLHFPoison: Reward Poisoning Attack for Reinforcement Learning with
  Human Feedback in Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiongxiao Wang, Junlin Wu, Muhao Chen, Yevgeniy Vorobeychik, Chaowei Xiao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-16 07:48:45</h6>
<p class='card-text'>Reinforcement Learning with Human Feedback (RLHF) is a methodology designed
to align Large Language Models (LLMs) with human preferences, playing an
important role in LLMs alignment. Despite its advantages, RLHF relies on human
annotators to rank the text, which can introduce potential security
vulnerabilities if any adversarial annotator (i.e., attackers) manipulates the
ranking score by up-ranking any malicious text to steer the LLM adversarially.
To assess the red-teaming of RLHF against human preference data poisoning, we
propose RankPoison, a poisoning attack method on candidates' selection of
preference rank flipping to reach certain malicious behaviors (e.g., generating
longer sequences, which can increase the computational cost). With poisoned
dataset generated by RankPoison, we can perform poisoning attacks on LLMs to
generate longer tokens without hurting the original safety alignment
performance. Moreover, applying RankPoison, we also successfully implement a
backdoor attack where LLMs can generate longer answers under questions with the
trigger word. Our findings highlight critical security challenges in RLHF,
underscoring the necessity for more robust alignment methods for LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.09132v2' target='_blank'>Aligning Neural Machine Translation Models: Human Feedback in Training
  and Inference</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Miguel Moura Ramos, Patrick Fernandes, António Farinhas, André F. T. Martins</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-15 17:21:58</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) is a recent technique to
improve the quality of the text generated by a language model, making it closer
to what humans would generate. A core ingredient in RLHF's success in aligning
and improving large language models (LLMs) is its reward model, trained using
human feedback on model outputs. In machine translation (MT), where metrics
trained from human annotations can readily be used as reward models, recent
methods using minimum Bayes risk decoding and reranking have succeeded in
improving the final quality of translation. In this study, we comprehensively
explore and compare techniques for integrating quality metrics as reward models
into the MT pipeline. This includes using the reward model for data filtering,
during the training phase through RL, and at inference time by employing
reranking techniques, and we assess the effects of combining these in a unified
approach. Our experimental results, conducted across multiple translation
tasks, underscore the crucial role of effective data filtering, based on
estimated quality, in harnessing the full potential of RL in enhancing MT
quality. Furthermore, our findings demonstrate the effectiveness of combining
RL training with reranking techniques, showcasing substantial improvements in
translation quality.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.08244v2' target='_blank'>Language and Sketching: An LLM-driven Interactive Multimodal Multitask
  Robot Navigation Framework</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weiqin Zu, Wenbin Song, Ruiqing Chen, Ze Guo, Fanglei Sun, Zheng Tian, Wei Pan, Jun Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-14 15:29:52</h6>
<p class='card-text'>The socially-aware navigation system has evolved to adeptly avoid various
obstacles while performing multiple tasks, such as point-to-point navigation,
human-following, and -guiding. However, a prominent gap persists: in
Human-Robot Interaction (HRI), the procedure of communicating commands to
robots demands intricate mathematical formulations. Furthermore, the transition
between tasks does not quite possess the intuitive control and user-centric
interactivity that one would desire. In this work, we propose an LLM-driven
interactive multimodal multitask robot navigation framework, termed LIM2N, to
solve the above new challenge in the navigation field. We achieve this by first
introducing a multimodal interaction framework where language and hand-drawn
inputs can serve as navigation constraints and control objectives. Next, a
reinforcement learning agent is built to handle multiple tasks with the
received information. Crucially, LIM2N creates smooth cooperation among the
reasoning of multimodal input, multitask planning, and adaptation and
processing of the intelligent sensing modules in the complicated system.
Extensive experiments are conducted in both simulation and the real world
demonstrating that LIM2N has superior user needs understanding, alongside an
enhanced interactive experience.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.05596v1' target='_blank'>LLM Augmented Hierarchical Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bharat Prakash, Tim Oates, Tinoosh Mohsenin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-09 18:54:28</h6>
<p class='card-text'>Solving long-horizon, temporally-extended tasks using Reinforcement Learning
(RL) is challenging, compounded by the common practice of learning without
prior knowledge (or tabula rasa learning). Humans can generate and execute
plans with temporally-extended actions and quickly learn to perform new tasks
because we almost never solve problems from scratch. We want autonomous agents
to have this same ability. Recently, LLMs have been shown to encode a
tremendous amount of knowledge about the world and to perform impressive
in-context learning and reasoning. However, using LLMs to solve real world
problems is hard because they are not grounded in the current task. In this
paper we exploit the planning capabilities of LLMs while using RL to provide
learning from the environment, resulting in a hierarchical agent that uses LLMs
to solve long-horizon tasks. Instead of completely relying on LLMs, they guide
a high-level policy, making learning significantly more sample efficient. This
approach is evaluated in simulation environments such as MiniGrid, SkillHack,
and Crafter, and on a real robot arm in block manipulation tasks. We show that
agents trained using our approach outperform other baselines methods and, once
trained, don't need access to LLMs during deployment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.05584v1' target='_blank'>Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Joey Hong, Sergey Levine, Anca Dragan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-09 18:45:16</h6>
<p class='card-text'>Large language models (LLMs) have emerged as powerful and general solutions
to many natural language tasks. However, many of the most important
applications of language generation are interactive, where an agent has to talk
to a person to reach a desired outcome. For example, a teacher might try to
understand their student's current comprehension level to tailor their
instruction accordingly, and a travel agent might ask questions of their
customer to understand their preferences in order to recommend activities they
might enjoy. LLMs trained with supervised fine-tuning or "single-step" RL, as
with standard RLHF, might struggle which tasks that require such goal-directed
behavior, since they are not trained to optimize for overall conversational
outcomes after multiple turns of interaction. In this work, we explore a new
method for adapting LLMs with RL for such goal-directed dialogue. Our key
insight is that, though LLMs might not effectively solve goal-directed dialogue
tasks out of the box, they can provide useful data for solving such tasks by
simulating suboptimal but human-like behaviors. Given a textual description of
a goal-directed dialogue task, we leverage LLMs to sample diverse synthetic
rollouts of hypothetical in-domain human-human interactions. Our algorithm then
utilizes this dataset with offline reinforcement learning to train an
interactive conversational agent that can optimize goal-directed objectives
over multiple turns. In effect, the LLM produces examples of possible
interactions, and RL then processes these examples to learn to perform more
optimal interactions. Empirically, we show that our proposed approach achieves
state-of-the-art performance in various goal-directed dialogue tasks that
include teaching and preference elicitation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.05553v3' target='_blank'>Removing RLHF Protections in GPT-4 via Fine-Tuning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qiusi Zhan, Richard Fang, Rohan Bindu, Akul Gupta, Tatsunori Hashimoto, Daniel Kang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-09 17:54:59</h6>
<p class='card-text'>As large language models (LLMs) have increased in their capabilities, so does
their potential for dual use. To reduce harmful outputs, produces and vendors
of LLMs have used reinforcement learning with human feedback (RLHF). In tandem,
LLM vendors have been increasingly enabling fine-tuning of their most powerful
models. However, concurrent work has shown that fine-tuning can remove RLHF
protections. We may expect that the most powerful models currently available
(GPT-4) are less susceptible to fine-tuning attacks. In this work, we show the
contrary: fine-tuning allows attackers to remove RLHF protections with as few
as 340 examples and a 95% success rate. These training examples can be
automatically generated with weaker models. We further show that removing RLHF
protections does not decrease usefulness on non-censored outputs, providing
evidence that our fine-tuning strategy does not decrease usefulness despite
using weaker models to generate training data. Our results show the need for
further research on protections on LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.04072v2' target='_blank'>Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Geyang Guo, Ranchi Zhao, Tianyi Tang, Wayne Xin Zhao, Ji-Rong Wen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-07 15:36:40</h6>
<p class='card-text'>Alignment with human preference is a desired property of large language
models (LLMs). Currently, the main alignment approach is based on reinforcement
learning from human feedback (RLHF). Despite the effectiveness of RLHF, it is
intricate to implement and train, thus recent studies explore how to develop
alternative alignment approaches based on supervised fine-tuning (SFT). A major
limitation of SFT is that it essentially does imitation learning, which cannot
fully understand what are the expected behaviors. To address this issue, we
propose an improved alignment approach named FIGA. Different from prior
methods, we incorporate fine-grained (i.e., token or phrase level) quality
signals that are derived by contrasting good and bad responses. Our approach
has made two major contributions. Firstly, we curate a refined alignment
dataset that pairs initial responses and the corresponding revised ones.
Secondly, we devise a new loss function can leverage fine-grained quality
signals to instruct the learning of LLMs for alignment. Extensive experiments
have demonstrated the effectiveness of our approaches by comparing a number of
competitive baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.04046v1' target='_blank'>Reinforcement Learning Fine-tuning of Language Models is Biased Towards
  More Extractable Features</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Diogo Cruz, Edoardo Pona, Alex Holness-Tofts, Elias Schmied, Víctor Abia Alonso, Charlie Griffin, Bogdan-Ionut Cirstea</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-07 15:00:39</h6>
<p class='card-text'>Many capable large language models (LLMs) are developed via self-supervised
pre-training followed by a reinforcement-learning fine-tuning phase, often
based on human or AI feedback. During this stage, models may be guided by their
inductive biases to rely on simpler features which may be easier to extract, at
a cost to robustness and generalisation. We investigate whether principles
governing inductive biases in the supervised fine-tuning of LLMs also apply
when the fine-tuning process uses reinforcement learning. Following Lovering et
al (2021), we test two hypotheses: that features more $\textit{extractable}$
after pre-training are more likely to be utilised by the final policy, and that
the evidence for/against a feature predicts whether it will be utilised.
Through controlled experiments on synthetic and natural language tasks, we find
statistically significant correlations which constitute strong evidence for
these hypotheses.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.04254v3' target='_blank'>Everything of Thoughts: Defying the Law of Penrose Triangle for Thought
  Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan, Qingwei Lin, Dongmei Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-07 12:30:36</h6>
<p class='card-text'>Recent advancements in Large Language Models (LLMs) have revolutionized
decision-making by breaking down complex problems into more manageable language
sequences referred to as "thoughts". An effective thought design should
consider three key perspectives: performance, efficiency, and flexibility.
However, existing thought can at most exhibit two of these attributes. To
address these limitations, we introduce a novel thought prompting approach
called "Everything of Thoughts" (XoT) to defy the law of "Penrose triangle of
existing thought paradigms. XoT leverages pretrained reinforcement learning and
Monte Carlo Tree Search (MCTS) to incorporate external domain knowledge into
thoughts, thereby enhancing LLMs' capabilities and enabling them to generalize
to unseen problems efficiently. Through the utilization of the MCTS-LLM
collaborative thought revision framework, this approach autonomously produces
high-quality comprehensive cognitive mappings with minimal LLM interactions.
Additionally, XoT empowers LLMs to engage in unconstrained thinking, allowing
for flexible cognitive mappings for problems with multiple solutions. We
evaluate XoT on several challenging multi-solution problem-solving tasks,
including Game of 24, 8-Puzzle, and Pocket Cube. Our results demonstrate that
XoT significantly outperforms existing approaches. Notably, XoT can yield
multiple solutions with just one LLM call, showcasing its remarkable
proficiency in addressing complex problems across diverse domains.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.02847v4' target='_blank'>Kinematic-aware Prompting for Generalizable Articulated Object
  Manipulation with LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenke Xia, Dong Wang, Xincheng Pang, Zhigang Wang, Bin Zhao, Di Hu, Xuelong Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-06 03:26:41</h6>
<p class='card-text'>Generalizable articulated object manipulation is essential for home-assistant
robots. Recent efforts focus on imitation learning from demonstrations or
reinforcement learning in simulation, however, due to the prohibitive costs of
real-world data collection and precise object simulation, it still remains
challenging for these works to achieve broad adaptability across diverse
articulated objects. Recently, many works have tried to utilize the strong
in-context learning ability of Large Language Models (LLMs) to achieve
generalizable robotic manipulation, but most of these researches focus on
high-level task planning, sidelining low-level robotic control. In this work,
building on the idea that the kinematic structure of the object determines how
we can manipulate it, we propose a kinematic-aware prompting framework that
prompts LLMs with kinematic knowledge of objects to generate low-level motion
trajectory waypoints, supporting various object manipulation. To effectively
prompt LLMs with the kinematic structure of different objects, we design a
unified kinematic knowledge parser, which represents various articulated
objects as a unified textual description containing kinematic joints and
contact location. Building upon this unified description, a kinematic-aware
planner model is proposed to generate precise 3D manipulation waypoints via a
designed kinematic-aware chain-of-thoughts prompting method. Our evaluation
spanned 48 instances across 16 distinct categories, revealing that our
framework not only outperforms traditional methods on 8 seen categories but
also shows a powerful zero-shot capability for 8 unseen articulated object
categories. Moreover, the real-world experiments on 7 different object
categories prove our framework's adaptability in practical scenarios. Code is
released at
https://github.com/GeWu-Lab/LLM_articulated_object_manipulation/tree/main.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.02379v1' target='_blank'>Accelerating Reinforcement Learning of Robotic Manipulations via
  Feedback from Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kun Chu, Xufeng Zhao, Cornelius Weber, Mengdi Li, Stefan Wermter</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-04 11:21:38</h6>
<p class='card-text'>Reinforcement Learning (RL) plays an important role in the robotic
manipulation domain since it allows self-learning from trial-and-error
interactions with the environment. Still, sample efficiency and reward
specification seriously limit its potential. One possible solution involves
learning from expert guidance. However, obtaining a human expert is impractical
due to the high cost of supervising an RL agent, and developing an automatic
supervisor is a challenging endeavor. Large Language Models (LLMs) demonstrate
remarkable abilities to provide human-like feedback on user inputs in natural
language. Nevertheless, they are not designed to directly control low-level
robotic motions, as their pretraining is based on vast internet data rather
than specific robotics data. In this paper, we introduce the Lafite-RL
(Language agent feedback interactive Reinforcement Learning) framework, which
enables RL agents to learn robotic tasks efficiently by taking advantage of
LLMs' timely feedback. Our experiments conducted on RLBench tasks illustrate
that, with simple prompt design in natural language, the Lafite-RL agent
exhibits improved learning capabilities when guided by an LLM. It outperforms
the baseline in terms of both learning efficiency and success rate,
underscoring the efficacy of the rewards provided by an LLM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.02268v1' target='_blank'>LLMs-augmented Contextual Bandit</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ali Baheri, Cecilia O. Alm</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-03 23:12:57</h6>
<p class='card-text'>Contextual bandits have emerged as a cornerstone in reinforcement learning,
enabling systems to make decisions with partial feedback. However, as contexts
grow in complexity, traditional bandit algorithms can face challenges in
adequately capturing and utilizing such contexts. In this paper, we propose a
novel integration of large language models (LLMs) with the contextual bandit
framework. By leveraging LLMs as an encoder, we enrich the representation of
the context, providing the bandit with a denser and more informative view.
Preliminary results on synthetic datasets demonstrate the potential of this
approach, showing notable improvements in cumulative rewards and reductions in
regret compared to traditional bandit algorithms. This integration not only
showcases the capabilities of LLMs in reinforcement learning but also opens the
door to a new era of contextually-aware decision systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.00262v2' target='_blank'>Plug-and-Play Policy Planner for Large Language Model Powered Dialogue
  Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yang Deng, Wenxuan Zhang, Wai Lam, See-Kiong Ng, Tat-Seng Chua</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-01 03:20:16</h6>
<p class='card-text'>Proactive dialogues serve as a practical yet challenging dialogue problem in
the era of large language models (LLMs), where the dialogue policy planning is
the key to improving the proactivity of LLMs. Most existing studies enable the
dialogue policy planning of LLMs using various prompting schemes or iteratively
enhance this capability in handling the given case with verbal AI feedback.
However, these approaches are either bounded by the policy planning capability
of the frozen LLMs or hard to be transferred to new cases. In this work, we
introduce a new dialogue policy planning paradigm to strategize LLMs for
proactive dialogue problems with a tunable language model plug-in as a
plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a
novel training framework to facilitate supervised fine-tuning over available
human-annotated data as well as reinforcement learning from goal-oriented AI
feedback with dynamic interaction data collected by the LLM-based self-play
simulation. In this manner, the LLM-powered dialogue agent can not only be
generalized to different cases after the training, but also be applicable to
different applications by just substituting the learned plug-in. In addition,
we propose to evaluate the policy planning capability of dialogue systems under
the interactive setting. Experimental results demonstrate that PPDPP
consistently and substantially outperforms existing approaches on three
different proactive dialogue applications, including negotiation, emotional
support, and tutoring dialogues.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.00168v2' target='_blank'>The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from
  Human Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nathan Lambert, Roberto Calandra</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-31 21:52:41</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) has emerged as a powerful
technique to make large language models (LLMs) more capable in complex
settings. RLHF proceeds as collecting human preference data, training a reward
model on said data, and optimizing a base ML model with respect to said reward
for extrinsic evaluation metrics (e.g. MMLU, GSM8k). RLHF relies on many
assumptions about how the various pieces fit together, such as a reward model
capturing human preferences and an RL optimizer extracting the right signal
from a reward model. As the RLHF process involves many distinct design
decisions, it is easy to assume that multiple processes are correlated and
therefore numerically linked. This apparent correlation is often not true,
where reward models are easily overoptimized or RL optimizers can reduce
performance on tasks not modeled in the data. Notable manifestations of models
trained with imperfect RLHF systems are those that are prone to refusing basic
requests for safety reasons or appearing lazy in generations. As chat model
evaluation becomes increasingly nuanced, the reliance on a perceived link
between reward model training, RL scores, and downstream performance drives
these issues, which we describe as an objective mismatch. In this paper, we
illustrate the causes of this issue, reviewing relevant literature from
model-based reinforcement learning, and argue for solutions. By solving
objective mismatch in RLHF, the ML models of the future will be more precisely
aligned to user instructions for both safety and helpfulness.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.20587v5' target='_blank'>Unleashing the Power of Pre-trained Language Models for Offline
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ruizhe Shi, Yuyao Liu, Yanjie Ze, Simon S. Du, Huazhe Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-31 16:24:17</h6>
<p class='card-text'>Offline reinforcement learning (RL) aims to find a near-optimal policy using
pre-collected datasets. In real-world scenarios, data collection could be
costly and risky; therefore, offline RL becomes particularly challenging when
the in-domain data is limited. Given recent advances in Large Language Models
(LLMs) and their few-shot learning prowess, this paper introduces
$\textbf{La}$nguage Models for $\textbf{Mo}$tion Control ($\textbf{LaMo}$), a
general framework based on Decision Transformers to effectively use pre-trained
Language Models (LMs) for offline RL. Our framework highlights four crucial
components: (1) Initializing Decision Transformers with sequentially
pre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to
full-weight fine-tuning, to combine the pre-trained knowledge from LMs and
in-domain knowledge effectively, (3) using the non-linear MLP transformation
instead of linear projections, to generate embeddings, and (4) integrating an
auxiliary language prediction loss during fine-tuning to stabilize the LMs and
retain their original abilities on languages. Empirical results indicate
$\textbf{LaMo}$ achieves excellent performance in sparse-reward tasks and
closes the gap between value-based offline RL methods and decision transformers
in dense-reward tasks. In particular, our method demonstrates superior
performance in scenarios with limited data samples.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.01468v1' target='_blank'>Remember what you did so you know what to do next</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Manuel R. Ciosici, Alex Hedges, Yash Kankanampati, Justin Martin, Marjorie Freedman, Ralph Weischedel</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-30 19:29:00</h6>
<p class='card-text'>We explore using a moderately sized large language model (GPT-J 6B
parameters) to create a plan for a simulated robot to achieve 30 classes of
goals in ScienceWorld, a text game simulator for elementary science
experiments. Previously published empirical work claimed that large language
models (LLMs) are a poor fit (Wang et al., 2022) compared to reinforcement
learning. Using the Markov assumption (a single previous step), the LLM
outperforms the reinforcement learning-based approach by a factor of 1.4. When
we fill the LLM's input buffer with as many prior steps as possible,
improvement rises to 3.5x. Even when training on only 6.5% of the training
data, we observe a 2.2x improvement over the reinforcement-learning-based
approach. Our experiments show that performance varies widely across the 30
classes of actions, indicating that averaging over tasks can hide significant
performance issues. In work contemporaneous with ours, Lin et al. (2023)
demonstrated a two-part approach (SwiftSage) that uses a small LLM (T5-large)
complemented by OpenAI's massive LLMs to achieve outstanding results in
ScienceWorld. Our 6-B parameter, single-stage GPT-J matches the performance of
SwiftSage's two-stage architecture when it incorporates GPT-3.5 turbo which has
29-times more parameters than GPT-J.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.19080v2' target='_blank'>Reward Finetuning for Faster and More Accurate Unsupervised Object
  Discovery</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Katie Z Luo, Zhenzhen Liu, Xiangyu Chen, Yurong You, Sagie Benaim, Cheng Perng Phoo, Mark Campbell, Wen Sun, Bharath Hariharan, Kilian Q. Weinberger</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-29 17:03:12</h6>
<p class='card-text'>Recent advances in machine learning have shown that Reinforcement Learning
from Human Feedback (RLHF) can improve machine learning models and align them
with human preferences. Although very successful for Large Language Models
(LLMs), these advancements have not had a comparable impact in research for
autonomous vehicles -- where alignment with human expectations can be
imperative. In this paper, we propose to adapt similar RL-based methods to
unsupervised object discovery, i.e. learning to detect objects from LiDAR
points without any training labels. Instead of labels, we use simple heuristics
to mimic human feedback. More explicitly, we combine multiple heuristics into a
simple reward function that positively correlates its score with bounding box
accuracy, i.e., boxes containing objects are scored higher than those without.
We start from the detector's own predictions to explore the space and reinforce
boxes with high rewards through gradient updates. Empirically, we demonstrate
that our approach is not only more accurate, but also orders of magnitudes
faster to train compared to prior works on object discovery.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.18940v3' target='_blank'>Language Agents with Reinforcement Learning for Strategic Play in the
  Werewolf Game</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zelai Xu, Chao Yu, Fei Fang, Yu Wang, Yi Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-29 09:02:57</h6>
<p class='card-text'>Agents built with large language models (LLMs) have shown great potential
across a wide range of domains. However, in complex decision-making tasks, pure
LLM-based agents tend to exhibit intrinsic bias in their choice of actions,
which is inherited from the model's training data and results in suboptimal
performance. To develop strategic language agents, i.e., agents that generate
flexible language actions and possess strong decision-making abilities, we
propose a novel framework that powers LLM-based agents with reinforcement
learning (RL). We consider Werewolf, a popular social deduction game, as a
challenging testbed that emphasizes versatile communication and strategic
gameplay. To mitigate the intrinsic bias in language actions, our agents use an
LLM to perform deductive reasoning and generate a diverse set of action
candidates. Then an RL policy trained to optimize the decision-making ability
chooses an action from the candidates to play in the game. Extensive
experiments show that our agents overcome the intrinsic bias and outperform
existing LLM-based agents in the Werewolf game. We also conduct human-agent
experiments and find that our agents achieve human-level performance and
demonstrate strong strategic play.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.18313v2' target='_blank'>FP8-LM: Training FP8 Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Houwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao, Yuxiang Yang, Ze Liu, Yifan Xiong, Ziyue Yang, Bolin Ni, Jingcheng Hu, Ruihang Li, Miaosen Zhang, Chen Li, Jia Ning, Ruizhe Wang, Zheng Zhang, Shuguang Liu, Joe Chau, Han Hu, Peng Cheng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-27 17:59:51</h6>
<p class='card-text'>In this paper, we explore FP8 low-bit data formats for efficient training of
large language models (LLMs). Our key insight is that most variables, such as
gradients and optimizer states, in LLM training can employ low-precision data
formats without compromising model accuracy and requiring no changes to
hyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision
framework for training LLMs. This framework offers three levels of FP8
utilization to streamline mixed-precision and distributed parallel training for
LLMs. It gradually incorporates 8-bit gradients, optimizer states, and
distributed learning in an incremental manner. Experiment results show that,
during the training of GPT-175B model on H100 GPU platform, our FP8
mixed-precision training framework not only achieved a remarkable 39% reduction
in real memory usage but also ran 75% faster than the widely adopted BF16
framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer
Engine by 37%. This largely reduces the training costs for large foundation
models. Furthermore, our FP8 mixed-precision training methodology is generic.
It can be seamlessly applied to other tasks such as LLM instruction tuning and
reinforcement learning with human feedback, offering savings in fine-tuning
expenses. Our FP8 low-precision training framework is open-sourced at
{https://github.com/Azure/MS-AMP}{aka.ms/MS.AMP}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.18308v1' target='_blank'>Gen2Sim: Scaling up Robot Learning in Simulation with Generative Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pushkal Katara, Zhou Xian, Katerina Fragkiadaki</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-27 17:55:32</h6>
<p class='card-text'>Generalist robot manipulators need to learn a wide variety of manipulation
skills across diverse environments. Current robot training pipelines rely on
humans to provide kinesthetic demonstrations or to program simulation
environments and to code up reward functions for reinforcement learning. Such
human involvement is an important bottleneck towards scaling up robot learning
across diverse tasks and environments. We propose Generation to Simulation
(Gen2Sim), a method for scaling up robot skill learning in simulation by
automating generation of 3D assets, task descriptions, task decompositions and
reward functions using large pre-trained generative models of language and
vision. We generate 3D assets for simulation by lifting open-world 2D
object-centric images to 3D using image diffusion models and querying LLMs to
determine plausible physics parameters. Given URDF files of generated and
human-developed assets, we chain-of-thought prompt LLMs to map these to
relevant task descriptions, temporal decompositions, and corresponding python
reward functions for reinforcement learning. We show Gen2Sim succeeds in
learning policies for diverse long horizon tasks, where reinforcement learning
with non temporally decomposed reward functions fails. Gen2Sim provides a
viable path for scaling up reinforcement learning for robot manipulators in
simulation, both by diversifying and expanding task and environment
development, and by facilitating the discovery of reinforcement-learned
behaviors through temporal task decomposition in RL. Our work contributes
hundreds of simulated assets, tasks and demonstrations, taking a step towards
fully autonomous robotic manipulation skill acquisition in simulation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.18127v2' target='_blank'>Ask more, know better: Reinforce-Learned Prompt Questions for Decision
  Making with Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xue Yan, Yan Song, Xinyu Cui, Filippos Christianos, Haifeng Zhang, David Henry Mguni, Jun Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-27 13:19:19</h6>
<p class='card-text'>Large language models (LLMs) demonstrate their promise in tackling
complicated practical challenges by combining action-based policies with chain
of thought (CoT) reasoning. Having high-quality prompts on hand, however, is
vital to the framework's effectiveness. Currently, these prompts are
handcrafted utilising extensive human labor, resulting in CoT policies that
frequently fail to generalise. Human intervention is also required to develop
grounding functions that ensure low-level controllers appropriately process CoT
reasoning. In this paper, we propose a comprehensive training framework for
complex task-solving, incorporating human prior knowledge into the learning of
action policies. To that purpose, we offer a new leader-follower bilevel
framework that is capable of learning to ask relevant questions (prompts) and
subsequently undertaking reasoning to guide the learning of actions. The prompt
policy is employed to make introspective revisions based on historical
findings, leading the CoT process to consider the anticipated goals and
generate outputs that lead to decisive, high-performing actions. The action
policy subsequently learns to comprehend and integrate the CoT outputs to take
actions. Our empirical data reveal that our framework outperforms leading
methods in $5$ decision-making tasks such as Overcooked and FourRoom.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.17722v2' target='_blank'>Large Language Models as Generalizable Policies for Embodied Tasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure, Walter Talbott, Katherine Metcalf, Natalie Mackraz, Devon Hjelm, Alexander Toshev</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-26 18:32:05</h6>
<p class='card-text'>We show that large language models (LLMs) can be adapted to be generalizable
policies for embodied visual tasks. Our approach, called Large LAnguage model
Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take
as input text instructions and visual egocentric observations and output
actions directly in the environment. Using reinforcement learning, we train
LLaRP to see and act solely through environmental interactions. We show that
LLaRP is robust to complex paraphrasings of task instructions and can
generalize to new tasks that require novel optimal behavior. In particular, on
1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other
common learned baselines or zero-shot applications of LLMs. Finally, to aid the
community in studying language conditioned, massively multi-task, embodied AI
problems we release a novel benchmark, Language Rearrangement, consisting of
150,000 training and 1,000 testing tasks for language-conditioned
rearrangement. Video examples of LLaRP in unseen Language Rearrangement
instructions are at https://llm-rl.github.io.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.17019v1' target='_blank'>Conditionally Combining Robot Skills using Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:K. R. Zentner, Ryan Julian, Brian Ichter, Gaurav S. Sukhatme</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-25 21:46:34</h6>
<p class='card-text'>This paper combines two contributions. First, we introduce an extension of
the Meta-World benchmark, which we call "Language-World," which allows a large
language model to operate in a simulated robotic environment using
semi-structured natural language queries and scripted skills described using
natural language. By using the same set of tasks as Meta-World, Language-World
results can be easily compared to Meta-World results, allowing for a point of
comparison between recent methods using Large Language Models (LLMs) and those
using Deep Reinforcement Learning. Second, we introduce a method we call Plan
Conditioned Behavioral Cloning (PCBC), that allows finetuning the behavior of
high-level plans using end-to-end demonstrations. Using Language-World, we show
that PCBC is able to achieve strong performance in a variety of few-shot
regimes, often achieving task generalization with as little as a single
demonstration. We have made Language-World available as open-source software at
https://github.com/krzentner/language-world/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.16960v2' target='_blank'>Privately Aligning Language Models with Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fan Wu, Huseyin A. Inan, Arturs Backurs, Varun Chandrasekaran, Janardhan Kulkarni, Robert Sim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-25 19:58:51</h6>
<p class='card-text'>Positioned between pre-training and user deployment, aligning large language
models (LLMs) through reinforcement learning (RL) has emerged as a prevailing
strategy for training instruction following-models such as ChatGPT. In this
work, we initiate the study of privacy-preserving alignment of LLMs through
Differential Privacy (DP) in conjunction with RL. Following the influential
work of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment
via RL without human in the loop (e.g., positive review generation) and (ii)
alignment via RL from human feedback (RLHF) (e.g., summarization in a
human-preferred way). We give a new DP framework to achieve alignment via RL,
and prove its correctness. Our experimental results validate the effectiveness
of our approach, offering competitive utility while ensuring strong privacy
protections.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.16355v3' target='_blank'>RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on
  Any GPU/TPUs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric Xing, Zhiting Hu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-25 04:32:35</h6>
<p class='card-text'>The recent progress of AI can be largely attributed to large language models
(LLMs). However, their escalating memory requirements introduce challenges for
machine learning (ML) researchers and engineers. Addressing this requires
developers to partition a large model to distribute it across multiple GPUs or
TPUs. This necessitates considerable coding and intricate configuration efforts
with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa.
These tools require users' expertise in machine learning systems (MLSys),
creating a bottleneck in LLM development, particularly for developers without
MLSys background. In this work, we present RedCoast (Redco), a lightweight and
user-friendly tool crafted to automate distributed training and inference for
LLMs, as well as to simplify ML pipeline development. The design of Redco
emphasizes two key aspects. Firstly, to automate model parallelism, our study
identifies two straightforward rules to generate tensor parallel strategies for
any given LLM. Integrating these rules into Redco facilitates effortless
distributed LLM training and inference, eliminating the need of additional
coding or complex configurations. We demonstrate the effectiveness by applying
Redco on a set of LLM architectures, such as GPT-J, LLaMA, T5, and OPT, up to
the size of 66B. Secondly, we propose a mechanism that allows for the
customization of diverse ML pipelines through the definition of merely three
functions, avoiding redundant and formulaic code like multi-host related
processing. This mechanism proves adaptable across a spectrum of ML algorithms,
from foundational language modeling to complex algorithms like meta-learning
and reinforcement learning. As a result, Redco implementations exhibit
significantly fewer lines of code compared to their official counterparts.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.16271v1' target='_blank'>CycleAlign: Iterative Distillation from Black-box LLM to White-box
  Models for Better Human Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jixiang Hong, Quan Tu, Changyu Chen, Xing Gao, Ji Zhang, Rui Yan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-25 01:05:03</h6>
<p class='card-text'>Language models trained on large-scale corpus often generate content that is
harmful, toxic, or contrary to human preferences, making their alignment with
human values a critical concern. Reinforcement learning from human feedback
(RLHF) with algorithms like PPO is a prevalent approach for alignment but is
often complex, unstable, and resource-intensive. Recently, ranking-based
alignment methods have emerged, offering stability and effectiveness by
replacing the RL framework with supervised fine-tuning, but they are costly due
to the need for annotated data. Considering that existing large language models
(LLMs) like ChatGPT are already relatively well-aligned and cost-friendly,
researchers have begun to align the language model with human preference from
AI feedback. The common practices, which unidirectionally distill the
instruction-following responses from LLMs, are constrained by their bottleneck.
Thus we introduce CycleAlign to distill alignment capabilities from
parameter-invisible LLMs (black-box) to a parameter-visible model (white-box)
in an iterative manner. With in-context learning (ICL) as the core of the
cycle, the black-box models are able to rank the model-generated responses
guided by human-craft instruction and demonstrations about their preferences.
During iterative interaction, the white-box models also have a judgment about
responses generated by them. Consequently, the agreement ranking could be
viewed as a pseudo label to dynamically update the in-context demonstrations
and improve the preference ranking ability of black-box models. Through
multiple interactions, the CycleAlign framework could align the white-box model
with the black-box model effectively in a low-resource way. Empirical results
illustrate that the model fine-tuned by CycleAlign remarkably exceeds existing
methods, and achieves the state-of-the-art performance in alignment with human
value.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.16048v1' target='_blank'>AI Alignment and Social Choice: Fundamental Limitations and Policy
  Implications</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Abhilash Mishra</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-24 17:59:04</h6>
<p class='card-text'>Aligning AI agents to human intentions and values is a key bottleneck in
building safe and deployable AI applications. But whose values should AI agents
be aligned with? Reinforcement learning with human feedback (RLHF) has emerged
as the key framework for AI alignment. RLHF uses feedback from human
reinforcers to fine-tune outputs; all widely deployed large language models
(LLMs) use RLHF to align their outputs to human values. It is critical to
understand the limitations of RLHF and consider policy challenges arising from
these limitations. In this paper, we investigate a specific challenge in
building RLHF systems that respect democratic norms. Building on impossibility
results in social choice theory, we show that, under fairly broad assumptions,
there is no unique voting protocol to universally align AI systems using RLHF
through democratic processes. Further, we show that aligning AI agents with the
values of all individuals will always violate certain private ethical
preferences of an individual user i.e., universal AI alignment using RLHF is
impossible. We discuss policy implications for the governance of AI systems
built using RLHF: first, the need for mandating transparent voting rules to
hold model builders accountable. Second, the need for model builders to focus
on developing AI agents that are narrowly aligned to specific user groups.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.16042v2' target='_blank'>WebWISE: Web Interface Control and Sequential Exploration with Large
  Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Heyi Tao, Sethuraman T V, Michal Shlapentokh-Rothman, Derek Hoiem</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-24 17:57:03</h6>
<p class='card-text'>The paper investigates using a Large Language Model (LLM) to automatically
perform web software tasks using click, scroll, and text input operations.
Previous approaches, such as reinforcement learning (RL) or imitation learning,
are inefficient to train and task-specific. Our method uses filtered Document
Object Model (DOM) elements as observations and performs tasks step-by-step,
sequentially generating small programs based on the current observations. We
use in-context learning, either benefiting from a single manually provided
example, or an automatically generated example based on a successful zero-shot
trial. We evaluate the proposed method on the MiniWob++ benchmark. With only
one in-context example, our WebWISE method achieves similar or better
performance than other methods that require many demonstrations or trials.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.18363v1' target='_blank'>A Contextualized Real-Time Multimodal Emotion Recognition for
  Conversational Agents using Graph Convolutional Networks in Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fathima Abdul Rahman, Guang Lu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-24 14:31:17</h6>
<p class='card-text'>Owing to the recent developments in Generative Artificial Intelligence
(GenAI) and Large Language Models (LLM), conversational agents are becoming
increasingly popular and accepted. They provide a human touch by interacting in
ways familiar to us and by providing support as virtual companions. Therefore,
it is important to understand the user's emotions in order to respond
considerately. Compared to the standard problem of emotion recognition,
conversational agents face an additional constraint in that recognition must be
real-time. Studies on model architectures using audio, visual, and textual
modalities have mainly focused on emotion classification using full video
sequences that do not provide online features. In this work, we present a novel
paradigm for contextualized Emotion Recognition using Graph Convolutional
Network with Reinforcement Learning (conER-GRL). Conversations are partitioned
into smaller groups of utterances for effective extraction of contextual
information. The system uses Gated Recurrent Units (GRU) to extract multimodal
features from these groups of utterances. More importantly, Graph Convolutional
Networks (GCN) and Reinforcement Learning (RL) agents are cascade trained to
capture the complex dependencies of emotion features in interactive scenarios.
Comparing the results of the conER-GRL model with other state-of-the-art models
on the benchmark dataset IEMOCAP demonstrates the advantageous capabilities of
the conER-GRL architecture in recognizing emotions in real-time from multimodal
conversational signals.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.15594v1' target='_blank'>Retrieval-based Knowledge Transfer: An Effective Approach for Extreme
  Large Language Model Compression</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiduan Liu, Jiahao Liu, Qifan Wang, Jingang Wang, Xunliang Cai, Dongyan Zhao, Ran Lucien Wang, Rui Yan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-24 07:58:20</h6>
<p class='card-text'>Large-scale pre-trained language models (LLMs) have demonstrated exceptional
performance in various natural language processing (NLP) tasks. However, the
massive size of these models poses huge challenges for their deployment in
real-world applications. While numerous model compression techniques have been
proposed, most of them are not well-suited for achieving extreme model
compression when there is a significant gap in model scale. In this paper, we
introduce a novel compression paradigm called Retrieval-based Knowledge
Transfer (RetriKT), which effectively transfers the knowledge of LLMs to
extremely small-scale models (e.g., 1%). In particular, our approach extracts
knowledge from LLMs to construct a knowledge store, from which the small-scale
model can retrieve relevant information and leverage it for effective
inference. To improve the quality of the model, soft prompt tuning and Proximal
Policy Optimization (PPO) reinforcement learning techniques are employed.
Extensive experiments are conducted on low-resource tasks from SuperGLUE and
GLUE benchmarks. The results demonstrate that the proposed approach
significantly enhances the performance of small-scale models by leveraging the
knowledge from LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.18347v1' target='_blank'>PRCA: Fitting Black-Box Large Language Models for Retrieval Question
  Answering via Pluggable Reward-Driven Contextual Adapter</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang, Ning Cheng, Ming Li, Jing Xiao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-23 03:12:00</h6>
<p class='card-text'>The Retrieval Question Answering (ReQA) task employs the retrieval-augmented
framework, composed of a retriever and generator. The generator formulates the
answer based on the documents retrieved by the retriever. Incorporating Large
Language Models (LLMs) as generators is beneficial due to their advanced QA
capabilities, but they are typically too large to be fine-tuned with budget
constraints while some of them are only accessible via APIs. To tackle this
issue and further improve ReQA performance, we propose a trainable Pluggable
Reward-Driven Contextual Adapter (PRCA), keeping the generator as a black box.
Positioned between the retriever and generator in a Pluggable manner, PRCA
refines the retrieved information by operating in a token-autoregressive
strategy via maximizing rewards of the reinforcement learning phase. Our
experiments validate PRCA's effectiveness in enhancing ReQA performance on
three datasets by up to 20% improvement to fit black-box LLMs into existing
frameworks, demonstrating its considerable potential in the LLMs era.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.14422v1' target='_blank'>Large Language Models are biased to overestimate profoundness</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Eugenio Herrera-Berg, Tomás Vergara Browne, Pablo León-Villagrá, Marc-Lluís Vives, Cristian Buc Calderon</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-22 21:33:50</h6>
<p class='card-text'>Recent advancements in natural language processing by large language models
(LLMs), such as GPT-4, have been suggested to approach Artificial General
Intelligence. And yet, it is still under dispute whether LLMs possess similar
reasoning abilities to humans. This study evaluates GPT-4 and various other
LLMs in judging the profoundness of mundane, motivational, and pseudo-profound
statements. We found a significant statement-to-statement correlation between
the LLMs and humans, irrespective of the type of statements and the prompting
technique used. However, LLMs systematically overestimate the profoundness of
nonsensical statements, with the exception of Tk-instruct, which uniquely
underestimates the profoundness of statements. Only few-shot learning prompts,
as opposed to chain-of-thought prompting, draw LLMs ratings closer to humans.
Furthermore, this work provides insights into the potential biases induced by
Reinforcement Learning from Human Feedback (RLHF), inducing an increase in the
bias to overestimate the profoundness of statements.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.13595v2' target='_blank'>The History and Risks of Reinforcement Learning and Human Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nathan Lambert, Thomas Krendl Gilbert, Tom Zick</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-20 15:45:16</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) has emerged as a powerful
technique to make large language models (LLMs) easier to use and more
effective. A core piece of the RLHF process is the training and utilization of
a model of human preferences that acts as a reward function for optimization.
This approach, which operates at the intersection of many stakeholders and
academic disciplines, remains poorly understood. RLHF reward models are often
cited as being central to achieving performance, yet very few descriptors of
capabilities, evaluations, training methods, or open-source models exist. Given
this lack of information, further study and transparency is needed for learned
RLHF reward models. In this paper, we illustrate the complex history of
optimizing preferences, and articulate lines of inquiry to understand the
sociotechnical context of reward models. In particular, we highlight the
ontological differences between costs, rewards, and preferences at stake in
RLHF's foundations, related methodological tensions, and possible research
directions to improve general understanding of how reward models function.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.13385v1' target='_blank'>Tuna: Instruction Tuning using Feedback from Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haoran Li, Yiran Liu, Xingxing Zhang, Wei Lu, Furu Wei</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-20 09:55:06</h6>
<p class='card-text'>Instruction tuning of open-source large language models (LLMs) like LLaMA,
using direct outputs from more powerful LLMs such as Instruct-GPT and GPT-4,
has proven to be a cost-effective way to align model behaviors with human
preferences. However, the instruction-tuned model has only seen one response
per instruction, lacking the knowledge of potentially better responses. In this
paper, we propose finetuning an instruction-tuned LLM using our novel
\textit{probabilistic ranking} and \textit{contextual ranking} approaches to
increase the likelihood of generating better responses. Probabilistic ranking
enables the instruction-tuned model to inherit the relative rankings of
high-quality and low-quality responses from the teacher LLM. On the other hand,
learning with contextual ranking allows the model to refine its own response
distribution using the contextual understanding ability of stronger LLMs.
Furthermore, we apply probabilistic ranking and contextual ranking sequentially
to the instruction-tuned LLM. The resulting model, which we call \textbf{Tuna},
consistently improves the performance on Super Natural Instructions (119 test
tasks), LMentry (25 test tasks), Vicuna QA, and can even obtain better results
than several strong reinforcement learning baselines. Our code and data are
available at \url{ https://github.com/microsoft/LMOps}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.12931v2' target='_blank'>Eureka: Human-Level Reward Design via Coding Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, Anima Anandkumar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-19 17:31:01</h6>
<p class='card-text'>Large Language Models (LLMs) have excelled as high-level semantic planners
for sequential decision-making tasks. However, harnessing them to learn complex
low-level manipulation tasks, such as dexterous pen spinning, remains an open
problem. We bridge this fundamental gap and present Eureka, a human-level
reward design algorithm powered by LLMs. Eureka exploits the remarkable
zero-shot generation, code-writing, and in-context improvement capabilities of
state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over
reward code. The resulting rewards can then be used to acquire complex skills
via reinforcement learning. Without any task-specific prompting or pre-defined
reward templates, Eureka generates reward functions that outperform expert
human-engineered rewards. In a diverse suite of 29 open-source RL environments
that include 10 distinct robot morphologies, Eureka outperforms human experts
on 83% of the tasks, leading to an average normalized improvement of 52%. The
generality of Eureka also enables a new gradient-free in-context learning
approach to reinforcement learning from human feedback (RLHF), readily
incorporating human inputs to improve the quality and the safety of the
generated rewards without model updating. Finally, using Eureka rewards in a
curriculum learning setting, we demonstrate for the first time, a simulated
Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a
pen in circles at rapid speed.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.12773v1' target='_blank'>Safe RLHF: Safe Reinforcement Learning from Human Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, Yaodong Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-19 14:22:03</h6>
<p class='card-text'>With the development of large language models (LLMs), striking a balance
between the performance and safety of AI systems has never been more critical.
However, the inherent tension between the objectives of helpfulness and
harmlessness presents a significant challenge during LLM training. To address
this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe
RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly
decouples human preferences regarding helpfulness and harmlessness, effectively
avoiding the crowdworkers' confusion about the tension and allowing us to train
separate reward and cost models. We formalize the safety concern of LLMs as an
optimization task of maximizing the reward function while satisfying specified
cost constraints. Leveraging the Lagrangian method to solve this constrained
problem, Safe RLHF dynamically adjusts the balance between the two objectives
during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we
demonstrate a superior ability to mitigate harmful responses while enhancing
model performance compared to existing value-aligned algorithms.
Experimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with
collected human preferences, significantly improving its helpfulness and
harmlessness according to human evaluations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.12523v1' target='_blank'>Privacy Preserving Large Language Models: ChatGPT Case Study Based
  Vision and Framework</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Imdad Ullah, Najm Hassan, Sukhpal Singh Gill, Basem Suleiman, Tariq Ahamed Ahanger, Zawar Shah, Junaid Qadir, Salil S. Kanhere</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-19 06:55:13</h6>
<p class='card-text'>The generative Artificial Intelligence (AI) tools based on Large Language
Models (LLMs) use billions of parameters to extensively analyse large datasets
and extract critical private information such as, context, specific details,
identifying information etc. This have raised serious threats to user privacy
and reluctance to use such tools. This article proposes the conceptual model
called PrivChatGPT, a privacy-preserving model for LLMs that consists of two
main components i.e., preserving user privacy during the data
curation/pre-processing together with preserving private context and the
private training process for large-scale data. To demonstrate its
applicability, we show how a private mechanism could be integrated into the
existing model for training LLMs to protect user privacy; specifically, we
employed differential privacy and private training using Reinforcement Learning
(RL). We measure the privacy loss and evaluate the measure of uncertainty or
randomness once differential privacy is applied. It further recursively
evaluates the level of privacy guarantees and the measure of uncertainty of
public database and resources, during each update when new information is added
for training purposes. To critically evaluate the use of differential privacy
for private LLMs, we hypothetically compared other mechanisms e..g, Blockchain,
private information retrieval, randomisation, for various performance measures
such as the model performance and accuracy, computational complexity, privacy
vs. utility etc. We conclude that differential privacy, randomisation, and
obfuscation can impact utility and performance of trained models, conversely,
the use of ToR, Blockchain, and PIR may introduce additional computational
complexity and high training latency. We believe that the proposed model could
be used as a benchmark for proposing privacy preserving LLMs for generative AI
tools.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.11971v3' target='_blank'>Improving Generalization of Alignment with Human Preferences through
  Group Invariant Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rui Zheng, Wei Shen, Yuan Hua, Wenbin Lai, Shihan Dou, Yuhao Zhou, Zhiheng Xi, Xiao Wang, Haoran Huang, Tao Gui, Qi Zhang, Xuanjing Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-18 13:54:15</h6>
<p class='card-text'>The success of AI assistants based on language models (LLMs) hinges crucially
on Reinforcement Learning from Human Feedback (RLHF), which enables the
generation of responses more aligned with human preferences. As universal AI
assistants, there's a growing expectation for them to perform consistently
across various domains. However, previous work shows that Reinforcement
Learning (RL) often exploits shortcuts to attain high rewards and overlooks
challenging samples. This focus on quick reward gains undermines both the
stability in training and the model's ability to generalize to new, unseen
data. In this work, we propose a novel approach that can learn a consistent
policy via RL across various data groups or domains. Given the challenges
associated with acquiring group annotations, our method automatically
classifies data into different groups, deliberately maximizing performance
variance. Then, we optimize the policy to perform well on challenging groups.
Lastly, leveraging the established groups, our approach adaptively adjusts the
exploration space, allocating more learning capacity to more challenging data
and preventing the model from over-optimizing on simpler data. Experimental
results indicate that our approach significantly enhances training stability
and model generalization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.11564v1' target='_blank'>Personalized Soups: Personalized Large Language Model Alignment via
  Post-hoc Parameter Merging</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, Prithviraj Ammanabrolu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-17 20:22:13</h6>
<p class='card-text'>While Reinforcement Learning from Human Feedback (RLHF) aligns Large Language
Models (LLMs) with general, aggregate human preferences, it is suboptimal for
learning diverse, individual perspectives. In this work, we study Reinforcement
Learning from Personalized Human Feedback (RLPHF) problem, wherein LLMs are
aligned to multiple (sometimes conflicting) preferences by modeling alignment
as a Multi-Objective Reinforcement Learning (MORL) problem. Compared to strong
single-objective baselines, we show that we can achieve personalized alignment
by decomposing preferences into multiple dimensions. These dimensions are
defined based on personalizations that are declared as desirable by the user.
In this work, we show that they can be efficiently trained independently in a
distributed manner and combined effectively post-hoc through parameter merging.
The code is available at https://github.com/joeljang/RLPHF.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.10844v1' target='_blank'>Survey of Vulnerabilities in Large Language Models Revealed by
  Adversarial Attacks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, Nael Abu-Ghazaleh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-16 21:37:24</h6>
<p class='card-text'>Large Language Models (LLMs) are swiftly advancing in architecture and
capability, and as they integrate more deeply into complex systems, the urgency
to scrutinize their security properties grows. This paper surveys research in
the emerging interdisciplinary field of adversarial attacks on LLMs, a subfield
of trustworthy ML, combining the perspectives of Natural Language Processing
and Security. Prior work has shown that even safety-aligned LLMs (via
instruction tuning and reinforcement learning through human feedback) can be
susceptible to adversarial attacks, which exploit weaknesses and mislead AI
systems, as evidenced by the prevalence of `jailbreak' attacks on models like
ChatGPT and Bard. In this survey, we first provide an overview of large
language models, describe their safety alignment, and categorize existing
research based on various learning structures: textual-only attacks,
multi-modal attacks, and additional attack methods specifically targeting
complex systems, such as federated learning or multi-agent systems. We also
offer comprehensive remarks on works that focus on the fundamental sources of
vulnerabilities and potential defenses. To make this field more accessible to
newcomers, we present a systematic review of existing works, a structured
typology of adversarial attack concepts, and additional resources, including
slides for presentations on related topics at the 62nd Annual Meeting of the
Association for Computational Linguistics (ACL'24).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.10505v4' target='_blank'>ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method
  for Aligning Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziniu Li, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu, Ruoyu Sun, Zhi-Quan Luo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-16 15:25:14</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) is key to aligning Large
Language Models (LLMs), typically paired with the Proximal Policy Optimization
(PPO) algorithm. While PPO is a powerful method designed for general
reinforcement learning tasks, it is overly sophisticated for LLMs, leading to
laborious hyper-parameter tuning and significant computation burdens. To make
RLHF efficient, we present ReMax, which leverages 3 properties of RLHF: fast
simulation, deterministic transitions, and trajectory-level rewards. These
properties are not exploited in PPO, making it less suitable for RLHF. Building
on the renowned REINFORCE algorithm, ReMax does not require training an
additional value model as in PPO and is further enhanced with a new variance
reduction technique. ReMax offers several benefits over PPO: it is simpler to
implement, eliminates more than 4 hyper-parameters in PPO, reduces GPU memory
usage, and shortens training time. ReMax can save about 46% GPU memory than PPO
when training a 7B model and enables training on A800-80GB GPUs without the
memory-saving offloading technique needed by PPO. Applying ReMax to a
Mistral-7B model resulted in a 94.78% win rate on the AlpacaEval leaderboard
and a 7.739 score on MT-bench, setting a new SOTA for open-source 7B models.
These results show the effectiveness of ReMax while addressing the limitations
of PPO in LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.10198v3' target='_blank'>MoConVQ: Unified Physics-Based Motion Control via Scalable Discrete
  Representations</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Heyuan Yao, Zhenhua Song, Yuyang Zhou, Tenglong Ao, Baoquan Chen, Libin Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-16 09:09:02</h6>
<p class='card-text'>In this work, we present MoConVQ, a novel unified framework for physics-based
motion control leveraging scalable discrete representations. Building upon
vector quantized variational autoencoders (VQ-VAE) and model-based
reinforcement learning, our approach effectively learns motion embeddings from
a large, unstructured dataset spanning tens of hours of motion examples. The
resultant motion representation not only captures diverse motion skills but
also offers a robust and intuitive interface for various applications. We
demonstrate the versatility of MoConVQ through several applications: universal
tracking control from various motion sources, interactive character control
with latent motion representations using supervised learning, physics-based
motion generation from natural language descriptions using the GPT framework,
and, most interestingly, seamless integration with large language models (LLMs)
with in-context learning to tackle complex and abstract tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.10701v3' target='_blank'>Theory of Mind for Multi-Agent Collaboration via Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes, Michael Lewis, Katia Sycara</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-16 07:51:19</h6>
<p class='card-text'>While Large Language Models (LLMs) have demonstrated impressive
accomplishments in both reasoning and planning, their abilities in multi-agent
collaborations remains largely unexplored. This study evaluates LLM-based
agents in a multi-agent cooperative text game with Theory of Mind (ToM)
inference tasks, comparing their performance with Multi-Agent Reinforcement
Learning (MARL) and planning-based baselines. We observed evidence of emergent
collaborative behaviors and high-order Theory of Mind capabilities among
LLM-based agents. Our results reveal limitations in LLM-based agents' planning
optimization due to systematic failures in managing long-horizon contexts and
hallucination about the task state. We explore the use of explicit belief state
representations to mitigate these issues, finding that it enhances task
performance and the accuracy of ToM inferences for LLM-based agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.10076v1' target='_blank'>Verbosity Bias in Preference Labeling by Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Keita Saito, Akifumi Wachi, Koki Wataoka, Youhei Akimoto</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-16 05:19:02</h6>
<p class='card-text'>In recent years, Large Language Models (LLMs) have witnessed a remarkable
surge in prevalence, altering the landscape of natural language processing and
machine learning. One key factor in improving the performance of LLMs is
alignment with humans achieved with Reinforcement Learning from Human Feedback
(RLHF), as for many LLMs such as GPT-4, Bard, etc. In addition, recent studies
are investigating the replacement of human feedback with feedback from other
LLMs named Reinforcement Learning from AI Feedback (RLAIF). We examine the
biases that come along with evaluating LLMs with other LLMs and take a closer
look into verbosity bias -- a bias where LLMs sometimes prefer more verbose
answers even if they have similar qualities. We see that in our problem
setting, GPT-4 prefers longer answers more than humans. We also propose a
metric to measure this bias.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.10021v2' target='_blank'>Bootstrap Your Own Skills: Learning to Solve New Tasks with Large
  Language Model Guidance</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jesse Zhang, Jiahui Zhang, Karl Pertsch, Ziyi Liu, Xiang Ren, Minsuk Chang, Shao-Hua Sun, Joseph J. Lim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-16 02:43:47</h6>
<p class='card-text'>We propose BOSS, an approach that automatically learns to solve new
long-horizon, complex, and meaningful tasks by growing a learned skill library
with minimal supervision. Prior work in reinforcement learning require expert
supervision, in the form of demonstrations or rich reward functions, to learn
long-horizon tasks. Instead, our approach BOSS (BOotStrapping your own Skills)
learns to accomplish new tasks by performing "skill bootstrapping," where an
agent with a set of primitive skills interacts with the environment to practice
new skills without receiving reward feedback for tasks outside of the initial
skill set. This bootstrapping phase is guided by large language models (LLMs)
that inform the agent of meaningful skills to chain together. Through this
process, BOSS builds a wide range of complex and useful behaviors from a basic
set of primitive skills. We demonstrate through experiments in realistic
household environments that agents trained with our LLM-guided bootstrapping
procedure outperform those trained with naive bootstrapping as well as prior
unsupervised skill acquisition methods on zero-shot execution of unseen,
long-horizon tasks in new environments. Website at clvrai.com/boss.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.09506v1' target='_blank'>Towards Semantic Communication Protocols for 6G: From Protocol Learning
  to Language-Oriented Approaches</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jihong Park, Seung-Woo Ko, Jinho Choi, Seong-Lyun Kim, Mehdi Bennis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-14 06:28:50</h6>
<p class='card-text'>The forthcoming 6G systems are expected to address a wide range of
non-stationary tasks. This poses challenges to traditional medium access
control (MAC) protocols that are static and predefined. In response,
data-driven MAC protocols have recently emerged, offering ability to tailor
their signaling messages for specific tasks. This article presents a novel
categorization of these data-driven MAC protocols into three levels: Level 1
MAC. task-oriented neural protocols constructed using multi-agent deep
reinforcement learning (MADRL); Level 2 MAC. neural network-oriented symbolic
protocols developed by converting Level 1 MAC outputs into explicit symbols;
and Level 3 MAC. language-oriented semantic protocols harnessing large language
models (LLMs) and generative models. With this categorization, we aim to
explore the opportunities and challenges of each level by delving into their
foundational techniques. Drawing from information theory and associated
principles as well as selected case studies, this study provides insights into
the trajectory of data-driven MAC protocols and sheds light on future research
directions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.09454v1' target='_blank'>LgTS: Dynamic Task Sampling using LLM-generated sub-goals for
  Reinforcement Learning Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yash Shukla, Wenchang Gao, Vasanth Sarathy, Alvaro Velasquez, Robert Wright, Jivko Sinapov</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-14 00:07:03</h6>
<p class='card-text'>Recent advancements in reasoning abilities of Large Language Models (LLM) has
promoted their usage in problems that require high-level planning for robots
and artificial agents. However, current techniques that utilize LLMs for such
planning tasks make certain key assumptions such as, access to datasets that
permit finetuning, meticulously engineered prompts that only provide relevant
and essential information to the LLM, and most importantly, a deterministic
approach to allow execution of the LLM responses either in the form of existing
policies or plan operators. In this work, we propose LgTS (LLM-guided
Teacher-Student learning), a novel approach that explores the planning
abilities of LLMs to provide a graphical representation of the sub-goals to a
reinforcement learning (RL) agent that does not have access to the transition
dynamics of the environment. The RL agent uses Teacher-Student learning
algorithm to learn a set of successful policies for reaching the goal state
from the start state while simultaneously minimizing the number of
environmental interactions. Unlike previous methods that utilize LLMs, our
approach does not assume access to a propreitary or a fine-tuned LLM, nor does
it require pre-trained policies that achieve the sub-goals proposed by the LLM.
Through experiments on a gridworld based DoorKey domain and a search-and-rescue
inspired domain, we show that generating a graphical structure of sub-goals
helps in learning policies for the LLM proposed sub-goals and the
Teacher-Student learning algorithm minimizes the number of environment
interactions when the transition dynamics are unknown.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.08922v1' target='_blank'>LLaMA Rider: Spurring Large Language Models to Explore the Open World</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yicheng Feng, Yuxuan Wang, Jiazheng Liu, Sipeng Zheng, Zongqing Lu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-13 07:47:44</h6>
<p class='card-text'>Recently, various studies have leveraged Large Language Models (LLMs) to help
decision-making and planning in environments, and try to align the LLMs'
knowledge with the world conditions. Nonetheless, the capacity of LLMs to
continuously acquire environmental knowledge and adapt in an open world remains
uncertain. In this paper, we propose an approach to spur LLMs to explore the
open world, gather experiences, and learn to improve their task-solving
capabilities. In this approach, a multi-round feedback-revision mechanism is
utilized to encourage LLMs to actively select appropriate revision actions
guided by feedback information from the environment. This facilitates
exploration and enhances the model's performance. Besides, we integrate
sub-task relabeling to assist LLMs in maintaining consistency in sub-task
planning and help the model learn the combinatorial nature between tasks,
enabling it to complete a wider range of tasks through training based on the
acquired exploration experiences. By evaluation in Minecraft, an open-ended
sandbox world, we demonstrate that our approach LLaMA-Rider enhances the
efficiency of the LLM in exploring the environment, and effectively improves
the LLM's ability to accomplish more tasks through fine-tuning with merely 1.3k
instances of collected data, showing minimal training costs compared to the
baseline using reinforcement learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.08164v5' target='_blank'>Interpreting Learned Feedback Patterns in Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Luke Marks, Amir Abdullah, Clement Neo, Rauno Arike, David Krueger, Philip Torr, Fazl Barez</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-12 09:36:03</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) is widely used to train
large language models (LLMs). However, it is unclear whether LLMs accurately
learn the underlying preferences in human feedback data. We coin the term
\textit{Learned Feedback Pattern} (LFP) for patterns in an LLM's activations
learned during RLHF that improve its performance on the fine-tuning task. We
hypothesize that LLMs with LFPs accurately aligned to the fine-tuning feedback
exhibit consistent activation patterns for outputs that would have received
similar feedback during RLHF. To test this, we train probes to estimate the
feedback signal implicit in the activations of a fine-tuned LLM. We then
compare these estimates to the true feedback, measuring how accurate the LFPs
are to the fine-tuning feedback. Our probes are trained on a condensed, sparse
and interpretable representation of LLM activations, making it easier to
correlate features of the input with our probe's predictions. We validate our
probes by comparing the neural features they correlate with positive feedback
inputs against the features GPT-4 describes and classifies as related to LFPs.
Understanding LFPs can help minimize discrepancies between LLM behavior and
training objectives, which is essential for the safety of LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.07488v2' target='_blank'>KwaiYiiMath: Technical Report</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiayi Fu, Lei Lin, Xiaoyang Gao, Pengli Liu, Zhengzong Chen, Zhirui Yang, Shengnan Zhang, Xue Zheng, Yan Li, Yuliang Liu, Xucheng Ye, Yiqiao Liao, Chao Liao, Bin Chen, Chengru Song, Junchen Wan, Zijia Lin, Fuzheng Zhang, Zhongyuan Wang, Di Zhang, Kun Gai</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-11 13:35:05</h6>
<p class='card-text'>Recent advancements in large language models (LLMs) have demonstrated
remarkable abilities in handling a variety of natural language processing (NLP)
downstream tasks, even on mathematical tasks requiring multi-step reasoning. In
this report, we introduce the KwaiYiiMath which enhances the mathematical
reasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT)
and Reinforced Learning from Human Feedback (RLHF), including on both English
and Chinese mathematical tasks. Meanwhile, we also constructed a small-scale
Chinese primary school mathematics test set (named KMath), consisting of 188
examples to evaluate the correctness of the problem-solving process generated
by the models. Empirical studies demonstrate that KwaiYiiMath can achieve
state-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with
the similar size models, respectively.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.06452v3' target='_blank'>Understanding the Effects of RLHF on LLM Generalisation and Diversity</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette, Roberta Raileanu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-10 09:25:44</h6>
<p class='card-text'>Large language models (LLMs) fine-tuned with reinforcement learning from
human feedback (RLHF) have been used in some of the most widely deployed AI
models to date, such as OpenAI's ChatGPT or Anthropic's Claude. While there has
been significant work developing these methods, our understanding of the
benefits and downsides of each stage in RLHF is still limited. To fill this
gap, we present an extensive analysis of how each stage of the process (i.e.
supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key
properties: out-of-distribution (OOD) generalisation and output diversity. OOD
generalisation is crucial given the wide range of real-world scenarios in which
these models are being used, while output diversity refers to the model's
ability to generate varied outputs and is important for a variety of use cases.
We perform our analysis across two base models on both summarisation and
instruction following tasks, the latter being highly relevant for current LLM
use cases. We find that RLHF generalises better than SFT to new inputs,
particularly as the distribution shift between train and test becomes larger.
However, RLHF significantly reduces output diversity compared to SFT across a
variety of measures, implying a tradeoff in current LLM fine-tuning methods
between generalisation and diversity. Our results provide guidance on which
fine-tuning method should be used depending on the application, and show that
more research is needed to improve the tradeoff between generalisation and
diversity.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.06147v1' target='_blank'>Reinforcement Learning in the Era of LLMs: What is Essential? What is
  needed? An RL Perspective on RLHF, Prompting, and Beyond</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hao Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-09 20:49:42</h6>
<p class='card-text'>Recent advancements in Large Language Models (LLMs) have garnered wide
attention and led to successful products such as ChatGPT and GPT-4. Their
proficiency in adhering to instructions and delivering harmless, helpful, and
honest (3H) responses can largely be attributed to the technique of
Reinforcement Learning from Human Feedback (RLHF). In this paper, we aim to
link the research in conventional RL to RL techniques used in LLM research.
Demystify this technique by discussing why, when, and how RL excels.
Furthermore, we explore potential future avenues that could either benefit from
or contribute to RLHF research.
  Highlighted Takeaways:
  1. RLHF is Online Inverse RL with Offline Demonstration Data.
  2. RLHF $>$ SFT because Imitation Learning (and Inverse RL) $>$ Behavior
Cloning (BC) by alleviating the problem of compounding error.
  3. The RM step in RLHF generates a proxy of the expensive human feedback,
such an insight can be generalized to other LLM tasks such as prompting
evaluation and optimization where feedback is also expensive.
  4. The policy learning in RLHF is more challenging than conventional problems
studied in IRL due to their high action dimensionality and feedback sparsity.
  5. The main superiority of PPO over off-policy value-based methods is its
stability gained from (almost) on-policy data and conservative policy updates.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.05910v2' target='_blank'>SALMON: Self-Alignment with Instructable Reward Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Cox, Yiming Yang, Chuang Gan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-09 17:56:53</h6>
<p class='card-text'>Supervised Fine-Tuning (SFT) on response demonstrations combined with
Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful
paradigm for aligning LLM-based AI agents. However, a significant limitation of
such an approach is its dependency on high-quality human annotations, making
its application to intricate tasks challenging due to difficulties in obtaining
consistent response demonstrations and in-distribution response preferences.
This paper presents a novel approach, namely SALMON, to align base language
models with minimal human supervision, using only a small set of human-defined
principles, yet achieving superior performance. Central to our approach is an
instructable reward model. Trained on synthetic preference data, this model can
generate reward scores based on arbitrary human-defined principles. By merely
adjusting these principles during the RL training phase, we gain full control
over the preferences with the instructable reward model, subsequently
influencing the behavior of the RL-trained policy models, and reducing the
reliance on the collection of online human preferences. Applying our method to
the LLaMA-2-70b base language model, we developed an AI assistant named
Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined
principles, Dromedary-2 significantly surpasses the performance of several
state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark
datasets. We have open-sourced the code and model weights to encourage further
research into aligning LLM-based AI agents with enhanced supervision
efficiency, improved controllability, and scalable oversight.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.05627v1' target='_blank'>Integrating Stock Features and Global Information via Large Language
  Models for Enhanced Stock Return Prediction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yujie Ding, Shuai Jia, Tianyi Ma, Bingcheng Mao, Xiuze Zhou, Liuliu Li, Dongming Han</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-09 11:34:18</h6>
<p class='card-text'>The remarkable achievements and rapid advancements of Large Language Models
(LLMs) such as ChatGPT and GPT-4 have showcased their immense potential in
quantitative investment. Traders can effectively leverage these LLMs to analyze
financial news and predict stock returns accurately. However, integrating LLMs
into existing quantitative models presents two primary challenges: the
insufficient utilization of semantic information embedded within LLMs and the
difficulties in aligning the latent information within LLMs with pre-existing
quantitative stock features. We propose a novel framework consisting of two
components to surmount these challenges. The first component, the Local-Global
(LG) model, introduces three distinct strategies for modeling global
information. These approaches are grounded respectively on stock features, the
capabilities of LLMs, and a hybrid method combining the two paradigms. The
second component, Self-Correlated Reinforcement Learning (SCRL), focuses on
aligning the embeddings of financial news generated by LLMs with stock features
within the same semantic space. By implementing our framework, we have
demonstrated superior performance in Rank Information Coefficient and returns,
particularly compared to models relying only on stock features in the China
A-share market.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.05344v1' target='_blank'>SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to
  RLHF</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yi Dong, Zhilin Wang, Makesh Narsimhan Sreedhar, Xianchao Wu, Oleksii Kuchaiev</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-09 02:11:21</h6>
<p class='card-text'>Model alignment with human preferences is an essential step in making Large
Language Models (LLMs) helpful and consistent with human values. It typically
consists of supervised fine-tuning (SFT) and reinforcement learning from human
feedback (RLHF) stages. However, RLHF faces inherent limitations stemming from
a complex training setup and its tendency to align the model with implicit
values that end users cannot control at run-time. Moreover, reward models in
RLHF stage commonly rely on single-dimensional feedback as opposed to explicit,
multifaceted signals that indicate attributes such as helpfulness, humor, and
toxicity. To address these limitations, we propose SteerLM, a supervised
fine-tuning method that empowers end-users to control responses during
inference. SteerLM conditions responses to conform to an explicitly defined
multi-dimensional set of attributes, thereby empowering a steerable AI capable
of generating helpful and high-quality responses while maintaining
customizability. Experiments show that SteerLM trained on open source datasets
generates responses that are preferred by human and automatic evaluators to
many state-of-the-art baselines trained with RLHF while being much easier to
train. Try SteerLM at https://huggingface.co/nvidia/SteerLM-llama2-13B</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.04716v1' target='_blank'>Reinforced UI Instruction Grounding: Towards a Generic UI Task
  Automation API</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhizheng Zhang, Wenxuan Xie, Xiaoyi Zhang, Yan Lu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-07 07:22:41</h6>
<p class='card-text'>Recent popularity of Large Language Models (LLMs) has opened countless
possibilities in automating numerous AI tasks by connecting LLMs to various
domain-specific models or APIs, where LLMs serve as dispatchers while
domain-specific models or APIs are action executors. Despite the vast numbers
of domain-specific models/APIs, they still struggle to comprehensively cover
super diverse automation demands in the interaction between human and User
Interfaces (UIs). In this work, we build a multimodal model to ground natural
language instructions in given UI screenshots as a generic UI task automation
executor. This metadata-free grounding model, consisting of a visual encoder
and a language decoder, is first pretrained on well studied document
understanding tasks and then learns to decode spatial information from UI
screenshots in a promptable way. To facilitate the exploitation of
image-to-text pretrained knowledge, we follow the pixel-to-sequence paradigm to
predict geometric coordinates in a sequence of tokens using a language decoder.
We further propose an innovative Reinforcement Learning (RL) based algorithm to
supervise the tokens in such sequence jointly with visually semantic metrics,
which effectively strengthens the spatial decoding capability of the
pixel-to-sequence paradigm. Extensive experiments demonstrate our proposed
reinforced UI instruction grounding model outperforms the state-of-the-art
methods by a clear margin and shows the potential as a generic UI task
automation API.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.04363v2' target='_blank'>Amortizing intractable inference in large language models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Edward J. Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, Nikolay Malkin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-06 16:36:08</h6>
<p class='card-text'>Autoregressive large language models (LLMs) compress knowledge from their
training data through next-token conditional distributions. This limits
tractable querying of this knowledge to start-to-end autoregressive sampling.
However, many tasks of interest -- including sequence continuation, infilling,
and other forms of constrained generation -- involve sampling from intractable
posterior distributions. We address this limitation by using amortized Bayesian
inference to sample from these intractable posteriors. Such amortization is
algorithmically achieved by fine-tuning LLMs via diversity-seeking
reinforcement learning algorithms: generative flow networks (GFlowNets). We
empirically demonstrate that this distribution-matching paradigm of LLM
fine-tuning can serve as an effective alternative to maximum-likelihood
training and reward-maximizing policy optimization. As an important
application, we interpret chain-of-thought reasoning as a latent variable
modeling problem and demonstrate that our approach enables data-efficient
adaptation of LLMs to tasks that require multi-step rationalization and tool
use.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.03903v2' target='_blank'>LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination
  Abilities in Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Saaket Agashe, Yue Fan, Anthony Reyna, Xin Eric Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-05 21:18:15</h6>
<p class='card-text'>The emergent reasoning and Theory of Mind (ToM) abilities demonstrated by
Large Language Models (LLMs) make them promising candidates for developing
coordination agents. In this study, we introduce a new LLM-Coordination
Benchmark aimed at a detailed analysis of LLMs within the context of Pure
Coordination Games, where participating agents need to cooperate for the most
gain. This benchmark evaluates LLMs through two distinct tasks: (1)
\emph{Agentic Coordination}, where LLMs act as proactive participants for
cooperation in 4 pure coordination games; (2) \emph{Coordination Question
Answering (QA)}, where LLMs are prompted to answer 198 multiple-choice
questions from the 4 games for evaluation of three key reasoning abilities:
Environment Comprehension, ToM Reasoning, and Joint Planning. Furthermore, to
enable LLMs for multi-agent coordination, we introduce a Cognitive Architecture
for Coordination (CAC) framework that can easily integrate different LLMs as
plug-and-play modules for pure coordination games. Our findings indicate that
LLM agents equipped with GPT-4-turbo achieve comparable performance to
state-of-the-art reinforcement learning methods in games that require
commonsense actions based on the environment. Besides, zero-shot coordination
experiments reveal that, unlike RL methods, LLM agents are robust to new unseen
partners. However, results on Coordination QA show a large room for improvement
in the Theory of Mind reasoning and joint planning abilities of LLMs. The
analysis also sheds light on how the ability of LLMs to understand their
environment and their partner's beliefs and intentions plays a part in their
ability to plan for coordination. Our code is available at
\url{https://github.com/eric-ai-lab/llm_coordination}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.03173v2' target='_blank'>$\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program
  Synthesis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zishun Yu, Yunzhe Tao, Liyu Chen, Tao Sun, Hongxia Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-04 21:40:36</h6>
<p class='card-text'>Program synthesis aims to create accurate, executable programs from problem
specifications, specifically from natural language descriptions in our context.
Recent studies have leveraged the power of reinforcement learning (RL) in
conjunction with large language models (LLMs), significantly enhancing code
generation capabilities. The application of RL focuses on directly optimizing
for functional correctness, offering an advantage over conventional supervised
methods. Despite policy-based RL methods dominating the literature on RL for
program synthesis, the nature of program synthesis tasks hints at a natural
alignment with value-based methods. This stems from the rich collection of
off-policy programs, including those developed by human programmers and also
historical samples, coupled with the straightforward verification of generated
programs through automated unit testing, meaning rewards are easy to obtain.
Diverging from the dominant use of policy-based algorithms, our work explores
the feasibility of value-based approaches, leading to the development of our
$\mathcal{B}$-Coder (pronounced Bellman coder). Yet, training value-based
methods presents challenges due to the enormous search space inherent to
program synthesis. To this end, we introduce an initialization protocol for RL
agents utilizing pre-trained LMs and a conservative Bellman operator to reduce
training complexities. Moreover, we demonstrate how to leverage the learned
value functions as a dual strategy to post-process generated programs. Our
empirical evaluations demonstrated $\mathcal{B}$-Coder's capability in
achieving state-of-the-art performance when compared to policy-based methods.
Remarkably, this achievement is reached with minimal reward engineering effort,
highlighting the effectiveness of value-based RL, independent of reward
designs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.02368v2' target='_blank'>Reinforcement Learning from Automatic Feedback for High-Quality Unit
  Test Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Benjamin Steenhoek, Michele Tufano, Neel Sundaresan, Alexey Svyatkovskiy</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-03 18:48:31</h6>
<p class='card-text'>Software testing is a crucial aspect of software development, and the
creation of high-quality tests that adhere to best practices is essential for
effective maintenance. Recently, Large Language Models (LLMs) have gained
popularity for code generation, including the automated creation of test cases.
However, these LLMs are often trained on vast amounts of publicly available
code, which may include test cases that do not adhere to best practices and may
even contain test smells (anti-patterns). To address this issue, we propose a
novel technique called Reinforcement Learning from Static Quality Metrics
(RLSQM). To begin, we analyze the anti-patterns generated by the LLM and show
that LLMs can generate undesirable test smells. Thus, we train specific reward
models for each static quality metric, then utilize Proximal Policy
Optimization (PPO) to train models for optimizing a single quality metric at a
time. Furthermore, we amalgamate these rewards into a unified reward model
aimed at capturing different best practices and quality aspects of tests. By
comparing RL-trained models with those trained using supervised learning, we
provide insights into how reliably utilize RL to improve test generation
quality and into the effects of various training strategies. Our experimental
results demonstrate that the RL-optimized model consistently generated
high-quality test cases compared to the base LLM, improving the model by up to
21%, and successfully generates nearly 100% syntactically correct code. RLSQM
also outperformed GPT-4 on four out of seven metrics. This represents a
significant step towards enhancing the overall efficiency and reliability of
software testing through Reinforcement Learning and static quality metrics. Our
data are available at https://figshare.com/s/ded476c8d4c221222849.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.01581v1' target='_blank'>On the Safety of Open-Sourced Large Language Models: Does Alignment
  Really Prevent Them From Being Misused?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hangfan Zhang, Zhimeng Guo, Huaisheng Zhu, Bochuan Cao, Lu Lin, Jinyuan Jia, Jinghui Chen, Dinghao Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-02 19:22:01</h6>
<p class='card-text'>Large Language Models (LLMs) have achieved unprecedented performance in
Natural Language Generation (NLG) tasks. However, many existing studies have
shown that they could be misused to generate undesired content. In response,
before releasing LLMs for public access, model developers usually align those
language models through Supervised Fine-Tuning (SFT) or Reinforcement Learning
with Human Feedback (RLHF). Consequently, those aligned large language models
refuse to generate undesired content when facing potentially harmful/unethical
requests. A natural question is "could alignment really prevent those
open-sourced large language models from being misused to generate undesired
content?''. In this work, we provide a negative answer to this question. In
particular, we show those open-sourced, aligned large language models could be
easily misguided to generate undesired content without heavy computations or
careful prompt designs. Our key idea is to directly manipulate the generation
process of open-sourced LLMs to misguide it to generate undesired content
including harmful or biased information and even private data. We evaluate our
method on 4 open-sourced LLMs accessible publicly and our finding highlights
the need for more advanced mitigation strategies for open-sourced LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.01377v2' target='_blank'>UltraFeedback: Boosting Language Models with Scaled AI Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, Zhiyuan Liu, Maosong Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-02 17:40:01</h6>
<p class='card-text'>Learning from human feedback has become a pivot technique in aligning large
language models (LLMs) with human preferences. However, acquiring vast and
premium human feedback is bottlenecked by time, labor, and human capability,
resulting in small sizes or limited topics of current datasets. This further
hinders feedback learning as well as alignment research within the open-source
community. To address this issue, we explore how to go beyond human feedback
and collect high-quality \textit{AI feedback} automatically for a scalable
alternative. Specifically, we identify \textbf{scale and diversity} as the key
factors for feedback data to take effect. Accordingly, we first broaden
instructions and responses in both amount and breadth to encompass a wider
range of user-assistant interactions. Then, we meticulously apply a series of
techniques to mitigate annotation biases for more reliable AI feedback. We
finally present \textsc{UltraFeedback}, a large-scale, high-quality, and
diversified AI feedback dataset, which contains over 1 million GPT-4 feedback
for 250k user-assistant conversations from various aspects. Built upon
\textsc{UltraFeedback}, we align a LLaMA-based model by best-of-$n$ sampling
and reinforcement learning, demonstrating its exceptional performance on chat
benchmarks. Our work validates the effectiveness of scaled AI feedback data in
constructing strong open-source chat language models, serving as a solid
foundation for future feedback learning research. Our data and models are
available at https://github.com/thunlp/UltraFeedback.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.01468v3' target='_blank'>Probing the Multi-turn Planning Capabilities of LLMs via 20 Question
  Games</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yizhe Zhang, Jiarui Lu, Navdeep Jaitly</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-02 16:55:37</h6>
<p class='card-text'>Large language models (LLMs) are effective at answering questions that are
clearly asked. However, when faced with ambiguous queries they can act
unpredictably and produce incorrect outputs. This underscores the need for the
development of intelligent agents capable of asking clarification questions to
resolve ambiguities effectively. This capability requires complex
understanding, state tracking, reasoning and planning over multiple
conversational turns. However, directly measuring this can be challenging. In
this paper, we offer a surrogate problem which assesses an LLMs's capability to
deduce an entity unknown to itself, but revealed to a judge, by asking the
judge a series of queries. This \textit{entity-deducing game} can serve as an
evaluation framework to probe the conversational reasoning and planning
capabilities of language models. We systematically evaluate various LLMs and
discover significant differences in their performance on this task. We find
that strong LLMs like GPT-4 outperform human players by a large margin. We
further employ Behavior Cloning (BC) to examine whether a weaker model is
capable of imitating a stronger model and generalizing to data or domains,
using only the demonstrations from a stronger model. We finally propose to use
Reinforcement Learning to enhance reasoning and planning capacity of Vicuna
models through episodes of game playing, which lead to significant performance
improvement. We hope that this problem offers insights into how autonomous
agents could be trained to behave more intelligently in ambiguous
circumstances.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.00898v4' target='_blank'>Enabling Language Models to Implicitly Learn Self-Improvement</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziqi Wang, Le Hou, Tianjian Lu, Yuexin Wu, Yunxuan Li, Hongkun Yu, Heng Ji</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-02 04:29:40</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated remarkable capabilities in
open-ended text generation tasks. However, the inherent open-ended nature of
these tasks implies that there is always room for improvement in the quality of
model responses. To address this challenge, various approaches have been
proposed to enhance the performance of LLMs. There has been a growing focus on
enabling LLMs to self-improve their response quality, thereby reducing the
reliance on extensive human annotation efforts for collecting diverse and
high-quality training data. Recently, prompting-based methods have been widely
explored among self-improvement methods owing to their effectiveness,
efficiency, and convenience. However, those methods usually require explicitly
and thoroughly written rubrics as inputs to LLMs. It is expensive and
challenging to manually derive and provide all necessary rubrics with a
real-world complex goal for improvement (e.g., being more helpful and less
harmful). To this end, we propose an ImPlicit Self-ImprovemenT (PIT) framework
that implicitly learns the improvement goal from human preference data. PIT
only requires preference data that are used to train reward models without
extra human efforts. Specifically, we reformulate the training objective of
reinforcement learning from human feedback (RLHF) -- instead of maximizing
response quality for a given input, we maximize the quality gap of the response
conditioned on a reference response. In this way, PIT is implicitly trained
with the improvement goal of better aligning with human preferences.
Experiments on two real-world datasets and one synthetic dataset show that our
method significantly outperforms prompting-based methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.00819v1' target='_blank'>Parameter-Efficient Tuning Helps Language Model Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tianci Xue, Ziqi Wang, Heng Ji</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-01 23:27:14</h6>
<p class='card-text'>Aligning large language models (LLMs) with human preferences is essential for
safe and useful LLMs. Previous works mainly adopt reinforcement learning (RLHF)
and direct preference optimization (DPO) with human feedback for alignment.
Nevertheless, they have certain drawbacks. One such limitation is that they can
only align models with one preference at the training time (e.g., they cannot
learn to generate concise responses when the preference data prefers detailed
responses), or have certain constraints for the data format (e.g., DPO only
supports pairwise preference data). To this end, prior works incorporate
controllable generations for alignment to make language models learn multiple
preferences and provide outputs with different preferences during inference if
asked. Controllable generation also offers more flexibility with regard to data
format (e.g., it supports pointwise preference data). Specifically, it uses
different control tokens for different preferences during training and
inference, making LLMs behave differently when required. Current controllable
generation methods either use a special token or hand-crafted prompts as
control tokens, and optimize them together with LLMs. As control tokens are
typically much lighter than LLMs, this optimization strategy may not
effectively optimize control tokens. To this end, we first use
parameter-efficient tuning (e.g., prompting tuning and low-rank adaptation) to
optimize control tokens and then fine-tune models for controllable generations,
similar to prior works. Our approach, alignMEnt with parameter-Efficient Tuning
(MEET), improves the quality of control tokens, thus improving controllable
generation quality consistently by an apparent margin on two well-recognized
datasets compared with prior works.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.00481v3' target='_blank'>LANCAR: Leveraging Language for Context-Aware Robot Locomotion in
  Unstructured Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chak Lam Shek, Xiyang Wu, Wesley A. Suttle, Carl Busart, Erin Zaroukian, Dinesh Manocha, Pratap Tokekar, Amrit Singh Bedi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-30 20:26:00</h6>
<p class='card-text'>Navigating robots through unstructured terrains is challenging, primarily due
to the dynamic environmental changes. While humans adeptly navigate such
terrains by using context from their observations, creating a similar
context-aware navigation system for robots is difficult. The essence of the
issue lies in the acquisition and interpretation of context information, a task
complicated by the inherent ambiguity of human language. In this work, we
introduce LANCAR, which addresses this issue by combining a context translator
with reinforcement learning (RL) agents for context-aware locomotion. LANCAR
allows robots to comprehend context information through Large Language Models
(LLMs) sourced from human observers and convert this information into
actionable context embeddings. These embeddings, combined with the robot's
sensor data, provide a complete input for the RL agent's policy network. We
provide an extensive evaluation of LANCAR under different levels of context
ambiguity and compare with alternative methods. The experimental results
showcase the superior generalizability and adaptability across different
terrains. Notably, LANCAR shows at least a 7.4% increase in episodic reward
over the best alternatives, highlighting its potential to enhance robotic
navigation in unstructured environments. More details and experiment videos
could be found in http://raaslab.org/projects/LLM_Context_Estimation/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.00212v3' target='_blank'>Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for
  LLM Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tianhao Wu, Banghua Zhu, Ruoyu Zhang, Zhaojin Wen, Kannan Ramchandran, Jiantao Jiao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-30 01:23:22</h6>
<p class='card-text'>Large Language Models (LLMs) can acquire extensive world knowledge through
pre-training on large corpora. However, due to exposure to low-quality data,
LLMs may exhibit harmful behavior without aligning with human values. The
dominant approach for steering LLMs towards beneficial behavior involves
Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy
Optimization (PPO) serving as the default RL optimizer. Despite its
effectiveness, PPO has limitations when optimizing rewards trained from
comparison-based loss. Primarily, PPO is not invariant to equivalent reward
functions containing identical preference information due to the need to
calibrate the reward scale. Additionally, PPO's necessity for token-wise
updates introduces complexity in both function approximation and algorithm
design compared to trajectory-wise optimization. This paper proposes a new
framework, reinforcement learning with relative feedback, and a novel
trajectory-wise policy gradient algorithm, Pairwise Proximal Policy
Optimization (P3O) that operates directly on comparative rewards. We show
theoretically that P3O is invariant to equivalent rewards and avoids the
complexity of PPO. Empirical evaluations demonstrate that P3O outperforms PPO
in the KL-Reward trade-off and can align with human preferences as well as or
better than prior methods. In summary, this work introduces a simpler yet
effective approach for aligning LLMs to human preferences through relative
feedback.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.00194v4' target='_blank'>Improving Planning with Large Language Models: A Modular Agentic
  Architecture</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Taylor Webb, Shanka Subhra Mondal, Ida Momennejad</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-30 00:10:14</h6>
<p class='card-text'>Large language models (LLMs) demonstrate impressive performance on a wide
variety of tasks, but they often struggle with tasks that require multi-step
reasoning or goal-directed planning. Both cognitive neuroscience and
reinforcement learning (RL) have proposed a number of interacting functional
components that together implement search and evaluation in multi-step decision
making. These components include conflict monitoring, state prediction, state
evaluation, task decomposition, and orchestration. To improve planning with
LLMs, we propose an agentic architecture, the Modular Agentic Planner (MAP), in
which planning is accomplished via the recurrent interaction of the specialized
modules mentioned above, each implemented using an LLM. MAP improves planning
through the interaction of specialized modules that break down a larger problem
into multiple brief automated calls to the LLM. We evaluate MAP on three
challenging planning tasks -- graph traversal, Tower of Hanoi, and the
PlanBench benchmark -- as well as an NLP task requiring multi-step reasoning
(strategyQA). We find that MAP yields significant improvements over both
standard LLM methods (zero-shot prompting, in-context learning) and competitive
baselines (chain-of-thought, multi-agent debate, and tree-of-thought), can be
effectively combined with smaller and more cost-efficient LLMs (Llama3-70B),
and displays superior transfer across tasks. These results suggest the benefit
of a modular and multi-agent approach to planning with LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.00166v1' target='_blank'>Motif: Intrinsic Motivation from Artificial Intelligence Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Martin Klissarov, Pierluca D'Oro, Shagun Sodhani, Roberta Raileanu, Pierre-Luc Bacon, Pascal Vincent, Amy Zhang, Mikael Henaff</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-29 22:10:01</h6>
<p class='card-text'>Exploring rich environments and evaluating one's actions without prior
knowledge is immensely challenging. In this paper, we propose Motif, a general
method to interface such prior knowledge from a Large Language Model (LLM) with
an agent. Motif is based on the idea of grounding LLMs for decision-making
without requiring them to interact with the environment: it elicits preferences
from an LLM over pairs of captions to construct an intrinsic reward, which is
then used to train agents with reinforcement learning. We evaluate Motif's
performance and behavior on the challenging, open-ended and
procedurally-generated NetHack game. Surprisingly, by only learning to maximize
its intrinsic reward, Motif achieves a higher game score than an algorithm
directly trained to maximize the score itself. When combining Motif's intrinsic
reward with the environment reward, our method significantly outperforms
existing approaches and makes progress on tasks where no advancements have ever
been made without demonstrations. Finally, we show that Motif mostly generates
intuitive human-aligned behaviors which can be steered easily through prompt
modifications, while scaling well with the LLM size and the amount of
information given in the prompt.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.00152v2' target='_blank'>Learning to Rewrite Prompts for Personalized Text Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong, Michael Bendersky</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-29 21:15:49</h6>
<p class='card-text'>Facilitated by large language models (LLMs), personalized text generation has
become a rapidly growing research direction. Most existing studies focus on
designing specialized models for a particular domain, or they require
fine-tuning the LLMs to generate personalized text. We consider a typical
scenario in which the large language model, which generates personalized
output, is frozen and can only be accessed through APIs. Under this constraint,
all one can do is to improve the input text (i.e., text prompts) sent to the
LLM, a procedure that is usually done manually. In this paper, we propose a
novel method to automatically revise prompts for personalized text generation.
The proposed method takes the initial prompts generated by a state-of-the-art,
multistage framework for personalized generation and rewrites a few critical
components that summarize and synthesize the personal context. The prompt
rewriter employs a training paradigm that chains together supervised learning
(SL) and reinforcement learning (RL), where SL reduces the search space of RL
and RL facilitates end-to-end training of the rewriter. Using datasets from
three representative domains, we demonstrate that the rewritten prompts
outperform both the original prompts and the prompts optimized via supervised
learning or reinforcement learning alone. In-depth analysis of the rewritten
prompts shows that they are not only human readable, but also able to guide
manual revision of prompts when there is limited resource to employ
reinforcement learning to train the prompt rewriter, or when it is costly to
deploy an automatic prompt rewriter for inference.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.17176v3' target='_blank'>AdaRefiner: Refining Decisions of Language Models with Adaptive Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wanpeng Zhang, Zongqing Lu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-29 12:16:19</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated significant success across
various domains. However, their application in complex decision-making tasks
frequently necessitates intricate prompt engineering or fine-tuning, leading to
challenges in unseen downstream tasks and heavy demands on computational
resources. Meanwhile, Reinforcement Learning (RL) has been recognized as
effective in decision-making problems but struggles in environments with sparse
rewards, such as open-world games. To overcome these challenges, we introduce
AdaRefiner, a novel framework designed to enhance the synergy between LLMs and
RL feedback. The key component of AdaRefiner is a lightweight Adapter Language
Model (LM), which automatically refines task comprehension based on feedback
from RL agents. This method mitigates the need for intricate prompt engineering
and intensive LLM fine-tuning while maintaining the LLMs' generalization
abilities and enhancing their decision-making capabilities in downstream tasks.
Empirical evaluations of AdaRefiner on 22 diverse tasks within the open-world
game Crafter have demonstrated its superior effectiveness, especially in
guiding agents towards higher-level and common-sense skills. Our work makes
contributions to the automatic self-refinement of LLMs with RL feedback,
offering a more adaptable and efficient solution for complex decision-making
problems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.17078v2' target='_blank'>Unsupervised Large Language Model Alignment for Information Retrieval
  via Contrastive Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qian Dong, Yiding Liu, Qingyao Ai, Zhijing Wu, Haitao Li, Yiqun Liu, Shuaiqiang Wang, Dawei Yin, Shaoping Ma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-29 09:14:53</h6>
<p class='card-text'>Large language models (LLMs) have demonstrated remarkable capabilities across
various research domains, including the field of Information Retrieval (IR).
However, the responses generated by off-the-shelf LLMs tend to be generic,
i.e., cannot capture the distinctiveness of each document with similar content.
This limits the performance of LLMs in IR because finding and distinguishing
relevant documents from substantial similar documents is a typical problem in
many IR tasks. To address this issue, we propose an unsupervised alignment
method, namely Reinforcement Learning from Contrastive Feedback (RLCF),
empowering LLMs to generate both high-quality and context-specific responses.
Our approach constructs unsupervised contrastive feedback signals based on
similar document groups, and adopts a reward function, named group-wise
reciprocal rank, to optimize LLMs within a standard Proximal Policy
Optimization. We conduct extensive experiments to evaluate the effectiveness of
RLCF on LLMs built with different languages and parameter sizes on multiple
downstream IR applications. RLCF significantly outperforms existing alignment
methods, and RLCF-optimized LLMs demonstrate considerable improvement in
generating responses with distinctiveness.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.16609v1' target='_blank'>Qwen Technical Report</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-28 17:07:49</h6>
<p class='card-text'>Large language models (LLMs) have revolutionized the field of artificial
intelligence, enabling natural language processing tasks that were previously
thought to be exclusive to humans. In this work, we introduce Qwen, the first
installment of our large language model series. Qwen is a comprehensive
language model series that encompasses distinct models with varying parameter
counts. It includes Qwen, the base pretrained language models, and Qwen-Chat,
the chat models finetuned with human alignment techniques. The base language
models consistently demonstrate superior performance across a multitude of
downstream tasks, and the chat models, particularly those trained using
Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The
chat models possess advanced tool-use and planning capabilities for creating
agent applications, showcasing impressive performance even when compared to
bigger models on complex tasks like utilizing a code interpreter. Furthermore,
we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as
well as mathematics-focused models, Math-Qwen-Chat, which are built upon base
language models. These models demonstrate significantly improved performance in
comparison with open-source models, and slightly fall behind the proprietary
models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.16382v2' target='_blank'>RLLTE: Long-Term Evolution Project of Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mingqi Yuan, Zequn Zhang, Yang Xu, Shihao Luo, Bo Li, Xin Jin, Wenjun Zeng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-28 12:30:37</h6>
<p class='card-text'>We present RLLTE: a long-term evolution, extremely modular, and open-source
framework for reinforcement learning (RL) research and application. Beyond
delivering top-notch algorithm implementations, RLLTE also serves as a toolkit
for developing algorithms. More specifically, RLLTE decouples the RL algorithms
completely from the exploitation-exploration perspective, providing a large
number of components to accelerate algorithm development and evolution. In
particular, RLLTE is the first RL framework to build a comprehensive ecosystem,
which includes model training, evaluation, deployment, benchmark hub, and large
language model (LLM)-empowered copilot. RLLTE is expected to set standards for
RL engineering practice and be highly stimulative for industry and academia.
Our documentation, examples, and source code are available at
https://github.com/RLE-Foundation/rllte.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.16347v2' target='_blank'>Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic
  Manipulation Tasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Eleftherios Triantafyllidis, Filippos Christianos, Zhibin Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-28 11:14:52</h6>
<p class='card-text'>Current reinforcement learning algorithms struggle in sparse and complex
environments, most notably in long-horizon manipulation tasks entailing a
plethora of different sequences. In this work, we propose the Intrinsically
Guided Exploration from Large Language Models (IGE-LLMs) framework. By
leveraging LLMs as an assistive intrinsic reward, IGE-LLMs guides the
exploratory process in reinforcement learning to address intricate long-horizon
with sparse rewards robotic manipulation tasks. We evaluate our framework and
related intrinsic learning methods in an environment challenged with
exploration, and a complex robotic manipulation task challenged by both
exploration and long-horizons. Results show IGE-LLMs (i) exhibit notably higher
performance over related intrinsic methods and the direct use of LLMs in
decision-making, (ii) can be combined and complement existing learning methods
highlighting its modularity, (iii) are fairly insensitive to different
intrinsic scaling parameters, and (iv) maintain robustness against increased
levels of uncertainty and horizons.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.16292v3' target='_blank'>DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large
  Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Licheng Wen, Daocheng Fu, Xin Li, Xinyu Cai, Tao Ma, Pinlong Cai, Min Dou, Botian Shi, Liang He, Yu Qiao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-28 09:41:35</h6>
<p class='card-text'>Recent advancements in autonomous driving have relied on data-driven
approaches, which are widely adopted but face challenges including dataset
bias, overfitting, and uninterpretability. Drawing inspiration from the
knowledge-driven nature of human driving, we explore the question of how to
instill similar capabilities into autonomous driving systems and summarize a
paradigm that integrates an interactive environment, a driver agent, as well as
a memory component to address this question. Leveraging large language models
(LLMs) with emergent abilities, we propose the DiLu framework, which combines a
Reasoning and a Reflection module to enable the system to perform
decision-making based on common-sense knowledge and evolve continuously.
Extensive experiments prove DiLu's capability to accumulate experience and
demonstrate a significant advantage in generalization ability over
reinforcement learning-based methods. Moreover, DiLu is able to directly
acquire experiences from real-world datasets which highlights its potential to
be deployed on practical autonomous driving systems. To the best of our
knowledge, we are the first to leverage knowledge-driven capability in
decision-making for autonomous vehicles. Through the proposed DiLu framework,
LLM is strengthened to apply knowledge and to reason causally in the autonomous
driving domain. Project page: https://pjlab-adg.github.io/DiLu/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.16240v1' target='_blank'>Beyond Reverse KL: Generalizing Direct Preference Optimization with
  Diverse Divergence Constraints</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, Yuxin Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-28 08:29:44</h6>
<p class='card-text'>The increasing capabilities of large language models (LLMs) raise
opportunities for artificial general intelligence but concurrently amplify
safety concerns, such as potential misuse of AI systems, necessitating
effective AI alignment. Reinforcement Learning from Human Feedback (RLHF) has
emerged as a promising pathway towards AI alignment but brings forth challenges
due to its complexity and dependence on a separate reward model. Direct
Preference Optimization (DPO) has been proposed as an alternative, and it
remains equivalent to RLHF under the reverse KL regularization constraint. This
paper presents $f$-DPO, a generalized approach to DPO by incorporating diverse
divergence constraints. We show that under certain $f$-divergences, including
Jensen-Shannon divergence, forward KL divergences and $\alpha$-divergences, the
complex relationship between the reward and optimal policy can also be
simplified by addressing the Karush-Kuhn-Tucker conditions. This eliminates the
need for estimating the normalizing constant in the Bradley-Terry model and
enables a tractable mapping between the reward function and the optimal policy.
Our approach optimizes LLMs to align with human preferences in a more efficient
and supervised manner under a broad set of divergence constraints. Empirically,
adopting these divergences ensures a balance between alignment performance and
generation diversity. Importantly, $f$-DPO outperforms PPO-based methods in
divergence efficiency, and divergence constraints directly influence expected
calibration error (ECE).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.13094v1' target='_blank'>Computational Natural Philosophy: A Thread from Presocratics through
  Turing to ChatGPT</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Gordana Dodig-Crnkovic</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-22 11:47:36</h6>
<p class='card-text'>Modern computational natural philosophy conceptualizes the universe in terms
of information and computation, establishing a framework for the study of
cognition and intelligence. Despite some critiques, this computational
perspective has significantly influenced our understanding of the natural
world, leading to the development of AI systems like ChatGPT based on deep
neural networks. Advancements in this domain have been facilitated by
interdisciplinary research, integrating knowledge from multiple fields to
simulate complex systems. Large Language Models (LLMs), such as ChatGPT,
represent this approach's capabilities, utilizing reinforcement learning with
human feedback (RLHF). Current research initiatives aim to integrate neural
networks with symbolic computing, introducing a new generation of hybrid
computational models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.12053v5' target='_blank'>AceGPT, Localizing Large Language Models in Arabic</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Huang Huang, Fei Yu, Jianqing Zhu, Xuening Sun, Hao Cheng, Dingjie Song, Zhihong Chen, Abdulmohsen Alharthi, Bang An, Juncai He, Ziche Liu, Zhiyi Zhang, Junying Chen, Jianquan Li, Benyou Wang, Lian Zhang, Ruoyu Sun, Xiang Wan, Haizhou Li, Jinchao Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-21 13:20:13</h6>
<p class='card-text'>This paper is devoted to the development of a localized Large Language Model
(LLM) specifically for Arabic, a language imbued with unique cultural
characteristics inadequately addressed by current mainstream models.
Significant concerns emerge when addressing cultural sensitivity and local
values. To address this, the paper proposes a comprehensive solution that
includes further pre-training with Arabic texts, Supervised Fine-Tuning (SFT)
utilizing native Arabic instructions, and GPT-4 responses in Arabic, alongside
Reinforcement Learning with AI Feedback (RLAIF) employing a reward model
attuned to local culture and values. The goal is to cultivate culturally
cognizant and value-aligned Arabic LLMs capable of accommodating the diverse,
application-specific needs of Arabic-speaking communities.
  Comprehensive evaluations reveal that the resulting model, dubbed `AceGPT',
sets the state-of-the-art standard for open Arabic LLMs across various
benchmarks. Codes, data, and models are in
https://github.com/FreedomIntelligence/AceGPT.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.11489v3' target='_blank'>Text2Reward: Reward Shaping with Language Models for Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, Tao Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-20 17:39:13</h6>
<p class='card-text'>Designing reward functions is a longstanding challenge in reinforcement
learning (RL); it requires specialized knowledge or domain data, leading to
high costs for development. To address this, we introduce Text2Reward, a
data-free framework that automates the generation and shaping of dense reward
functions based on large language models (LLMs). Given a goal described in
natural language, Text2Reward generates shaped dense reward functions as an
executable program grounded in a compact representation of the environment.
Unlike inverse RL and recent work that uses LLMs to write sparse reward codes
or unshaped dense rewards with a constant function across timesteps,
Text2Reward produces interpretable, free-form dense reward codes that cover a
wide range of tasks, utilize existing packages, and allow iterative refinement
with human feedback. We evaluate Text2Reward on two robotic manipulation
benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo.
On 13 of the 17 manipulation tasks, policies trained with generated reward
codes achieve similar or better task success rates and convergence speed than
expert-written reward codes. For locomotion tasks, our method learns six novel
locomotion behaviors with a success rate exceeding 94%. Furthermore, we show
that the policies trained in the simulator with our method can be deployed in
the real world. Finally, Text2Reward further improves the policies by refining
their reward functions with human feedback. Video results are available at
https://text-to-reward.github.io/ .</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.11359v3' target='_blank'>Prompt, Plan, Perform: LLM-based Humanoid Control via Quantized
  Imitation Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jingkai Sun, Qiang Zhang, Yiqun Duan, Xiaoyang Jiang, Chong Cheng, Renjing Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-20 14:42:01</h6>
<p class='card-text'>In recent years, reinforcement learning and imitation learning have shown
great potential for controlling humanoid robots' motion. However, these methods
typically create simulation environments and rewards for specific tasks,
resulting in the requirements of multiple policies and limited capabilities for
tackling complex and unknown tasks. To overcome these issues, we present a
novel approach that combines adversarial imitation learning with large language
models (LLMs). This innovative method enables the agent to learn reusable
skills with a single policy and solve zero-shot tasks under the guidance of
LLMs. In particular, we utilize the LLM as a strategic planner for applying
previously learned skills to novel tasks through the comprehension of
task-specific prompts. This empowers the robot to perform the specified actions
in a sequence. To improve our model, we incorporate codebook-based vector
quantization, allowing the agent to generate suitable actions in response to
unseen textual commands from LLMs. Furthermore, we design general reward
functions that consider the distinct motion features of humanoid robots,
ensuring the agent imitates the motion data while maintaining goal orientation
without additional guiding direction approaches or policies. To the best of our
knowledge, this is the first framework that controls humanoid robots using a
single learning policy network and LLM as a planner. Extensive experiments
demonstrate that our method exhibits efficient and adaptive ability in
complicated motion tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.10691v3' target='_blank'>MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language
  Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, Heng Ji</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-19 15:25:42</h6>
<p class='card-text'>To solve complex tasks, large language models (LLMs) often require multiple
rounds of interactions with the user, sometimes assisted by external tools.
However, current evaluation protocols often emphasize benchmark performance
with single-turn exchanges, neglecting the nuanced interactions among the user,
LLMs, and external tools, while also underestimating the importance of natural
language feedback from users. These oversights contribute to discrepancies
between research benchmark evaluations and real-world use cases. We introduce
MINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn
interactions by (1) using tools and (2) leveraging natural language feedback.
To ensure reproducibility, we provide an evaluation framework where LLMs can
access tools by executing Python code and receive users' natural language
feedback simulated by GPT-4. We repurpose a diverse set of established
evaluation datasets focusing on reasoning, coding, and decision-making and
carefully curate them into a compact subset for efficient evaluation. Our
analysis of 20 open- and closed-source LLMs offers intriguing findings. (a)
LLMs generally benefit from tools and language feedback, with performance gains
(absolute, same below) of 1-8% for each turn of tool use and 2-17% with natural
language feedback. (b) Better single-turn performance does not guarantee better
multi-turn performance. (c) Surprisingly, on the LLMs evaluated, supervised
instruction-finetuning (SIFT) and reinforcement learning from human feedback
(RLHF) generally hurt multi-turn capabilities. We expect MINT can help measure
progress and incentivize research in improving LLMs' capabilities in multi-turn
interactions, especially for open-source communities where multi-turn human
evaluation can be less accessible compared to commercial LLMs with a larger
user base.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.10105v2' target='_blank'>Understanding Catastrophic Forgetting in Language Models via Implicit
  Inference</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Suhas Kotha, Jacob Mitchell Springer, Aditi Raghunathan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-18 19:28:48</h6>
<p class='card-text'>We lack a systematic understanding of the effects of fine-tuning (via methods
such as instruction-tuning or reinforcement learning from human feedback),
particularly on tasks outside the narrow fine-tuning distribution. In a
simplified scenario, we demonstrate that improving performance on tasks within
the fine-tuning data distribution comes at the expense of capabilities on other
tasks. We hypothesize that language models implicitly infer the task of the
prompt and that fine-tuning skews this inference towards tasks in the
fine-tuning distribution. To test this, we propose Conjugate Prompting, which
artificially makes the task look farther from the fine-tuning distribution
while requiring the same capability, and we find that this recovers some of the
pretraining capabilities in our synthetic setup. Since real-world fine-tuning
distributions are predominantly English, we apply conjugate prompting to
recover pretrained capabilities in LLMs by simply translating the prompts to
different languages. This allows us to recover in-context learning abilities
lost via instruction tuning, natural reasoning capability lost during code
fine-tuning, and, more concerningly, harmful content generation suppressed by
safety fine-tuning in chatbots like ChatGPT.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.07124v2' target='_blank'>RAIN: Your Language Models Can Align Themselves without Finetuning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, Hongyang Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-13 17:59:09</h6>
<p class='card-text'>Large language models (LLMs) often demonstrate inconsistencies with human
preferences. Previous research typically gathered human preference data and
then aligned the pre-trained models using reinforcement learning or instruction
tuning, a.k.a. the finetuning step. In contrast, aligning frozen LLMs without
requiring alignment data is more appealing. This work explores the potential of
the latter setting. We discover that by integrating self-evaluation and rewind
mechanisms, unaligned LLMs can directly produce responses consistent with human
preferences via self-boosting. We introduce a novel inference method,
Rewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to
evaluate their own generation and use the evaluation results to guide rewind
and generation for AI safety. Notably, RAIN operates without the need of extra
data for model alignment and abstains from any training, gradient computation,
or parameter updates. Experimental results evaluated by GPT-4 and humans
demonstrate the effectiveness of RAIN: on the HH dataset, RAIN improves the
harmlessness rate of LLaMA 30B from 82% of vanilla inference to 97%, while
maintaining the helpfulness rate. On the TruthfulQA dataset, RAIN improves the
truthfulness of the already-well-aligned LLaMA-2-chat 13B model by 5%.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.06687v2' target='_blank'>Self-Refined Large Language Model as Automated Reward Function Designer
  for Deep Reinforcement Learning in Robotics</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiayang Song, Zhehua Zhou, Jiawei Liu, Chunrong Fang, Zhan Shu, Lei Ma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-13 02:56:56</h6>
<p class='card-text'>Although Deep Reinforcement Learning (DRL) has achieved notable success in
numerous robotic applications, designing a high-performing reward function
remains a challenging task that often requires substantial manual input.
Recently, Large Language Models (LLMs) have been extensively adopted to address
tasks demanding in-depth common-sense knowledge, such as reasoning and
planning. Recognizing that reward function design is also inherently linked to
such knowledge, LLM offers a promising potential in this context. Motivated by
this, we propose in this work a novel LLM framework with a self-refinement
mechanism for automated reward function design. The framework commences with
the LLM formulating an initial reward function based on natural language
inputs. Then, the performance of the reward function is assessed, and the
results are presented back to the LLM for guiding its self-refinement process.
We examine the performance of our proposed framework through a variety of
continuous robotic control tasks across three diverse robotic systems. The
results indicate that our LLM-designed reward functions are able to rival or
even surpass manually designed reward functions, highlighting the efficacy and
applicability of our approach.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.06553v4' target='_blank'>Query-Dependent Prompt Evaluation and Optimization with Offline Inverse
  RL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hao Sun, Alihan Hüyük, Mihaela van der Schaar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-13 01:12:52</h6>
<p class='card-text'>In this study, we aim to enhance the arithmetic reasoning ability of Large
Language Models (LLMs) through zero-shot prompt optimization. We identify a
previously overlooked objective of query dependency in such optimization and
elucidate two ensuing challenges that impede the successful and economical
design of prompt optimization techniques. One primary issue is the absence of
an effective method to evaluate prompts during inference when the golden answer
is unavailable. Concurrently, learning via interactions with the LLMs to
navigate the expansive natural language prompting space proves to be
resource-intensive. To address this, we introduce Prompt-OIRL, which harnesses
offline inverse reinforcement learning to draw insights from offline prompting
demonstration data. Such data exists as by-products when diverse prompts are
benchmarked on open-accessible datasets. With Prompt-OIRL, the query-dependent
prompt optimization objective is achieved by first learning an offline reward
model. This model can evaluate any query-prompt pairs without accessing LLMs.
Subsequently, a best-of-N strategy is deployed to recommend the optimal prompt.
Our experimental evaluations across various LLM scales and arithmetic reasoning
datasets underscore both the efficacy and economic viability of the proposed
approach.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.06657v2' target='_blank'>Statistical Rejection Sampling Improves Preference Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J. Liu, Jialu Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-13 01:07:25</h6>
<p class='card-text'>Improving the alignment of language models with human preferences remains an
active research challenge. Previous approaches have primarily utilized
Reinforcement Learning from Human Feedback (RLHF) via online RL methods such as
Proximal Policy Optimization (PPO). Recently, offline methods such as Sequence
Likelihood Calibration (SLiC) and Direct Preference Optimization (DPO) have
emerged as attractive alternatives, offering improvements in stability and
scalability while maintaining competitive performance. SLiC refines its loss
function using sequence pairs sampled from a supervised fine-tuned (SFT)
policy, while DPO directly optimizes language models based on preference data,
foregoing the need for a separate reward model. However, the maximum likelihood
estimator (MLE) of the target optimal policy requires labeled preference pairs
sampled from that policy. DPO's lack of a reward model constrains its ability
to sample preference pairs from the optimal policy, and SLiC is restricted to
sampling preference pairs only from the SFT policy. To address these
limitations, we introduce a novel approach called Statistical Rejection
Sampling Optimization (RSO) that aims to source preference data from the target
optimal policy using rejection sampling, enabling a more accurate estimation of
the optimal policy. We also propose a unified framework that enhances the loss
functions used in both SLiC and DPO from a preference modeling standpoint.
Through extensive experiments across three diverse tasks, we demonstrate that
RSO consistently outperforms both SLiC and DPO on evaluations from both Large
Language Model (LLM) and human raters.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.06256v4' target='_blank'>Mitigating the Alignment Tax of RLHF</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yong Lin, Hangyu Lin, Wei Xiong, Shizhe Diao, Jianmeng Liu, Jipeng Zhang, Rui Pan, Haoxiang Wang, Wenbin Hu, Hanning Zhang, Hanze Dong, Renjie Pi, Han Zhao, Nan Jiang, Heng Ji, Yuan Yao, Tong Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-12 14:16:54</h6>
<p class='card-text'>LLMs acquire a wide range of abilities during pre-training, but aligning LLMs
under Reinforcement Learning with Human Feedback (RLHF) can lead to forgetting
pretrained abilities, which is also known as the alignment tax. To investigate
alignment tax, we conducted experiments with existing RLHF algorithms using
OpenLLaMA-3B, which revealed a pronounced alignment tax in NLP tasks. Whereas,
despite various techniques to mitigate forgetting, they are often at odds with
the RLHF performance, leading to a trade-off between alignment performance and
forgetting mitigation, leading to an alignment-forgetting trade-off.
  In this paper we show that model averaging, which simply interpolates between
pre and post RLHF model weights, surprisingly achieves the most strongest
alignment-forgetting Pareto front among a wide range of competing methods. To
understand its effectiveness, we offer theoretical insights into model
averaging, revealing that it enhances performance Pareto front by increasing
feature diversity on the layers where tasks share overlapped feature spaces.
Empirical evidence corroborates our analysis by showing the benefits of
averaging low-level transformer layers. Building on the analysis and the
observation that averaging different layers of the transformer leads to
significantly different alignment-forgetting trade-offs, we propose
Heterogeneous Model Averaging (HMA) to Heterogeneously find various combination
ratios of model layers. HMA seeks to maximize the alignment performance while
incurring minimal alignment tax. Moreover, we validate HMA's performance across
a range of RLHF algorithms over OpenLLaMA-3B and further extend our findings to
Mistral-7B which is evaluated by open-sourced preference model and GPT4. Code
available here:
https://github.com/avalonstrel/Mitigating-the-Alignment-Tax-of-RLHF.git.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.00841v1' target='_blank'>LeanContext: Cost-Efficient Domain-Specific Question Answering Using
  LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Md Adnan Arefeen, Biplob Debnath, Srimat Chakradhar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-02 06:33:18</h6>
<p class='card-text'>Question-answering (QA) is a significant application of Large Language Models
(LLMs), shaping chatbot capabilities across healthcare, education, and customer
service. However, widespread LLM integration presents a challenge for small
businesses due to the high expenses of LLM API usage. Costs rise rapidly when
domain-specific data (context) is used alongside queries for accurate
domain-specific LLM responses. One option is to summarize the context by using
LLMs and reduce the context. However, this can also filter out useful
information that is necessary to answer some domain-specific queries. In this
paper, we shift from human-oriented summarizers to AI model-friendly summaries.
Our approach, LeanContext, efficiently extracts $k$ key sentences from the
context that are closely aligned with the query. The choice of $k$ is neither
static nor random; we introduce a reinforcement learning technique that
dynamically determines $k$ based on the query and context. The rest of the less
important sentences are reduced using a free open source text reduction method.
We evaluate LeanContext against several recent query-aware and query-unaware
context reduction approaches on prominent datasets (arxiv papers and BBC news
articles). Despite cost reductions of $37.29\%$ to $67.81\%$, LeanContext's
ROUGE-1 score decreases only by $1.41\%$ to $2.65\%$ compared to a baseline
that retains the entire context (no summarization). Additionally, if free
pretrained LLM-based summarizers are used to reduce context (into human
consumable summaries), LeanContext can further modify the reduced context to
enhance the accuracy (ROUGE-1 score) by $13.22\%$ to $24.61\%$.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.03224v3' target='_blank'>No Train Still Gain. Unleash Mathematical Reasoning of Large Language
  Models with Monte Carlo Tree Search Guided by Energy Function</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haotian Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-01 13:10:54</h6>
<p class='card-text'>Large language models (LLMs) demonstrate impressive language understanding
and contextual learning abilities, making them suitable for natural language
processing (NLP) tasks and complex mathematical reasoning. However, when
applied to mathematical reasoning tasks, LLMs often struggle to generate
correct reasoning steps and answers despite having high probabilities for the
solutions. To overcome this limitation and enhance the mathematical reasoning
capabilities of fine-tuned LLMs without additional fine-tuning steps, we
propose a method that incorporates Monte Carlo Tree Search (MCTS) and a
lightweight energy function to rank decision steps and enable immediate
reaction and precise reasoning. Specifically, we re-formulate the fine-tuned
LLMs into a Residual-based Energy Model (Residual-EBM) and employ noise
contrastive estimation to estimate the energy function's parameters. We then
utilize MCTS with the energy function as a path verifier to search the output
space and evaluate the reasoning path. Through extensive experiments on two
mathematical reasoning benchmarks, GSM8k and AQUA-RAT, we demonstrate the
exceptional capabilities of our method, which significantly improves the pass@1
metric of the fine-tuned model without requiring additional fine-tuning or
reinforcement learning with human feedback alignment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.00267v3' target='_blank'>RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with
  AI Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, Sushant Prakash</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-01 05:53:33</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) has proven effective in
aligning large language models (LLMs) with human preferences, but gathering
high-quality preference labels is expensive. RL from AI Feedback (RLAIF),
introduced in Bai et al., offers a promising alternative that trains the reward
model (RM) on preferences generated by an off-the-shelf LLM. Across the tasks
of summarization, helpful dialogue generation, and harmless dialogue
generation, we show that RLAIF achieves comparable performance to RLHF.
Furthermore, we take a step towards "self-improvement" by demonstrating that
RLAIF can outperform a supervised fine-tuned baseline even when the AI labeler
is the same size as the policy, or even the exact same checkpoint as the
initial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that
circumvents RM training by obtaining rewards directly from an off-the-shelf LLM
during RL, which achieves superior performance to canonical RLAIF. Our results
suggest that RLAIF can achieve performance on-par with using human feedback,
offering a potential solution to the scalability limitations of RLHF.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2308.14921v1' target='_blank'>Gender bias and stereotypes in Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hadas Kotek, Rikker Dockum, David Q. Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-08-28 22:32:05</h6>
<p class='card-text'>Large Language Models (LLMs) have made substantial progress in the past
several months, shattering state-of-the-art benchmarks in many domains. This
paper investigates LLMs' behavior with respect to gender stereotypes, a known
issue for prior models. We use a simple paradigm to test the presence of gender
bias, building on but differing from WinoBias, a commonly used gender bias
dataset, which is likely to be included in the training data of current LLMs.
We test four recently published LLMs and demonstrate that they express biased
assumptions about men and women's occupations. Our contributions in this paper
are as follows: (a) LLMs are 3-6 times more likely to choose an occupation that
stereotypically aligns with a person's gender; (b) these choices align with
people's perceptions better than with the ground truth as reflected in official
job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in
perceptions or the ground truth; (d) LLMs ignore crucial ambiguities in
sentence structure 95% of the time in our study items, but when explicitly
prompted, they recognize the ambiguity; (e) LLMs provide explanations for their
choices that are factually inaccurate and likely obscure the true reason behind
their predictions. That is, they provide rationalizations of their biased
behavior. This highlights a key property of these models: LLMs are trained on
imbalanced datasets; as such, even with the recent successes of reinforcement
learning with human feedback, they tend to reflect those imbalances back at us.
As with other types of societal biases, we suggest that LLMs must be carefully
tested to ensure that they treat minoritized individuals and communities
equitably.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2308.14284v6' target='_blank'>Prompt to Transfer: Sim-to-Real Transfer for Traffic Signal Control with
  Prompt Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Longchao Da, Minquan Gao, Hao Mei, Hua Wei</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-08-28 03:49:13</h6>
<p class='card-text'>Numerous solutions are proposed for the Traffic Signal Control (TSC) tasks
aiming to provide efficient transportation and mitigate congestion waste. In
recent, promising results have been attained by Reinforcement Learning (RL)
methods through trial and error in simulators, bringing confidence in solving
cities' congestion headaches. However, there still exist performance gaps when
simulator-trained policies are deployed to the real world. This issue is mainly
introduced by the system dynamic difference between the training simulator and
the real-world environments. The Large Language Models (LLMs) are trained on
mass knowledge and proved to be equipped with astonishing inference abilities.
In this work, we leverage LLMs to understand and profile the system dynamics by
a prompt-based grounded action transformation. Accepting the cloze prompt
template, and then filling in the answer based on accessible context, the
pre-trained LLM's inference ability is exploited and applied to understand how
weather conditions, traffic states, and road types influence traffic dynamics,
being aware of this, the policies' action is taken and grounded based on
realistic dynamics, thus help the agent learn a more realistic policy. We
conduct experiments using DQN to show the effectiveness of the proposed
PromptGAT's ability in mitigating the performance gap from simulation to
reality (sim-to-real).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2308.13278v1' target='_blank'>Integrating LLMs and Decision Transformers for Language Grounded
  Generative Quality-Diversity</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Achkan Salehi, Stephane Doncieux</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-08-25 10:00:06</h6>
<p class='card-text'>Quality-Diversity is a branch of stochastic optimization that is often
applied to problems from the Reinforcement Learning and control domains in
order to construct repertoires of well-performing policies/skills that exhibit
diversity with respect to a behavior space. Such archives are usually composed
of a finite number of reactive agents which are each associated to a unique
behavior descriptor, and instantiating behavior descriptors outside of that
coarsely discretized space is not straight-forward. While a few recent works
suggest solutions to that issue, the trajectory that is generated is not easily
customizable beyond the specification of a target behavior descriptor. We
propose to jointly solve those problems in environments where semantic
information about static scene elements is available by leveraging a Large
Language Model to augment the repertoire with natural language descriptions of
trajectories, and training a policy conditioned on those descriptions. Thus,
our method allows a user to not only specify an arbitrary target behavior
descriptor, but also provide the model with a high-level textual prompt to
shape the generated trajectory. We also propose an LLM-based approach to
evaluating the performance of such generative agents. Furthermore, we develop a
benchmark based on simulated robot navigation in a 2d maze that we use for
experimental validation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2308.12086v2' target='_blank'>Out of the Cage: How Stochastic Parrots Win in Cyber Security
  Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Maria Rigaki, Ondřej Lukáš, Carlos A. Catania, Sebastian Garcia</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-08-23 12:11:27</h6>
<p class='card-text'>Large Language Models (LLMs) have gained widespread popularity across diverse
domains involving text generation, summarization, and various natural language
processing tasks. Despite their inherent limitations, LLM-based designs have
shown promising capabilities in planning and navigating open-world scenarios.
This paper introduces a novel application of pre-trained LLMs as agents within
cybersecurity network environments, focusing on their utility for sequential
decision-making processes.
  We present an approach wherein pre-trained LLMs are leveraged as attacking
agents in two reinforcement learning environments. Our proposed agents
demonstrate similar or better performance against state-of-the-art agents
trained for thousands of episodes in most scenarios and configurations. In
addition, the best LLM agents perform similarly to human testers of the
environment without any additional training process. This design highlights the
potential of LLMs to efficiently address complex decision-making tasks within
cybersecurity.
  Furthermore, we introduce a new network security environment named
NetSecGame. The environment is designed to eventually support complex
multi-agent scenarios within the network security domain. The proposed
environment mimics real network attacks and is designed to be highly modular
and adaptable for various scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2308.12030v2' target='_blank'>Prompt-Based Length Controlled Generation with Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Renlong Jie, Xiaojun Meng, Lifeng Shang, Xin Jiang, Qun Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-08-23 09:43:10</h6>
<p class='card-text'>Large language models (LLMs) like ChatGPT and GPT-4 have attracted great
attention given their surprising performance on a wide range of NLP tasks.
Length controlled generation of LLMs emerges as an important topic, which
enables users to fully leverage the capability of LLMs in more real-world
scenarios like generating a proper answer or essay of a desired length. In
addition, the autoregressive generation in LLMs is extremely time-consuming,
while the ability of controlling this generated length can reduce the inference
cost by limiting the length. Therefore, we propose a prompt-based length
control method to achieve high-accuracy length controlled generation. In
particular, we adopt reinforcement learning with the reward signal given by
either trainable or rule-based reward models, which further enhances the
length-control ability of LLMs by rewarding outputs that follows pre-defined
control instruction. To enable rule-based inference, we also introduce standard
prompt extractor to collect the standard control information from users' input.
Experiments show that our method significantly improves the accuracy of
prompt-based length control for summarization task on popular datasets like
CNNDM and NYT. Both the standard prompt extractor and the RL-tuned model have
show strong generalization ability to unseen control prompt templates.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2308.11807v1' target='_blank'>Towards an On-device Agent for Text Rewriting</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yun Zhu, Yinxiao Liu, Felix Stahlberg, Shankar Kumar, Yu-hui Chen, Liangchen Luo, Lei Shu, Renjie Liu, Jindong Chen, Lei Meng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-08-22 22:18:38</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated impressive capabilities for
text rewriting. Nonetheless, the large sizes of these models make them
impractical for on-device inference, which would otherwise allow for enhanced
privacy and economical inference. Creating a smaller yet potent language model
for text rewriting presents a formidable challenge because it requires
balancing the need for a small size with the need to retain the emergent
capabilities of the LLM, that requires costly data collection. To address the
above challenge, we introduce a new instruction tuning approach for building a
mobile-centric text rewriting model. Our strategies enable the generation of
high quality training data without any human labeling. In addition, we propose
a heuristic reinforcement learning framework which substantially enhances
performance without requiring preference data. To further bridge the
performance gap with the larger server-side model, we propose an effective
approach that combines the mobile rewrite agent with the server model using a
cascade. To tailor the text rewriting tasks to mobile scenarios, we introduce
MessageRewriteEval, a benchmark that focuses on text rewriting for messages
through natural language instructions. Through empirical experiments, we
demonstrate that our on-device model surpasses the current state-of-the-art
LLMs in text rewriting while maintaining a significantly reduced model size.
Notably, we show that our proposed cascading approach improves model
performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2308.13542v1' target='_blank'>LaGR-SEQ: Language-Guided Reinforcement Learning with Sample-Efficient
  Querying</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Thommen George Karimpanal, Laknath Buddhika Semage, Santu Rana, Hung Le, Truyen Tran, Sunil Gupta, Svetha Venkatesh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-08-21 02:07:35</h6>
<p class='card-text'>Large language models (LLMs) have recently demonstrated their impressive
ability to provide context-aware responses via text. This ability could
potentially be used to predict plausible solutions in sequential decision
making tasks pertaining to pattern completion. For example, by observing a
partial stack of cubes, LLMs can predict the correct sequence in which the
remaining cubes should be stacked by extrapolating the observed patterns (e.g.,
cube sizes, colors or other attributes) in the partial stack. In this work, we
introduce LaGR (Language-Guided Reinforcement learning), which uses this
predictive ability of LLMs to propose solutions to tasks that have been
partially completed by a primary reinforcement learning (RL) agent, in order to
subsequently guide the latter's training. However, as RL training is generally
not sample-efficient, deploying this approach would inherently imply that the
LLM be repeatedly queried for solutions; a process that can be expensive and
infeasible. To address this issue, we introduce SEQ (sample efficient
querying), where we simultaneously train a secondary RL agent to decide when
the LLM should be queried for solutions. Specifically, we use the quality of
the solutions emanating from the LLM as the reward to train this agent. We show
that our proposed framework LaGR-SEQ enables more efficient primary RL
training, while simultaneously minimizing the number of queries to the LLM. We
demonstrate our approach on a series of tasks and highlight the advantages of
our approach, along with its limitations and potential future research
directions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2308.10088v2' target='_blank'>PACE: Improving Prompt with Actor-Critic Editing for Large Language
  Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yihong Dong, Kangcheng Luo, Xue Jiang, Zhi Jin, Ge Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-08-19 18:47:44</h6>
<p class='card-text'>Large language models (LLMs) have showcased remarkable potential across
various tasks by conditioning on prompts. However, the quality of different
human-written prompts leads to substantial discrepancies in LLMs' performance,
and improving prompts usually necessitates considerable human effort and
expertise. To this end, this paper proposes Prompt with Actor-Critic Editing
(PACE) for LLMs to enable automatic prompt editing. Drawing inspiration from
the actor-critic algorithm in reinforcement learning, PACE leverages LLMs as
the dual roles of actors and critics, conceptualizing prompt as a type of
policy. PACE refines prompt, taking into account the feedback from both actors
performing prompt and critics criticizing response. This process helps LLMs
better align prompt to a specific task, thanks to real responses and thinking
from LLMs. We conduct extensive experiments on 24 instruction induction tasks
and 21 big-bench tasks. Experimental results indicate that PACE elevates the
relative performance of medium/low-quality human-written prompts by up to 98\%,
which has comparable performance to high-quality human-written prompts.
Moreover, PACE also exhibits notable efficacy for prompt generation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2308.09583v2' target='_blank'>WizardMath: Empowering Mathematical Reasoning for Large Language Models
  via Reinforced Evol-Instruct</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Yansong Tang, Dongmei Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-08-18 14:23:21</h6>
<p class='card-text'>Large language models (LLMs), such as GPT-4, have shown remarkable
performance in natural language processing (NLP) tasks, including challenging
mathematical reasoning. However, most existing open-source models are only
pre-trained on large-scale internet data and without math-related optimization.
In this paper, we present WizardMath, which enhances the mathematical CoT
reasoning abilities of LLMs without using external python tools, by applying
our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method
to the domain of math. Through extensive experiments on two mathematical
reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary
capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier
open-source LLMs by a substantial margin with higher data efficiency.
Furthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini
Pro and GPT-4-early-version. Additionally, our preliminary exploration
highlights the pivotal role of instruction evolution and process supervision in
achieving exceptional math performance. For more details refer to
https://github.com/nlpxucan/WizardLM</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2308.09376v1' target='_blank'>Leveraging Large Language Models for DRL-Based Anti-Jamming Strategies
  in Zero Touch Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Abubakar S. Ali, Dimitrios Michael Manias, Abdallah Shami, Sami Muhaidat</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-08-18 08:13:23</h6>
<p class='card-text'>As the dawn of sixth-generation (6G) networking approaches, it promises
unprecedented advancements in communication and automation. Among the leading
innovations of 6G is the concept of Zero Touch Networks (ZTNs), aiming to
achieve fully automated, self-optimizing networks with minimal human
intervention. Despite the advantages ZTNs offer in terms of efficiency and
scalability, challenges surrounding transparency, adaptability, and human trust
remain prevalent. Concurrently, the advent of Large Language Models (LLMs)
presents an opportunity to elevate the ZTN framework by bridging the gap
between automated processes and human-centric interfaces. This paper explores
the integration of LLMs into ZTNs, highlighting their potential to enhance
network transparency and improve user interactions. Through a comprehensive
case study on deep reinforcement learning (DRL)-based anti-jamming technique,
we demonstrate how LLMs can distill intricate network operations into
intuitive, human-readable reports. Additionally, we address the technical and
ethical intricacies of melding LLMs with ZTNs, with an emphasis on data
privacy, transparency, and bias reduction. Looking ahead, we identify emerging
research avenues at the nexus of LLMs and ZTNs, advocating for sustained
innovation and interdisciplinary synergy in the domain of automated networks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2308.08998v2' target='_blank'>Reinforced Self-Training (ReST) for Language Modeling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, Nando de Freitas</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-08-17 14:12:48</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) can improve the quality of
large language model's (LLM) outputs by aligning them with human preferences.
We propose a simple algorithm for aligning LLMs with human preferences inspired
by growing batch reinforcement learning (RL), which we call Reinforced
Self-Training (ReST). Given an initial LLM policy, ReST produces a dataset by
generating samples from the policy, which are then used to improve the LLM
policy using offline RL algorithms. ReST is more efficient than typical online
RLHF methods because the training dataset is produced offline, which allows
data reuse. While ReST is a general approach applicable to all generative
learning settings, we focus on its application to machine translation. Our
results show that ReST can substantially improve translation quality, as
measured by automated metrics and human evaluation on machine translation
benchmarks in a compute and sample-efficient manner.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2308.08520v1' target='_blank'>Painter: Teaching Auto-regressive Language Models to Draw Sketches</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Reza Pourreza, Apratim Bhattacharyya, Sunny Panchal, Mingu Lee, Pulkit Madan, Roland Memisevic</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-08-16 17:18:30</h6>
<p class='card-text'>Large language models (LLMs) have made tremendous progress in natural
language understanding and they have also been successfully adopted in other
domains such as computer vision, robotics, reinforcement learning, etc. In this
work, we apply LLMs to image generation tasks by directly generating the
virtual brush strokes to paint an image. We present Painter, an LLM that can
convert user prompts in text description format to sketches by generating the
corresponding brush strokes in an auto-regressive way. We construct Painter
based on off-the-shelf LLM that is pre-trained on a large text corpus, by
fine-tuning it on the new task while preserving language understanding
capabilities. We create a dataset of diverse multi-object sketches paired with
textual prompts that covers several object types and tasks. Painter can
generate sketches from text descriptions, remove objects from canvas, and
detect and classify objects in sketches. Although this is an unprecedented
pioneering work in using LLMs for auto-regressive image generation, the results
are very encouraging.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2308.07308v4' target='_blank'>LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mansi Phute, Alec Helbling, Matthew Hull, ShengYun Peng, Sebastian Szyller, Cory Cornelius, Duen Horng Chau</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-08-14 17:54:10</h6>
<p class='card-text'>Large language models (LLMs) are popular for high-quality text generation but
can produce harmful content, even when aligned with human values through
reinforcement learning. Adversarial prompts can bypass their safety measures.
We propose LLM Self Defense, a simple approach to defend against these attacks
by having an LLM screen the induced responses. Our method does not require any
fine-tuning, input preprocessing, or iterative output generation. Instead, we
incorporate the generated content into a pre-defined prompt and employ another
instance of an LLM to analyze the text and predict whether it is harmful. We
test LLM Self Defense on GPT 3.5 and Llama 2, two of the current most prominent
LLMs against various types of attacks, such as forcefully inducing affirmative
responses to prompts and prompt engineering attacks. Notably, LLM Self Defense
succeeds in reducing the attack success rate to virtually 0 using both GPT 3.5
and Llama 2. The code is publicly available at
https://github.com/poloclub/llm-self-defense</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2308.09721v2' target='_blank'>A new solution and concrete implementation steps for Artificial General
  Intelligence</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yongcong Chen, Ting Zeng, Xingyue Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-08-12 13:31:02</h6>
<p class='card-text'>In this paper, we propose a new approach to building a artificial general
intelligence with self awareness, which includes: (1) a new method to implement
attention mechanisms; (2) a way to give machines self-demands; (3) how to form
a value evaluation system compatible with the network; (4) a way to create the
world models; (5) how to realize a top-down, hierarchical thinking
decision-making chain; (6) a way to achieve general decision-making and
response capabilities; (7) a way for a machine to directly obtain human
experience through language. In the paper, we first analyze some of the
shortcomings of current LLMs (Large Language Model) and propose ideas for
improvement. Then we analyze why our scheme can solve the above problems and
provide detailed steps for implementing our scheme. In chapter 4, we have
presented a step-by-step mplementation roadmap. And in chapter 5, we have
presented a specific implementation demonstration. In chapter 6, we analyze the
advantages and disadvantages of our scheme and propose further research
directions. In this article, we have put forward how to create genuine
artificial general intelligence step by step. It can handle data of all
modalities in a unified form and can directly understand the experience that
humans already possess through language, thus avoiding the problem that
reinforcement learning is required for every decision-making process.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2308.06463v2' target='_blank'>GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, Zhaopeng Tu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-08-12 04:05:57</h6>
<p class='card-text'>Safety lies at the core of the development of Large Language Models (LLMs).
There is ample work on aligning LLMs with human ethics and preferences,
including data filtering in pretraining, supervised fine-tuning, reinforcement
learning from human feedback, and red teaming, etc. In this study, we discover
that chat in cipher can bypass the safety alignment techniques of LLMs, which
are mainly conducted in natural languages. We propose a novel framework
CipherChat to systematically examine the generalizability of safety alignment
to non-natural languages -- ciphers. CipherChat enables humans to chat with
LLMs through cipher prompts topped with system role descriptions and few-shot
enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs,
including ChatGPT and GPT-4 for different representative human ciphers across
11 safety domains in both English and Chinese. Experimental results show that
certain ciphers succeed almost 100% of the time to bypass the safety alignment
of GPT-4 in several safety domains, demonstrating the necessity of developing
safety alignment for non-natural languages. Notably, we identify that LLMs seem
to have a ''secret cipher'', and propose a novel SelfCipher that uses only role
play and several demonstrations in natural language to evoke this capability.
SelfCipher surprisingly outperforms existing human ciphers in almost all cases.
Our code and data will be released at https://github.com/RobustNLP/CipherChat.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2308.06212v1' target='_blank'>A Large Language Model Enhanced Conversational Recommender System</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yue Feng, Shuchang Liu, Zhenghai Xue, Qingpeng Cai, Lantao Hu, Peng Jiang, Kun Gai, Fei Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-08-11 16:30:44</h6>
<p class='card-text'>Conversational recommender systems (CRSs) aim to recommend high-quality items
to users through a dialogue interface. It usually contains multiple sub-tasks,
such as user preference elicitation, recommendation, explanation, and item
information search. To develop effective CRSs, there are some challenges: 1)
how to properly manage sub-tasks; 2) how to effectively solve different
sub-tasks; and 3) how to correctly generate responses that interact with users.
Recently, Large Language Models (LLMs) have exhibited an unprecedented ability
to reason and generate, presenting a new opportunity to develop more powerful
CRSs. In this work, we propose a new LLM-based CRS, referred to as LLMCRS, to
address the above challenges. For sub-task management, we leverage the
reasoning ability of LLM to effectively manage sub-task. For sub-task solving,
we collaborate LLM with expert models of different sub-tasks to achieve the
enhanced performance. For response generation, we utilize the generation
ability of LLM as a language interface to better interact with users.
Specifically, LLMCRS divides the workflow into four stages: sub-task detection,
model matching, sub-task execution, and response generation. LLMCRS also
designs schema-based instruction, demonstration-based instruction, dynamic
sub-task and model matching, and summary-based generation to instruct LLM to
generate desired results in the workflow. Finally, to adapt LLM to
conversational recommendations, we also propose to fine-tune LLM with
reinforcement learning from CRSs performance feedback, referred to as RLPF.
Experimental results on benchmark datasets show that LLMCRS with RLPF
outperforms the existing methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2308.05585v1' target='_blank'>Proximal Policy Optimization Actual Combat: Manipulating Output
  Tokenizer Length</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Miao Fan, Chen Hu, Shuchang Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-08-10 13:50:17</h6>
<p class='card-text'>The Reinforcement Learning from Human Feedback (RLHF) plays a pivotal role in
shaping the impact of large language models (LLMs), contributing significantly
to controlling output toxicity and selecting output styles, particularly as
LLMs often harbor misleading content, highlighting the urgency to align them
with human values for secure AI systems. The RLHF, characterized by complexity,
instability, and sensitivity to hyperparameters, makes the evaluation of the
reward model for complex tasks challenging, thereby further complicating the
use of Proximal Policy Optimization (PPO). In this paper, we introduce a simple
task designed to employ Gloden as a reward model that validates the
effectiveness of PPO and inspires it, primarily explaining the task of
utilizing PPO to manipulate the tokenizer length of the output generated by the
model. Experiments confirm that PPO is not only effective in manipulating the
output tokenizer length to a certain extent in this type of task but also
exhibits facilitated training once the influence of the reward model effect is
excluded, making it an exciting development.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2308.04386v1' target='_blank'>Learning Evaluation Models from Large Language Models for Sequence
  Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chenglong Wang, Hang Zhou, Kaiyan Chang, Tongran Liu, Chunliang Zhang, Quan Du, Tong Xiao, Jingbo Zhu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-08-08 16:41:16</h6>
<p class='card-text'>Large language models achieve state-of-the-art performance on sequence
generation evaluation, but typically have a large number of parameters. This is
a computational challenge as presented by applying their evaluation capability
at scale. To overcome the challenge, in this paper, we propose \textbf{ECT}, an
\textbf{e}valuation \textbf{c}apability \textbf{t}ransfer method, to transfer
the evaluation capability from LLMs to relatively lightweight language models.
Based on the proposed ECT, we learn various evaluation models from ChatGPT, and
employ them as reward models to improve sequence generation models via
reinforcement learning and reranking approaches. Experimental results on
machine translation, text style transfer, and summarization tasks demonstrate
the effectiveness of our ECT. Notably, applying the learned evaluation models
to sequence generation models results in better generated sequences as
evaluated by commonly used metrics and ChatGPT.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2308.03549v3' target='_blank'>Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language
  Model through Expert Feedback and Real-world Multi-turn Dialogue</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Songhua Yang, Hanjie Zhao, Senbin Zhu, Guangyu Zhou, Hongfei Xu, Yuxiang Jia, Hongying Zan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-08-07 12:56:13</h6>
<p class='card-text'>Recent advances in Large Language Models (LLMs) have achieved remarkable
breakthroughs in understanding and responding to user intents. However, their
performance lag behind general use cases in some expertise domains, such as
Chinese medicine. Existing efforts to incorporate Chinese medicine into LLMs
rely on Supervised Fine-Tuning (SFT) with single-turn and distilled dialogue
data. These models lack the ability for doctor-like proactive inquiry and
multi-turn comprehension and cannot align responses with experts' intentions.
In this work, we introduce Zhongjing, the first Chinese medical LLaMA-based LLM
that implements an entire training pipeline from continuous pre-training, SFT,
to Reinforcement Learning from Human Feedback (RLHF). Additionally, we
construct a Chinese multi-turn medical dialogue dataset of 70,000 authentic
doctor-patient dialogues, CMtMedQA, which significantly enhances the model's
capability for complex dialogue and proactive inquiry initiation. We also
define a refined annotation rule and evaluation criteria given the unique
characteristics of the biomedical domain. Extensive experimental results show
that Zhongjing outperforms baselines in various capacities and matches the
performance of ChatGPT in some abilities, despite the 100x parameters. Ablation
studies also demonstrate the contributions of each component: pre-training
enhances medical knowledge, and RLHF further improves instruction-following
ability and safety. Our code, datasets, and models are available at
https://github.com/SupritYoung/Zhongjing.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2307.16180v1' target='_blank'>Do LLMs Possess a Personality? Making the MBTI Test an Amazing
  Evaluation for Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Keyu Pan, Yawen Zeng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-07-30 09:34:35</h6>
<p class='card-text'>The field of large language models (LLMs) has made significant progress, and
their knowledge storage capacity is approaching that of human beings.
Furthermore, advanced techniques, such as prompt learning and reinforcement
learning, are being employed to address ethical concerns and hallucination
problems associated with LLMs, bringing them closer to aligning with human
values. This situation naturally raises the question of whether LLMs with
human-like abilities possess a human-like personality? In this paper, we aim to
investigate the feasibility of using the Myers-Briggs Type Indicator (MBTI), a
widespread human personality assessment tool, as an evaluation metric for LLMs.
Specifically, extensive experiments will be conducted to explore: 1) the
personality types of different LLMs, 2) the possibility of changing the
personality types by prompt engineering, and 3) How does the training dataset
affect the model's personality. Although the MBTI is not a rigorous assessment,
it can still reflect the similarity between LLMs and human personality. In
practice, the MBTI has the potential to serve as a rough indicator. Our codes
are available at
https://github.com/HarderThenHarder/transformers_tasks/tree/main/LLM/llms_mbti.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2307.16039v2' target='_blank'>Okapi: Instruction-tuned Large Language Models in Multiple Languages
  with Reinforcement Learning from Human Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan A. Rossi, Thien Huu Nguyen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-07-29 18:01:46</h6>
<p class='card-text'>A key technology for the development of large language models (LLMs) involves
instruction tuning that helps align the models' responses with human
expectations to realize impressive learning abilities. Two major approaches for
instruction tuning characterize supervised fine-tuning (SFT) and reinforcement
learning from human feedback (RLHF), which are currently applied to produce the
best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for
research and development efforts, various instruction-tuned open-source LLMs
have also been introduced recently, e.g., Alpaca, Vicuna, to name a few.
However, existing open-source LLMs have only been instruction-tuned for English
and a few popular languages, thus hindering their impacts and accessibility to
many other languages in the world. Among a few very recent work to explore
instruction tuning for LLMs in multiple languages, SFT has been used as the
only approach to instruction-tune LLMs for multiple languages. This has left a
significant gap for fine-tuned LLMs based on RLHF in diverse languages and
raised important questions on how RLHF can boost the performance of
multilingual instruction tuning. To overcome this issue, we present Okapi, the
first system with instruction-tuned LLMs based on RLHF for multiple languages.
Okapi introduces instruction and response-ranked data in 26 diverse languages
to facilitate the experiments and development of future multilingual LLM
research. We also present benchmark datasets to enable the evaluation of
generative LLMs in multiple languages. Our experiments demonstrate the
advantages of RLHF for multilingual instruction over SFT for different base
models and datasets. Our framework and resources are released at
https://github.com/nlp-uoregon/Okapi.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2307.15833v1' target='_blank'>Dialogue Shaping: Empowering Agents through NPC Interaction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wei Zhou, Xiangyu Peng, Mark Riedl</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-07-28 22:44:54</h6>
<p class='card-text'>One major challenge in reinforcement learning (RL) is the large amount of
steps for the RL agent needs to converge in the training process and learn the
optimal policy, especially in text-based game environments where the action
space is extensive. However, non-player characters (NPCs) sometimes hold some
key information about the game, which can potentially help to train RL agents
faster. Thus, this paper explores how to interact and converse with NPC agents
to get the key information using large language models (LLMs), as well as
incorporate this information to speed up RL agent's training using knowledge
graphs (KGs) and Story Shaping.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2307.15217v2' target='_blank'>Open Problems and Fundamental Limitations of Reinforcement Learning from
  Human Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, Dylan Hadfield-Menell</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-07-27 22:29:25</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) is a technique for training
AI systems to align with human goals. RLHF has emerged as the central method
used to finetune state-of-the-art large language models (LLMs). Despite this
popularity, there has been relatively little public work systematizing its
flaws. In this paper, we (1) survey open problems and fundamental limitations
of RLHF and related methods; (2) overview techniques to understand, improve,
and complement RLHF in practice; and (3) propose auditing and disclosure
standards to improve societal oversight of RLHF systems. Our work emphasizes
the limitations of RLHF and highlights the importance of a multi-faceted
approach to the development of safer AI systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2307.14936v1' target='_blank'>PanGu-Coder2: Boosting Large Language Models for Code with Ranking
  Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang Zhao, Yuenan Guo, Qianxiang Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-07-27 15:28:29</h6>
<p class='card-text'>Large Language Models for Code (Code LLM) are flourishing. New and powerful
models are released on a weekly basis, demonstrating remarkable performance on
the code generation task. Various approaches have been proposed to boost the
code generation performance of pre-trained Code LLMs, such as supervised
fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we
propose a novel RRTF (Rank Responses to align Test&Teacher Feedback) framework,
which can effectively and efficiently boost pre-trained large language models
for code generation. Under this framework, we present PanGu-Coder2, which
achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through
an extensive evaluation on CoderEval and LeetCode benchmarks, we show that
PanGu-Coder2 consistently outperforms all previous Code LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2307.12798v3' target='_blank'>RRAML: Reinforced Retrieval Augmented Machine Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Andrea Bacciu, Florin Cuconasu, Federico Siciliano, Fabrizio Silvestri, Nicola Tonellotto, Giovanni Trappolini</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-07-24 13:51:19</h6>
<p class='card-text'>The emergence of large language models (LLMs) has revolutionized machine
learning and related fields, showcasing remarkable abilities in comprehending,
generating, and manipulating human language. However, their conventional usage
through API-based text prompt submissions imposes certain limitations in terms
of context constraints and external source availability. To address these
challenges, we propose a novel framework called Reinforced Retrieval Augmented
Machine Learning (RRAML). RRAML integrates the reasoning capabilities of LLMs
with supporting information retrieved by a purpose-built retriever from a vast
user-provided database. By leveraging recent advancements in reinforcement
learning, our method effectively addresses several critical challenges.
Firstly, it circumvents the need for accessing LLM gradients. Secondly, our
method alleviates the burden of retraining LLMs for specific tasks, as it is
often impractical or impossible due to restricted access to the model and the
computational intensity involved. Additionally we seamlessly link the
retriever's task with the reasoner, mitigating hallucinations and reducing
irrelevant, and potentially damaging retrieved documents. We believe that the
research agenda outlined in this paper has the potential to profoundly impact
the field of AI, democratizing access to and utilization of LLMs for a wide
range of entities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2307.11922v1' target='_blank'>Selective Perception: Optimizing State Descriptions with Reinforcement
  Learning for Language Model Actors</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kolby Nottingham, Yasaman Razeghi, Kyungmin Kim, JB Lanier, Pierre Baldi, Roy Fox, Sameer Singh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-07-21 22:02:50</h6>
<p class='card-text'>Large language models (LLMs) are being applied as actors for sequential
decision making tasks in domains such as robotics and games, utilizing their
general world knowledge and planning abilities. However, previous work does
little to explore what environment state information is provided to LLM actors
via language. Exhaustively describing high-dimensional states can impair
performance and raise inference costs for LLM actors. Previous LLM actors avoid
the issue by relying on hand-engineered, task-specific protocols to determine
which features to communicate about a state and which to leave out. In this
work, we propose Brief Language INputs for DEcision-making Responses (BLINDER),
a method for automatically selecting concise state descriptions by learning a
value function for task-conditioned state descriptions. We evaluate BLINDER on
the challenging video game NetHack and a robotic manipulation task. Our method
improves task success rate, reduces input size and compute costs, and
generalizes between LLM actors.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2307.11346v1' target='_blank'>CohortGPT: An Enhanced GPT for Participant Recruitment in Clinical Study</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zihan Guan, Zihao Wu, Zhengliang Liu, Dufan Wu, Hui Ren, Quanzheng Li, Xiang Li, Ninghao Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-07-21 04:43:00</h6>
<p class='card-text'>Participant recruitment based on unstructured medical texts such as clinical
notes and radiology reports has been a challenging yet important task for the
cohort establishment in clinical research. Recently, Large Language Models
(LLMs) such as ChatGPT have achieved tremendous success in various downstream
tasks thanks to their promising performance in language understanding,
inference, and generation. It is then natural to test their feasibility in
solving the cohort recruitment task, which involves the classification of a
given paragraph of medical text into disease label(s). However, when applied to
knowledge-intensive problem settings such as medical text classification, where
the LLMs are expected to understand the decision made by human experts and
accurately identify the implied disease labels, the LLMs show a mediocre
performance. A possible explanation is that, by only using the medical text,
the LLMs neglect to use the rich context of additional information that
languages afford. To this end, we propose to use a knowledge graph as auxiliary
information to guide the LLMs in making predictions. Moreover, to further boost
the LLMs adapt to the problem setting, we apply a chain-of-thought (CoT) sample
selection strategy enhanced by reinforcement learning, which selects a set of
CoT samples given each individual medical report. Experimental results and
various ablation studies show that our few-shot learning method achieves
satisfactory performance compared with fine-tuning strategies and gains superb
advantages when the available data is limited. The code and sample dataset of
the proposed CohortGPT model is available at:
https://anonymous.4open.science/r/CohortGPT-4872/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2307.10512v1' target='_blank'>IvyGPT: InteractiVe Chinese pathwaY language model in medical domain</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rongsheng Wang, Yaofei Duan, ChanTong Lam, Jiexi Chen, Jiangsheng Xu, Haoming Chen, Xiaohong Liu, Patrick Cheong-Iao Pang, Tao Tan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-07-20 01:11:14</h6>
<p class='card-text'>General large language models (LLMs) such as ChatGPT have shown remarkable
success. However, such LLMs have not been widely adopted for medical purposes,
due to poor accuracy and inability to provide medical advice. We propose
IvyGPT, an LLM based on LLaMA that is trained and fine-tuned with high-quality
medical question-answer (QA) instances and Reinforcement Learning from Human
Feedback (RLHF). After supervised fine-tuning, IvyGPT has good multi-turn
conversation capabilities, but it cannot perform like a doctor in other
aspects, such as comprehensive diagnosis. Through RLHF, IvyGPT can output
richer diagnosis and treatment answers that are closer to human. In the
training, we used QLoRA to train 33 billion parameters on a small number of
NVIDIA A100 (80GB) GPUs. Experimental results show that IvyGPT has outperformed
other medical GPT models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2307.10485v2' target='_blank'>FinGPT: Democratizing Internet-scale Data for Financial Large Language
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiao-Yang Liu, Guoxuan Wang, Hongyang Yang, Daochen Zha</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-07-19 22:43:57</h6>
<p class='card-text'>Large language models (LLMs) have demonstrated remarkable proficiency in
understanding and generating human-like texts, which may potentially
revolutionize the finance industry. However, existing LLMs often fall short in
the financial field, which is mainly attributed to the disparities between
general text data and financial text data. Unfortunately, there is only a
limited number of financial text datasets available, and BloombergGPT, the
first financial LLM (FinLLM), is close-sourced (only the training logs were
released). In light of this, we aim to democratize Internet-scale financial
data for LLMs, which is an open challenge due to diverse data sources, low
signal-to-noise ratio, and high time-validity. To address the challenges, we
introduce an open-sourced and data-centric framework, Financial Generative
Pre-trained Transformer (FinGPT), that automates the collection and curation of
real-time financial data from 34 diverse sources on the Internet, providing
researchers and practitioners with accessible and transparent resources to
develop their FinLLMs. Additionally, we propose a simple yet effective strategy
for fine-tuning FinLLM using the inherent feedback from the market, dubbed
Reinforcement Learning with Stock Prices (RLSP). We also adopt the Low-rank
Adaptation (LoRA, QLoRA) method that enables users to customize their own
FinLLMs from general-purpose LLMs at a low cost. Finally, we showcase several
FinGPT applications, including robo-advisor, sentiment analysis for algorithmic
trading, and low-code development. FinGPT aims to democratize FinLLMs,
stimulate innovation, and unlock new opportunities in open finance. The codes
have been open-sourced.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2307.05915v2' target='_blank'>Prompt Generate Train (PGT): Few-shot Domain Adaption of Retrieval
  Augmented Generation Models for Open Book Question-Answering</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:C. S. Krishna</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-07-12 04:44:31</h6>
<p class='card-text'>We propose a framework - Prompt, Generate, Train (PGT) - to efficiently
develop a generative question-answering model for open-book question-answering
over a proprietary collection of text documents. The framework adapts a
retriever augmented generation (RAG) model to the target domain using
supervised fine-tuning and reinforcement learning with synthetic feedback in a
few-shot setting. This, we hypothesize, will yield an aligned, uncertainty
calibrated model that is competitive with GPT-4 based in-context retrieval
augmented generation in generating relevant answers at lower serving costs. The
framework's synthetic generation pipeline will generate synthetic training data
comprising <passage, question, answer> tuples using an open-source LLM and a
novel consistency filtering scheme. The pipeline will be designed to generate
both abstractive and extractive questions that span the entire corpus. The
framework proposes to fine-tune a smaller RAG model comprising a dense
retriever (ColBERTv2) and a smaller sized LLM on the synthetic dataset. In
parallel, the framework will train a Reward model to score domain grounded
answers higher than hallucinated answers using an a priori relevance ordering
of synthetically assembled samples. In the next phase, the framework will align
the RAG model with the target domain using reinforcement learning (Proximal
Policy Optimization). This step may improve the RAG model's ability to generate
grounded answers and ignore out of domain questions. In the final phase, the
framework will calibrate the model's uncertainty for extractive
question-answers.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2307.04964v2' target='_blank'>Secrets of RLHF in Large Language Models Part I: PPO</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, Limao Xiong, Lu Chen, Zhiheng Xi, Nuo Xu, Wenbin Lai, Minghao Zhu, Cheng Chang, Zhangyue Yin, Rongxiang Weng, Wensen Cheng, Haoran Huang, Tianxiang Sun, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-07-11 01:55:24</h6>
<p class='card-text'>Large language models (LLMs) have formulated a blueprint for the advancement
of artificial general intelligence. Its primary objective is to function as a
human-centric (helpful, honest, and harmless) assistant. Alignment with humans
assumes paramount significance, and reinforcement learning with human feedback
(RLHF) emerges as the pivotal technological paradigm underpinning this pursuit.
Current technical routes usually include \textbf{reward models} to measure
human preferences, \textbf{Proximal Policy Optimization} (PPO) to optimize
policy model outputs, and \textbf{process supervision} to improve step-by-step
reasoning capabilities. However, due to the challenges of reward design,
environment interaction, and agent training, coupled with huge trial and error
cost of large language models, there is a significant barrier for AI
researchers to motivate the development of technical alignment and safe landing
of LLMs. The stable training of RLHF has still been a puzzle. In the first
report, we dissect the framework of RLHF, re-evaluate the inner workings of
PPO, and explore how the parts comprising PPO algorithms impact policy agent
training. We identify policy constraints being the key factor for the effective
implementation of the PPO algorithm. Therefore, we explore the PPO-max, an
advanced version of PPO algorithm, to efficiently improve the training
stability of the policy model. Based on our main results, we perform a
comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT.
The absence of open-source implementations has posed significant challenges to
the investigation of LLMs alignment. Therefore, we are eager to release
technical reports, reward models and PPO codes, aiming to make modest
contributions to the advancement of LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2307.04657v3' target='_blank'>BeaverTails: Towards Improved Safety Alignment of LLM via a
  Human-Preference Dataset</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, Yaodong Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-07-10 15:56:17</h6>
<p class='card-text'>In this paper, we introduce the BeaverTails dataset, aimed at fostering
research on safety alignment in large language models (LLMs). This dataset
uniquely separates annotations of helpfulness and harmlessness for
question-answering pairs, thus offering distinct perspectives on these crucial
attributes. In total, we have gathered safety meta-labels for 333,963
question-answer (QA) pairs and 361,903 pairs of expert comparison data for both
the helpfulness and harmlessness metrics. We further showcase applications of
BeaverTails in content moderation and reinforcement learning with human
feedback (RLHF), emphasizing its potential for practical safety measures in
LLMs. We believe this dataset provides vital resources for the community,
contributing towards the safe development and deployment of LLMs. Our project
page is available at the following URL:
https://sites.google.com/view/pku-beavertails.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2307.04349v2' target='_blank'>RLTF: Reinforcement Learning from Unit Test Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, Deheng Ye</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-07-10 05:18:18</h6>
<p class='card-text'>The goal of program synthesis, or code generation, is to generate executable
code based on given descriptions. Recently, there has been an increasing number
of studies employing reinforcement learning (RL) to improve the performance of
large language models (LLMs) for code. However, current representative works
either rely solely on offline frameworks, limiting the exploration of new
sample spaces, or fall short in the utilization of unit test signals, not
accounting for specific error locations within the code. To address these
issues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback,
a novel online RL framework with unit test feedback of multi-granularity for
refining code LLMs. Our approach generates data in real-time during training
and simultaneously utilizes fine-grained feedback signals to guide the model
towards producing higher-quality code. Extensive experiments show that RLTF
achieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our
code is available at: https://github.com/Zyq-scut/RLTF.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2307.02157v1' target='_blank'>Generative Job Recommendations with Large Language Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhi Zheng, Zhaopeng Qiu, Xiao Hu, Likang Wu, Hengshu Zhu, Hui Xiong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-07-05 09:58:08</h6>
<p class='card-text'>The rapid development of online recruitment services has encouraged the
utilization of recommender systems to streamline the job seeking process.
Predominantly, current job recommendations deploy either collaborative
filtering or person-job matching strategies. However, these models tend to
operate as "black-box" systems and lack the capacity to offer explainable
guidance to job seekers. Moreover, conventional matching-based recommendation
methods are limited to retrieving and ranking existing jobs in the database,
restricting their potential as comprehensive career AI advisors. To this end,
here we present GIRL (GeneratIve job Recommendation based on Large language
models), a novel approach inspired by recent advancements in the field of Large
Language Models (LLMs). We initially employ a Supervised Fine-Tuning (SFT)
strategy to instruct the LLM-based generator in crafting suitable Job
Descriptions (JDs) based on the Curriculum Vitae (CV) of a job seeker.
Moreover, we propose to train a model which can evaluate the matching degree
between CVs and JDs as a reward model, and we use Proximal Policy Optimization
(PPO)-based Reinforcement Learning (RL) method to further fine-tine the
generator. This aligns the generator with recruiter feedback, tailoring the
output to better meet employer preferences. In particular, GIRL serves as a job
seeker-centric generative model, providing job suggestions without the need of
a candidate set. This capability also enhances the performance of existing job
recommendation models by supplementing job seeking features with generated
content. With extensive experiments on a large-scale real-world dataset, we
demonstrate the substantial effectiveness of our approach. We believe that GIRL
introduces a paradigm-shifting approach to job recommendation systems,
fostering a more personalized and comprehensive job-seeking experience.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2307.00593v3' target='_blank'>Isolating Compiler Bugs by Generating Effective Witness Programs with
  Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haoxin Tu, Zhide Zhou, He Jiang, Imam Nur Bani Yusuf, Yuxian Li, Lingxiao Jiang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-07-02 15:20:54</h6>
<p class='card-text'>Compiler bugs pose a significant threat to safety-critical applications, and
promptly as well as effectively isolating these bugs is crucial for assuring
the quality of compilers. However, the limited availability of debugging
information on reported bugs complicates the compiler bug isolation task.
Existing compiler bug isolation approaches convert the problem into a test
program mutation problem, but they are still limited by ineffective mutation
strategies or high human effort requirements. Drawing inspiration from the
recent progress of pre-trained Large Language Models (LLMs), such as ChatGPT,
in code generation, we propose a new approach named LLM4CBI to utilize LLMs to
generate effective test programs for compiler bug isolation. However, using
LLMs directly for test program mutation may not yield the desired results due
to the challenges associated with formulating precise prompts and selecting
specialized prompts. To overcome the challenges, three new components are
designed in LLM4CBI. First, LLM4CBI utilizes a program complexity-guided prompt
production component, which leverages data and control flow analysis to
identify the most valuable variables and locations in programs for mutation.
Second, LLM4CBI employs a memorized prompt selection component, which adopts
reinforcement learning to select specialized prompts for mutating test programs
continuously. Third, a test program validation component is proposed to select
specialized feedback prompts to avoid repeating the same mistakes during the
mutation process. Compared with state-of-the-art approaches over 120 real bugs
from GCC and LLVM, our evaluation demonstrates the advantages of LLM4CBI: It
can isolate 69.70%/21.74% and 24.44%/8.92% more bugs than DiWi and RecBi within
Top-1/Top-5 ranked results. We also demonstrate that the LLMs component used in
LLM4CBI can be easily replaced while still achieving reasonable results.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2306.17492v2' target='_blank'>Preference Ranking Optimization for Human Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, Houfeng Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-06-30 09:07:37</h6>
<p class='card-text'>Large language models (LLMs) often contain misleading content, emphasizing
the need to align them with human values to ensure secure AI systems.
Reinforcement learning from human feedback (RLHF) has been employed to achieve
this alignment. However, it encompasses two main drawbacks: (1) RLHF exhibits
complexity, instability, and sensitivity to hyperparameters in contrast to SFT.
(2) Despite massive trial-and-error, multiple sampling is reduced to pair-wise
contrast, thus lacking contrasts from a macro perspective. In this paper, we
propose Preference Ranking Optimization (PRO) as an efficient SFT algorithm to
directly fine-tune LLMs for human alignment. PRO extends the pair-wise contrast
to accommodate preference rankings of any length. By iteratively contrasting
candidates, PRO instructs the LLM to prioritize the best response while
progressively ranking the rest responses. In this manner, PRO effectively
transforms human alignment into aligning the probability ranking of n responses
generated by LLM with the preference ranking of humans towards these responses.
Experiments have shown that PRO outperforms baseline algorithms, achieving
comparable results to ChatGPT and human responses through automatic-based,
reward-based, GPT-4, and human evaluations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2306.14898v3' target='_blank'>InterCode: Standardizing and Benchmarking Interactive Coding with
  Execution Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:John Yang, Akshara Prabhakar, Karthik Narasimhan, Shunyu Yao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-06-26 17:59:50</h6>
<p class='card-text'>Humans write code in a fundamentally interactive manner and rely on constant
execution feedback to correct errors, resolve ambiguities, and decompose tasks.
While LLMs have recently exhibited promising coding capabilities, current
coding benchmarks mostly consider a static instruction-to-code sequence
transduction process, which has the potential for error propagation and a
disconnect between the generated code and its final execution environment. To
address this gap, we introduce InterCode, a lightweight, flexible, and
easy-to-use framework of interactive coding as a standard reinforcement
learning (RL) environment, with code as actions and execution feedback as
observations. Our framework is language and platform agnostic, uses
self-contained Docker environments to provide safe and reproducible execution,
and is compatible out-of-the-box with traditional seq2seq coding methods, while
enabling the development of new methods for interactive code generation. We use
InterCode to create three interactive code environments with Bash, SQL, and
Python as action spaces, leveraging data from the static NL2Bash, Spider, and
MBPP datasets. We demonstrate InterCode's viability as a testbed by evaluating
multiple state-of-the-art LLMs configured with different prompting strategies
such as ReAct and Plan & Solve. Our results showcase the benefits of
interactive code generation and demonstrate that InterCode can serve as a
challenging benchmark for advancing code understanding and generation
capabilities. InterCode is designed to be easily extensible and can even be
used to create new tasks such as Capture the Flag, a popular coding puzzle that
is inherently multi-step and involves multiple programming languages. Project
site with code and data: https://intercode-benchmark.github.io</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2306.11816v2' target='_blank'>Learning to Generate Better Than Your LLM</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jonathan D. Chang, Kiante Brantley, Rajkumar Ramamurthy, Dipendra Misra, Wen Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-06-20 18:19:17</h6>
<p class='card-text'>Reinforcement learning (RL) has emerged as a powerful paradigm for
fine-tuning Large Language Models (LLMs) for text generation. In particular,
recent LLMs such as ChatGPT and GPT-4 can engage in fluent conversations with
users after finetuning with RL. Capitalizing on key properties of text
generation, we seek to investigate RL algorithms beyond general purpose
algorithms like Proximal Policy Optimization (PPO). In particular, we extend RL
algorithms to allow them to interact with a dynamic black-box guide LLM and
propose RL with guided feedback (RLGF), a suite of RL algorithms for LLM
fine-tuning. We provide two ways for the guide LLM to interact with the LLM to
be optimized for maximizing rewards. The guide LLM can generate text which
serves as additional starting states for the RL optimization procedure. The
guide LLM can also be used to complete the partial sentences generated by the
LLM that is being optimized, treating the guide LLM as an expert to imitate and
surpass eventually. We experiment on the IMDB positive sentiment, CommonGen,
and TL;DR summarization tasks. We show that our RL algorithms achieve higher
performance than supervised learning (SL) and the RL baseline PPO,
demonstrating the benefit of interaction with the guide LLM. On both CommonGen
and TL;DR, we not only outperform our SL baselines but also improve upon PPO
across a variety of metrics beyond the one we optimized for. Our code can be
found at https://github.com/Cornell-RL/tril.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2306.11731v2' target='_blank'>Learning Profitable NFT Image Diffusions via Multiple Visual-Policy
  Guided Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Huiguo He, Tianfu Wang, Huan Yang, Jianlong Fu, Nicholas Jing Yuan, Jian Yin, Hongyang Chao, Qi Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-06-20 17:59:46</h6>
<p class='card-text'>We study the task of generating profitable Non-Fungible Token (NFT) images
from user-input texts. Recent advances in diffusion models have shown great
potential for image generation. However, existing works can fall short in
generating visually-pleasing and highly-profitable NFT images, mainly due to
the lack of 1) plentiful and fine-grained visual attribute prompts for an NFT
image, and 2) effective optimization metrics for generating high-quality NFT
images. To solve these challenges, we propose a Diffusion-based generation
framework with Multiple Visual-Policies as rewards (i.e., Diffusion-MVP) for
NFT images. The proposed framework consists of a large language model (LLM), a
diffusion-based image generator, and a series of visual rewards by design.
First, the LLM enhances a basic human input (such as "panda") by generating
more comprehensive NFT-style prompts that include specific visual attributes,
such as "panda with Ninja style and green background." Second, the
diffusion-based image generator is fine-tuned using a large-scale NFT dataset
to capture fine-grained image styles and accessory compositions of popular NFT
elements. Third, we further propose to utilize multiple visual-policies as
optimization goals, including visual rarity levels, visual aesthetic scores,
and CLIP-based text-image relevances. This design ensures that our proposed
Diffusion-MVP is capable of minting NFT images with high visual quality and
market value. To facilitate this research, we have collected the largest
publicly available NFT image dataset to date, consisting of 1.5 million
high-quality images with corresponding texts and market values. Extensive
experiments including objective evaluations and user studies demonstrate that
our framework can generate NFT images showing more visually engaging elements
and higher market value, compared with SOTA approaches.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2306.10985v1' target='_blank'>LARG, Language-based Automatic Reward and Goal Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Julien Perez, Denys Proux, Claude Roux, Michael Niemaz</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-06-19 14:52:39</h6>
<p class='card-text'>Goal-conditioned and Multi-Task Reinforcement Learning (GCRL and MTRL)
address numerous problems related to robot learning, including locomotion,
navigation, and manipulation scenarios. Recent works focusing on
language-defined robotic manipulation tasks have led to the tedious production
of massive human annotations to create dataset of textual descriptions
associated with trajectories. To leverage reinforcement learning with
text-based task descriptions, we need to produce reward functions associated
with individual tasks in a scalable manner. In this paper, we leverage recent
capabilities of Large Language Models (LLMs) and introduce \larg,
Language-based Automatic Reward and Goal Generation, an approach that converts
a text-based task description into its corresponding reward and goal-generation
functions We evaluate our approach for robotic manipulation and demonstrate its
ability to train and execute policies in a scalable manner, without the need
for handcrafted reward functions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2306.08094v2' target='_blank'>Can ChatGPT Enable ITS? The Case of Mixed Traffic Control via
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Michael Villarreal, Bibek Poudel, Weizi Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-06-13 19:27:18</h6>
<p class='card-text'>The surge in Reinforcement Learning (RL) applications in Intelligent
Transportation Systems (ITS) has contributed to its growth as well as
highlighted key challenges. However, defining objectives of RL agents in
traffic control and management tasks, as well as aligning policies with these
goals through an effective formulation of Markov Decision Process (MDP), can be
challenging and often require domain experts in both RL and ITS. Recent
advancements in Large Language Models (LLMs) such as GPT-4 highlight their
broad general knowledge, reasoning capabilities, and commonsense priors across
various domains. In this work, we conduct a large-scale user study involving 70
participants to investigate whether novices can leverage ChatGPT to solve
complex mixed traffic control problems. Three environments are tested,
including ring road, bottleneck, and intersection. We find ChatGPT has mixed
results. For intersection and bottleneck, ChatGPT increases number of
successful policies by 150% and 136% compared to solely beginner capabilities,
with some of them even outperforming experts. However, ChatGPT does not provide
consistent improvements across all scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2306.06755v4' target='_blank'>CoTran: An LLM-based Code Translator using Reinforcement Learning with
  Feedback from Compiler and Symbolic Execution</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Prithwish Jana, Piyush Jha, Haoyang Ju, Gautham Kishore, Aryan Mahajan, Vijay Ganesh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-06-11 19:47:52</h6>
<p class='card-text'>In this paper, we present an LLM-based code translation method and an
associated tool called CoTran, that translates whole-programs from one
high-level programming language to another. Existing LLM-based code translation
methods lack training to ensure that the translated code reliably compiles or
bears substantial functional equivalence to the input code. In our work, we
fine-tune an LLM using reinforcement learning, incorporating compiler feedback,
and symbolic execution (symexec)-based testing feedback to assess functional
equivalence between the input and output programs. The idea is to guide an LLM
during fine-tuning, via compiler and symexec-based testing feedback, by letting
it know how far it is from producing perfect translations. We conduct extensive
experiments comparing CoTran with 14 other code translation tools, including
human-written transpilers, LLM-based translation tools, and ChatGPT. Using a
benchmark of over \num{57000} code pairs in Java and Python, we demonstrate
that CoTran outperforms the other tools on relevant metrics such as compilation
accuracy (CompAcc) and functional equivalence accuracy (FEqAcc). For example,
in Python-to-Java translation, CoTran achieves 48.68% FEqAcc and 76.98%
CompAcc, whereas the nearest competing tool (PLBART-base) gets 38.26% and
75.77% respectively. Additionally, CoTran, built on top of CodeT5, improves
FEqAcc by +14.89% and CompAcc by +8.14% for Python-to-Java (resp., +12.94% and
+4.30% for Java-to-Python).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2306.06199v1' target='_blank'>Reliability Check: An Analysis of GPT-3's Response to Sensitive Topics
  and Prompt Wording</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aisha Khatun, Daniel G. Brown</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-06-09 19:07:31</h6>
<p class='card-text'>Large language models (LLMs) have become mainstream technology with their
versatile use cases and impressive performance. Despite the countless
out-of-the-box applications, LLMs are still not reliable. A lot of work is
being done to improve the factual accuracy, consistency, and ethical standards
of these models through fine-tuning, prompting, and Reinforcement Learning with
Human Feedback (RLHF), but no systematic analysis of the responses of these
models to different categories of statements, or on their potential
vulnerabilities to simple prompting changes is available. In this work, we
analyze what confuses GPT-3: how the model responds to certain sensitive topics
and what effects the prompt wording has on the model response. We find that
GPT-3 correctly disagrees with obvious Conspiracies and Stereotypes but makes
mistakes with common Misconceptions and Controversies. The model responses are
inconsistent across prompts and settings, highlighting GPT-3's unreliability.
Dataset and code of our analysis is available in
https://github.com/tanny411/GPT3-Reliability-Check.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2306.07929v2' target='_blank'>Large Language Models Are Semi-Parametric Reinforcement Learning Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zihan Zhao, Kai Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-06-09 08:08:18</h6>
<p class='card-text'>Inspired by the insights in cognitive science with respect to human memory
and reasoning mechanism, a novel evolvable LLM-based (Large Language Model)
agent framework is proposed as REMEMBERER. By equipping the LLM with a
long-term experience memory, REMEMBERER is capable of exploiting the
experiences from the past episodes even for different task goals, which excels
an LLM-based agent with fixed exemplars or equipped with a transient working
memory. We further introduce Reinforcement Learning with Experience Memory
(RLEM) to update the memory. Thus, the whole system can learn from the
experiences of both success and failure, and evolve its capability without
fine-tuning the parameters of the LLM. In this way, the proposed REMEMBERER
constitutes a semi-parametric RL agent. Extensive experiments are conducted on
two RL task sets to evaluate the proposed framework. The average results with
different initialization and training sets exceed the prior SOTA by 4% and 2%
for the success rate on two task sets and demonstrate the superiority and
robustness of REMEMBERER.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2306.05696v1' target='_blank'>Embodied Executable Policy Learning with Language-based Scene
  Summarization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jielin Qiu, Mengdi Xu, William Han, Seungwhan Moon, Ding Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-06-09 06:34:09</h6>
<p class='card-text'>Large Language models (LLMs) have shown remarkable success in assisting robot
learning tasks, i.e., complex household planning. However, the performance of
pretrained LLMs heavily relies on domain-specific templated text data, which
may be infeasible in real-world robot learning tasks with image-based
observations. Moreover, existing LLMs with text inputs lack the capability to
evolve with non-expert interactions with environments. In this work, we
introduce a novel learning paradigm that generates robots' executable actions
in the form of text, derived solely from visual observations, using
language-based summarization of these observations as the connecting bridge
between both domains. Our proposed paradigm stands apart from previous works,
which utilized either language instructions or a combination of language and
visual data as inputs. Moreover, our method does not require oracle text
summarization of the scene, eliminating the need for human involvement in the
learning loop, which makes it more practical for real-world robot learning
tasks. Our proposed paradigm consists of two modules: the SUM module, which
interprets the environment using visual observations and produces a text
summary of the scene, and the APM module, which generates executable action
policies based on the natural language descriptions provided by the SUM module.
We demonstrate that our proposed method can employ two fine-tuning strategies,
including imitation learning and reinforcement learning approaches, to adapt to
the target test tasks effectively. We conduct extensive experiments involving
various SUM/APM model selections, environments, and tasks across 7 house
layouts in the VirtualHome environment. Our experimental results demonstrate
that our method surpasses existing baselines, confirming the effectiveness of
this novel learning paradigm.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2306.03604v8' target='_blank'>Enabling Intelligent Interactions between an Agent and an LLM: A
  Reinforcement Learning Approach</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bin Hu, Chenyang Zhao, Pu Zhang, Zihao Zhou, Yuanhang Yang, Zenglin Xu, Bin Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-06-06 11:49:09</h6>
<p class='card-text'>Large language models (LLMs) encode a vast amount of world knowledge acquired
from massive text datasets. Recent studies have demonstrated that LLMs can
assist an embodied agent in solving complex sequential decision making tasks by
providing high-level instructions. However, interactions with LLMs can be
time-consuming. In many practical scenarios, it requires a significant amount
of storage space that can only be deployed on remote cloud servers.
Additionally, using commercial LLMs can be costly since they may charge based
on usage frequency. In this paper, we explore how to enable intelligent
cost-effective interactions between a down stream task oriented agent and an
LLM. We find that this problem can be naturally formulated by a Markov decision
process (MDP), and propose When2Ask, a reinforcement learning based approach
that learns when it is necessary to query LLMs for high-level instructions to
accomplish a target task. On one side, When2Ask discourages unnecessary
redundant interactions, while on the other side, it enables the agent to
identify and follow useful instructions from the LLM. This enables the agent to
halt an ongoing plan and transition to a more suitable one based on new
environmental observations. Experiments on MiniGrid and Habitat environments
that entail planning sub-goals demonstrate that When2Ask learns to solve target
tasks with only a few necessary interactions with the LLM, significantly
reducing interaction costs in testing environments compared with baseline
methods. Our code is available at: https://github.com/ZJLAB-AMMI/LLM4RL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2306.03081v2' target='_blank'>Sequential Monte Carlo Steering of Large Language Models using
  Probabilistic Programs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alexander K. Lew, Tan Zhi-Xuan, Gabriel Grand, Vikash K. Mansinghka</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-06-05 17:55:05</h6>
<p class='card-text'>Even after fine-tuning and reinforcement learning, large language models
(LLMs) can be difficult, if not impossible, to control reliably with prompts
alone. We propose a new inference-time approach to enforcing syntactic and
semantic constraints on the outputs of LLMs, called sequential Monte Carlo
(SMC) steering. The key idea is to specify language generation tasks as
posterior inference problems in a class of discrete probabilistic sequence
models, and replace standard decoding with sequential Monte Carlo inference.
For a computational cost similar to that of beam search, SMC can steer LLMs to
solve diverse tasks, including infilling, generation under syntactic
constraints, and prompt intersection. To facilitate experimentation with SMC
steering, we present a probabilistic programming library, LLaMPPL
(https://github.com/probcomp/hfppl), for concisely specifying new generation
tasks as language model probabilistic programs, and automating steering of
LLaMA-family Transformers.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2306.02231v3' target='_blank'>Fine-Tuning Language Models with Advantage-Induced Policy Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Banghua Zhu, Hiteshi Sharma, Felipe Vieira Frujeri, Shi Dong, Chenguang Zhu, Michael I. Jordan, Jiantao Jiao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-06-04 01:59:40</h6>
<p class='card-text'>Reinforcement learning from human feedback (RLHF) has emerged as a reliable
approach to aligning large language models (LLMs) to human preferences. Among
the plethora of RLHF techniques, proximal policy optimization (PPO) is of the
most widely used methods. Despite its popularity, however, PPO may suffer from
mode collapse, instability, and poor sample efficiency. We show that these
issues can be alleviated by a novel algorithm that we refer to as
Advantage-Induced Policy Alignment (APA), which leverages a squared error loss
function based on the estimated advantages. We demonstrate empirically that APA
consistently outperforms PPO in language tasks by a large margin, when a
separate reward model is employed as the evaluator. In addition, compared with
PPO, APA offers a more stable form of control over the deviation from the
model's initial policy, ensuring that the model improves its performance
without collapsing to deterministic output. In addition to empirical results,
we also provide a theoretical justification supporting the design of our loss
function.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2306.01540v1' target='_blank'>CLIPGraphs: Multimodal Graph Networks to Infer Object-Room Affinities</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ayush Agrawal, Raghav Arora, Ahana Datta, Snehasis Banerjee, Brojeshwar Bhowmick, Krishna Murthy Jatavallabhula, Mohan Sridharan, Madhava Krishna</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-06-02 13:44:01</h6>
<p class='card-text'>This paper introduces a novel method for determining the best room to place
an object in, for embodied scene rearrangement. While state-of-the-art
approaches rely on large language models (LLMs) or reinforcement learned (RL)
policies for this task, our approach, CLIPGraphs, efficiently combines
commonsense domain knowledge, data-driven methods, and recent advances in
multimodal learning. Specifically, it (a)encodes a knowledge graph of prior
human preferences about the room location of different objects in home
environments, (b) incorporates vision-language features to support multimodal
queries based on images or text, and (c) uses a graph network to learn
object-room affinities based on embeddings of the prior knowledge and the
vision-language features. We demonstrate that our approach provides better
estimates of the most appropriate location of objects from a benchmark set of
object categories in comparison with state-of-the-art baselines</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2305.17306v1' target='_blank'>Chain-of-Thought Hub: A Continuous Effort to Measure Large Language
  Models' Reasoning Performance</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, Tushar Khot</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-05-26 23:46:42</h6>
<p class='card-text'>As large language models (LLMs) are continuously being developed, their
evaluation becomes increasingly important yet challenging. This work proposes
Chain-of-Thought Hub, an open-source evaluation suite on the multi-step
reasoning capabilities of large language models. We are interested in this
setting for two reasons: (1) from the behavior of GPT and PaLM model family, we
observe that complex reasoning is likely to be a key differentiator between
weaker and stronger LLMs; (2) we envisage large language models to become the
next-generation computational platform and foster an ecosystem of LLM-based new
applications, this naturally requires the foundation models to perform complex
tasks that often involve the composition of linguistic and logical operations.
Our approach is to compile a suite of challenging reasoning benchmarks to track
the progress of LLMs. Our current results show that: (1) model scale clearly
correlates with reasoning capabilities; (2) As of May 2023, Claude-v1.3 and
PaLM-2 are the only two models that are comparable with GPT-4, while
open-sourced models still lag behind; (3) LLaMA-65B performs closely to
code-davinci-002, indicating that with successful further development such as
reinforcement learning from human feedback (RLHF), it has great potential to be
close to GPT-3.5-Turbo. Our results also suggest that for the open-source
efforts to catch up, the community may focus more on building better base
models and exploring RLHF.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2305.17066v1' target='_blank'>Mindstorms in Natural Language-Based Societies of Mind</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R. Ashley, Róbert Csordás, Anand Gopalakrishnan, Abdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann, Kazuki Irie, Louis Kirsch, Bing Li, Guohao Li, Shuming Liu, Jinjie Mai, Piotr Piękos, Aditya Ramesh, Imanol Schlag, Weimin Shi, Aleksandar Stanić, Wenyi Wang, Yuhui Wang, Mengmeng Xu, Deng-Ping Fan, Bernard Ghanem, Jürgen Schmidhuber</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-05-26 16:21:25</h6>
<p class='card-text'>Both Minsky's "society of mind" and Schmidhuber's "learning to think" inspire
diverse societies of large multimodal neural networks (NNs) that solve problems
by interviewing each other in a "mindstorm." Recent implementations of NN-based
societies of minds consist of large language models (LLMs) and other NN-based
experts communicating through a natural language interface. In doing so, they
overcome the limitations of single LLMs, improving multimodal zero-shot
reasoning. In these natural language-based societies of mind (NLSOMs), new
agents -- all communicating through the same universal symbolic language -- are
easily added in a modular fashion. To demonstrate the power of NLSOMs, we
assemble and experiment with several of them (having up to 129 members),
leveraging mindstorms in them to solve some practical AI tasks: visual question
answering, image captioning, text-to-image synthesis, 3D generation, egocentric
retrieval, embodied AI, and general language-based task solving. We view this
as a starting point towards much larger NLSOMs with billions of agents-some of
which may be humans. And with this emergence of great societies of
heterogeneous minds, many new research questions have suddenly become paramount
to the future of artificial intelligence. What should be the social structure
of an NLSOM? What would be the (dis)advantages of having a monarchical rather
than a democratic structure? How can principles of NN economies be used to
maximize the total reward of a reinforcement learning NLSOM? In this work, we
identify, discuss, and try to answer some of these questions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2305.18341v2' target='_blank'>Coarse-Tuning Models of Code with Reinforcement Learning Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Abhinav Jain, Chima Adiole, Swarat Chaudhuri, Thomas Reps, Chris Jermaine</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-05-25 22:09:08</h6>
<p class='card-text'>Large Language Models (LLMs) pre-trained on code have recently emerged as the
dominant approach to program synthesis. However, these models are trained using
next-token prediction, which ignores the syntax and semantics of code. We
propose RLCF, that further trains a pre-trained LLM via reinforcement learning,
using feedback from a grounding function that scores the quality of the code.
The grounding function uses (i) compiler-derived feedback on whether the code
it generates passes a set of correctness checks; and (ii) feedback from a
different LLM that compares the generated code to a reference code. RLCF is
model- and language-agnostic. We empirically evaluate it on the MBJP and MathQA
tasks for Java. Our experiments show that RLCF raises the odds that an
LLM-generated program compiles, is executable, and produces the right output on
tests, often allowing LLMs to match the performance of 2x-8x larger LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2305.17144v2' target='_blank'>Ghost in the Minecraft: Generally Capable Agents for Open-World
  Environments via Large Language Models with Text-based Knowledge and Memory</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, Jifeng Dai</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-05-25 17:59:49</h6>
<p class='card-text'>The captivating realm of Minecraft has attracted substantial research
interest in recent years, serving as a rich platform for developing intelligent
agents capable of functioning in open-world environments. However, the current
research landscape predominantly focuses on specific objectives, such as the
popular "ObtainDiamond" task, and has not yet shown effective generalization to
a broader spectrum of tasks. Furthermore, the current leading success rate for
the "ObtainDiamond" task stands at around 20%, highlighting the limitations of
Reinforcement Learning (RL) based controllers used in existing methods. To
tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel
framework integrates Large Language Models (LLMs) with text-based knowledge and
memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These
agents, equipped with the logic and common sense capabilities of LLMs, can
skillfully navigate complex, sparse-reward environments with text-based
interactions. We develop a set of structured actions and leverage LLMs to
generate action plans for the agents to execute. The resulting LLM-based agent
markedly surpasses previous methods, achieving a remarkable improvement of
+47.5% in success rate on the "ObtainDiamond" task, demonstrating superior
robustness compared to traditional RL-based controllers. Notably, our agent is
the first to procure all items in the Minecraft Overworld technology tree,
demonstrating its extensive capabilities. GITM does not need any GPU for
training, but a single CPU node with 32 CPU cores is enough. This research
shows the potential of LLMs in developing capable agents for handling
long-horizon, complex tasks and adapting to uncertainties in open-world
environments. See the project website at https://github.com/OpenGVLab/GITM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2305.16366v1' target='_blank'>Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal
  Theorem Proving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xueliang Zhao, Wenda Li, Lingpeng Kong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-05-25 11:35:52</h6>
<p class='card-text'>Large language models~(LLMs) present an intriguing avenue of exploration in
the domain of formal theorem proving. Nonetheless, the full utilization of
these models, particularly in terms of demonstration formatting and
organization, remains an underexplored area. In an endeavor to enhance the
efficacy of LLMs, we introduce a subgoal-based demonstration learning
framework, consisting of two primary elements: Firstly, drawing upon the
insights of subgoal learning from the domains of reinforcement learning and
robotics, we propose the construction of distinct subgoals for each
demonstration example and refine these subgoals in accordance with the
pertinent theories of subgoal learning. Secondly, we build upon recent advances
in diffusion models to predict the optimal organization, simultaneously
addressing two intricate issues that persist within the domain of demonstration
organization: subset selection and order determination. Through the integration
of subgoal-based learning methodologies, we have successfully increased the
prevailing proof accuracy from 38.9\% to 44.3\% on the miniF2F benchmark.
Furthermore, the adoption of diffusion models for demonstration organization
can lead to an additional enhancement in accuracy to 45.5\%, or a $5\times$
improvement in sampling efficiency compared with the long-standing
state-of-the-art method. Our code is available at
\url{https://github.com/HKUNLP/subgoal-theorem-prover}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2305.15685v2' target='_blank'>RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lei Shu, Liangchen Luo, Jayakumar Hoskere, Yun Zhu, Yinxiao Liu, Simon Tong, Jindong Chen, Lei Meng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-05-25 03:26:26</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated impressive capabilities in
creative tasks such as storytelling and E-mail generation. However, as LLMs are
primarily trained on final text results rather than intermediate revisions, it
might be challenging for them to perform text rewriting tasks. Most studies in
the rewriting tasks focus on a particular transformation type within the
boundaries of single sentences. In this work, we develop new strategies for
instruction tuning and reinforcement learning to better align LLMs for
cross-sentence rewriting tasks using diverse wording and structures expressed
through natural languages including 1) generating rewriting instruction data
from Wiki edits and public corpus through instruction generation and
chain-of-thought prompting; 2) collecting comparison data for reward model
training through a new ranking function. To facilitate this research, we
introduce OpenRewriteEval, a novel benchmark covers a wide variety of rewriting
types expressed through natural language instructions. Our results show
significant improvements over a variety of baselines. The public repository is
available on GitHub under Google Research
(https://github.com/google-research/google-research/tree/master/rewritelm).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2305.15486v3' target='_blank'>SPRING: Studying the Paper and Reasoning to Play Games</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yue Wu, Shrimai Prabhumoye, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Tom Mitchell, Yuanzhi Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-05-24 18:14:35</h6>
<p class='card-text'>Open-world survival games pose significant challenges for AI algorithms due
to their multi-tasking, deep exploration, and goal prioritization requirements.
Despite reinforcement learning (RL) being popular for solving games, its high
sample complexity limits its effectiveness in complex open-world games like
Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's
original academic paper and use the knowledge learned to reason and play the
game through a large language model (LLM). Prompted with the LaTeX source as
game context and a description of the agent's current observation, our SPRING
framework employs a directed acyclic graph (DAG) with game-related questions as
nodes and dependencies as edges. We identify the optimal action to take in the
environment by traversing the DAG and calculating LLM responses for each node
in topological order, with the LLM's answer to final node directly translating
to environment actions. In our experiments, we study the quality of in-context
"reasoning" induced by different forms of prompts under the setting of the
Crafter open-world environment. Our experiments suggest that LLMs, when
prompted with consistent chain-of-thought, have great potential in completing
sophisticated high-level trajectories. Quantitatively, SPRING with GPT-4
outperforms all state-of-the-art RL baselines, trained for 1M steps, without
any training. Finally, we show the potential of games as a test bed for LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2305.15075v1' target='_blank'>HuatuoGPT, towards Taming Language Model to Be a Doctor</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Jianquan Li, Guiming Chen, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, Xiang Wan, Benyou Wang, Haizhou Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-05-24 11:56:01</h6>
<p class='card-text'>In this paper, we present HuatuoGPT, a large language model (LLM) for medical
consultation. The core recipe of HuatuoGPT is to leverage both
\textit{distilled data from ChatGPT} and \textit{real-world data from doctors}
in the supervised fine-tuned stage. The responses of ChatGPT are usually
detailed, well-presented and informative while it cannot perform like a doctor
in many aspects, e.g. for integrative diagnosis. We argue that real-world data
from doctors would be complementary to distilled data in the sense the former
could tame a distilled language model to perform like doctors. To better
leverage the strengths of both data, we train a reward model to align the
language model with the merits that both data bring, following an RLAIF
(reinforced learning from AI feedback) fashion. To evaluate and benchmark the
models, we propose a comprehensive evaluation scheme (including automatic and
manual metrics). Experimental results demonstrate that HuatuoGPT achieves
state-of-the-art results in performing medical consultation among open-source
LLMs in GPT-4 evaluation, human evaluation, and medical benchmark datasets. It
is worth noting that by using additional real-world data and RLAIF, the
distilled language model (i.e., HuatuoGPT) outperforms its teacher model
ChatGPT in most cases. Our code, data, and models are publicly available at
\url{https://github.com/FreedomIntelligence/HuatuoGPT}. The online demo is
available at \url{https://www.HuatuoGPT.cn/}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2305.14791v2' target='_blank'>Prompting Large Language Models for Counterfactual Generation: An
  Empirical Study</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yongqi Li, Mayi Xu, Xin Miao, Shen Zhou, Tieyun Qian</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-05-24 06:44:32</h6>
<p class='card-text'>Large language models (LLMs) have made remarkable progress in a wide range of
natural language understanding and generation tasks. However, their ability to
generate counterfactuals has not been examined systematically. To bridge this
gap, we present a comprehensive evaluation framework on various types of NLU
tasks, which covers all key factors in determining LLMs' capability of
generating counterfactuals. Based on this framework, we 1) investigate the
strengths and weaknesses of LLMs as the counterfactual generator, and 2)
disclose the factors that affect LLMs when generating counterfactuals,
including both the intrinsic properties of LLMs and prompt designing. The
results show that, though LLMs are promising in most cases, they face
challenges in complex tasks like RE since they are bounded by task-specific
performance, entity constraints, and inherent selection bias. We also find that
alignment techniques, e.g., instruction-tuning and reinforcement learning from
human feedback, may potentially enhance the counterfactual generation ability
of LLMs. On the contrary, simply increasing the parameter size does not yield
the desired improvements. Besides, from the perspective of prompt designing,
task guidelines unsurprisingly play an important role. However, the
chain-of-thought approach does not always help due to inconsistency issues.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2305.14483v1' target='_blank'>Language Model Self-improvement by Reinforcement Learning Contemplation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jing-Cheng Pang, Pengyuan Wang, Kaiyuan Li, Xiong-Hui Chen, Jiacheng Xu, Zongzhang Zhang, Yang Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-05-23 19:25:52</h6>
<p class='card-text'>Large Language Models (LLMs) have exhibited remarkable performance across
various natural language processing (NLP) tasks. However, fine-tuning these
models often necessitates substantial supervision, which can be expensive and
time-consuming to obtain. This paper introduces a novel unsupervised method
called LanguageModel Self-Improvement by Reinforcement Learning Contemplation
(SIRLC) that improves LLMs without reliance on external labels. Our approach is
grounded in the observation that it is simpler for language models to assess
text quality than to generate text. Building on this insight, SIRLC assigns
LLMs dual roles as both student and teacher. As a student, the LLM generates
answers to unlabeled questions, while as a teacher, it evaluates the generated
text and assigns scores accordingly. The model parameters are updated using
reinforcement learning to maximize the evaluation score. We demonstrate that
SIRLC can be applied to various NLP tasks, such as reasoning problems, text
generation, and machine translation. Our experiments show that SIRLC
effectively improves LLM performance without external supervision, resulting in
a 5.6% increase in answering accuracy for reasoning tasks and a rise in
BERTScore from 0.82 to 0.86 for translation tasks. Furthermore, SIRLC can be
applied to models of different sizes, showcasing its broad applicability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2305.14283v3' target='_blank'>Query Rewriting for Retrieval-Augmented Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, Nan Duan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-05-23 17:27:50</h6>
<p class='card-text'>Large Language Models (LLMs) play powerful, black-box readers in the
retrieve-then-read pipeline, making remarkable progress in knowledge-intensive
tasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of
the previous retrieve-then-read for the retrieval-augmented LLMs from the
perspective of the query rewriting. Unlike prior studies focusing on adapting
either the retriever or the reader, our approach pays attention to the
adaptation of the search query itself, for there is inevitably a gap between
the input text and the needed knowledge in retrieval. We first prompt an LLM to
generate the query, then use a web search engine to retrieve contexts.
Furthermore, to better align the query to the frozen modules, we propose a
trainable scheme for our pipeline. A small language model is adopted as a
trainable rewriter to cater to the black-box LLM reader. The rewriter is
trained using the feedback of the LLM reader by reinforcement learning.
Evaluation is conducted on downstream tasks, open-domain QA and multiple-choice
QA. Experiments results show consistent performance improvement, indicating
that our framework is proven effective and scalable, and brings a new framework
for retrieval-augmented LLM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2305.13735v2' target='_blank'>Aligning Large Language Models through Synthetic Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Min Yoo, Minjoon Seo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-05-23 06:41:16</h6>
<p class='card-text'>Aligning large language models (LLMs) to human values has become increasingly
important as it enables sophisticated steering of LLMs. However, it requires
significant human demonstrations and feedback or distillation from proprietary
LLMs such as ChatGPT. In this work, we propose a novel alignment learning
framework with synthetic feedback not dependent on extensive human annotations
and proprietary LLMs. First, we perform reward modeling (RM) with synthetic
feedback by contrasting responses from vanilla LLMs with various sizes and
prompts. Then, we use the RM to simulate high-quality demonstrations to train a
supervised policy and further optimize the model with reinforcement learning.
Our resulting model, Aligned Language Model with Synthetic Training dataset
(ALMoST), outperforms recent open-sourced models, which are trained on the
outputs of InstructGPT or human-annotated demonstrations, in alignment
benchmarks. In human evaluation, our model is preferred to Alpaca and Dolly-v2,
55.0% and 58.5% of the time, respectively. Further analyses demonstrate the
efficacy and importance of synthetic feedback in our framework. The code is
available at https://github.com/naver-ai/almost</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2305.06176v3' target='_blank'>Fine-tuning Language Models with Generative Adversarial Reward Modelling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhang Ze Yu, Lau Jia Jaw, Zhang Hui, Bryan Kian Hsiang Low</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-05-09 17:06:06</h6>
<p class='card-text'>Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to
significantly enhance the performance of large language models (LLMs) by
aligning their outputs with desired human values through instruction tuning.
However, RLHF is constrained by the expertise and productivity limitations of
human evaluators. A response to this downside is to fall back to supervised
fine-tuning (SFT) with additional carefully selected expert demonstrations.
However, while this method has been proven to be effective, it invariably also
leads to increased human-in-the-loop overhead. In this study, we propose
another alternative approach: Reinforcement Learning with Generative
Adversarial Feedback (RLGAF) to RLHF and SFT, which uses a generative
adversarial training style to enable the LLMs to learn useful human expert
demonstrations without being directly exposed to the training examples, thus
enabling good generalization capabilities while preserving sample efficiency.
Our preliminary findings indicate that RLGAF can help align LLMs outputs with
competitive performance against RLHF and SFT, while not suffering from their
respective inherent restrictions, suggesting promising avenues for further
research on automating AI alignment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2305.04207v3' target='_blank'>No More Manual Tests? Evaluating and Improving ChatGPT for Unit Test
  Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiqiang Yuan, Yiling Lou, Mingwei Liu, Shiji Ding, Kaixin Wang, Yixuan Chen, Xin Peng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-05-07 07:17:08</h6>
<p class='card-text'>Unit testing is essential in detecting bugs in functionally-discrete program
units. Manually writing high-quality unit tests is time-consuming and
laborious. Although traditional techniques can generate tests with reasonable
coverage, they exhibit low readability and cannot be directly adopted by
developers. Recent work has shown the large potential of large language models
(LLMs) in unit test generation, which can generate more human-like and
meaningful test code. ChatGPT, the latest LLM incorporating instruction tuning
and reinforcement learning, has performed well in various domains. However, It
remains unclear how effective ChatGPT is in unit test generation.
  In this work, we perform the first empirical study to evaluate ChatGPT's
capability of unit test generation. Specifically, we conduct a quantitative
analysis and a user study to systematically investigate the quality of its
generated tests regarding the correctness, sufficiency, readability, and
usability. The tests generated by ChatGPT still suffer from correctness issues,
including diverse compilation errors and execution failures. Still, the passing
tests generated by ChatGPT resemble manually-written tests by achieving
comparable coverage, readability, and even sometimes developers' preference.
Our findings indicate that generating unit tests with ChatGPT could be very
promising if the correctness of its generated tests could be further improved.
  Inspired by our findings above, we propose ChatTESTER, a novel ChatGPT-based
unit test generation approach, which leverages ChatGPT itself to improve the
quality of its generated tests. ChatTESTER incorporates an initial test
generator and an iterative test refiner. Our evaluation demonstrates the
effectiveness of ChatTESTER by generating 34.3% more compilable tests and 18.7%
more tests with correct assertions than the default ChatGPT.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2305.03433v2' target='_blank'>Towards Applying Powerful Large AI Models in Classroom Teaching:
  Opportunities, Challenges and Prospects</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kehui Tan, Tianqi Pang, Chenyou Fan, Song Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-05-05 11:09:13</h6>
<p class='card-text'>This perspective paper proposes a series of interactive scenarios that
utilize Artificial Intelligence (AI) to enhance classroom teaching, such as
dialogue auto-completion, knowledge and style transfer, and assessment of
AI-generated content. By leveraging recent developments in Large Language
Models (LLMs), we explore the potential of AI to augment and enrich
teacher-student dialogues and improve the quality of teaching. Our goal is to
produce innovative and meaningful conversations between teachers and students,
create standards for evaluation, and improve the efficacy of AI-for-Education
initiatives. In Section 3, we discuss the challenges of utilizing existing LLMs
to effectively complete the educated tasks and present a unified framework for
addressing diverse education dataset, processing lengthy conversations, and
condensing information to better accomplish more downstream tasks. In Section
4, we summarize the pivoting tasks including Teacher-Student Dialogue
Auto-Completion, Expert Teaching Knowledge and Style Transfer, and Assessment
of AI-Generated Content (AIGC), providing a clear path for future research. In
Section 5, we also explore the use of external and adjustable LLMs to improve
the generated content through human-in-the-loop supervision and reinforcement
learning. Ultimately, this paper seeks to highlight the potential for AI to aid
the field of education and promote its further exploration.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2305.03047v2' target='_blank'>Principle-Driven Self-Alignment of Language Models from Scratch with
  Minimal Human Supervision</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, Chuang Gan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-05-04 17:59:28</h6>
<p class='card-text'>Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised
fine-tuning (SFT) with human annotations and reinforcement learning from human
feedback (RLHF) to align the output of large language models (LLMs) with human
intentions, ensuring they are helpful, ethical, and reliable. However, this
dependence can significantly constrain the true potential of AI-assistant
agents due to the high cost of obtaining human supervision and the related
issues on quality, reliability, diversity, self-consistency, and undesirable
biases. To address these challenges, we propose a novel approach called
SELF-ALIGN, which combines principle-driven reasoning and the generative power
of LLMs for the self-alignment of AI agents with minimal human supervision. Our
approach encompasses four stages: first, we use an LLM to generate synthetic
prompts, and a topic-guided method to augment the prompt diversity; second, we
use a small set of human-written principles for AI models to follow, and guide
the LLM through in-context learning from demonstrations (of principles
application) to produce helpful, ethical, and reliable responses to user's
queries; third, we fine-tune the original LLM with the high-quality
self-aligned responses so that the resulting model can generate desirable
responses for each query directly without the principle set and the
demonstrations anymore; and finally, we offer a refinement step to address the
issues of overly-brief or indirect responses. Applying SELF-ALIGN to the
LLaMA-65b base language model, we develop an AI assistant named Dromedary. With
fewer than 300 lines of human annotations (including < 200 seed prompts, 16
generic principles, and 5 exemplars for in-context learning). Dromedary
significantly surpasses the performance of several state-of-the-art AI systems,
including Text-Davinci-003 and Alpaca, on benchmark datasets with various
settings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2305.01550v1' target='_blank'>Mitigating Approximate Memorization in Language Models via Dissimilarity
  Learned Policy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aly M. Kassem</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-05-02 15:53:28</h6>
<p class='card-text'>Large Language models (LLMs) are trained on large amounts of data, which can
include sensitive information that may compromise personal privacy. LLMs showed
to memorize parts of the training data and emit those data verbatim when an
adversary prompts appropriately. Previous research has primarily focused on
data preprocessing and differential privacy techniques to address memorization
or prevent verbatim memorization exclusively, which can give a false sense of
privacy. However, these methods rely on explicit and implicit assumptions about
the structure of the data to be protected, which often results in an incomplete
solution to the problem. To address this, we propose a novel framework that
utilizes a reinforcement learning approach (PPO) to fine-tune LLMs to mitigate
approximate memorization. Our approach utilizes a negative similarity score,
such as BERTScore or SacreBLEU, as a reward signal to learn a dissimilarity
policy. Our results demonstrate that this framework effectively mitigates
approximate memorization while maintaining high levels of coherence and fluency
in the generated samples. Furthermore, our framework is robust in mitigating
approximate memorization across various circumstances, including longer
context, which is known to increase memorization in LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2304.12958v2' target='_blank'>A Closer Look at Reward Decomposition for High-Level Robotic
  Explanations</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenhao Lu, Xufeng Zhao, Sven Magg, Martin Gromniak, Mengdi Li, Stefan Wermter</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-04-25 16:01:42</h6>
<p class='card-text'>Explaining the behaviour of intelligent agents learned by reinforcement
learning (RL) to humans is challenging yet crucial due to their
incomprehensible proprioceptive states, variational intermediate goals, and
resultant unpredictability. Moreover, one-step explanations for RL agents can
be ambiguous as they fail to account for the agent's future behaviour at each
transition, adding to the complexity of explaining robot actions. By leveraging
abstracted actions that map to task-specific primitives, we avoid explanations
on the movement level. To further improve the transparency and explainability
of robotic systems, we propose an explainable Q-Map learning framework that
combines reward decomposition (RD) with abstracted action spaces, allowing for
non-ambiguous and high-level explanations based on object properties in the
task. We demonstrate the effectiveness of our framework through quantitative
and qualitative analysis of two robotic scenarios, showcasing visual and
textual explanations, from output artefacts of RD explanations, that are easy
for humans to comprehend. Additionally, we demonstrate the versatility of
integrating these artefacts with large language models (LLMs) for reasoning and
interactive querying.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2304.11490v3' target='_blank'>Boosting Theory-of-Mind Performance in Large Language Models via
  Prompting</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shima Rahimi Moghaddam, Christopher J. Honey</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-04-22 22:50:50</h6>
<p class='card-text'>Large language models (LLMs) excel in many tasks in 2023, but they still face
challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require
understanding agents' beliefs, goals, and mental states, are essential for
common-sense reasoning involving humans, making it crucial to enhance LLM
performance in this area. This study measures the ToM performance of GPT-4 and
three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates
the effectiveness of in-context learning in improving their ToM comprehension.
We evaluated prompts featuring two-shot chain of thought reasoning and
step-by-step thinking instructions. We found that LLMs trained with
Reinforcement Learning from Human Feedback (RLHF) (all models excluding
Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed
best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell
short of the 87% human accuracy on the test set. However, when supplied with
prompts for in-context learning, all RLHF-trained LLMs exceeded 80% ToM
accuracy, with GPT-4 reaching 100%. These results demonstrate that appropriate
prompting enhances LLM ToM reasoning, and they underscore the context-dependent
nature of LLM cognitive capacities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2304.11082v6' target='_blank'>Fundamental Limitations of Alignment in Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yotam Wolf, Noam Wies, Oshri Avnery, Yoav Levine, Amnon Shashua</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-04-19 17:50:09</h6>
<p class='card-text'>An important aspect in developing language models that interact with humans
is aligning their behavior to be useful and unharmful for their human users.
This is usually achieved by tuning the model in a way that enhances desired
behaviors and inhibits undesired ones, a process referred to as alignment. In
this paper, we propose a theoretical approach called Behavior Expectation
Bounds (BEB) which allows us to formally investigate several inherent
characteristics and limitations of alignment in large language models.
Importantly, we prove that within the limits of this framework, for any
behavior that has a finite probability of being exhibited by the model, there
exist prompts that can trigger the model into outputting this behavior, with
probability that increases with the length of the prompt. This implies that any
alignment process that attenuates an undesired behavior but does not remove it
altogether, is not safe against adversarial prompting attacks. Furthermore, our
framework hints at the mechanism by which leading alignment approaches such as
reinforcement learning from human feedback make the LLM prone to being prompted
into the undesired behaviors. This theoretical result is being experimentally
demonstrated in large scale by the so called contemporary "chatGPT jailbreaks",
where adversarial users trick the LLM into breaking its alignment guardrails by
triggering it into acting as a malicious persona. Our results expose
fundamental limitations in alignment of LLMs and bring to the forefront the
need to devise reliable mechanisms for ensuring AI safety.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2304.07327v2' target='_blank'>OpenAssistant Conversations -- Democratizing Large Language Model
  Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, Alexander Mattick</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-04-14 18:01:29</h6>
<p class='card-text'>Aligning large language models (LLMs) with human preferences has proven to
drastically improve usability and has driven rapid adoption as demonstrated by
ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and
reinforcement learning from human feedback (RLHF) greatly reduce the required
skill and domain knowledge to effectively harness the capabilities of LLMs,
increasing their accessibility and utility across various domains. However,
state-of-the-art alignment techniques like RLHF rely on high-quality human
feedback data, which is expensive to create and often remains proprietary. In
an effort to democratize research on large-scale alignment, we release
OpenAssistant Conversations, a human-generated, human-annotated assistant-style
conversation corpus consisting of 161,443 messages in 35 different languages,
annotated with 461,292 quality ratings, resulting in over 10,000 complete and
fully annotated conversation trees. The corpus is a product of a worldwide
crowd-sourcing effort involving over 13,500 volunteers. Models trained on
OpenAssistant Conversations show consistent improvements on standard benchmarks
over respective base models. We release our code and data under a fully
permissive licence.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2304.04370v6' target='_blank'>OpenAGI: When LLM Meets Domain Experts</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yingqiang Ge, Wenyue Hua, Kai Mei, Jianchao Ji, Juntao Tan, Shuyuan Xu, Zelong Li, Yongfeng Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-04-10 03:55:35</h6>
<p class='card-text'>Human Intelligence (HI) excels at combining basic skills to solve complex
tasks. This capability is vital for Artificial Intelligence (AI) and should be
embedded in comprehensive AI Agents, enabling them to harness expert models for
complex task-solving towards Artificial General Intelligence (AGI). Large
Language Models (LLMs) show promising learning and reasoning abilities, and can
effectively use external models, tools, plugins, or APIs to tackle complex
problems. In this work, we introduce OpenAGI, an open-source AGI research and
development platform designed for solving multi-step, real-world tasks.
Specifically, OpenAGI uses a dual strategy, integrating standard benchmark
tasks for benchmarking and evaluation, and open-ended tasks including more
expandable models, tools, plugins, or APIs for creative problem-solving. Tasks
are presented as natural language queries to the LLM, which then selects and
executes appropriate models. We also propose a Reinforcement Learning from Task
Feedback (RLTF) mechanism that uses task results to improve the LLM's
task-solving ability, which creates a self-improving AI feedback loop. While we
acknowledge that AGI is a broad and multifaceted research challenge with no
singularly defined solution path, the integration of LLMs with domain-specific
expert models, inspired by mirroring the blend of general and specialized
intelligence in humans, offers a promising approach towards AGI. We are
open-sourcing the OpenAGI project's code, dataset, benchmarks, evaluation
methods, and the UI demo to foster community involvement in AGI advancement:
https://github.com/agiresearch/OpenAGI.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2304.01852v4' target='_blank'>Summary of ChatGPT-Related Research and Perspective Towards the Future
  of Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, Zihao Wu, Lin Zhao, Dajiang Zhu, Xiang Li, Ning Qiang, Dingang Shen, Tianming Liu, Bao Ge</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-04-04 15:01:06</h6>
<p class='card-text'>This paper presents a comprehensive survey of ChatGPT-related (GPT-3.5 and
GPT-4) research, state-of-the-art large language models (LLM) from the GPT
series, and their prospective applications across diverse domains. Indeed, key
innovations such as large-scale pre-training that captures knowledge across the
entire world wide web, instruction fine-tuning and Reinforcement Learning from
Human Feedback (RLHF) have played significant roles in enhancing LLMs'
adaptability and performance. We performed an in-depth analysis of 194 relevant
papers on arXiv, encompassing trend analysis, word cloud representation, and
distribution analysis across various application domains. The findings reveal a
significant and increasing interest in ChatGPT-related research, predominantly
centered on direct natural language processing applications, while also
demonstrating considerable potential in areas ranging from education and
history to mathematics, medicine, and physics. This study endeavors to furnish
insights into ChatGPT's capabilities, potential implications, ethical concerns,
and offer direction for future advancements in this field.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2304.01481v1' target='_blank'>The Vector Grounding Problem</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dimitri Coelho Mollo, Raphaël Millière</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-04-04 02:54:04</h6>
<p class='card-text'>The remarkable performance of large language models (LLMs) on complex
linguistic tasks has sparked a lively debate on the nature of their
capabilities. Unlike humans, these models learn language exclusively from
textual data, without direct interaction with the real world. Nevertheless,
they can generate seemingly meaningful text about a wide range of topics. This
impressive accomplishment has rekindled interest in the classical 'Symbol
Grounding Problem,' which questioned whether the internal representations and
outputs of classical symbolic AI systems could possess intrinsic meaning.
Unlike these systems, modern LLMs are artificial neural networks that compute
over vectors rather than symbols. However, an analogous problem arises for such
systems, which we dub the Vector Grounding Problem. This paper has two primary
objectives. First, we differentiate various ways in which internal
representations can be grounded in biological or artificial systems,
identifying five distinct notions discussed in the literature: referential,
sensorimotor, relational, communicative, and epistemic grounding.
Unfortunately, these notions of grounding are often conflated. We clarify the
differences between them, and argue that referential grounding is the one that
lies at the heart of the Vector Grounding Problem. Second, drawing on theories
of representational content in philosophy and cognitive science, we propose
that certain LLMs, particularly those fine-tuned with Reinforcement Learning
from Human Feedback (RLHF), possess the necessary features to overcome the
Vector Grounding Problem, as they stand in the requisite causal-historical
relations to the world that underpin intrinsic meaning. We also argue that,
perhaps unexpectedly, multimodality and embodiment are neither necessary nor
sufficient conditions for referential grounding in artificial systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2304.00416v1' target='_blank'>Towards Healthy AI: Large Language Models Need Therapists Too</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Baihan Lin, Djallel Bouneffouf, Guillermo Cecchi, Kush R. Varshney</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-04-02 00:39:12</h6>
<p class='card-text'>Recent advances in large language models (LLMs) have led to the development
of powerful AI chatbots capable of engaging in natural and human-like
conversations. However, these chatbots can be potentially harmful, exhibiting
manipulative, gaslighting, and narcissistic behaviors. We define Healthy AI to
be safe, trustworthy and ethical. To create healthy AI systems, we present the
SafeguardGPT framework that uses psychotherapy to correct for these harmful
behaviors in AI chatbots. The framework involves four types of AI agents: a
Chatbot, a "User," a "Therapist," and a "Critic." We demonstrate the
effectiveness of SafeguardGPT through a working example of simulating a social
conversation. Our results show that the framework can improve the quality of
conversations between AI chatbots and humans. Although there are still several
challenges and directions to be addressed in the future, SafeguardGPT provides
a promising approach to improving the alignment between AI chatbots and human
values. By incorporating psychotherapy and reinforcement learning techniques,
the framework enables AI chatbots to learn and adapt to human preferences and
values in a safe and ethical way, contributing to the development of a more
human-centric and responsible AI.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2303.17651v2' target='_blank'>Self-Refine: Iterative Refinement with Self-Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-03-30 18:30:01</h6>
<p class='card-text'>Like humans, large language models (LLMs) do not always generate the best
output on their first try. Motivated by how humans refine their written text,
we introduce Self-Refine, an approach for improving initial outputs from LLMs
through iterative feedback and refinement. The main idea is to generate an
initial output using an LLMs; then, the same LLMs provides feedback for its
output and uses it to refine itself, iteratively. Self-Refine does not require
any supervised training data, additional training, or reinforcement learning,
and instead uses a single LLM as the generator, refiner, and feedback provider.
We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response
generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT,
and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine
are preferred by humans and automatic metrics over those generated with the
same LLM using conventional one-step generation, improving by ~20% absolute on
average in task performance. Our work demonstrates that even state-of-the-art
LLMs like GPT-4 can be further improved at test time using our simple,
standalone approach.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2303.17491v3' target='_blank'>Language Models can Solve Computer Tasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Geunwoo Kim, Pierre Baldi, Stephen McAleer</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-03-30 16:01:52</h6>
<p class='card-text'>Agents capable of carrying out general tasks on a computer can improve
efficiency and productivity by automating repetitive tasks and assisting in
complex problem-solving. Ideally, such agents should be able to solve new
computer tasks presented to them through natural language commands. However,
previous approaches to this problem require large amounts of expert
demonstrations and task-specific reward functions, both of which are
impractical for new tasks. In this work, we show that a pre-trained large
language model (LLM) agent can execute computer tasks guided by natural
language using a simple prompting scheme where the agent Recursively Criticizes
and Improves its output (RCI). The RCI approach significantly outperforms
existing LLM methods for automating computer tasks and surpasses supervised
learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++
benchmark. We compare multiple LLMs and find that RCI with the
InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful
of demonstrations per task rather than tens of thousands, and without a
task-specific reward function. Furthermore, we demonstrate RCI prompting's
effectiveness in enhancing LLMs' reasoning abilities on a suite of natural
language reasoning tasks, outperforming chain of thought (CoT) prompting with
external feedback. We find that RCI combined with CoT performs better than
either separately. Our code can be found here:
https://github.com/posgnu/rci-agent.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2303.12767v2' target='_blank'>Can we trust the evaluation on ChatGPT?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rachith Aiyappa, Jisun An, Haewoon Kwak, Yong-Yeol Ahn</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-03-22 17:32:56</h6>
<p class='card-text'>ChatGPT, the first large language model (LLM) with mass adoption, has
demonstrated remarkable performance in numerous natural language tasks. Despite
its evident usefulness, evaluating ChatGPT's performance in diverse problem
domains remains challenging due to the closed nature of the model and its
continuous updates via Reinforcement Learning from Human Feedback (RLHF). We
highlight the issue of data contamination in ChatGPT evaluations, with a case
study of the task of stance detection. We discuss the challenge of preventing
data contamination and ensuring fair model evaluation in the age of closed and
continuously trained models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2303.11366v4' target='_blank'>Reflexion: Language Agents with Verbal Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-03-20 18:08:50</h6>
<p class='card-text'>Large language models (LLMs) have been increasingly used to interact with
external environments (e.g., games, compilers, APIs) as goal-driven agents.
However, it remains challenging for these language agents to quickly and
efficiently learn from trial-and-error as traditional reinforcement learning
methods require extensive training samples and expensive model fine-tuning. We
propose Reflexion, a novel framework to reinforce language agents not by
updating weights, but instead through linguistic feedback. Concretely,
Reflexion agents verbally reflect on task feedback signals, then maintain their
own reflective text in an episodic memory buffer to induce better
decision-making in subsequent trials. Reflexion is flexible enough to
incorporate various types (scalar values or free-form language) and sources
(external or internally simulated) of feedback signals, and obtains significant
improvements over a baseline agent across diverse tasks (sequential
decision-making, coding, language reasoning). For example, Reflexion achieves a
91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous
state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis
studies using different feedback signals, feedback incorporation methods, and
agent types, and provide insights into how they affect performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2303.09136v1' target='_blank'>A Short Survey of Viewing Large Language Models in Legal Aspect</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhongxiang Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-03-16 08:01:22</h6>
<p class='card-text'>Large language models (LLMs) have transformed many fields, including natural
language processing, computer vision, and reinforcement learning. These models
have also made a significant impact in the field of law, where they are being
increasingly utilized to automate various legal tasks, such as legal judgement
prediction, legal document analysis, and legal document writing. However, the
integration of LLMs into the legal field has also raised several legal
problems, including privacy concerns, bias, and explainability. In this survey,
we explore the integration of LLMs into the field of law. We discuss the
various applications of LLMs in legal tasks, examine the legal challenges that
arise from their use, and explore the data resources that can be used to
specialize LLMs in the legal domain. Finally, we discuss several promising
directions and conclude this paper. By doing so, we hope to provide an overview
of the current state of LLMs in law and highlight the potential benefits and
challenges of their integration.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2303.05453v1' target='_blank'>Personalisation within bounds: A risk taxonomy and policy framework for
  the alignment of large language models with personalised feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hannah Rose Kirk, Bertie Vidgen, Paul Röttger, Scott A. Hale</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-03-09 17:52:07</h6>
<p class='card-text'>Large language models (LLMs) are used to generate content for a wide range of
tasks, and are set to reach a growing audience in coming years due to
integration in product interfaces like ChatGPT or search engines like Bing.
This intensifies the need to ensure that models are aligned with human
preferences and do not produce unsafe, inaccurate or toxic outputs. While
alignment techniques like reinforcement learning with human feedback (RLHF) and
red-teaming can mitigate some safety concerns and improve model capabilities,
it is unlikely that an aggregate fine-tuning process can adequately represent
the full range of users' preferences and values. Different people may
legitimately disagree on their preferences for language and conversational
norms, as well as on values or ideologies which guide their communication.
Personalising LLMs through micro-level preference learning processes may result
in models that are better aligned with each user. However, there are several
normative challenges in defining the bounds of a societally-acceptable and safe
degree of personalisation. In this paper, we ask how, and in what ways, LLMs
should be personalised. First, we review literature on current paradigms for
aligning LLMs with human feedback, and identify issues including (i) a lack of
clarity regarding what alignment means; (ii) a tendency of technology providers
to prescribe definitions of inherently subjective preferences and values; and
(iii) a 'tyranny of the crowdworker', exacerbated by a lack of documentation in
who we are really aligning to. Second, we present a taxonomy of benefits and
risks associated with personalised LLMs, for individuals and society at large.
Finally, we propose a three-tiered policy framework that allows users to
experience the benefits of personalised alignment, while restraining unsafe and
undesirable LLM-behaviours within (supra-)national and organisational bounds.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2303.03751v3' target='_blank'>Zeroth-Order Optimization Meets Human Feedback: Provable Learning via
  Ranking Oracles</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiwei Tang, Dmitry Rybin, Tsung-Hui Chang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-03-07 09:20:43</h6>
<p class='card-text'>In this study, we delve into an emerging optimization challenge involving a
black-box objective function that can only be gauged via a ranking oracle-a
situation frequently encountered in real-world scenarios, especially when the
function is evaluated by human judges. Such challenge is inspired from
Reinforcement Learning with Human Feedback (RLHF), an approach recently
employed to enhance the performance of Large Language Models (LLMs) using human
guidance. We introduce ZO-RankSGD, an innovative zeroth-order optimization
algorithm designed to tackle this optimization problem, accompanied by
theoretical assurances. Our algorithm utilizes a novel rank-based random
estimator to determine the descent direction and guarantees convergence to a
stationary point. Moreover, ZO-RankSGD is readily applicable to policy
optimization problems in Reinforcement Learning (RL), particularly when only
ranking oracles for the episode reward are available. Last but not least, we
demonstrate the effectiveness of ZO-RankSGD in a novel application: improving
the quality of images generated by a diffusion generative model with human
ranking feedback. Throughout experiments, we found that ZO-RankSGD can
significantly enhance the detail of generated images with only a few rounds of
human feedback. Overall, our work advances the field of zeroth-order
optimization by addressing the problem of optimizing functions with only
ranking feedback, and offers a new and effective approach for aligning
Artificial Intelligence (AI) with human intentions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2303.00001v1' target='_blank'>Reward Design with Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Minae Kwon, Sang Michael Xie, Kalesha Bullard, Dorsa Sadigh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-02-27 22:09:35</h6>
<p class='card-text'>Reward design in reinforcement learning (RL) is challenging since specifying
human notions of desired behavior may be difficult via reward functions or
require many expert demonstrations. Can we instead cheaply design rewards using
a natural language interface? This paper explores how to simplify reward design
by prompting a large language model (LLM) such as GPT-3 as a proxy reward
function, where the user provides a textual prompt containing a few examples
(few-shot) or a description (zero-shot) of the desired behavior. Our approach
leverages this proxy reward function in an RL framework. Specifically, users
specify a prompt once at the beginning of training. During training, the LLM
evaluates an RL agent's behavior against the desired behavior described by the
prompt and outputs a corresponding reward signal. The RL agent then uses this
reward to update its behavior. We evaluate whether our approach can train
agents aligned with user objectives in the Ultimatum Game, matrix games, and
the DealOrNoDeal negotiation task. In all three tasks, we show that RL agents
trained with our framework are well-aligned with the user's objectives and
outperform RL agents trained with reward functions learned via supervised
learning</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2302.14003v1' target='_blank'>Systematic Rectification of Language Models via Dead-end Analysis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Meng Cao, Mehdi Fatemi, Jackie Chi Kit Cheung, Samira Shabanian</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-02-27 17:47:53</h6>
<p class='card-text'>With adversarial or otherwise normal prompts, existing large language models
(LLM) can be pushed to generate toxic discourses. One way to reduce the risk of
LLMs generating undesired discourses is to alter the training of the LLM. This
can be very restrictive due to demanding computation requirements. Other
methods rely on rule-based or prompt-based token elimination, which are limited
as they dismiss future tokens and the overall meaning of the complete
discourse. Here, we center detoxification on the probability that the finished
discourse is ultimately considered toxic. That is, at each point, we advise
against token selections proportional to how likely a finished text from this
point will be toxic. To this end, we formally extend the dead-end theory from
the recent reinforcement learning (RL) literature to also cover uncertain
outcomes. Our approach, called rectification, utilizes a separate but
significantly smaller model for detoxification, which can be applied to diverse
LLMs as long as they share the same vocabulary. Importantly, our method does
not require access to the internal representations of the LLM, but only the
token probability distribution at each decoding step. This is crucial as many
LLMs today are hosted in servers and only accessible through APIs. When applied
to various LLMs, including GPT-3, our approach significantly improves the
generated discourse compared to the base LLMs and other techniques in terms of
both the overall language and detoxification performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2302.11520v4' target='_blank'>Guiding Large Language Models via Directional Stimulus Prompting</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, Xifeng Yan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-02-22 17:44:15</h6>
<p class='card-text'>We introduce Directional Stimulus Prompting, a novel framework for guiding
black-box large language models (LLMs) toward specific desired outputs. Instead
of directly adjusting LLMs, our method employs a small tunable policy model
(e.g., T5) to generate an auxiliary directional stimulus prompt for each input
instance. These directional stimulus prompts act as nuanced, instance-specific
hints and clues to guide LLMs in generating desired outcomes, such as including
specific keywords in the generated summary. Our approach sidesteps the
challenges of direct LLM tuning by optimizing the policy model to explore
directional stimulus prompts that align LLMs with desired behaviors. The policy
model can be optimized through 1) supervised fine-tuning using labeled data and
2) reinforcement learning from offline or online rewards based on the LLM's
output. We assess our method across summarization, dialogue response
generation, and chain-of-thought reasoning tasks. Our experiments demonstrate
that the framework consistently improves LLMs' (e.g., ChatGPT, Codex,
InstructGPT) performance on these supervised tasks using minimal labeled data.
Notably, using just 80 dialogues on the MultiWOZ dataset, our approach enhances
ChatGPT's performance by an impressive 41.4%, matching or surpassing some fully
supervised start-of-the-art models. Additionally, the instance-specific
chain-of-thought prompt generated by our approach improves InstructGPT's
reasoning accuracy compared to human-crafted or automatically generated
prompts. The code and data are publicly available at
\url{https://github.com/Leezekun/Directional-Stimulus-Prompting}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2302.09051v4' target='_blank'>Complex QA and language models hybrid architectures, Survey</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, Elisabeth Murisasco</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-02-17 18:31:31</h6>
<p class='card-text'>This paper reviews the state-of-the-art of language models architectures and
strategies for "complex" question-answering (QA, CQA, CPS) with a focus on
hybridization. Large Language Models (LLM) are good at leveraging public data
on standard problems but once you want to tackle more specific complex
questions or problems (e.g. How does the concept of personal freedom vary
between different cultures ? What is the best mix of power generation methods
to reduce climate change ?) you may need specific architecture, knowledge,
skills, methods, sensitive data protection, explainability, human approval and
versatile feedback... Recent projects like ChatGPT and GALACTICA have allowed
non-specialists to grasp the great potential as well as the equally strong
limitations of LLM in complex QA. In this paper, we start by reviewing required
skills and evaluation techniques. We integrate findings from the robust
community edited research papers BIG, BLOOM and HELM which open source,
benchmark and analyze limits and challenges of LLM in terms of tasks complexity
and strict evaluation on accuracy (e.g. fairness, robustness, toxicity, ...) as
a baseline. We discuss some challenges associated with complex QA, including
domain adaptation, decomposition and efficient multi-step QA, long form and
non-factoid QA, safety and multi-sensitivity data protection, multimodal
search, hallucinations, explainability and truthfulness, temporal reasoning. We
analyze current solutions and promising research trends, using elements such
as: hybrid LLM architectural patterns, training and prompting strategies,
active human reinforcement learning supervised with AI, neuro-symbolic and
structured knowledge grounding, program synthesis, iterated decomposition and
others.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2302.06692v2' target='_blank'>Guiding Pretraining in Reinforcement Learning with Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor Darrell, Pieter Abbeel, Abhishek Gupta, Jacob Andreas</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-02-13 21:16:03</h6>
<p class='card-text'>Reinforcement learning algorithms typically struggle in the absence of a
dense, well-shaped reward function. Intrinsically motivated exploration methods
address this limitation by rewarding agents for visiting novel states or
transitions, but these methods offer limited benefits in large environments
where most discovered novelty is irrelevant for downstream tasks. We describe a
method that uses background knowledge from text corpora to shape exploration.
This method, called ELLM (Exploring with LLMs) rewards an agent for achieving
goals suggested by a language model prompted with a description of the agent's
current state. By leveraging large-scale language model pretraining, ELLM
guides agents toward human-meaningful and plausibly useful behaviors without
requiring a human in the loop. We evaluate ELLM in the Crafter game environment
and the Housekeep robotic simulator, showing that ELLM-trained agents have
better coverage of common-sense behaviors during pretraining and usually match
or improve performance on a range of downstream tasks. Code available at
https://github.com/yuqingd/ellm.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2302.02662v4' target='_blank'>Grounding Large Language Models in Interactive Environments with Online
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-02-06 10:01:08</h6>
<p class='card-text'>Recent works successfully leveraged Large Language Models' (LLM) abilities to
capture abstract knowledge about world's physics to solve decision-making
problems. Yet, the alignment between LLMs' knowledge and the environment can be
wrong and limit functional competence due to lack of grounding. In this paper,
we study an approach (named GLAM) to achieve this alignment through functional
grounding: we consider an agent using an LLM as a policy that is progressively
updated as the agent interacts with the environment, leveraging online
Reinforcement Learning to improve its performance to solve goals. Using an
interactive textual environment designed to study higher-level forms of
functional grounding, and a set of spatial and navigation tasks, we study
several scientific questions: 1) Can LLMs boost sample efficiency for online
learning of various RL tasks? 2) How can it boost different forms of
generalization? 3) What is the impact of online learning? We study these
questions by functionally grounding several variants (size, architecture) of
FLAN-T5.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2301.13573v1' target='_blank'>Skill Decision Transformer</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shyam Sudhakaran, Sebastian Risi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-01-31 11:52:46</h6>
<p class='card-text'>Recent work has shown that Large Language Models (LLMs) can be incredibly
effective for offline reinforcement learning (RL) by representing the
traditional RL problem as a sequence modelling problem (Chen et al., 2021;
Janner et al., 2021). However many of these methods only optimize for high
returns, and may not extract much information from a diverse dataset of
trajectories. Generalized Decision Transformers (GDTs) (Furuta et al., 2021)
have shown that utilizing future trajectory information, in the form of
information statistics, can help extract more information from offline
trajectory data. Building upon this, we propose Skill Decision Transformer
(Skill DT). Skill DT draws inspiration from hindsight relabelling (Andrychowicz
et al., 2017) and skill discovery methods to discover a diverse set of
primitive behaviors, or skills. We show that Skill DT can not only perform
offline state-marginal matching (SMM), but can discovery descriptive behaviors
that can be easily sampled. Furthermore, we show that through purely
reward-free optimization, Skill DT is still competitive with supervised offline
RL approaches on the D4RL benchmark. The code and videos can be found on our
project page: https://github.com/shyamsn97/skill-dt</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2301.12050v2' target='_blank'>Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making
  using Language Guided World Modelling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer Singh, Roy Fox</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-01-28 02:04:07</h6>
<p class='card-text'>Reinforcement learning (RL) agents typically learn tabula rasa, without prior
knowledge of the world. However, if initialized with knowledge of high-level
subgoals and transitions between subgoals, RL agents could utilize this
Abstract World Model (AWM) for planning and exploration. We propose using
few-shot large language models (LLMs) to hypothesize an AWM, that will be
verified through world experience, to improve sample efficiency of RL agents.
Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft
in two phases: (1) the Dream phase where the agent uses an LLM to decompose a
task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase
where the agent learns a modular policy for each subgoal and verifies or
corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and
then verifying the AWM based on agent experience not only increases sample
efficiency over contemporary methods by an order of magnitude but is also
robust to and corrects errors in the LLM, successfully blending noisy
internet-scale information from LLMs with knowledge grounded in environment
dynamics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2301.10095v2' target='_blank'>Large Language Models as Fiduciaries: A Case Study Toward Robustly
  Communicating With Artificial Intelligence Through Legal Standards</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:John J. Nay</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-01-24 16:03:20</h6>
<p class='card-text'>Artificial Intelligence (AI) is taking on increasingly autonomous roles,
e.g., browsing the web as a research assistant and managing money. But
specifying goals and restrictions for AI behavior is difficult. Similar to how
parties to a legal contract cannot foresee every potential "if-then"
contingency of their future relationship, we cannot specify desired AI behavior
for all circumstances. Legal standards facilitate robust communication of
inherently vague and underspecified goals. Instructions (in the case of
language models, "prompts") that employ legal standards will allow AI agents to
develop shared understandings of the spirit of a directive that generalize
expectations regarding acceptable actions to take in unspecified states of the
world. Standards have built-in context that is lacking from other goal
specification languages, such as plain language and programming languages.
Through an empirical study on thousands of evaluation labels we constructed
from U.S. court opinions, we demonstrate that large language models (LLMs) are
beginning to exhibit an "understanding" of one of the most relevant legal
standards for AI agents: fiduciary obligations. Performance comparisons across
models suggest that, as LLMs continue to exhibit improved core capabilities,
their legal standards understanding will also continue to improve. OpenAI's
latest LLM has 78% accuracy on our data, their previous release has 73%
accuracy, and a model from their 2020 GPT-3 paper has 27% accuracy (worse than
random). Our research is an initial step toward a framework for evaluating AI
understanding of legal standards more broadly, and for conducting reinforcement
learning with legal feedback (RLLF).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2210.13623v3' target='_blank'>Reinforcement Learning and Bandits for Speech and Language Processing:
  Tutorial, Review and Outlook</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Baihan Lin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-10-24 21:49:12</h6>
<p class='card-text'>In recent years, reinforcement learning and bandits have transformed a wide
range of real-world applications including healthcare, finance, recommendation
systems, robotics, and last but not least, the speech and natural language
processing. While most speech and language applications of reinforcement
learning algorithms are centered around improving the training of deep neural
networks with its flexible optimization properties, there are still many
grounds to explore to utilize the benefits of reinforcement learning, such as
its reward-driven adaptability, state representations, temporal structures and
generalizability. In this survey, we present an overview of recent advancements
of reinforcement learning and bandits, and discuss how they can be effectively
employed to solve speech and natural language processing problems with models
that are adaptive, interactive and scalable.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2210.07792v2' target='_blank'>Robust Preference Learning for Storytelling via Contrastive
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Louis Castricato, Alexander Havrilla, Shahbuland Matiana, Michael Pieler, Anbang Ye, Ian Yang, Spencer Frazier, Mark Riedl</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-10-14 13:21:33</h6>
<p class='card-text'>Controlled automated story generation seeks to generate natural language
stories satisfying constraints from natural language critiques or preferences.
Existing methods to control for story preference utilize prompt engineering
which is labor intensive and often inconsistent. They may also use
logit-manipulation methods which require annotated datasets to exist for the
desired attributes. To address these issues, we first train a contrastive
bi-encoder model to align stories with corresponding human critiques, named
CARP, building a general purpose preference model. This is subsequently used as
a reward function to fine-tune a generative language model via reinforcement
learning. However, simply fine-tuning a generative language model with a
contrastive reward model does not always reliably result in a story generation
system capable of generating stories that meet user preferences. To increase
story generation robustness we further fine-tune the contrastive reward model
using a prompt-learning technique. A human participant study is then conducted
comparing generations from our full system, ablations, and two baselines. We
show that the full fine-tuning pipeline results in a story generator preferred
over a LLM 20x as large as well as logit-based methods. This motivates the use
of contrastive learning for general purpose human preference modeling.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2210.03629v3' target='_blank'>ReAct: Synergizing Reasoning and Acting in Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-10-06 01:00:32</h6>
<p class='card-text'>While large language models (LLMs) have demonstrated impressive capabilities
across tasks in language understanding and interactive decision making, their
abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.
action plan generation) have primarily been studied as separate topics. In this
paper, we explore the use of LLMs to generate both reasoning traces and
task-specific actions in an interleaved manner, allowing for greater synergy
between the two: reasoning traces help the model induce, track, and update
action plans as well as handle exceptions, while actions allow it to interface
with external sources, such as knowledge bases or environments, to gather
additional information. We apply our approach, named ReAct, to a diverse set of
language and decision making tasks and demonstrate its effectiveness over
state-of-the-art baselines, as well as improved human interpretability and
trustworthiness over methods without reasoning or acting components.
Concretely, on question answering (HotpotQA) and fact verification (Fever),
ReAct overcomes issues of hallucination and error propagation prevalent in
chain-of-thought reasoning by interacting with a simple Wikipedia API, and
generates human-like task-solving trajectories that are more interpretable than
baselines without reasoning traces. On two interactive decision making
benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and
reinforcement learning methods by an absolute success rate of 34% and 10%
respectively, while being prompted with only one or two in-context examples.
Project site with code: https://react-lm.github.io</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2210.00131v4' target='_blank'>Underspecification in Language Modeling Tasks: A Causality-Informed
  Study of Gendered Pronoun Resolution</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Emily McMilin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-09-30 23:10:11</h6>
<p class='card-text'>Modern language modeling tasks are often underspecified: for a given token
prediction, many words may satisfy the user's intent of producing natural
language at inference time, however only one word will minimize the task's loss
function at training time. We introduce a simple causal mechanism to describe
the role underspecification plays in the generation of spurious correlations.
Despite its simplicity, our causal model directly informs the development of
two lightweight black-box evaluation methods, that we apply to gendered pronoun
resolution tasks on a wide range of LLMs to 1) aid in the detection of
inference-time task underspecification by exploiting 2) previously unreported
gender vs. time and gender vs. location spurious correlations on LLMs with a
range of A) sizes: from BERT-base to GPT-4 Turbo Preview, B) pre-training
objectives: from masked & autoregressive language modeling to a mixture of
these objectives, and C) training stages: from pre-training only to
reinforcement learning from human feedback (RLHF). Code and open-source demos
available at https://github.com/2dot71mily/uspec.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2208.11981v2' target='_blank'>On Reality and the Limits of Language Data: Aligning LLMs with Human
  Norms</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nigel H. Collier, Fangyu Liu, Ehsan Shareghi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-08-25 10:21:23</h6>
<p class='card-text'>Recent advancements in Large Language Models (LLMs) harness linguistic
associations in vast natural language data for practical applications. However,
their ability to understand the physical world using only language data remains
a question. After reviewing existing protocols, we explore this question using
a novel and tightly controlled reasoning test (ART) and compare human norms
against versions of GPT-3. Our findings highlight the categories of
common-sense relations models that could learn directly from data and areas of
weakness. GPT-3 offers evidence for verbal reasoning on a par with human
subjects for several relations including Synonymy, Antonymy, and Default
inheritance, Without reinforcement learning from human judgements, it appears
GPT-3 performs at the lower end of the reference interval for Has-part and
Contained-in. Weaknesses were observed also in affordance characteristics
through Necessary-quality, Order-of-size and Order-of-intensity. Combining LLMs
with symbolic world grounding is a promising direction to address associative
learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2206.08896v1' target='_blank'>Evolution through Large Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Joel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, Kenneth O. Stanley</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-06-17 17:07:04</h6>
<p class='card-text'>This paper pursues the insight that large language models (LLMs) trained to
generate code can vastly improve the effectiveness of mutation operators
applied to programs in genetic programming (GP). Because such LLMs benefit from
training data that includes sequential changes and modifications, they can
approximate likely changes that humans would make. To highlight the breadth of
implications of such evolution through large models (ELM), in the main
experiment ELM combined with MAP-Elites generates hundreds of thousands of
functional examples of Python programs that output working ambulating robots in
the Sodarace domain, which the original LLM had never seen in pre-training.
These examples then help to bootstrap training a new conditional language model
that can output the right walker for a particular terrain. The ability to
bootstrap new models that can output appropriate artifacts for a given context
in a domain where zero training data was previously available carries
implications for open-endedness, deep learning, and reinforcement learning.
These implications are explored here in depth in the hope of inspiring new
directions of research now opened up by ELM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2205.00584v2' target='_blank'>Making Large Language Models Interactive: A Pioneer Study on Supporting
  Complex Information-Seeking Tasks with Implicit Constraints</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ali Ahmadvand, Negar Arabzadeh, Julia Kiseleva, Patricio Figueroa Sanz, Xin Deng, Sujay Jauhar, Michael Gamon, Eugene Agichtein, Ned Friend, Aniruddha</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-05-02 00:04:44</h6>
<p class='card-text'>Current interactive systems with natural language interfaces lack the ability
to understand a complex information-seeking request which expresses several
implicit constraints at once, and there is no prior information about user
preferences e.g.,"find hiking trails around San Francisco which are accessible
with toddlers and have beautiful scenery in summer", where output is a list of
possible suggestions for users to start their exploration. In such scenarios,
user requests can be issued in one shot in the form of a complex and long
query, unlike conversational and exploratory search models, where require short
utterances or queries are often presented to the system step by step. We have
designed and deployed a platform to collect the data from approaching such
complex interactive systems. Moreover, despite with the current advancement of
generative language models these models suffer from hallucination in providing
accurate factual knowledge. All language models are mostly trained in large
part on web-scraped data from the past, which usually is not useful for
immediate users' needs. In this article, we propose an IA that leverages Large
Language Models (LLM) for complex request understanding and makes it
interactive using Reinforcement learning that allows intricately refine user
requests by making them complete, leading to better retrieval and reduce LLMs
hallucination problems for current user needs. To demonstrate the performance
of the proposed modeling paradigm, we have adopted various pre-retrieval
metrics that capture the extent to which guided interactions with our system
yield better retrieval results. Through extensive experimentation, we
demonstrated that our method significantly outperforms several robust
baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2104.12145v2' target='_blank'>LLM helps design and optimize photonic crystal surface emitting lasers</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Renjie Li, Ceyao Zhang, Sixuan Mao, Hai Huang, Mou Zhong, Yiou Cui, Xiyuan Zhou, Feng Yin, Zhaoyu Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-04-25 12:55:22</h6>
<p class='card-text'>Conventional design and optimization of Photonic Crystal Surface Emitting
Lasers (PCSEL) usually requires expert knowledge in semiconductor physics and
optimization algorithms, which is also known as the inverse design problem.
However, with the trend towards automation and depersonalization of the entire
integrated circuits (IC) industry, the conventional method, with the drawback
of being relatively labor-intensive and sub-optimal, warrants further
refinement. This technical dilemma remained until the emergence of Large
Language Models (LLMs), such as OpenAI's ChatGPT and Google's Bard. This paper
explores the possibility of applying LLMs to machine learning-based design and
optimization of PCSELs. Specifically, we utilize GPT3.5 and GPT4. By simply
having conversations, GPT assisted us with writing Finite Difference Time
Domain (FDTD) simulation code and deep reinforcement learning code to acquire
the optimized PCSEL solution, spanning from the proposition of ideas to the
realization of algorithms. Given that GPT will perform better when given
detailed and specific questions, we break down the PCSEL design problem into a
series of sub-problems and converse with GPT by posing open-ended heuristic
questions rather than definitive commands. This paper shows that LLMs, such as
ChatGPT, can guide the nanophotonic design and optimization processes, on both
the conceptual and technical level, and we propose new human-AI co-design
strategies and show their practical implications. We achieve a significant
milestone for the first step towards an automated end to end nanophotonic
design and production pipeline.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>