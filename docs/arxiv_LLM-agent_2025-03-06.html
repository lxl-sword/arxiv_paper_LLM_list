<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>LLM-agent - 2025-03-06</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>LLM-agent - 2025-03-06</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03750v1' target='_blank'>The MASK Benchmark: Disentangling Honesty From Accuracy in AI Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Richard Ren, Arunim Agarwal, Mantas Mazeika, Cristina Menghini, Robert Vacareanu, Brad Kenstler, Mick Yang, Isabelle Barrass, Alice Gatti, Xuwang Yin, Eduardo Trevino, Matias Geralnik, Adam Khoja, Dean Lee, Summer Yue, Dan Hendrycks</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 18:59:23</h6>
<p class='card-text'>As large language models (LLMs) become more capable and agentic, the
requirement for trust in their outputs grows significantly, yet at the same
time concerns have been mounting that models may learn to lie in pursuit of
their goals. To address these concerns, a body of work has emerged around the
notion of "honesty" in LLMs, along with interventions aimed at mitigating
deceptive behaviors. However, evaluations of honesty are currently highly
limited, with no benchmark combining large scale and applicability to all
models. Moreover, many benchmarks claiming to measure honesty in fact simply
measure accuracy--the correctness of a model's beliefs--in disguise. In this
work, we introduce a large-scale human-collected dataset for measuring honesty
directly, allowing us to disentangle accuracy from honesty for the first time.
Across a diverse set of LLMs, we find that while larger models obtain higher
accuracy on our benchmark, they do not become more honest. Surprisingly, while
most frontier LLMs obtain high scores on truthfulness benchmarks, we find a
substantial propensity in frontier LLMs to lie when pressured to do so,
resulting in low honesty scores on our benchmark. We find that simple methods,
such as representation engineering interventions, can improve honesty. These
results underscore the growing need for robust evaluations and effective
interventions to ensure LLMs remain trustworthy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03704v1' target='_blank'>A Practical Memory Injection Attack against LLM Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shen Dong, Shaocheng Xu, Pengfei He, Yige Li, Jiliang Tang, Tianming Liu, Hui Liu, Zhen Xiang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 17:53:24</h6>
<p class='card-text'>Agents based on large language models (LLMs) have demonstrated strong
capabilities in a wide range of complex, real-world applications. However, LLM
agents with a compromised memory bank may easily produce harmful outputs when
the past records retrieved for demonstration are malicious. In this paper, we
propose a novel Memory INJection Attack, MINJA, that enables the injection of
malicious records into the memory bank by only interacting with the agent via
queries and output observations. These malicious records are designed to elicit
a sequence of malicious reasoning steps leading to undesirable agent actions
when executing the victim user's query. Specifically, we introduce a sequence
of bridging steps to link the victim query to the malicious reasoning steps.
During the injection of the malicious record, we propose an indication prompt
to guide the agent to autonomously generate our designed bridging steps. We
also propose a progressive shortening strategy that gradually removes the
indication prompt, such that the malicious record will be easily retrieved when
processing the victim query comes after. Our extensive experiments across
diverse agents demonstrate the effectiveness of MINJA in compromising agent
memory. With minimal requirements for execution, MINJA enables any user to
influence agent memory, highlighting practical risks of LLM agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03686v1' target='_blank'>MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rui Ye, Shuo Tang, Rui Ge, Yaxin Du, Zhenfei Yin, Siheng Chen, Jing Shao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 17:27:59</h6>
<p class='card-text'>LLM-based multi-agent systems (MAS) have shown significant potential in
tackling diverse tasks. However, to design effective MAS, existing approaches
heavily rely on manual configurations or multiple calls of advanced LLMs,
resulting in inadaptability and high inference costs. In this paper, we
simplify the process of building an MAS by reframing it as a generative
language task, where the input is a user query and the output is a
corresponding MAS. To address this novel task, we unify the representation of
MAS as executable code and propose a consistency-oriented data construction
pipeline to create a high-quality dataset comprising coherent and consistent
query-MAS pairs. Using this dataset, we train MAS-GPT, an open-source
medium-sized LLM that is capable of generating query-adaptive MAS within a
single LLM inference. The generated MAS can be seamlessly applied to process
user queries and deliver high-quality responses. Extensive experiments on 9
benchmarks and 5 LLMs show that the proposed MAS-GPT consistently outperforms
10+ baseline MAS methods on diverse settings, indicating MAS-GPT's high
effectiveness, efficiency and strong generalization ability. Code will be
available at https://github.com/rui-ye/MAS-GPT.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03669v1' target='_blank'>Attentive Reasoning Queries: A Systematic Method for Optimizing
  Instruction-Following in Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bar Karov, Dor Zohar, Yam Marcovitz</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 17:03:48</h6>
<p class='card-text'>We present Attentive Reasoning Queries (ARQs), a novel structured reasoning
approach that significantly improves instruction-following in Large Language
Models through domain-specialized reasoning blueprints. While LLMs demonstrate
remarkable capabilities across diverse tasks, they often fail to maintain
adherence to complex, use-case-specific instructions during multi-turn
conversations, presenting challenges for business-critical applications. ARQs
address this limitation by guiding LLMs through systematic reasoning steps with
targeted queries that reinstate critical instructions and facilitate
intermediate reasoning throughout the completion process. In extensive testing
within Parlant, our framework for reliable customer-facing agents in which ARQs
were born out of necessity, they achieved a 90.2% success rate across 87 test
scenarios, outperforming both Chain-of-Thought reasoning (86.1%) and direct
response generation (81.5%). ARQs showed particular strength in addressing
persistent failure modes like guideline re-application and hallucination
prevention. Our analysis also revealed that ARQs can potentially be more
computationally efficient than free-form reasoning when carefully designed.
These findings demonstrate that structured reasoning approaches provide
effective mechanisms for controlling how LLMs process information and make
decisions in complex scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03586v1' target='_blank'>Benchmarking LLMs and LLM-based Agents in Practical Vulnerability
  Detection for Code Repositories</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alperen Yildiz, Sin G. Teo, Yiling Lou, Yebo Feng, Chong Wang, Dinil M. Divakaran</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 15:22:24</h6>
<p class='card-text'>Large Language Models (LLMs) have shown promise in software vulnerability
detection, particularly on function-level benchmarks like Devign and BigVul.
However, real-world detection requires interprocedural analysis, as
vulnerabilities often emerge through multi-hop function calls rather than
isolated functions. While repository-level benchmarks like ReposVul and VulEval
introduce interprocedural context, they remain computationally expensive, lack
pairwise evaluation of vulnerability fixes, and explore limited context
retrieval, limiting their practicality.
  We introduce JitVul, a JIT vulnerability detection benchmark linking each
function to its vulnerability-introducing and fixing commits. Built from 879
CVEs spanning 91 vulnerability types, JitVul enables comprehensive evaluation
of detection capabilities. Our results show that ReAct Agents, leveraging
thought-action-observation and interprocedural context, perform better than
LLMs in distinguishing vulnerable from benign code. While prompting strategies
like Chain-of-Thought help LLMs, ReAct Agents require further refinement. Both
methods show inconsistencies, either misidentifying vulnerabilities or
over-analyzing security guards, indicating significant room for improvement.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03505v1' target='_blank'>Parallelized Planning-Acting for Efficient LLM-based Multi-Agent Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yaoru Li, Shunyu Liu, Tongya Zheng, Mingli Song</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 13:53:10</h6>
<p class='card-text'>Recent advancements in Large Language Model(LLM)-based Multi-Agent
Systems(MAS) have demonstrated remarkable potential for tackling complex
decision-making tasks. However, existing frameworks inevitably rely on
serialized execution paradigms, where agents must complete sequential LLM
planning before taking action. This fundamental constraint severely limits
real-time responsiveness and adaptation, which is crucial in dynamic
environments with ever-changing scenarios. In this paper, we propose a novel
parallelized planning-acting framework for LLM-based MAS, featuring a
dual-thread architecture with interruptible execution to enable concurrent
planning and acting. Specifically, our framework comprises two core threads:(1)
a planning thread driven by a centralized memory system, maintaining
synchronization of environmental states and agent communication to support
dynamic decision-making; and (2) an acting thread equipped with a comprehensive
skill library, enabling automated task execution through recursive
decomposition. Extensive experiments on challenging Minecraft demonstrate the
effectiveness of the proposed framework.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03503v1' target='_blank'>Collaborative Expert LLMs Guided Multi-Objective Molecular Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiajun Yu, Yizhen Zheng, Huan Yee Koh, Shirui Pan, Tianyue Wang, Haishuai Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 13:47:55</h6>
<p class='card-text'>Molecular optimization is a crucial yet complex and time-intensive process
that often acts as a bottleneck for drug development. Traditional methods rely
heavily on trial and error, making multi-objective optimization both
time-consuming and resource-intensive. Current AI-based methods have shown
limited success in handling multi-objective optimization tasks, hampering their
practical utilization. To address this challenge, we present MultiMol, a
collaborative large language model (LLM) system designed to guide
multi-objective molecular optimization. MultiMol comprises two agents,
including a data-driven worker agent and a literature-guided research agent.
The data-driven worker agent is a large language model being fine-tuned to
learn how to generate optimized molecules considering multiple objectives,
while the literature-guided research agent is responsible for searching
task-related literature to find useful prior knowledge that facilitates
identifying the most promising optimized candidates. In evaluations across six
multi-objective optimization tasks, MultiMol significantly outperforms existing
methods, achieving a 82.30% success rate, in sharp contrast to the 27.50%
success rate of current strongest methods. To further validate its practical
impact, we tested MultiMol on two real-world challenges. First, we enhanced the
selectivity of Xanthine Amine Congener (XAC), a promiscuous ligand that binds
both A1R and A2AR, successfully biasing it towards A1R. Second, we improved the
bioavailability of Saquinavir, an HIV-1 protease inhibitor with known
bioavailability limitations. Overall, these results indicate that MultiMol
represents a highly promising approach for multi-objective molecular
optimization, holding great potential to accelerate the drug development
process and contribute to the advancement of pharmaceutical research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03462v1' target='_blank'>Open-Source Large Language Models as Multilingual Crowdworkers:
  Synthesizing Open-Domain Dialogues in Several Languages With No Examples in
  Targets and No Machine Translation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ahmed Njifenjou, Virgile Sucal, Bassam Jabaian, Fabrice Lefèvre</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 12:52:14</h6>
<p class='card-text'>The prevailing paradigm in the domain of Open-Domain Dialogue agents
predominantly focuses on the English language, encompassing both models and
datasets. Furthermore, the financial and temporal investments required for
crowdsourcing such datasets for finetuning are substantial, particularly when
multiple languages are involved. Fortunately, advancements in Large Language
Models (LLMs) have unveiled a plethora of possibilities across diverse tasks.
Specifically, instruction-tuning has enabled LLMs to execute tasks based on
natural language instructions, occasionally surpassing the performance of human
crowdworkers. Additionally, these models possess the capability to function in
various languages within a single thread. Consequently, to generate new samples
in different languages, we propose leveraging these capabilities to replicate
the data collection process. We introduce a pipeline for generating Open-Domain
Dialogue data in multiple Target Languages using LLMs, with demonstrations
provided in a unique Source Language. By eschewing explicit Machine Translation
in this approach, we enhance the adherence to language-specific nuances. We
apply this methodology to the PersonaChat dataset. To enhance the openness of
generated dialogues and mimic real life scenarii, we added the notion of speech
events corresponding to the type of conversation the speakers are involved in
and also that of common ground which represents the premises of a conversation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03459v1' target='_blank'>Unified Mind Model: Reimagining Autonomous Agents in the LLM Era</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pengbo Hu, Xiang Ying</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 12:49:44</h6>
<p class='card-text'>Large language models (LLMs) have recently demonstrated remarkable
capabilities across domains, tasks, and languages (e.g., ChatGPT and GPT-4),
reviving the research of general autonomous agents with human-like cognitive
abilities.Such human-level agents require semantic comprehension and
instruction-following capabilities, which exactly fall into the strengths of
LLMs.Although there have been several initial attempts to build human-level
agents based on LLMs, the theoretical foundation remains a challenging open
problem. In this paper, we propose a novel theoretical cognitive architecture,
the Unified Mind Model (UMM), which offers guidance to facilitate the rapid
creation of autonomous agents with human-level cognitive abilities.
Specifically, our UMM starts with the global workspace theory and further
leverage LLMs to enable the agent with various cognitive abilities, such as
multi-modal perception, planning, reasoning, tool use, learning, memory,
reflection and motivation. Building upon UMM, we then develop an agent-building
engine, MindOS, which allows users to quickly create domain-/task-specific
autonomous agents without any programming effort.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03444v1' target='_blank'>Taxation Perspectives from Large Language Models: A Case Study on
  Additional Tax Penalties</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Eunkyung Choi, Young Jin Suh, Hun Park, Wonseok Hwang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 12:24:20</h6>
<p class='card-text'>How capable are large language models (LLMs) in the domain of taxation?
Although numerous studies have explored the legal domain in general, research
dedicated to taxation remain scarce. Moreover, the datasets used in these
studies are either simplified, failing to reflect the real-world complexities,
or unavailable as open source. To address this gap, we introduce PLAT, a new
benchmark designed to assess the ability of LLMs to predict the legitimacy of
additional tax penalties. PLAT is constructed to evaluate LLMs' understanding
of tax law, particularly in cases where resolving the issue requires more than
just applying related statutes. Our experiments with six LLMs reveal that their
baseline capabilities are limited, especially when dealing with conflicting
issues that demand a comprehensive understanding. However, we found that
enabling retrieval, self-reasoning, and discussion among multiple agents with
specific role assignments, this limitation can be mitigated.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03303v1' target='_blank'>SEOE: A Scalable and Reliable Semantic Evaluation Framework for Open
  Domain Event Detection</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yi-Fan Lu, Xian-Ling Mao, Tian Lan, Tong Zhang, Yu-Shi Zhu, Heyan Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 09:37:05</h6>
<p class='card-text'>Automatic evaluation for Open Domain Event Detection (ODED) is a highly
challenging task, because ODED is characterized by a vast diversity of
un-constrained output labels from various domains. Nearly all existing
evaluation methods for ODED usually first construct evaluation benchmarks with
limited labels and domain coverage, and then evaluate ODED methods using
metrics based on token-level label matching rules. However, this kind of
evaluation framework faces two issues: (1) The limited evaluation benchmarks
lack representatives of the real world, making it difficult to accurately
reflect the performance of various ODED methods in real-world scenarios; (2)
Evaluation metrics based on token-level matching rules fail to capture semantic
similarity between predictions and golden labels. To address these two problems
above, we propose a scalable and reliable Semantic-level Evaluation framework
for Open domain Event detection (SEOE) by constructing a more representative
evaluation benchmark and introducing a semantic evaluation metric.
Specifically, our proposed framework first constructs a scalable evaluation
benchmark that currently includes 564 event types covering 7 major domains,
with a cost-effective supplementary annotation strategy to ensure the
benchmark's representativeness. The strategy also allows for the supplement of
new event types and domains in the future. Then, the proposed SEOE leverages
large language models (LLMs) as automatic evaluation agents to compute a
semantic F1-score, incorporating fine-grained definitions of semantically
similar labels to enhance the reliability of the evaluation. Extensive
experiments validate the representatives of the benchmark and the reliability
of the semantic evaluation metric. Existing ODED methods are thoroughly
evaluated, and the error patterns of predictions are analyzed, revealing
several insightful findings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03258v1' target='_blank'>Exploring the Potential of Large Language Models as Predictors in
  Dynamic Text-Attributed Graphs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Runlin Lei, Jiarui Ji, Haipeng Ding, Lu Yi, Zhewei Wei, Yongchao Liu, Chuntao Hong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 08:28:11</h6>
<p class='card-text'>With the rise of large language models (LLMs), there has been growing
interest in Graph Foundation Models (GFMs) for graph-based tasks. By leveraging
LLMs as predictors, GFMs have demonstrated impressive generalizability across
various tasks and datasets. However, existing research on LLMs as predictors
has predominantly focused on static graphs, leaving their potential in dynamic
graph prediction unexplored. In this work, we pioneer using LLMs for predictive
tasks on dynamic graphs. We identify two key challenges: the constraints
imposed by context length when processing large-scale historical data and the
significant variability in domain characteristics, both of which complicate the
development of a unified predictor. To address these challenges, we propose the
GraphAgent-Dynamic (GAD) Framework, a multi-agent system that leverages
collaborative LLMs. In contrast to using a single LLM as the predictor, GAD
incorporates global and local summary agents to generate domain-specific
knowledge, enhancing its transferability across domains. Additionally,
knowledge reflection agents enable adaptive updates to GAD's knowledge,
maintaining a unified and self-consistent architecture. In experiments, GAD
demonstrates performance comparable to or even exceeds that of full-supervised
graph neural networks without dataset-specific training. Finally, to enhance
the task-specific performance of LLM-based predictors, we discuss potential
improvements, such as dataset-specific fine-tuning to LLMs. By developing
tailored strategies for different tasks, we provide new insights for the future
design of LLM-based predictors.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03205v1' target='_blank'>MA-LoT: Multi-Agent Lean-based Long Chain-of-Thought Reasoning enhances
  Formal Theorem Proving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ruida Wang, Rui Pan, Yuxin Li, Jipeng Zhang, Yizhen Jia, Shizhe Diao, Renjie Pi, Junjie Hu, Tong Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 05:50:31</h6>
<p class='card-text'>Solving mathematical problems using computer-verifiable languages like Lean
has significantly impacted mathematical and computer science communities.
State-of-the-art methods utilize single Large Language Models (LLMs) as agents
or provers to either generate complete proof or perform tree searches. However,
single-agent methods inherently lack a structured way to combine high-level
reasoning in Natural Language (NL) with Formal Language (FL) verification
feedback. To solve these issues, we propose MA-LoT: Multi-Agent Lean-based Long
Chain-of-Thought framework, (to the best of our knowledge), the first
multi-agent framework for Lean4 theorem proving that balance high-level NL
reasoning and FL verification in Long CoT. Using this structured interaction,
our approach enables deeper insights and long-term coherence in proof
generation, with which past methods struggle. We do this by leveraging emergent
formal reasoning ability in Long CoT using our novel LoT-Transfer Learning
training-inference pipeline. Extensive experiments show that our framework
achieves 54.51% accuracy rate on the Lean4 version of MiniF2F-Test dataset,
largely outperforming GPT-4 (22.95%), single-agent tree search
(InternLM-Step-Prover, 50.70%), and whole-proof generation
(DeepSeek-Prover-v1.5, 48.36%) baselines. Furthermore, our findings highlight
the potential of combining Long CoT with formal verification for a more
insightful generation in a broader perspective.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03154v1' target='_blank'>Dango: A Mixed-Initiative Data Wrangling System using Large Language
  Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wei-Hao Chen, Weixi Tong, Amanda Case, Tianyi Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 03:54:51</h6>
<p class='card-text'>Data wrangling is a time-consuming and challenging task in a data science
pipeline. While many tools have been proposed to automate or facilitate data
wrangling, they often misinterpret user intent, especially in complex tasks. We
propose Dango, a mixed-initiative multi-agent system for data wrangling.
Compared to existing tools, Dango enhances user communication of intent by
allowing users to demonstrate on multiple tables and use natural language
prompts in a conversation interface, enabling users to clarify their intent by
answering LLM-posed multiple-choice clarification questions, and providing
multiple forms of feedback such as step-by-step natural language explanations
and data provenance to help users evaluate the data wrangling scripts. We
conducted a within-subjects user study with 38 participants and demonstrated
that Dango's features can significantly improve intent clarification, accuracy,
and efficiency in data wrangling. Furthermore, we demonstrated the
generalizability of Dango by applying it to a broader set of data wrangling
tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02976v1' target='_blank'>Teaching AI to Handle Exceptions: Supervised Fine-Tuning with
  Human-Aligned Judgment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Matthew DosSantos DiSorbo, Harang Ju, Sinan Aral</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 20:00:37</h6>
<p class='card-text'>Large language models (LLMs), initially developed for generative AI, are now
evolving into agentic AI systems, which make decisions in complex, real-world
contexts. Unfortunately, while their generative capabilities are
well-documented, their decision-making processes remain poorly understood. This
is particularly evident when models are handling exceptions, a critical and
challenging aspect of decision-making made relevant by the inherent
incompleteness of contracts. Here we demonstrate that LLMs, even ones that
excel at reasoning, deviate significantly from human judgments because they
adhere strictly to policies, even when such adherence is impractical,
suboptimal, or even counterproductive. We then evaluate three approaches to
tuning AI agents to handle exceptions: ethical framework prompting,
chain-of-thought reasoning, and supervised fine-tuning. We find that while
ethical framework prompting fails and chain-of-thought prompting provides only
slight improvements, supervised fine-tuning, specifically with human
explanations, yields markedly better results. Surprisingly, in our experiments,
supervised fine-tuning even enabled models to generalize human-like
decision-making to novel scenarios, demonstrating transfer learning of
human-aligned decision-making across contexts. Furthermore, fine-tuning with
explanations, not just labels, was critical for alignment, suggesting that
aligning LLMs with human judgment requires explicit training on how decisions
are made, not just which decisions are made. These findings highlight the need
to address LLMs' shortcomings in handling exceptions in order to guide the
development of agentic AI toward models that can effectively align with human
judgment and simultaneously adapt to novel contexts.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02760v1' target='_blank'>From Metaphor to Mechanism: How LLMs Decode Traditional Chinese Medicine
  Symbolic Language for Modern Clinical Relevance</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiacheng Tang, Nankai Wu, Fan Gao, Chengxiao Dai, Mengyao Zhao, Xinjie Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 16:22:49</h6>
<p class='card-text'>Metaphorical expressions are abundant in Traditional Chinese Medicine (TCM),
conveying complex disease mechanisms and holistic health concepts through
culturally rich and often abstract terminology. Bridging these metaphors to
anatomically driven Western medical (WM) concepts poses significant challenges
for both automated language processing and real-world clinical practice. To
address this gap, we propose a novel multi-agent and chain-of-thought (CoT)
framework designed to interpret TCM metaphors accurately and map them to WM
pathophysiology. Specifically, our approach combines domain-specialized agents
(TCM Expert, WM Expert) with a Coordinator Agent, leveraging stepwise
chain-of-thought prompts to ensure transparent reasoning and conflict
resolution. We detail a methodology for building a metaphor-rich TCM dataset,
discuss strategies for effectively integrating multi-agent collaboration and
CoT reasoning, and articulate the theoretical underpinnings that guide metaphor
interpretation across distinct medical paradigms. We present a comprehensive
system design and highlight both the potential benefits and limitations of our
approach, while leaving placeholders for future experimental validation. Our
work aims to support clinical decision-making, cross-system educational
initiatives, and integrated healthcare research, ultimately offering a robust
scaffold for reconciling TCM's symbolic language with the mechanistic focus of
Western medicine.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02692v1' target='_blank'>FinArena: A Human-Agent Collaboration Framework for Financial Market
  Analysis and Forecasting</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Congluo Xu, Zhaobin Liu, Ziyang Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 15:04:40</h6>
<p class='card-text'>To improve stock trend predictions and support personalized investment
decisions, this paper proposes FinArena, a novel Human-Agent collaboration
framework. Inspired by the mixture of experts (MoE) approach, FinArena combines
multimodal financial data analysis with user interaction. The human module
features an interactive interface that captures individual risk preferences,
allowing personalized investment strategies. The machine module utilizes a
Large Language Model-based (LLM-based) multi-agent system to integrate diverse
data sources, such as stock prices, news articles, and financial statements. To
address hallucinations in LLMs, FinArena employs the adaptive
Retrieval-Augmented Generative (RAG) method for processing unstructured news
data. Finally, a universal expert agent makes investment decisions based on the
features extracted from multimodal data and investors' individual risk
preferences. Extensive experiments show that FinArena surpasses both
traditional and state-of-the-art benchmarks in stock trend prediction and
yields promising results in trading simulations across various risk profiles.
These findings highlight FinArena's potential to enhance investment outcomes by
aligning strategic insights with personalized risk considerations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02682v1' target='_blank'>MPO: Boosting LLM Agents with Meta Plan Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weimin Xiong, Yifan Song, Qingxiu Dong, Bingchan Zhao, Feifan Song, Xun Wang, Sujian Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 14:54:45</h6>
<p class='card-text'>Recent advancements in large language models (LLMs) have enabled LLM-based
agents to successfully tackle interactive planning tasks. However, despite
their successes, existing approaches often suffer from planning hallucinations
and require retraining for each new agent. To address these challenges, we
propose the Meta Plan Optimization (MPO) framework, which enhances agent
planning capabilities by directly incorporating explicit guidance. Unlike
previous methods that rely on complex knowledge, which either require
significant human effort or lack quality assurance, MPO leverages high-level
general guidance through meta plans to assist agent planning and enables
continuous optimization of the meta plans based on feedback from the agent's
task execution. Our experiments conducted on two representative tasks
demonstrate that MPO significantly outperforms existing baselines. Moreover,
our analysis indicates that MPO provides a plug-and-play solution that enhances
both task completion efficiency and generalization capabilities in previous
unseen scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02582v1' target='_blank'>Playing games with Large language models: Randomness and strategy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alicia Vidler, Toby Walsh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 13:04:48</h6>
<p class='card-text'>Playing games has a long history of describing intricate interactions in
simplified forms. In this paper we explore if large language models (LLMs) can
play games, investigating their capabilities for randomisation and strategic
adaptation through both simultaneous and sequential game interactions. We focus
on GPT-4o-Mini-2024-08-17 and test two games between LLMs: Rock Paper Scissors
(RPS) and games of strategy (Prisoners Dilemma PD). LLMs are often described as
stochastic parrots, and while they may indeed be parrots, our results suggest
that they are not very stochastic in the sense that their outputs - when
prompted to be random - are often very biased. Our research reveals that LLMs
appear to develop loss aversion strategies in repeated games, with RPS
converging to stalemate conditions while PD shows systematic shifts between
cooperative and competitive outcomes based on prompt design. We detail
programmatic tools for independent agent interactions and the Agentic AI
challenges faced in implementation. We show that LLMs can indeed play games,
just not very well. These results have implications for the use of LLMs in
multi-agent LLM systems and showcase limitations in current approaches to model
output for strategic decision-making.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02519v1' target='_blank'>Generator-Assistant Stepwise Rollback Framework for Large Language Model
  Agent</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xingzuo Li, Kehai Chen, Yunfei Long, Xuefeng Bai, Yong Xu, Min Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 11:31:05</h6>
<p class='card-text'>Large language model (LLM) agents typically adopt a step-by-step reasoning
framework, in which they interleave the processes of thinking and acting to
accomplish the given task. However, this paradigm faces a deep-rooted one-pass
issue whereby each generated intermediate thought is plugged into the
trajectory regardless of its correctness, which can cause irreversible error
propagation. To address the issue, this paper proposes a novel framework called
Generator-Assistant Stepwise Rollback (GA-Rollback) to induce better
decision-making for LLM agents. Particularly, GA-Rollback utilizes a generator
to interact with the environment and an assistant to examine each action
produced by the generator, where the assistant triggers a rollback operation
upon detection of incorrect actions. Moreover, we introduce two additional
strategies tailored for the rollback scenario to further improve its
effectiveness. Extensive experiments show that GA-Rollback achieves significant
improvements over several strong baselines on three widely used benchmarks. Our
analysis further reveals that GA-Rollback can function as a robust
plug-and-play module, integrating seamlessly with other methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02445v2' target='_blank'>BRIDGE: Bootstrapping Text to Control Time-Series Generation via
  Multi-Agent Iterative Optimization and Diffusion Modelling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hao Li, Yu-Hao Huang, Chang Xu, Viktor Schlegel, Ren-He Jiang, Riza Batista-Navarro, Goran Nenadic, Jiang Bian</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 09:40:00</h6>
<p class='card-text'>Time-series Generation (TSG) is a prominent research area with broad
applications in simulations, data augmentation, and counterfactual analysis.
While existing methods have shown promise in unconditional single-domain TSG,
real-world applications demand for cross-domain approaches capable of
controlled generation tailored to domain-specific constraints and
instance-level requirements. In this paper, we argue that text can provide
semantic insights, domain information and instance-specific temporal patterns,
to guide and improve TSG. We introduce ``Text-Controlled TSG'', a task focused
on generating realistic time series by incorporating textual descriptions. To
address data scarcity in this setting, we propose a novel LLM-based Multi-Agent
framework that synthesizes diverse, realistic text-to-TS datasets. Furthermore,
we introduce BRIDGE, a hybrid text-controlled TSG framework that integrates
semantic prototypes with text description for supporting domain-level guidance.
This approach achieves state-of-the-art generation fidelity on 11 of 12
datasets, and improves controllability by 12.52% on MSE and 6.34% MAE compared
to no text input generation, highlighting its potential for generating tailored
time-series data.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02398v1' target='_blank'>PersonaX: A Recommendation Agent Oriented User Modeling Framework for
  Long Behavior Sequence</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yunxiao Shi, Wujiang Xu, Zeqi Zhang, Xing Zi, Qiang Wu, Min Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 08:41:40</h6>
<p class='card-text'>Recommendation agents leverage large language models for user modeling LLM UM
to construct textual personas guiding alignment with real users. However
existing LLM UM methods struggle with long user generated content UGC due to
context limitations and performance degradation. To address this sampling
strategies prioritize relevance or recency are often applied yet they
inevitably neglect the diverse user interests embedded within the discarded
behaviors resulting in incomplete modeling and degraded profiling quality.
Furthermore relevance based sampling requires real time retrieval forcing the
user modeling process to operate online which introduces significant latency
overhead. In this paper we propose PersonaX an agent agnostic LLM UM framework
that tackles these challenges through sub behavior sequence SBS selection and
offline multi persona construction. PersonaX extracts compact SBS segments
offline to capture diverse user interests generating fine grained textual
personas that are cached for efficient online retrieval. This approach ensures
that the user persona used for prompting remains highly relevant to the current
context while eliminating the need for online user modeling. For SBS selection
we ensure both efficiency length less than five and high representational
quality by balancing prototypicality and diversity within the sampled data.
Extensive experiments validate the effectiveness and versatility of PersonaX in
high quality user profiling. Utilizing only 30 to 50 percent of the behavioral
data with a sequence length of 480 integrating PersonaX with AgentCF yields an
absolute performance improvement of 3 to 11 percent while integration with
Agent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic
framework sets a new benchmark for scalable user modeling paving the way for
more accurate and efficient LLM driven recommendation agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02390v1' target='_blank'>ReSo: A Reward-driven Self-organizing LLM-based Multi-Agent System for
  Reasoning Tasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Heng Zhou, Hejia Geng, Xiangyuan Xue, Zhenfei Yin, Lei Bai</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 08:28:04</h6>
<p class='card-text'>Multi-agent systems have emerged as a promising approach for enhancing the
reasoning capabilities of large language models in complex problem-solving.
However, current MAS frameworks are limited by poor flexibility and
scalability, with underdeveloped optimization strategies. To address these
challenges, we propose ReSo, which integrates task graph generation with a
reward-driven two-stage agent selection process. The core of ReSo is the
proposed Collaborative Reward Model, which can provide fine-grained reward
signals for MAS cooperation for optimization. We also introduce an automated
data synthesis framework for generating MAS benchmarks, without human
annotations. Experimentally, ReSo matches or outperforms existing methods. ReSo
achieves \textbf{33.7\%} and \textbf{32.3\%} accuracy on Math-MAS and
SciBench-MAS SciBench, while other methods completely fail. Code is available
at: \href{https://github.com/hengzzzhou/ReSo}{ReSo}</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02365v1' target='_blank'>EchoQA: A Large Collection of Instruction Tuning Data for Echocardiogram
  Reports</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lama Moukheiber, Mira Moukheiber, Dana Moukheiiber, Hyung-Chul Lee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 07:45:45</h6>
<p class='card-text'>We introduce a novel question-answering (QA) dataset using echocardiogram
reports sourced from the Medical Information Mart for Intensive Care database.
This dataset is specifically designed to enhance QA systems in cardiology,
consisting of 771,244 QA pairs addressing a wide array of cardiac abnormalities
and their severity. We compare large language models (LLMs), including
open-source and biomedical-specific models for zero-shot evaluation, and
closed-source models for zero-shot and three-shot evaluation. Our results show
that fine-tuning LLMs improves performance across various QA metrics,
validating the value of our dataset. Clinicians also qualitatively evaluate the
best-performing model to assess the LLM responses for correctness. Further, we
conduct fine-grained fairness audits to assess the bias-performance trade-off
of LLMs across various social determinants of health. Our objective is to
propel the field forward by establishing a benchmark for LLM AI agents aimed at
supporting clinicians with cardiac differential diagnoses, thereby reducing the
documentation burden that contributes to clinician burnout and enabling
healthcare professionals to focus more on patient care.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02268v1' target='_blank'>AppAgentX: Evolving GUI Agents as Proficient Smartphone Users</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenjia Jiang, Yangyang Zhuang, Chenxi Song, Xu Yang, Chi Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 04:34:09</h6>
<p class='card-text'>Recent advancements in Large Language Models (LLMs) have led to the
development of intelligent LLM-based agents capable of interacting with
graphical user interfaces (GUIs). These agents demonstrate strong reasoning and
adaptability, enabling them to perform complex tasks that traditionally
required predefined rules. However, the reliance on step-by-step reasoning in
LLM-based agents often results in inefficiencies, particularly for routine
tasks. In contrast, traditional rule-based systems excel in efficiency but lack
the intelligence and flexibility to adapt to novel scenarios. To address this
challenge, we propose a novel evolutionary framework for GUI agents that
enhances operational efficiency while retaining intelligence and flexibility.
Our approach incorporates a memory mechanism that records the agent's task
execution history. By analyzing this history, the agent identifies repetitive
action sequences and evolves high-level actions that act as shortcuts,
replacing these low-level operations and improving efficiency. This allows the
agent to focus on tasks requiring more complex reasoning, while simplifying
routine actions. Experimental results on multiple benchmark tasks demonstrate
that our approach significantly outperforms existing methods in both efficiency
and accuracy. The code will be open-sourced to support further research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02238v1' target='_blank'>Haste Makes Waste: Evaluating Planning Abilities of LLMs for Efficient
  and Feasible Multitasking with Time Constraints Between Actions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zirui Wu, Xiao Liu, Jiayi Li, Lingpeng Kong, Yansong Feng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 03:27:02</h6>
<p class='card-text'>While Large Language Model-based agents have demonstrated substantial
progress in task completion, existing evaluation benchmarks tend to
overemphasize single-task performance, with insufficient attention given to the
crucial aspects of multitask planning and execution efficiency required in
real-world scenarios. To bridge this gap, we present Recipe2Plan, a novel
benchmark framework based on real-world cooking scenarios. Unlike conventional
benchmarks, Recipe2Plan challenges agents to optimize cooking time through
parallel task execution while respecting temporal constraints i.e. specific
actions need to be performed within a particular time intervals following the
preceding steps. Overly aggressive local parallelization may disrupt this
constraint, potentially compromising the entire cooking process. This strict
time constraint between actions raises a unique challenge for agents to balance
between maximizing concurrent operations and adhering to critical timing
constraints. Extensive experiments with state-of-the-art models reveal
challenges in maintaining this balance between efficiency and feasibility. The
results highlight the need for improved temporal awareness and global
multitasking capabilities in large language models. We open-source our
benchmark and code at https://github.com/WilliamZR/Recipe2Plan.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02197v1' target='_blank'>ATLaS: Agent Tuning via Learning Critical Steps</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhixun Chen, Ming Li, Yuxuan Huang, Yali Du, Meng Fang, Tianyi Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 02:14:55</h6>
<p class='card-text'>Large Language Model (LLM) agents have demonstrated remarkable generalization
capabilities across multi-domain tasks. Existing agent tuning approaches
typically employ supervised finetuning on entire expert trajectories. However,
behavior-cloning of full trajectories can introduce expert bias and weaken
generalization to states not covered by the expert data. Additionally, critical
steps, such as planning, complex reasoning for intermediate subtasks, and
strategic decision-making, are essential to success in agent tasks, so learning
these steps is the key to improving LLM agents. For more effective and
efficient agent tuning, we propose ATLaS that identifies the critical steps in
expert trajectories and finetunes LLMs solely on these steps with reduced
costs. By steering the training's focus to a few critical steps, our method
mitigates the risk of overfitting entire trajectories and promotes
generalization across different environments and tasks. In extensive
experiments, an LLM finetuned on only 30% critical steps selected by ATLaS
outperforms the LLM finetuned on all steps and recent open-source LLM agents.
ATLaS maintains and improves base LLM skills as generalist agents interacting
with diverse environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02076v1' target='_blank'>CorrA: Leveraging Large Language Models for Dynamic Obstacle Avoidance
  of Autonomous Vehicles</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shanting Wang, Panagiotis Typaldos, Andreas A. Malikopoulos</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 21:57:28</h6>
<p class='card-text'>In this paper, we present Corridor-Agent (CorrA), a framework that integrates
large language models (LLMs) with model predictive control (MPC) to address the
challenges of dynamic obstacle avoidance in autonomous vehicles. Our approach
leverages LLM reasoning ability to generate appropriate parameters for
sigmoid-based boundary functions that define safe corridors around obstacles,
effectively reducing the state-space of the controlled vehicle. The proposed
framework adjusts these boundaries dynamically based on real-time vehicle data
that guarantees collision-free trajectories while also ensuring both
computational efficiency and trajectory optimality. The problem is formulated
as an optimal control problem and solved with differential dynamic programming
(DDP) for constrained optimization, and the proposed approach is embedded
within an MPC framework. Extensive simulation and real-world experiments
demonstrate that the proposed framework achieves superior performance in
maintaining safety and efficiency in complex, dynamic environments compared to
a baseline MPC approach.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02068v1' target='_blank'>Interactive Debugging and Steering of Multi-Agent AI Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Will Epperson, Gagan Bansal, Victor Dibia, Adam Fourney, Jack Gerrits, Erkang Zhu, Saleema Amershi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 21:42:54</h6>
<p class='card-text'>Fully autonomous teams of LLM-powered AI agents are emerging that collaborate
to perform complex tasks for users. What challenges do developers face when
trying to build and debug these AI agent teams? In formative interviews with
five AI agent developers, we identify core challenges: difficulty reviewing
long agent conversations to localize errors, lack of support in current tools
for interactive debugging, and the need for tool support to iterate on agent
configuration. Based on these needs, we developed an interactive multi-agent
debugging tool, AGDebugger, with a UI for browsing and sending messages, the
ability to edit and reset prior agent messages, and an overview visualization
for navigating complex message histories. In a two-part user study with 14
participants, we identify common user strategies for steering agents and
highlight the importance of interactive message resets for debugging. Our
studies deepen understanding of interfaces for debugging increasingly important
agentic workflows.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02067v1' target='_blank'>AI persuading AI vs AI persuading Humans: LLMs' Differential
  Effectiveness in Promoting Pro-Environmental Behavior</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alexander Doudkin, Pat Pataranutaporn, Pattie Maes</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 21:40:55</h6>
<p class='card-text'>Pro-environmental behavior (PEB) is vital to combat climate change, yet
turning awareness into intention and action remains elusive. We explore large
language models (LLMs) as tools to promote PEB, comparing their impact across
3,200 participants: real humans (n=1,200), simulated humans based on actual
participant data (n=1,200), and fully synthetic personas (n=1,200). All three
participant groups faced personalized or standard chatbots, or static
statements, employing four persuasion strategies (moral foundations, future
self-continuity, action orientation, or "freestyle" chosen by the LLM). Results
reveal a "synthetic persuasion paradox": synthetic and simulated agents
significantly affect their post-intervention PEB stance, while human responses
barely shift. Simulated participants better approximate human trends but still
overestimate effects. This disconnect underscores LLM's potential for
pre-evaluating PEB interventions but warns of its limits in predicting
real-world behavior. We call for refined synthetic modeling and sustained and
extended human trials to align conversational AI's promise with tangible
sustainability outcomes.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>