<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>LLM-agent - 2025-03-05</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>LLM-agent - 2025-03-05</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02760v1' target='_blank'>From Metaphor to Mechanism: How LLMs Decode Traditional Chinese Medicine
  Symbolic Language for Modern Clinical Relevance</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiacheng Tang, Nankai Wu, Fan Gao, Chengxiao Dai, Mengyao Zhao, Xinjie Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 16:22:49</h6>
<p class='card-text'>Metaphorical expressions are abundant in Traditional Chinese Medicine (TCM),
conveying complex disease mechanisms and holistic health concepts through
culturally rich and often abstract terminology. Bridging these metaphors to
anatomically driven Western medical (WM) concepts poses significant challenges
for both automated language processing and real-world clinical practice. To
address this gap, we propose a novel multi-agent and chain-of-thought (CoT)
framework designed to interpret TCM metaphors accurately and map them to WM
pathophysiology. Specifically, our approach combines domain-specialized agents
(TCM Expert, WM Expert) with a Coordinator Agent, leveraging stepwise
chain-of-thought prompts to ensure transparent reasoning and conflict
resolution. We detail a methodology for building a metaphor-rich TCM dataset,
discuss strategies for effectively integrating multi-agent collaboration and
CoT reasoning, and articulate the theoretical underpinnings that guide metaphor
interpretation across distinct medical paradigms. We present a comprehensive
system design and highlight both the potential benefits and limitations of our
approach, while leaving placeholders for future experimental validation. Our
work aims to support clinical decision-making, cross-system educational
initiatives, and integrated healthcare research, ultimately offering a robust
scaffold for reconciling TCM's symbolic language with the mechanistic focus of
Western medicine.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02692v1' target='_blank'>FinArena: A Human-Agent Collaboration Framework for Financial Market
  Analysis and Forecasting</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Congluo Xu, Zhaobin Liu, Ziyang Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 15:04:40</h6>
<p class='card-text'>To improve stock trend predictions and support personalized investment
decisions, this paper proposes FinArena, a novel Human-Agent collaboration
framework. Inspired by the mixture of experts (MoE) approach, FinArena combines
multimodal financial data analysis with user interaction. The human module
features an interactive interface that captures individual risk preferences,
allowing personalized investment strategies. The machine module utilizes a
Large Language Model-based (LLM-based) multi-agent system to integrate diverse
data sources, such as stock prices, news articles, and financial statements. To
address hallucinations in LLMs, FinArena employs the adaptive
Retrieval-Augmented Generative (RAG) method for processing unstructured news
data. Finally, a universal expert agent makes investment decisions based on the
features extracted from multimodal data and investors' individual risk
preferences. Extensive experiments show that FinArena surpasses both
traditional and state-of-the-art benchmarks in stock trend prediction and
yields promising results in trading simulations across various risk profiles.
These findings highlight FinArena's potential to enhance investment outcomes by
aligning strategic insights with personalized risk considerations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02682v1' target='_blank'>MPO: Boosting LLM Agents with Meta Plan Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weimin Xiong, Yifan Song, Qingxiu Dong, Bingchan Zhao, Feifan Song, Xun Wang, Sujian Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 14:54:45</h6>
<p class='card-text'>Recent advancements in large language models (LLMs) have enabled LLM-based
agents to successfully tackle interactive planning tasks. However, despite
their successes, existing approaches often suffer from planning hallucinations
and require retraining for each new agent. To address these challenges, we
propose the Meta Plan Optimization (MPO) framework, which enhances agent
planning capabilities by directly incorporating explicit guidance. Unlike
previous methods that rely on complex knowledge, which either require
significant human effort or lack quality assurance, MPO leverages high-level
general guidance through meta plans to assist agent planning and enables
continuous optimization of the meta plans based on feedback from the agent's
task execution. Our experiments conducted on two representative tasks
demonstrate that MPO significantly outperforms existing baselines. Moreover,
our analysis indicates that MPO provides a plug-and-play solution that enhances
both task completion efficiency and generalization capabilities in previous
unseen scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02582v1' target='_blank'>Playing games with Large language models: Randomness and strategy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alicia Vidler, Toby Walsh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 13:04:48</h6>
<p class='card-text'>Playing games has a long history of describing intricate interactions in
simplified forms. In this paper we explore if large language models (LLMs) can
play games, investigating their capabilities for randomisation and strategic
adaptation through both simultaneous and sequential game interactions. We focus
on GPT-4o-Mini-2024-08-17 and test two games between LLMs: Rock Paper Scissors
(RPS) and games of strategy (Prisoners Dilemma PD). LLMs are often described as
stochastic parrots, and while they may indeed be parrots, our results suggest
that they are not very stochastic in the sense that their outputs - when
prompted to be random - are often very biased. Our research reveals that LLMs
appear to develop loss aversion strategies in repeated games, with RPS
converging to stalemate conditions while PD shows systematic shifts between
cooperative and competitive outcomes based on prompt design. We detail
programmatic tools for independent agent interactions and the Agentic AI
challenges faced in implementation. We show that LLMs can indeed play games,
just not very well. These results have implications for the use of LLMs in
multi-agent LLM systems and showcase limitations in current approaches to model
output for strategic decision-making.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02519v1' target='_blank'>Generator-Assistant Stepwise Rollback Framework for Large Language Model
  Agent</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xingzuo Li, Kehai Chen, Yunfei Long, Xuefeng Bai, Yong Xu, Min Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 11:31:05</h6>
<p class='card-text'>Large language model (LLM) agents typically adopt a step-by-step reasoning
framework, in which they interleave the processes of thinking and acting to
accomplish the given task. However, this paradigm faces a deep-rooted one-pass
issue whereby each generated intermediate thought is plugged into the
trajectory regardless of its correctness, which can cause irreversible error
propagation. To address the issue, this paper proposes a novel framework called
Generator-Assistant Stepwise Rollback (GA-Rollback) to induce better
decision-making for LLM agents. Particularly, GA-Rollback utilizes a generator
to interact with the environment and an assistant to examine each action
produced by the generator, where the assistant triggers a rollback operation
upon detection of incorrect actions. Moreover, we introduce two additional
strategies tailored for the rollback scenario to further improve its
effectiveness. Extensive experiments show that GA-Rollback achieves significant
improvements over several strong baselines on three widely used benchmarks. Our
analysis further reveals that GA-Rollback can function as a robust
plug-and-play module, integrating seamlessly with other methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02445v1' target='_blank'>BRIDGE: Bootstrapping Text to Control Time-Series Generation via
  Multi-Agent Iterative Optimization and Diffusion Modelling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hao Li, Yu-Hao Huang, Chang Xu, Viktor Schlegel, Ren-He Jiang, Riza Batista-Navarro, Goran Nenadic, Jiang Bian</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 09:40:00</h6>
<p class='card-text'>Time-series Generation (TSG) is a prominent research area with broad
applications in simulations, data augmentation, and counterfactual analysis.
While existing methods have shown promise in unconditional single-domain TSG,
real-world applications demand for cross-domain approaches capable of
controlled generation tailored to domain-specific constraints and
instance-level requirements. In this paper, we argue that text can provide
semantic insights, domain information and instance-specific temporal patterns,
to guide and improve TSG. We introduce ``Text-Controlled TSG'', a task focused
on generating realistic time series by incorporating textual descriptions. To
address data scarcity in this setting, we propose a novel LLM-based Multi-Agent
framework that synthesizes diverse, realistic text-to-TS datasets. Furthermore,
we introduce BRIDGE, a hybrid text-controlled TSG framework that integrates
semantic prototypes with text description for supporting domain-level guidance.
This approach achieves state-of-the-art generation fidelity on 11 of 12
datasets, and improves controllability by 12.52% on MSE and 6.34% MAE compared
to no text input generation, highlighting its potential for generating tailored
time-series data.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02398v1' target='_blank'>PersonaX: A Recommendation Agent Oriented User Modeling Framework for
  Long Behavior Sequence</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yunxiao Shi, Wujiang Xu, Zeqi Zhang, Xing Zi, Qiang Wu, Min Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 08:41:40</h6>
<p class='card-text'>Recommendation agents leverage large language models for user modeling LLM UM
to construct textual personas guiding alignment with real users. However
existing LLM UM methods struggle with long user generated content UGC due to
context limitations and performance degradation. To address this sampling
strategies prioritize relevance or recency are often applied yet they
inevitably neglect the diverse user interests embedded within the discarded
behaviors resulting in incomplete modeling and degraded profiling quality.
Furthermore relevance based sampling requires real time retrieval forcing the
user modeling process to operate online which introduces significant latency
overhead. In this paper we propose PersonaX an agent agnostic LLM UM framework
that tackles these challenges through sub behavior sequence SBS selection and
offline multi persona construction. PersonaX extracts compact SBS segments
offline to capture diverse user interests generating fine grained textual
personas that are cached for efficient online retrieval. This approach ensures
that the user persona used for prompting remains highly relevant to the current
context while eliminating the need for online user modeling. For SBS selection
we ensure both efficiency length less than five and high representational
quality by balancing prototypicality and diversity within the sampled data.
Extensive experiments validate the effectiveness and versatility of PersonaX in
high quality user profiling. Utilizing only 30 to 50 percent of the behavioral
data with a sequence length of 480 integrating PersonaX with AgentCF yields an
absolute performance improvement of 3 to 11 percent while integration with
Agent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic
framework sets a new benchmark for scalable user modeling paving the way for
more accurate and efficient LLM driven recommendation agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02390v1' target='_blank'>ReSo: A Reward-driven Self-organizing LLM-based Multi-Agent System for
  Reasoning Tasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Heng Zhou, Hejia Geng, Xiangyuan Xue, Zhenfei Yin, Lei Bai</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 08:28:04</h6>
<p class='card-text'>Multi-agent systems have emerged as a promising approach for enhancing the
reasoning capabilities of large language models in complex problem-solving.
However, current MAS frameworks are limited by poor flexibility and
scalability, with underdeveloped optimization strategies. To address these
challenges, we propose ReSo, which integrates task graph generation with a
reward-driven two-stage agent selection process. The core of ReSo is the
proposed Collaborative Reward Model, which can provide fine-grained reward
signals for MAS cooperation for optimization. We also introduce an automated
data synthesis framework for generating MAS benchmarks, without human
annotations. Experimentally, ReSo matches or outperforms existing methods. ReSo
achieves \textbf{33.7\%} and \textbf{32.3\%} accuracy on Math-MAS and
SciBench-MAS SciBench, while other methods completely fail. Code is available
at: \href{https://github.com/hengzzzhou/ReSo}{ReSo}</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02365v1' target='_blank'>EchoQA: A Large Collection of Instruction Tuning Data for Echocardiogram
  Reports</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lama Moukheiber, Mira Moukheiber, Dana Moukheiiber, Hyung-Chul Lee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 07:45:45</h6>
<p class='card-text'>We introduce a novel question-answering (QA) dataset using echocardiogram
reports sourced from the Medical Information Mart for Intensive Care database.
This dataset is specifically designed to enhance QA systems in cardiology,
consisting of 771,244 QA pairs addressing a wide array of cardiac abnormalities
and their severity. We compare large language models (LLMs), including
open-source and biomedical-specific models for zero-shot evaluation, and
closed-source models for zero-shot and three-shot evaluation. Our results show
that fine-tuning LLMs improves performance across various QA metrics,
validating the value of our dataset. Clinicians also qualitatively evaluate the
best-performing model to assess the LLM responses for correctness. Further, we
conduct fine-grained fairness audits to assess the bias-performance trade-off
of LLMs across various social determinants of health. Our objective is to
propel the field forward by establishing a benchmark for LLM AI agents aimed at
supporting clinicians with cardiac differential diagnoses, thereby reducing the
documentation burden that contributes to clinician burnout and enabling
healthcare professionals to focus more on patient care.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02268v1' target='_blank'>AppAgentX: Evolving GUI Agents as Proficient Smartphone Users</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenjia Jiang, Yangyang Zhuang, Chenxi Song, Xu Yang, Chi Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 04:34:09</h6>
<p class='card-text'>Recent advancements in Large Language Models (LLMs) have led to the
development of intelligent LLM-based agents capable of interacting with
graphical user interfaces (GUIs). These agents demonstrate strong reasoning and
adaptability, enabling them to perform complex tasks that traditionally
required predefined rules. However, the reliance on step-by-step reasoning in
LLM-based agents often results in inefficiencies, particularly for routine
tasks. In contrast, traditional rule-based systems excel in efficiency but lack
the intelligence and flexibility to adapt to novel scenarios. To address this
challenge, we propose a novel evolutionary framework for GUI agents that
enhances operational efficiency while retaining intelligence and flexibility.
Our approach incorporates a memory mechanism that records the agent's task
execution history. By analyzing this history, the agent identifies repetitive
action sequences and evolves high-level actions that act as shortcuts,
replacing these low-level operations and improving efficiency. This allows the
agent to focus on tasks requiring more complex reasoning, while simplifying
routine actions. Experimental results on multiple benchmark tasks demonstrate
that our approach significantly outperforms existing methods in both efficiency
and accuracy. The code will be open-sourced to support further research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02238v1' target='_blank'>Haste Makes Waste: Evaluating Planning Abilities of LLMs for Efficient
  and Feasible Multitasking with Time Constraints Between Actions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zirui Wu, Xiao Liu, Jiayi Li, Lingpeng Kong, Yansong Feng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 03:27:02</h6>
<p class='card-text'>While Large Language Model-based agents have demonstrated substantial
progress in task completion, existing evaluation benchmarks tend to
overemphasize single-task performance, with insufficient attention given to the
crucial aspects of multitask planning and execution efficiency required in
real-world scenarios. To bridge this gap, we present Recipe2Plan, a novel
benchmark framework based on real-world cooking scenarios. Unlike conventional
benchmarks, Recipe2Plan challenges agents to optimize cooking time through
parallel task execution while respecting temporal constraints i.e. specific
actions need to be performed within a particular time intervals following the
preceding steps. Overly aggressive local parallelization may disrupt this
constraint, potentially compromising the entire cooking process. This strict
time constraint between actions raises a unique challenge for agents to balance
between maximizing concurrent operations and adhering to critical timing
constraints. Extensive experiments with state-of-the-art models reveal
challenges in maintaining this balance between efficiency and feasibility. The
results highlight the need for improved temporal awareness and global
multitasking capabilities in large language models. We open-source our
benchmark and code at https://github.com/WilliamZR/Recipe2Plan.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02197v1' target='_blank'>ATLaS: Agent Tuning via Learning Critical Steps</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhixun Chen, Ming Li, Yuxuan Huang, Yali Du, Meng Fang, Tianyi Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 02:14:55</h6>
<p class='card-text'>Large Language Model (LLM) agents have demonstrated remarkable generalization
capabilities across multi-domain tasks. Existing agent tuning approaches
typically employ supervised finetuning on entire expert trajectories. However,
behavior-cloning of full trajectories can introduce expert bias and weaken
generalization to states not covered by the expert data. Additionally, critical
steps, such as planning, complex reasoning for intermediate subtasks, and
strategic decision-making, are essential to success in agent tasks, so learning
these steps is the key to improving LLM agents. For more effective and
efficient agent tuning, we propose ATLaS that identifies the critical steps in
expert trajectories and finetunes LLMs solely on these steps with reduced
costs. By steering the training's focus to a few critical steps, our method
mitigates the risk of overfitting entire trajectories and promotes
generalization across different environments and tasks. In extensive
experiments, an LLM finetuned on only 30% critical steps selected by ATLaS
outperforms the LLM finetuned on all steps and recent open-source LLM agents.
ATLaS maintains and improves base LLM skills as generalist agents interacting
with diverse environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02076v1' target='_blank'>CorrA: Leveraging Large Language Models for Dynamic Obstacle Avoidance
  of Autonomous Vehicles</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shanting Wang, Panagiotis Typaldos, Andreas A. Malikopoulos</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 21:57:28</h6>
<p class='card-text'>In this paper, we present Corridor-Agent (CorrA), a framework that integrates
large language models (LLMs) with model predictive control (MPC) to address the
challenges of dynamic obstacle avoidance in autonomous vehicles. Our approach
leverages LLM reasoning ability to generate appropriate parameters for
sigmoid-based boundary functions that define safe corridors around obstacles,
effectively reducing the state-space of the controlled vehicle. The proposed
framework adjusts these boundaries dynamically based on real-time vehicle data
that guarantees collision-free trajectories while also ensuring both
computational efficiency and trajectory optimality. The problem is formulated
as an optimal control problem and solved with differential dynamic programming
(DDP) for constrained optimization, and the proposed approach is embedded
within an MPC framework. Extensive simulation and real-world experiments
demonstrate that the proposed framework achieves superior performance in
maintaining safety and efficiency in complex, dynamic environments compared to
a baseline MPC approach.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02068v1' target='_blank'>Interactive Debugging and Steering of Multi-Agent AI Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Will Epperson, Gagan Bansal, Victor Dibia, Adam Fourney, Jack Gerrits, Erkang Zhu, Saleema Amershi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 21:42:54</h6>
<p class='card-text'>Fully autonomous teams of LLM-powered AI agents are emerging that collaborate
to perform complex tasks for users. What challenges do developers face when
trying to build and debug these AI agent teams? In formative interviews with
five AI agent developers, we identify core challenges: difficulty reviewing
long agent conversations to localize errors, lack of support in current tools
for interactive debugging, and the need for tool support to iterate on agent
configuration. Based on these needs, we developed an interactive multi-agent
debugging tool, AGDebugger, with a UI for browsing and sending messages, the
ability to edit and reset prior agent messages, and an overview visualization
for navigating complex message histories. In a two-part user study with 14
participants, we identify common user strategies for steering agents and
highlight the importance of interactive message resets for debugging. Our
studies deepen understanding of interfaces for debugging increasingly important
agentic workflows.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02067v1' target='_blank'>AI persuading AI vs AI persuading Humans: LLMs' Differential
  Effectiveness in Promoting Pro-Environmental Behavior</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alexander Doudkin, Pat Pataranutaporn, Pattie Maes</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 21:40:55</h6>
<p class='card-text'>Pro-environmental behavior (PEB) is vital to combat climate change, yet
turning awareness into intention and action remains elusive. We explore large
language models (LLMs) as tools to promote PEB, comparing their impact across
3,200 participants: real humans (n=1,200), simulated humans based on actual
participant data (n=1,200), and fully synthetic personas (n=1,200). All three
participant groups faced personalized or standard chatbots, or static
statements, employing four persuasion strategies (moral foundations, future
self-continuity, action orientation, or "freestyle" chosen by the LLM). Results
reveal a "synthetic persuasion paradox": synthetic and simulated agents
significantly affect their post-intervention PEB stance, while human responses
barely shift. Simulated participants better approximate human trends but still
overestimate effects. This disconnect underscores LLM's potential for
pre-evaluating PEB interventions but warns of its limits in predicting
real-world behavior. We call for refined synthetic modeling and sustained and
extended human trials to align conversational AI's promise with tangible
sustainability outcomes.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02038v1' target='_blank'>Persuasion at Play: Understanding Misinformation Dynamics in
  Demographic-Aware Human-LLM Interactions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Angana Borah, Rada Mihalcea, Verónica Pérez-Rosas</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 20:30:22</h6>
<p class='card-text'>Existing challenges in misinformation exposure and susceptibility vary across
demographic groups, as some populations are more vulnerable to misinformation
than others. Large language models (LLMs) introduce new dimensions to these
challenges through their ability to generate persuasive content at scale and
reinforcing existing biases. This study investigates the bidirectional
persuasion dynamics between LLMs and humans when exposed to misinformative
content. We analyze human-to-LLM influence using human-stance datasets and
assess LLM-to-human influence by generating LLM-based persuasive arguments.
Additionally, we use a multi-agent LLM framework to analyze the spread of
misinformation under persuasion among demographic-oriented LLM agents. Our
findings show that demographic factors influence susceptibility to
misinformation in LLMs, closely reflecting the demographic-based patterns seen
in human susceptibility. We also find that, similar to human demographic
groups, multi-agent LLMs exhibit echo chamber behavior. This research explores
the interplay between humans and LLMs, highlighting demographic differences in
the context of misinformation and offering insights for future interventions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02016v1' target='_blank'>Mind the (Belief) Gap: Group Identity in the World of LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Angana Borah, Marwa Houalla, Rada Mihalcea</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 19:50:52</h6>
<p class='card-text'>Social biases and belief-driven behaviors can significantly impact Large
Language Models (LLMs) decisions on several tasks. As LLMs are increasingly
used in multi-agent systems for societal simulations, their ability to model
fundamental group psychological characteristics remains critical yet
under-explored. In this study, we present a multi-agent framework that
simulates belief congruence, a classical group psychology theory that plays a
crucial role in shaping societal interactions and preferences. Our findings
reveal that LLMs exhibit amplified belief congruence compared to humans, across
diverse contexts. We further investigate the implications of this behavior on
two downstream tasks: (1) misinformation dissemination and (2) LLM learning,
finding that belief congruence in LLMs increases misinformation dissemination
and impedes learning. To mitigate these negative impacts, we propose strategies
inspired by: (1) contact hypothesis, (2) accuracy nudges, and (3) global
citizenship framework. Our results show that the best strategies reduce
misinformation dissemination by up to 37% and enhance learning by 11%. Bridging
social psychology and AI, our work provides insights to navigate real-world
interactions using LLMs while addressing belief-driven biases.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01986v1' target='_blank'>Adaptively evaluating models with task elicitation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Davis Brown, Prithvi Balehannina, Helen Jin, Shreya Havaldar, Hamed Hassani, Eric Wong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 19:04:10</h6>
<p class='card-text'>Manual curation of evaluation datasets is struggling to keep up with the
rapidly expanding capabilities and deployment scenarios of language models.
Towards scalable model profiling, we introduce and validate a framework for
evaluating LLMs, called Adaptive Evaluations. Adaptive evaluations use
scaffolded language models (evaluator agents) to search through a target
model's behavior on a domain dataset and create difficult questions (tasks)
that can discover and probe the model's failure modes. We find that frontier
models lack consistency when adaptively probed with our framework on a diverse
suite of datasets and tasks, including but not limited to legal reasoning,
forecasting, and online harassment. Generated questions pass human validity
checks and often transfer to other models with different capability profiles,
demonstrating that adaptive evaluations can also be used to create difficult
domain-specific datasets.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01844v1' target='_blank'>Can (A)I Change Your Mind?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Miriam Havin, Timna Wharton Kleinman, Moran Koren, Yaniv Dover, Ariel Goldstein</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 18:59:54</h6>
<p class='card-text'>The increasing integration of large language model (LLM) based conversational
agents into everyday life raises critical cognitive and social questions about
their potential to influence human opinions. Although previous studies have
shown that LLM-based agents can generate persuasive content, these typically
involve controlled, English-language settings. Addressing this, our
preregistered study explored LLM's persuasive capabilities in more ecological,
unconstrained scenarios, examining both static (written paragraphs) and dynamic
(conversations via Telegram) interaction types. Conducted entirely in Hebrew
with 200 participants, the study assessed the persuasive effects of both LLM
and human interlocutors on controversial civil policy topics. Results indicated
that participants adopted LLM and human perspectives similarly, with
significant opinion changes evident across all conditions, regardless of
interlocutor type or interaction mode. Confidence levels increased
significantly in most scenarios, except in static LLM interactions. These
findings demonstrate LLM-based agents' robust persuasive capabilities across
diverse sources and settings, highlighting their potential impact on shaping
public opinions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01829v1' target='_blank'>Persuade Me if You Can: A Framework for Evaluating Persuasion
  Effectiveness and Susceptibility Among Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nimet Beyza Bozdag, Shuhaib Mehri, Gokhan Tur, Dilek Hakkani-Tür</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 18:53:21</h6>
<p class='card-text'>Large Language Models (LLMs) demonstrate persuasive capabilities that rival
human-level persuasion. While these capabilities can be used for social good,
they also present risks of potential misuse. Moreover, LLMs' susceptibility to
persuasion raises concerns about alignment with ethical principles. To study
these dynamics, we introduce Persuade Me If You Can (PMIYC), an automated
framework for evaluating persuasion through multi-agent interactions. Here,
Persuader agents engage in multi-turn conversations with the Persuadee agents,
allowing us to measure LLMs' persuasive effectiveness and their susceptibility
to persuasion. We conduct comprehensive evaluations across diverse LLMs,
ensuring each model is assessed against others in both subjective and
misinformation contexts. We validate the efficacy of our framework through
human evaluations and show alignment with prior work. PMIYC offers a scalable
alternative to human annotation for studying persuasion in LLMs. Through PMIYC,
we find that Llama-3.3-70B and GPT-4o exhibit similar persuasive effectiveness,
outperforming Claude 3 Haiku by 30%. However, GPT-4o demonstrates over 50%
greater resistance to persuasion for misinformation compared to Llama-3.3-70B.
These findings provide empirical insights into the persuasive dynamics of LLMs
and contribute to the development of safer AI systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01811v1' target='_blank'>AutoAdvExBench: Benchmarking autonomous exploitation of adversarial
  example defenses</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nicholas Carlini, Javier Rando, Edoardo Debenedetti, Milad Nasr, Florian Tramèr</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 18:39:48</h6>
<p class='card-text'>We introduce AutoAdvExBench, a benchmark to evaluate if large language models
(LLMs) can autonomously exploit defenses to adversarial examples. Unlike
existing security benchmarks that often serve as proxies for real-world tasks,
bench directly measures LLMs' success on tasks regularly performed by machine
learning security experts. This approach offers a significant advantage: if a
LLM could solve the challenges presented in bench, it would immediately present
practical utility for adversarial machine learning researchers. We then design
a strong agent that is capable of breaking 75% of CTF-like ("homework
exercise") adversarial example defenses. However, we show that this agent is
only able to succeed on 13% of the real-world defenses in our benchmark,
indicating the large gap between difficulty in attacking "real" code, and
CTF-like code. In contrast, a stronger LLM that can attack 21% of real defenses
only succeeds on 54% of CTF-like defenses. We make this benchmark available at
https://github.com/ethz-spylab/AutoAdvExBench.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01767v1' target='_blank'>Designing VR Simulation System for Clinical Communication Training with
  LLMs-Based Embodied Conversational Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiuqi Tommy Zhu, Heidi Cheerman, Minxin Cheng, Sheri Kiami, Leanne Chukoskie, Eileen McGivney</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 17:45:06</h6>
<p class='card-text'>VR simulation in Health Professions (HP) education demonstrates huge
potential, but fixed learning content with little customization limits its
application beyond lab environments. To address these limitations in the
context of VR for patient communication training, we conducted a user-centered
study involving semi-structured interviews with advanced HP students to
understand their challenges in clinical communication training and perceptions
of VR-based solutions. From this, we derived design insights emphasizing the
importance of realistic scenarios, simple interactions, and unpredictable
dialogues. Building on these insights, we developed the Virtual AI Patient
Simulator (VAPS), a novel VR system powered by Large Language Models (LLMs) and
Embodied Conversational Agents (ECAs), supporting dynamic and customizable
patient interactions for immersive learning. We also provided an example of how
clinical professors could use user-friendly design forms to create personalized
scenarios that align with course objectives in VAPS and discuss future
implications of integrating AI-driven technologies into VR education.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01763v1' target='_blank'>Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for
  Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhengliang Shi, Yuhan Wang, Lingyong Yan, Pengjie Ren, Shuaiqiang Wang, Dawei Yin, Zhaochun Ren</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 17:37:16</h6>
<p class='card-text'>Tool learning aims to augment large language models (LLMs) with diverse
tools, enabling them to act as agents for solving practical tasks. Due to the
limited context length of tool-using LLMs, adopting information retrieval (IR)
models to select useful tools from large toolsets is a critical initial step.
However, the performance of IR models in tool retrieval tasks remains
underexplored and unclear. Most tool-use benchmarks simplify this step by
manually pre-annotating a small set of relevant tools for each task, which is
far from the real-world scenarios. In this paper, we propose ToolRet, a
heterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks,
and a corpus of 43k tools, collected from existing datasets. We benchmark six
types of models on ToolRet. Surprisingly, even the models with strong
performance in conventional IR benchmarks, exhibit poor performance on ToolRet.
This low retrieval quality degrades the task pass rate of tool-use LLMs. As a
further step, we contribute a large-scale training dataset with over 200k
instances, which substantially optimizes the tool retrieval ability of IR
models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01694v1' target='_blank'>Student engagement in collaborative learning with AI agents in an
  LLM-empowered learning environment: A cluster analysis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhanxin Hao, Jianxiao Jiang, Jifan Yu, Zhiyuan Liu, Yu Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 16:08:28</h6>
<p class='card-text'>Integrating LLM models into educational practice fosters personalized
learning by accommodating the diverse behavioral patterns of different learner
types. This study aims to explore these learner types within a novel
interactive setting, providing a detailed analysis of their distinctive
characteristics and interaction dynamics. The research involved 110 students
from a university in China, who engaged with multiple LLM agents in an
LLM-empowered learning environment, completing coursework across six modules.
Data on the students' non-cognitive traits, course engagement, and AI
interaction patterns were collected and analyzed. Using hierarchical cluster
analysis, the students were classified into three distinct groups: active
questioners, responsive navigators, and silent listeners. Epistemic network
analysis was then applied to further delineate the interaction profiles and
cognitive engagement of different types of learners. The findings underscore
how different learner types engage with human-AI interactive learning and offer
practical implications for the design of adaptive educational systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01513v1' target='_blank'>Evaluation and Facilitation of Online Discussions in the LLM Era: A
  Survey</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Katerina Korre, Dimitris Tsirmpas, Nikos Gkoumas, Emma Cabalé, Dionysis Kontarinis, Danai Myrtzani, Theodoros Evgeniou, Ion Androutsopoulos, John Pavlopoulos</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 13:26:01</h6>
<p class='card-text'>We present a survey of methods for assessing and enhancing the quality of
online discussions, focusing on the potential of Large Language Models (LLMs).
While online discourses aim, at least in theory, to foster mutual
understanding, they often devolve into harmful exchanges, such as hate speech,
threatening social cohesion and democratic values. Recent advancements in LLMs
enable facilitation agents that not only moderate content, but also actively
improve the quality of interactions. Our survey synthesizes ideas from Natural
Language Processing (NLP) and Social Sciences to provide (a) a new taxonomy on
discussion quality evaluation, (b) an overview of intervention and facilitation
strategies, along with a new taxonomy on conversation facilitation datasets,
(c) an LLM-oriented roadmap of good practices and future research directions,
from technological and societal perspectives.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01490v1' target='_blank'>Improving Retrospective Language Agents via Joint Policy Gradient
  Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xueyang Feng, Bo Lan, Quanyu Dai, Lei Wang, Jiakai Tang, Xu Chen, Zhenhua Dong, Ji-Rong Wen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 12:54:54</h6>
<p class='card-text'>In recent research advancements within the community, large language models
(LLMs) have sparked great interest in creating autonomous agents. However,
current prompt-based agents often heavily rely on large-scale LLMs. Meanwhile,
although fine-tuning methods significantly enhance the capabilities of smaller
LLMs, the fine-tuned agents often lack the potential for self-reflection and
self-improvement. To address these challenges, we introduce a novel agent
framework named RetroAct, which is a framework that jointly optimizes both
task-planning and self-reflective evolution capabilities in language agents.
Specifically, we develop a two-stage joint optimization process that integrates
imitation learning and reinforcement learning, and design an off-policy joint
policy gradient optimization algorithm with imitation learning regularization
to enhance the data efficiency and training stability in agent tasks. RetroAct
significantly improves the performance of open-source models, reduces
dependency on closed-source LLMs, and enables fine-tuned agents to learn and
evolve continuously. We conduct extensive experiments across various testing
environments, demonstrating RetroAct has substantial improvements in task
performance and decision-making processes.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01935v1' target='_blank'>MultiAgentBench: Evaluating the Collaboration and Competition of LLM
  agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kunlun Zhu, Hongyi Du, Zhaochen Hong, Xiaocheng Yang, Shuyi Guo, Zhe Wang, Zhenhailong Wang, Cheng Qian, Xiangru Tang, Heng Ji, Jiaxuan You</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 05:18:50</h6>
<p class='card-text'>Large Language Models (LLMs) have shown remarkable capabilities as autonomous
agents, yet existing benchmarks either focus on single-agent tasks or are
confined to narrow domains, failing to capture the dynamics of multi-agent
coordination and competition. In this paper, we introduce MultiAgentBench, a
comprehensive benchmark designed to evaluate LLM-based multi-agent systems
across diverse, interactive scenarios. Our framework measures not only task
completion but also the quality of collaboration and competition using novel,
milestone-based key performance indicators. Moreover, we evaluate various
coordination protocols (including star, chain, tree, and graph topologies) and
innovative strategies such as group discussion and cognitive planning. Notably,
gpt-4o-mini reaches the average highest task score, graph structure performs
the best among coordination protocols in the research scenario, and cognitive
planning improves milestone achievement rates by 3%. Code and datasets are
public available at https://github.com/MultiagentBench/MARBLE.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.00870v1' target='_blank'>NeSyC: A Neuro-symbolic Continual Learner For Complex Embodied Tasks In
  Open Domains</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wonje Choi, Jinwoo Park, Sanghyun Ahn, Daehee Lee, Honguk Woo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-02 12:16:20</h6>
<p class='card-text'>We explore neuro-symbolic approaches to generalize actionable knowledge,
enabling embodied agents to tackle complex tasks more effectively in
open-domain environments. A key challenge for embodied agents is the
generalization of knowledge across diverse environments and situations, as
limited experiences often confine them to their prior knowledge. To address
this issue, we introduce a novel framework, NeSyC, a neuro-symbolic continual
learner that emulates the hypothetico-deductive model by continually
formulating and validating knowledge from limited experiences through the
combined use of Large Language Models (LLMs) and symbolic tools. Specifically,
we devise a contrastive generality improvement scheme within NeSyC, which
iteratively generates hypotheses using LLMs and conducts contrastive validation
via symbolic tools. This scheme reinforces the justification for admissible
actions while minimizing the inference of inadmissible ones. Additionally, we
incorporate a memory-based monitoring scheme that efficiently detects action
errors and triggers the knowledge refinement process across domains.
Experiments conducted on diverse embodied task benchmarks-including ALFWorld,
VirtualHome, Minecraft, RLBench, and a real-world robotic scenario-demonstrate
that NeSyC is highly effective in solving complex embodied tasks across a range
of open-domain environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.00841v1' target='_blank'>A Law Reasoning Benchmark for LLM with Tree-Organized Structures
  including Factum Probandum, Evidence and Experiences</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiaxin Shen, Jinan Xu, Huiqi Hu, Luyi Lin, Fei Zheng, Guoyang Ma, Fandong Meng, Jie Zhou, Wenjuan Han</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-02 10:26:54</h6>
<p class='card-text'>While progress has been made in legal applications, law reasoning, crucial
for fair adjudication, remains unexplored. We propose a transparent law
reasoning schema enriched with hierarchical factum probandum, evidence, and
implicit experience, enabling public scrutiny and preventing bias. Inspired by
this schema, we introduce the challenging task, which takes a textual case
description and outputs a hierarchical structure justifying the final decision.
We also create the first crowd-sourced dataset for this task, enabling
comprehensive evaluation. Simultaneously, we propose an agent framework that
employs a comprehensive suite of legal analysis tools to address the challenge
task. This benchmark paves the way for transparent and accountable AI-assisted
law reasoning in the ``Intelligent Court''.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.00821v1' target='_blank'>AI Agents for Ground-Based Gamma Astronomy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:D. Kostunin, V. Sotnikov, S. Golovachev, A. Strube</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-02 09:55:54</h6>
<p class='card-text'>Next-generation instruments for ground-based gamma-ray astronomy are marked
by a substantial increase in complexity, featuring dozens of telescopes. This
leap in scale introduces significant challenges in managing system operations
and offline data analysis. Methods, which depend on advanced personnel training
and sophisticated software, become increasingly strained as system complexity
grows, making it more challenging to effectively support users in such a
multifaceted environment. To address these challenges, we propose the
development of AI agents based on instruction-finetuned large language models
(LLMs). These agents align with specific documentation and codebases,
understand the environmental context, operate with external APIs, and
communicate with humans in natural language. Leveraging the advanced
capabilities of modern LLMs, which can process and retain vast amounts of
information, these AI agents offer a transformative approach to system
management and data analysis by automating complex tasks and providing
intelligent assistance. We present two prototypes that integrate with the
Cherenkov Telescope Array Observatory pipelines for operations and offline data
analysis. The first prototype automates data model implementation and
maintenance for the Configuration Database of the Array Control and Data
Acquisition (ACADA). The second prototype is an open-access code generation
application tailored for data analysis based on the Gammapy framework.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>