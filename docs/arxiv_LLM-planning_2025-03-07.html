<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>LLM-planning - 2025-03-07</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>LLM-planning - 2025-03-07</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04723v1' target='_blank'>Shifting Long-Context LLMs Research from Input to Output</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuhao Wu, Yushi Bai, Zhiqing Hu, Shangqing Tu, Ming Shan Hee, Juanzi Li, Roy Ka-Wei Lee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 18:59:37</h6>
<p class='card-text'>Recent advancements in long-context Large Language Models (LLMs) have
primarily concentrated on processing extended input contexts, resulting in
significant strides in long-context comprehension. However, the equally
critical aspect of generating long-form outputs has received comparatively less
attention. This paper advocates for a paradigm shift in NLP research toward
addressing the challenges of long-output generation. Tasks such as novel
writing, long-term planning, and complex reasoning require models to understand
extensive contexts and produce coherent, contextually rich, and logically
consistent extended text. These demands highlight a critical gap in current LLM
capabilities. We underscore the importance of this under-explored domain and
call for focused efforts to develop foundational LLMs tailored for generating
high-quality, long-form outputs, which hold immense potential for real-world
applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04691v1' target='_blank'>Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pengcheng Qiu, Chaoyi Wu, Shuyu Liu, Weike Zhao, Ya Zhang, Yanfeng Wang, Weidi Xie</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 18:35:39</h6>
<p class='card-text'>The latest reasoning-enhanced large language models (reasoning LLMs), such as
DeepSeek-R1 and OpenAI-o3, have demonstrated remarkable success. However, the
application of such reasoning enhancements to the highly professional medical
domain has not been clearly evaluated, particularly regarding with not only
assessing the final generation but also examining the quality of their
reasoning processes. In this study, we present MedR-Bench, a reasoning-focused
medical evaluation benchmark comprising 1,453 structured patient cases with
reasoning references mined from case reports. Our benchmark spans 13 body
systems and 10 specialty disorders, encompassing both common and rare diseases.
In our evaluation, we introduce a versatile framework consisting of three
critical clinical stages: assessment recommendation, diagnostic
decision-making, and treatment planning, comprehensively capturing the LLMs'
performance across the entire patient journey in healthcare. For metrics, we
propose a novel agentic system, Reasoning Evaluator, designed to automate and
objectively quantify free-text reasoning responses in a scalable manner from
the perspectives of efficiency, factuality, and completeness by dynamically
searching and performing cross-referencing checks. As a result, we assess five
state-of-the-art reasoning LLMs, including DeepSeek-R1, OpenAI-o3-mini, and
others. Our results reveal that current LLMs can handle relatively simple
diagnostic tasks with sufficient critical assessment results, achieving
accuracy generally over 85%. However, they still struggle with more complex
tasks, such as assessment recommendation and treatment planning. In reasoning,
their reasoning processes are generally reliable, with factuality scores
exceeding 90%, though they often omit critical reasoning steps. Our study
clearly reveals further development directions for current clinical LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04675v1' target='_blank'>LLM-guided Plan and Retrieval: A Strategic Alignment for Interpretable
  User Satisfaction Estimation in Dialogue</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sangyeop Kim, Sohhyung Park, Jaewon Jung, Jinseok Kim, Sungzoon Cho</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 18:12:33</h6>
<p class='card-text'>Understanding user satisfaction with conversational systems, known as User
Satisfaction Estimation (USE), is essential for assessing dialogue quality and
enhancing user experiences. However, existing methods for USE face challenges
due to limited understanding of underlying reasons for user dissatisfaction and
the high costs of annotating user intentions. To address these challenges, we
propose PRAISE (Plan and Retrieval Alignment for Interpretable Satisfaction
Estimation), an interpretable framework for effective user satisfaction
prediction. PRAISE operates through three key modules. The Strategy Planner
develops strategies, which are natural language criteria for classifying user
satisfaction. The Feature Retriever then incorporates knowledge on user
satisfaction from Large Language Models (LLMs) and retrieves relevance features
from utterances. Finally, the Score Analyzer evaluates strategy predictions and
classifies user satisfaction. Experimental results demonstrate that PRAISE
achieves state-of-the-art performance on three benchmarks for the USE task.
Beyond its superior performance, PRAISE offers additional benefits. It enhances
interpretability by providing instance-level explanations through effective
alignment of utterances with strategies. Moreover, PRAISE operates more
efficiently than existing approaches by eliminating the need for LLMs during
the inference phase.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04280v1' target='_blank'>Towards Autonomous Reinforcement Learning for Real-World Robotic
  Manipulation with Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Niccol√≤ Turcato, Matteo Iovino, Aris Synodinos, Alberto Dalla Libera, Ruggero Carli, Pietro Falco</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 10:08:44</h6>
<p class='card-text'>Recent advancements in Large Language Models (LLMs) and Visual Language
Models (VLMs) have significantly impacted robotics, enabling high-level
semantic motion planning applications. Reinforcement Learning (RL), a
complementary paradigm, enables agents to autonomously optimize complex
behaviors through interaction and reward signals. However, designing effective
reward functions for RL remains challenging, especially in real-world tasks
where sparse rewards are insufficient and dense rewards require elaborate
design. In this work, we propose Autonomous Reinforcement learning for Complex
HumanInformed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4,
a pre-trained LLM, to generate reward functions directly from natural language
task descriptions. The rewards are used to train RL agents in simulated
environments, where we formalize the reward generation process to enhance
feasibility. Additionally, GPT-4 automates the coding of task success criteria,
creating a fully automated, one-shot procedure for translating human-readable
text into deployable robot skills. Our approach is validated through extensive
simulated experiments on single-arm and bi-manual manipulation tasks using an
ABB YuMi collaborative robot, highlighting its practicality and effectiveness.
Tasks are demonstrated on the real robot setup.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04250v1' target='_blank'>An Egocentric Vision-Language Model based Portable Real-time Smart
  Assistant</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yifei Huang, Jilan Xu, Baoqi Pei, Yuping He, Guo Chen, Mingfang Zhang, Lijin Yang, Zheng Nie, Jinyao Liu, Guoshun Fan, Dechen Lin, Fang Fang, Kunpeng Li, Chang Yuan, Xinyuan Chen, Yaohui Wang, Yali Wang, Yu Qiao, Limin Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 09:33:46</h6>
<p class='card-text'>We present Vinci, a vision-language system designed to provide real-time,
comprehensive AI assistance on portable devices. At its core, Vinci leverages
EgoVideo-VL, a novel model that integrates an egocentric vision foundation
model with a large language model (LLM), enabling advanced functionalities such
as scene understanding, temporal grounding, video summarization, and future
planning. To enhance its utility, Vinci incorporates a memory module for
processing long video streams in real time while retaining contextual history,
a generation module for producing visual action demonstrations, and a retrieval
module that bridges egocentric and third-person perspectives to provide
relevant how-to videos for skill acquisition. Unlike existing systems that
often depend on specialized hardware, Vinci is hardware-agnostic, supporting
deployment across a wide range of devices, including smartphones and wearable
cameras. In our experiments, we first demonstrate the superior performance of
EgoVideo-VL on multiple public benchmarks, showcasing its vision-language
reasoning and contextual understanding capabilities. We then conduct a series
of user studies to evaluate the real-world effectiveness of Vinci, highlighting
its adaptability and usability in diverse scenarios. We hope Vinci can
establish a new framework for portable, real-time egocentric AI systems,
empowering users with contextual and actionable insights. Including the
frontend, backend, and models, all codes of Vinci are available at
https://github.com/OpenGVLab/vinci.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03911v1' target='_blank'>Safe LLM-Controlled Robots with Formal Guarantees via Reachability
  Analysis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ahmad Hafez, Alireza Naderi Akhormeh, Amr Hegazy, Amr Alanwar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 21:23:15</h6>
<p class='card-text'>The deployment of Large Language Models (LLMs) in robotic systems presents
unique safety challenges, particularly in unpredictable environments. Although
LLMs, leveraging zero-shot learning, enhance human-robot interaction and
decision-making capabilities, their inherent probabilistic nature and lack of
formal guarantees raise significant concerns for safety-critical applications.
Traditional model-based verification approaches often rely on precise system
models, which are difficult to obtain for real-world robotic systems and may
not be fully trusted due to modeling inaccuracies, unmodeled dynamics, or
environmental uncertainties. To address these challenges, this paper introduces
a safety assurance framework for LLM-controlled robots based on data-driven
reachability analysis, a formal verification technique that ensures all
possible system trajectories remain within safe operational limits. Our
framework specifically investigates the problem of instructing an LLM to
navigate the robot to a specified goal and assesses its ability to generate
low-level control actions that successfully guide the robot safely toward that
goal. By leveraging historical data to construct reachable sets of states for
the robot-LLM system, our approach provides rigorous safety guarantees against
unsafe behaviors without relying on explicit analytical models. We validate the
framework through experimental case studies in autonomous navigation and task
planning, demonstrating its effectiveness in mitigating risks associated with
LLM-generated commands. This work advances the integration of formal methods
into LLM-based robotics, offering a principled and practical approach to
ensuring safety in next-generation autonomous systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03556v1' target='_blank'>Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented
  Manipulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaomeng Zhu, Yuyang Li, Leiyao Cui, Pengfei Li, Huan-ang Gao, Yixin Zhu, Hao Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 14:44:53</h6>
<p class='card-text'>Object affordance reasoning, the ability to infer object functionalities
based on physical properties, is fundamental for task-oriented planning and
activities in both humans and Artificial Intelligence (AI). This capability,
required for planning and executing daily activities in a task-oriented manner,
relies on commonsense knowledge of object physics and functionalities,
extending beyond simple object recognition. Current computational models for
affordance reasoning from perception lack generalizability, limiting their
applicability in novel scenarios. Meanwhile, comprehensive Large Language
Models (LLMs) with emerging reasoning capabilities are challenging to deploy on
local devices for task-oriented manipulations. Here, we introduce LVIS-Aff, a
large-scale dataset comprising 1,496 tasks and 119k images, designed to enhance
the generalizability of affordance reasoning from perception. Utilizing this
dataset, we develop Afford-X, an end-to-end trainable affordance reasoning
model that incorporates Verb Attention and Bi-Fusion modules to improve
multi-modal understanding. This model achieves up to a 12.1% performance
improvement over the best-reported results from non-LLM methods, while also
demonstrating a 1.2% enhancement compared to our previous conference paper.
Additionally, it maintains a compact 187M parameter size and infers nearly 50
times faster than the GPT-4V API. Our work demonstrates the potential for
efficient, generalizable affordance reasoning models that can be deployed on
local devices for task-oriented manipulations. We showcase Afford-X's
effectiveness in enabling task-oriented manipulations for robots across various
tasks and environments, underscoring its efficiency and broad implications for
advancing robotics and AI systems in real-world applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03505v1' target='_blank'>Parallelized Planning-Acting for Efficient LLM-based Multi-Agent Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yaoru Li, Shunyu Liu, Tongya Zheng, Mingli Song</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 13:53:10</h6>
<p class='card-text'>Recent advancements in Large Language Model(LLM)-based Multi-Agent
Systems(MAS) have demonstrated remarkable potential for tackling complex
decision-making tasks. However, existing frameworks inevitably rely on
serialized execution paradigms, where agents must complete sequential LLM
planning before taking action. This fundamental constraint severely limits
real-time responsiveness and adaptation, which is crucial in dynamic
environments with ever-changing scenarios. In this paper, we propose a novel
parallelized planning-acting framework for LLM-based MAS, featuring a
dual-thread architecture with interruptible execution to enable concurrent
planning and acting. Specifically, our framework comprises two core threads:(1)
a planning thread driven by a centralized memory system, maintaining
synchronization of environmental states and agent communication to support
dynamic decision-making; and (2) an acting thread equipped with a comprehensive
skill library, enabling automated task execution through recursive
decomposition. Extensive experiments on challenging Minecraft demonstrate the
effectiveness of the proposed framework.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03459v2' target='_blank'>Unified Mind Model: Reimagining Autonomous Agents in the LLM Era</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pengbo Hu, Xiang Ying</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 12:49:44</h6>
<p class='card-text'>Large language models (LLMs) have recently demonstrated remarkable
capabilities across domains, tasks, and languages (e.g., ChatGPT and GPT-4),
reviving the research of general autonomous agents with human-like cognitive
abilities. Such human-level agents require semantic comprehension and
instruction-following capabilities, which exactly fall into the strengths of
LLMs. Although there have been several initial attempts to build human-level
agents based on LLMs, the theoretical foundation remains a challenging open
problem. In this paper, we propose a novel theoretical cognitive architecture,
the Unified Mind Model (UMM), which offers guidance to facilitate the rapid
creation of autonomous agents with human-level cognitive abilities.
Specifically, our UMM starts with the global workspace theory and further
leverage LLMs to enable the agent with various cognitive abilities, such as
multi-modal perception, planning, reasoning, tool use, learning, memory,
reflection and motivation. Building upon UMM, we then develop an agent-building
engine, MindOS, which allows users to quickly create domain-/task-specific
autonomous agents without any programming effort.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03074v1' target='_blank'>BEVDriver: Leveraging BEV Maps in LLMs for Robust Closed-Loop Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Katharina Winter, Mark Azer, Fabian B. Flohr</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 00:27:32</h6>
<p class='card-text'>Autonomous driving has the potential to set the stage for more efficient
future mobility, requiring the research domain to establish trust through safe,
reliable and transparent driving. Large Language Models (LLMs) possess
reasoning capabilities and natural language understanding, presenting the
potential to serve as generalized decision-makers for ego-motion planning that
can interact with humans and navigate environments designed for human drivers.
While this research avenue is promising, current autonomous driving approaches
are challenged by combining 3D spatial grounding and the reasoning and language
capabilities of LLMs. We introduce BEVDriver, an LLM-based model for end-to-end
closed-loop driving in CARLA that utilizes latent BEV features as perception
input. BEVDriver includes a BEV encoder to efficiently process multi-view
images and 3D LiDAR point clouds. Within a common latent space, the BEV
features are propagated through a Q-Former to align with natural language
instructions and passed to the LLM that predicts and plans precise future
trajectories while considering navigation instructions and critical scenarios.
On the LangAuto benchmark, our model reaches up to 18.9% higher performance on
the Driving Score compared to SoTA methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02756v1' target='_blank'>BatchGEMBA: Token-Efficient Machine Translation Evaluation with Batched
  Prompting and Prompt Compression</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Daniil Larionov, Steffen Eger</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 16:20:52</h6>
<p class='card-text'>Recent advancements in Large Language Model (LLM)-based Natural Language
Generation evaluation have largely focused on single-example prompting,
resulting in significant token overhead and computational inefficiencies. In
this work, we introduce BatchGEMBA-MQM, a framework that integrates batched
prompting with the GEMBA-MQM metric for machine translation evaluation. Our
approach aggregates multiple translation examples into a single prompt,
reducing token usage by 2-4 times (depending on the batch size) relative to
single-example prompting. Furthermore, we propose a batching-aware prompt
compression model that achieves an additional token reduction of 13-15% on
average while also showing ability to help mitigate batching-induced quality
degradation. Evaluations across several LLMs (GPT-4o, GPT-4o-mini, Mistral
Small, Phi4, and CommandR7B) and varying batch sizes reveal that while batching
generally negatively affects quality (but sometimes not substantially), prompt
compression does not degrade further, and in some cases, recovers quality loss.
For instance, GPT-4o retains over 90% of its baseline performance at a batch
size of 4 when compression is applied, compared to a 44.6% drop without
compression. We plan to release our code and trained models at
https://github.com/NL2G/batchgemba to support future research in this domain.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02698v1' target='_blank'>FlowPlan: Zero-Shot Task Planning with LLM Flow Engineering for Robotic
  Instruction Following</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zijun Lin, Chao Tang, Hanjing Ye, Hong Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 15:14:41</h6>
<p class='card-text'>Robotic instruction following tasks require seamless integration of visual
perception, task planning, target localization, and motion execution. However,
existing task planning methods for instruction following are either data-driven
or underperform in zero-shot scenarios due to difficulties in grounding lengthy
instructions into actionable plans under operational constraints. To address
this, we propose FlowPlan, a structured multi-stage LLM workflow that elevates
zero-shot pipeline and bridges the performance gap between zero-shot and
data-driven in-context learning methods. By decomposing the planning process
into modular stages--task information retrieval, language-level reasoning,
symbolic-level planning, and logical evaluation--FlowPlan generates logically
coherent action sequences while adhering to operational constraints and further
extracts contextual guidance for precise instance-level target localization.
Benchmarked on the ALFRED and validated in real-world applications, our method
achieves competitive performance relative to data-driven in-context learning
methods and demonstrates adaptability across diverse environments. This work
advances zero-shot task planning in robotic systems without reliance on labeled
data. Project website: https://instruction-following-project.github.io/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02682v1' target='_blank'>MPO: Boosting LLM Agents with Meta Plan Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weimin Xiong, Yifan Song, Qingxiu Dong, Bingchan Zhao, Feifan Song, Xun Wang, Sujian Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 14:54:45</h6>
<p class='card-text'>Recent advancements in large language models (LLMs) have enabled LLM-based
agents to successfully tackle interactive planning tasks. However, despite
their successes, existing approaches often suffer from planning hallucinations
and require retraining for each new agent. To address these challenges, we
propose the Meta Plan Optimization (MPO) framework, which enhances agent
planning capabilities by directly incorporating explicit guidance. Unlike
previous methods that rely on complex knowledge, which either require
significant human effort or lack quality assurance, MPO leverages high-level
general guidance through meta plans to assist agent planning and enables
continuous optimization of the meta plans based on feedback from the agent's
task execution. Our experiments conducted on two representative tasks
demonstrate that MPO significantly outperforms existing baselines. Moreover,
our analysis indicates that MPO provides a plug-and-play solution that enhances
both task completion efficiency and generalization capabilities in previous
unseen scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02465v1' target='_blank'>UAV-VLRR: Vision-Language Informed NMPC for Rapid Response in UAV Search
  and Rescue</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yasheerah Yaqoot, Muhammad Ahsan Mustafa, Oleg Sautenkov, Dzmitry Tsetserukou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 10:21:58</h6>
<p class='card-text'>Emergency search and rescue (SAR) operations often require rapid and precise
target identification in complex environments where traditional manual drone
control is inefficient. In order to address these scenarios, a rapid SAR
system, UAV-VLRR (Vision-Language-Rapid-Response), is developed in this
research. This system consists of two aspects: 1) A multimodal system which
harnesses the power of Visual Language Model (VLM) and the natural language
processing capabilities of ChatGPT-4o (LLM) for scene interpretation. 2) A
non-linearmodel predictive control (NMPC) with built-in obstacle avoidance for
rapid response by a drone to fly according to the output of the multimodal
system. This work aims at improving response times in emergency SAR operations
by providing a more intuitive and natural approach to the operator to plan the
SAR mission while allowing the drone to carry out that mission in a rapid and
safe manner. When tested, our approach was faster on an average by 33.75% when
compared with an off-the-shelf autopilot and 54.6% when compared with a human
pilot. Video of UAV-VLRR: https://youtu.be/KJqQGKKt1xY</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02238v1' target='_blank'>Haste Makes Waste: Evaluating Planning Abilities of LLMs for Efficient
  and Feasible Multitasking with Time Constraints Between Actions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zirui Wu, Xiao Liu, Jiayi Li, Lingpeng Kong, Yansong Feng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 03:27:02</h6>
<p class='card-text'>While Large Language Model-based agents have demonstrated substantial
progress in task completion, existing evaluation benchmarks tend to
overemphasize single-task performance, with insufficient attention given to the
crucial aspects of multitask planning and execution efficiency required in
real-world scenarios. To bridge this gap, we present Recipe2Plan, a novel
benchmark framework based on real-world cooking scenarios. Unlike conventional
benchmarks, Recipe2Plan challenges agents to optimize cooking time through
parallel task execution while respecting temporal constraints i.e. specific
actions need to be performed within a particular time intervals following the
preceding steps. Overly aggressive local parallelization may disrupt this
constraint, potentially compromising the entire cooking process. This strict
time constraint between actions raises a unique challenge for agents to balance
between maximizing concurrent operations and adhering to critical timing
constraints. Extensive experiments with state-of-the-art models reveal
challenges in maintaining this balance between efficiency and feasibility. The
results highlight the need for improved temporal awareness and global
multitasking capabilities in large language models. We open-source our
benchmark and code at https://github.com/WilliamZR/Recipe2Plan.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02197v1' target='_blank'>ATLaS: Agent Tuning via Learning Critical Steps</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhixun Chen, Ming Li, Yuxuan Huang, Yali Du, Meng Fang, Tianyi Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 02:14:55</h6>
<p class='card-text'>Large Language Model (LLM) agents have demonstrated remarkable generalization
capabilities across multi-domain tasks. Existing agent tuning approaches
typically employ supervised finetuning on entire expert trajectories. However,
behavior-cloning of full trajectories can introduce expert bias and weaken
generalization to states not covered by the expert data. Additionally, critical
steps, such as planning, complex reasoning for intermediate subtasks, and
strategic decision-making, are essential to success in agent tasks, so learning
these steps is the key to improving LLM agents. For more effective and
efficient agent tuning, we propose ATLaS that identifies the critical steps in
expert trajectories and finetunes LLMs solely on these steps with reduced
costs. By steering the training's focus to a few critical steps, our method
mitigates the risk of overfitting entire trajectories and promotes
generalization across different environments and tasks. In extensive
experiments, an LLM finetuned on only 30% critical steps selected by ATLaS
outperforms the LLM finetuned on all steps and recent open-source LLM agents.
ATLaS maintains and improves base LLM skills as generalist agents interacting
with diverse environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01804v2' target='_blank'>$\texttt{SEM-CTRL}$: Semantically Controlled Decoding</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mohammad Albinhassan, Pranava Madhyastha, Alessandra Russo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 18:33:46</h6>
<p class='card-text'>Ensuring both syntactic and semantic correctness in Large Language Model
(LLM) outputs remains a significant challenge, despite being critical for
real-world deployment. In this paper, we introduce $\texttt{SEM-CTRL}$, a
unified approach that enforces rich context-sensitive constraints and task- and
instance-specific semantics directly on an LLM decoder. Our approach integrates
token-level MCTS, which is guided by specific syntactic and semantic
constraints. The constraints over the desired outputs are expressed using
Answer Set Grammars -- a logic-based formalism that generalizes
context-sensitive grammars while incorporating background knowledge to
represent task-specific semantics. We show that our approach guarantees correct
completions for any off-the-shelf LLM without the need for fine-tuning. We
evaluate $\texttt{SEM-CTRL}$ on a range of tasks, including synthetic grammar
synthesis, combinatorial reasoning, and planning. Our results demonstrate that
$\texttt{SEM-CTRL}$ allows small pre-trained LLMs to efficiently outperform
larger variants and state-of-the-art reasoning models (e.g., o1-preview) while
simultaneously guaranteeing solution correctness.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01700v1' target='_blank'>Code-as-Symbolic-Planner: Foundation Model-Based Robot Planning via
  Symbolic Code Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yongchao Chen, Yilun Hao, Yang Zhang, Chuchu Fan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 16:13:41</h6>
<p class='card-text'>Recent works have shown great potentials of Large Language Models (LLMs) in
robot task and motion planning (TAMP). Current LLM approaches generate text- or
code-based reasoning chains with sub-goals and action plans. However, they do
not fully leverage LLMs' symbolic computing and code generation capabilities.
Many robot TAMP tasks involve complex optimization under multiple constraints,
where pure textual reasoning is insufficient. While augmenting LLMs with
predefined solvers and planners improves performance, it lacks generalization
across tasks. Given LLMs' growing coding proficiency, we enhance their TAMP
capabilities by steering them to generate code as symbolic planners for
optimization and constraint verification. Unlike prior work that uses code to
interface with robot action modules, we steer LLMs to generate code as solvers,
planners, and checkers for TAMP tasks requiring symbolic computing, while still
leveraging textual reasoning to incorporate common sense. With a multi-round
guidance and answer evolution framework, the proposed Code-as-Symbolic-Planner
improves success rates by average 24.1\% over best baseline methods across
seven typical TAMP tasks and three popular LLMs. Code-as-Symbolic-Planner shows
strong effectiveness and generalizability across discrete and continuous
environments, 2D/3D simulations and real-world settings, as well as single- and
multi-robot tasks with diverse requirements. See our project website
https://yongchao98.github.io/Code-Symbol-Planner/ for prompts, videos, and
code.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01490v1' target='_blank'>Improving Retrospective Language Agents via Joint Policy Gradient
  Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xueyang Feng, Bo Lan, Quanyu Dai, Lei Wang, Jiakai Tang, Xu Chen, Zhenhua Dong, Ji-Rong Wen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 12:54:54</h6>
<p class='card-text'>In recent research advancements within the community, large language models
(LLMs) have sparked great interest in creating autonomous agents. However,
current prompt-based agents often heavily rely on large-scale LLMs. Meanwhile,
although fine-tuning methods significantly enhance the capabilities of smaller
LLMs, the fine-tuned agents often lack the potential for self-reflection and
self-improvement. To address these challenges, we introduce a novel agent
framework named RetroAct, which is a framework that jointly optimizes both
task-planning and self-reflective evolution capabilities in language agents.
Specifically, we develop a two-stage joint optimization process that integrates
imitation learning and reinforcement learning, and design an off-policy joint
policy gradient optimization algorithm with imitation learning regularization
to enhance the data efficiency and training stability in agent tasks. RetroAct
significantly improves the performance of open-source models, reduces
dependency on closed-source LLMs, and enables fine-tuned agents to learn and
evolve continuously. We conduct extensive experiments across various testing
environments, demonstrating RetroAct has substantial improvements in task
performance and decision-making processes.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01236v1' target='_blank'>LLM-Advisor: An LLM Benchmark for Cost-efficient Path Planning across
  Multiple Terrains</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ling Xiao, Toshihiko Yamasaki</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 07:02:10</h6>
<p class='card-text'>Multi-terrain cost-efficient path planning is a crucial task in robot
navigation, requiring the identification of a path from the start to the goal
that not only avoids obstacles but also minimizes travel costs. This is
especially crucial for real-world applications where robots need to navigate
diverse terrains in outdoor environments, where recharging or refueling is
difficult. However, there is very limited research on this topic. In this
paper, we develop a prompt-based approach, LLM-Advisor, which leverages large
language models (LLMs) as effective advisors for path planning. The LLM-Advisor
selectively provides suggestions, demonstrating its ability to recognize when
no modifications are necessary. When suggestions are made, 70.59% of the paths
suggested for the A* algorithm, 69.47% for the RRT* algorithm, and 78.70% for
the LLM-A* algorithm achieve greater cost efficiency. Since LLM-Advisor may
occasionally lack common sense in their suggestions, we propose two
hallucination-mitigation strategies. Furthermore, we experimentally verified
that GPT-4o performs poorly in zero-shot path planning, even when terrain
descriptions are clearly provided, demonstrating its low spatial awareness. We
also experimentally demonstrate that using an LLM as an advisor is more
effective than directly integrating it into the path-planning loop. Since LLMs
may generate hallucinations, using LLMs in the loop of a search-based method
(such as A*) may lead to a higher number of failed paths, demonstrating that
our proposed LLM-Advisor is a better choice.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01935v1' target='_blank'>MultiAgentBench: Evaluating the Collaboration and Competition of LLM
  agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kunlun Zhu, Hongyi Du, Zhaochen Hong, Xiaocheng Yang, Shuyi Guo, Zhe Wang, Zhenhailong Wang, Cheng Qian, Xiangru Tang, Heng Ji, Jiaxuan You</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 05:18:50</h6>
<p class='card-text'>Large Language Models (LLMs) have shown remarkable capabilities as autonomous
agents, yet existing benchmarks either focus on single-agent tasks or are
confined to narrow domains, failing to capture the dynamics of multi-agent
coordination and competition. In this paper, we introduce MultiAgentBench, a
comprehensive benchmark designed to evaluate LLM-based multi-agent systems
across diverse, interactive scenarios. Our framework measures not only task
completion but also the quality of collaboration and competition using novel,
milestone-based key performance indicators. Moreover, we evaluate various
coordination protocols (including star, chain, tree, and graph topologies) and
innovative strategies such as group discussion and cognitive planning. Notably,
gpt-4o-mini reaches the average highest task score, graph structure performs
the best among coordination protocols in the research scenario, and cognitive
planning improves milestone achievement rates by 3%. Code and datasets are
public available at https://github.com/MultiagentBench/MARBLE.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01068v1' target='_blank'>Language-Guided Object Search in Agricultural Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Advaith Balaji, Saket Pradhan, Dmitry Berenson</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 00:15:45</h6>
<p class='card-text'>Creating robots that can assist in farms and gardens can help reduce the
mental and physical workload experienced by farm workers. We tackle the problem
of object search in a farm environment, providing a method that allows a robot
to semantically reason about the location of an unseen target object among a
set of previously seen objects in the environment using a Large Language Model
(LLM). We leverage object-to-object semantic relationships to plan a path
through the environment that will allow us to accurately and efficiently locate
our target object while also reducing the overall distance traveled, without
needing high-level room or area-level semantic relationships. During our
evaluations, we found that our method outperformed a current state-of-the-art
baseline and our ablations. Our offline testing yielded an average path
efficiency of 84%, reflecting how closely the predicted path aligns with the
ideal path. Upon deploying our system on the Boston Dynamics Spot robot in a
real-world farm environment, we found that our system had a success rate of
80%, with a success weighted by path length of 0.67, which demonstrates a
reasonable trade-off between task success and path efficiency under real-world
conditions. The project website can be viewed at
https://adi-balaji.github.io/losae/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01007v1' target='_blank'>From Vague Instructions to Task Plans: A Feedback-Driven HRC Task
  Planning Framework based on LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Afagh Mehri Shervedani, Matthew R. Walter, Milos Zefran</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-02 20:22:32</h6>
<p class='card-text'>Recent advances in large language models (LLMs) have demonstrated their
potential as planners in human-robot collaboration (HRC) scenarios, offering a
promising alternative to traditional planning methods. LLMs, which can generate
structured plans by reasoning over natural language inputs, have the ability to
generalize across diverse tasks and adapt to human instructions. This paper
investigates the potential of LLMs to facilitate planning in the context of
human-robot collaborative tasks, with a focus on their ability to reason from
high-level, vague human inputs, and fine-tune plans based on real-time
feedback. We propose a novel hybrid framework that combines LLMs with human
feedback to create dynamic, context-aware task plans. Our work also highlights
how a single, concise prompt can be used for a wide range of tasks and
environments, overcoming the limitations of long, detailed structured prompts
typically used in prior studies. By integrating user preferences into the
planning loop, we ensure that the generated plans are not only effective but
aligned with human intentions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.00767v1' target='_blank'>LLMs are everywhere: Ubiquitous Utilization of AI Models through Air
  Computing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Baris Yamansavascilar, Atay Ozgovde, Cem Ersoy</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-02 07:24:34</h6>
<p class='card-text'>We are witnessing a new era where problem-solving and cognitive tasks are
being increasingly delegated to Large Language Models (LLMs) across diverse
domains, ranging from code generation to holiday planning. This trend also
creates a demand for the ubiquitous execution of LLM-powered applications in a
wide variety of environments in which traditional terrestrial 2D networking
infrastructures may prove insufficient. A promising solution in this context is
to extend edge computing into a 3D setting to include aerial platforms
organized in multiple layers, a paradigm we refer to as air computing, to
augment local devices for running LLM and Generative AI (GenAI) applications.
This approach alleviates the strain on existing infrastructure while enhancing
service efficiency by offloading computational tasks to the corresponding air
units such as UAVs. Furthermore, the coordinated deployment of various air
units can significantly improve the Quality of Experience (QoE) by ensuring
seamless, adaptive, and resilient task execution. In this study, we investigate
the synergy between LLM-based applications and air computing, exploring their
potential across various use cases. Additionally, we present a disaster
response case study demonstrating how the collaborative utilization of LLMs and
air computing can significantly improve outcomes in critical situations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.00729v1' target='_blank'>CLEA: Closed-Loop Embodied Agent for Enhancing Task Execution in Dynamic
  Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mingcong Lei, Ge Wang, Yiming Zhao, Zhixin Mai, Qing Zhao, Yao Guo, Zhen Li, Shuguang Cui, Yatong Han, Jinke Ren</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-02 04:50:59</h6>
<p class='card-text'>Large Language Models (LLMs) exhibit remarkable capabilities in the
hierarchical decomposition of complex tasks through semantic reasoning.
However, their application in embodied systems faces challenges in ensuring
reliable execution of subtask sequences and achieving one-shot success in
long-term task completion. To address these limitations in dynamic
environments, we propose Closed-Loop Embodied Agent (CLEA) -- a novel
architecture incorporating four specialized open-source LLMs with functional
decoupling for closed-loop task management. The framework features two core
innovations: (1) Interactive task planner that dynamically generates executable
subtasks based on the environmental memory, and (2) Multimodal execution critic
employing an evaluation framework to conduct a probabilistic assessment of
action feasibility, triggering hierarchical re-planning mechanisms when
environmental perturbations exceed preset thresholds. To validate CLEA's
effectiveness, we conduct experiments in a real environment with manipulable
objects, using two heterogeneous robots for object search, manipulation, and
search-manipulation integration tasks. Across 12 task trials, CLEA outperforms
the baseline model, achieving a 67.3% improvement in success rate and a 52.8%
increase in task completion rate. These results demonstrate that CLEA
significantly enhances the robustness of task planning and execution in dynamic
environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.00717v1' target='_blank'>LLMDR: LLM-Driven Deadlock Detection and Resolution in Multi-Agent
  Pathfinding</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Seungbae Seo, Junghwan Kim, Minjeong Shin, Bongwon Suh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-02 03:49:15</h6>
<p class='card-text'>Multi-Agent Pathfinding (MAPF) is a core challenge in multi-agent systems.
Existing learning-based MAPF methods often struggle with scalability,
particularly when addressing complex scenarios that are prone to deadlocks. To
address these challenges, we introduce LLMDR (LLM-Driven Deadlock Detection and
Resolution), an approach designed to resolve deadlocks and improve the
performance of learnt MAPF models. LLMDR integrates the inference capabilities
of large language models (LLMs) with learnt MAPF models and prioritized
planning, enabling it to detect deadlocks and provide customized resolution
strategies. We evaluate LLMDR on standard MAPF benchmark maps with varying
agent numbers, measuring its performance when combined with several base
models. The results demonstrate that LLMDR improves the performance of learnt
MAPF models, particularly in deadlock-prone scenarios, with notable
improvements in success rates. These findings show the potential of integrating
LLMs to improve the scalability of learning-based MAPF methods.
  The source code for LLMDR is available at:
https://github.com/ssbacc/llmdr-dhc</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.00714v1' target='_blank'>Speculative Ad-hoc Querying</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haoyu Li, Srikanth Kandula, Maria Angels de Luis Balaguer, Aditya Akella, Venkat Arun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-02 03:44:31</h6>
<p class='card-text'>Analyzing large datasets requires responsive query execution, but executing
SQL queries on massive datasets can be slow. This paper explores whether query
execution can begin even before the user has finished typing, allowing results
to appear almost instantly. We propose SpeQL, a system that leverages Large
Language Models (LLMs) to predict likely queries based on the database schema,
the user's past queries, and their incomplete query. Since exact query
prediction is infeasible, SpeQL speculates on partial queries in two ways: 1)
it predicts the query structure to compile and plan queries in advance, and 2)
it precomputes smaller temporary tables that are much smaller than the original
database, but are still predicted to contain all information necessary to
answer the user's final query. Additionally, SpeQL continuously displays
results for speculated queries and subqueries in real time, aiding exploratory
analysis. A utility/user study showed that SpeQL improved task completion time,
and participants reported that its speculative display of results helped them
discover patterns in the data more quickly. In the study, SpeQL improves user's
query latency by up to $289\times$ and kept the overhead reasonable, at $\$4$
per hour.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.00624v1' target='_blank'>An evaluation of DeepSeek Models in Biomedical Natural Language
  Processing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zaifu Zhan, Shuang Zhou, Huixue Zhou, Jiawen Deng, Yu Hou, Jeremy Yeung, Rui Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-01 21:26:29</h6>
<p class='card-text'>The advancement of Large Language Models (LLMs) has significantly impacted
biomedical Natural Language Processing (NLP), enhancing tasks such as named
entity recognition, relation extraction, event extraction, and text
classification. In this context, the DeepSeek series of models have shown
promising potential in general NLP tasks, yet their capabilities in the
biomedical domain remain underexplored. This study evaluates multiple DeepSeek
models (Distilled-DeepSeek-R1 series and Deepseek-LLMs) across four key
biomedical NLP tasks using 12 datasets, benchmarking them against
state-of-the-art alternatives (Llama3-8B, Qwen2.5-7B, Mistral-7B, Phi-4-14B,
Gemma-2-9B). Our results reveal that while DeepSeek models perform
competitively in named entity recognition and text classification, challenges
persist in event and relation extraction due to precision-recall trade-offs. We
provide task-specific model recommendations and highlight future research
directions. This evaluation underscores the strengths and limitations of
DeepSeek models in biomedical NLP, guiding their future deployment and
optimization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.00203v2' target='_blank'>Llamarine: Open-source Maritime Industry-specific Large Language Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:William Nguyen, An Phan, Konobu Kimura, Hitoshi Maeno, Mika Tanaka, Quynh Le, William Poucher, Christopher Nguyen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-28 21:39:22</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated substantial potential in
addressing complex reasoning tasks, yet their general-purpose nature often
limits their effectiveness in specialized domains such as maritime navigation.
To bridge this gap, we introduce Llamarine, the first open-source LLM designed
specifically for maritime navigation. Llamarine 1.0 is developed through
continued pretraining and fine-tuning on a high-quality corpus comprising
maritime textbooks, research publications, and web text from Wikipedia. This
domain-specific training enables the model to acquire expert-level knowledge in
navigational principles, collision avoidance, route optimization, and
regulatory compliance. Our key contributions include (a) the curation of a
comprehensive maritime dataset from authoritative sources, ensuring depth and
reliability in the model's knowledge base; (b) the development of a
foundational model capable of reasoning about complex navigational challenges
with greater accuracy than general-purpose LLMs; and (c) the establishment of a
benchmark to evaluate performance in maritime-specific decision-making tasks.
Experimental results demonstrate that Llamarine outperforms both
general-purpose and commercial LLMs in critical navigation-related tasks, such
as trajectory planning, risk assessment, and compliance with maritime
regulations. By providing an open-source foundation model trained exclusively
on high-quality maritime literature, Llamarine paves the way for AI-driven
advancements in maritime safety, efficiency, and operational decision-making.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01908v1' target='_blank'>UDora: A Unified Red Teaming Framework against LLM Agents by Dynamically
  Hijacking Their Own Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiawei Zhang, Shuang Yang, Bo Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-28 21:30:28</h6>
<p class='card-text'>Large Language Model (LLM) agents equipped with external tools have become
increasingly powerful for handling complex tasks such as web shopping,
automated email replies, and financial trading. However, these advancements
also amplify the risks of adversarial attacks, particularly when LLM agents can
access sensitive external functionalities. Moreover, because LLM agents engage
in extensive reasoning or planning before executing final actions, manipulating
them into performing targeted malicious actions or invoking specific tools
remains a significant challenge. Consequently, directly embedding adversarial
strings in malicious instructions or injecting malicious prompts into tool
interactions has become less effective against modern LLM agents. In this work,
we present UDora, a unified red teaming framework designed for LLM Agents that
dynamically leverages the agent's own reasoning processes to compel it toward
malicious behavior. Specifically, UDora first samples the model's reasoning for
the given task, then automatically identifies multiple optimal positions within
these reasoning traces to insert targeted perturbations. Subsequently, it uses
the modified reasoning as the objective to optimize the adversarial strings. By
iteratively applying this process, the LLM agent will then be induced to
undertake designated malicious actions or to invoke specific malicious tools.
Our approach demonstrates superior effectiveness compared to existing methods
across three LLM agent datasets.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>