<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>LLM-RL - 2025-03-15</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>LLM-RL - 2025-03-15</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.10509v1' target='_blank'>SySLLM: Generating Synthesized Policy Summaries for Reinforcement
  Learning Agents Using Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sahar Admoni, Omer Ben-Porat, Ofra Amir</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-13 16:10:14</h6>
<p class='card-text'>Policies generated by Reinforcement Learning (RL) algorithms can be difficult
to describe to users, as they result from the interplay between complex reward
structures and neural network-based representations. This combination often
leads to unpredictable behaviors, making policies challenging to analyze and
posing significant obstacles to fostering human trust in real-world
applications. Global policy summarization methods aim to describe agent
behavior through a demonstration of actions in a subset of world-states.
However, users can only watch a limited number of demonstrations, restricting
their understanding of policies. Moreover, those methods overly rely on user
interpretation, as they do not synthesize observations into coherent patterns.
In this work, we present SySLLM (Synthesized Summary using LLMs), a novel
method that employs synthesis summarization, utilizing large language models'
(LLMs) extensive world knowledge and ability to capture patterns, to generate
textual summaries of policies. Specifically, an expert evaluation demonstrates
that the proposed approach generates summaries that capture the main insights
generated by experts while not resulting in significant hallucinations.
Additionally, a user study shows that SySLLM summaries are preferred over
demonstration-based policy summaries and match or surpass their performance in
objective agent identification tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.10177v1' target='_blank'>PRISM: Preference Refinement via Implicit Scene Modeling for 3D
  Vision-Language Preference-Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yirong Sun, Yanjun Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-13 08:58:10</h6>
<p class='card-text'>We propose PRISM, a novel framework designed to overcome the limitations of
2D-based Preference-Based Reinforcement Learning (PBRL) by unifying 3D point
cloud modeling and future-aware preference refinement. At its core, PRISM
adopts a 3D Point Cloud-Language Model (3D-PC-LLM) to mitigate occlusion and
viewpoint biases, ensuring more stable and spatially consistent preference
signals. Additionally, PRISM leverages Chain-of-Thought (CoT) reasoning to
incorporate long-horizon considerations, thereby preventing the short-sighted
feedback often seen in static preference comparisons. In contrast to
conventional PBRL techniques, this integration of 3D perception and
future-oriented reasoning leads to significant gains in preference agreement
rates, faster policy convergence, and robust generalization across unseen
robotic environments. Our empirical results, spanning tasks such as robotic
manipulation and autonomous navigation, highlight PRISM's potential for
real-world applications where precise spatial understanding and reliable
long-term decision-making are critical. By bridging 3D geometric awareness with
CoT-driven preference modeling, PRISM establishes a comprehensive foundation
for scalable, human-aligned reinforcement learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.10093v1' target='_blank'>Representation-based Reward Modeling for Efficient Safety Alignment of
  Large Language Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qiyuan Deng, Xuefeng Bai, Kehai Chen, Yaowei Wang, Liqiang Nie, Min Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-13 06:40:34</h6>
<p class='card-text'>Reinforcement Learning (RL) algorithms for safety alignment of Large Language
Models (LLMs), such as Direct Preference Optimization (DPO), encounter the
challenge of distribution shift. Current approaches typically address this
issue through online sampling from the target policy, which requires
significant computational resources. In this paper, we hypothesize that during
off-policy training, while the ranking order of output generated by policy
changes, their overall distribution remains relatively stable. This stability
allows the transformation of the sampling process from the target policy into a
re-ranking of preference data. Building on this hypothesis, We propose a new
framework that leverages the model's intrinsic safety judgment capability to
extract reward signals, which are then used to calculate label confidence for
preferences reordering. Extensive experimental results and theoretical analysis
demonstrate that the proposed method effectively addresses the distribution
shift issue, remarkably enhancing the safety performance while reducing about
300x computational overheads.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.10049v1' target='_blank'>Enhancing Multi-Agent Systems via Reinforcement Learning with LLM-based
  Planner and Graph-based Policy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziqi Jia, Junjie Li, Xiaoyang Qu, Jianzong Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-13 05:02:49</h6>
<p class='card-text'>Multi-agent systems (MAS) have shown great potential in executing complex
tasks, but coordination and safety remain significant challenges. Multi-Agent
Reinforcement Learning (MARL) offers a promising framework for agent
collaboration, but it faces difficulties in handling complex tasks and
designing reward functions. The introduction of Large Language Models (LLMs)
has brought stronger reasoning and cognitive abilities to MAS, but existing
LLM-based systems struggle to respond quickly and accurately in dynamic
environments. To address these challenges, we propose LLM-based Graph
Collaboration MARL (LGC-MARL), a framework that efficiently combines LLMs and
MARL. This framework decomposes complex tasks into executable subtasks and
achieves efficient collaboration among multiple agents through graph-based
coordination. Specifically, LGC-MARL consists of two main components: an LLM
planner and a graph-based collaboration meta policy. The LLM planner transforms
complex task instructions into a series of executable subtasks, evaluates the
rationality of these subtasks using a critic model, and generates an action
dependency graph. The graph-based collaboration meta policy facilitates
communication and collaboration among agents based on the action dependency
graph, and adapts to new task environments through meta-learning. Experimental
results on the AI2-THOR simulation platform demonstrate the superior
performance and scalability of LGC-MARL in completing various complex tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.09956v1' target='_blank'>Exploring Mutual Empowerment Between Wireless Networks and RL-based
  LLMs: A Survey</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yu Qiao, Phuong-Nam Tran, Ji Su Yoon, Loc X. Nguyen, Choong Seon Hong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-13 01:59:11</h6>
<p class='card-text'>Reinforcement learning (RL)-based large language models (LLMs), such as
ChatGPT, DeepSeek, and Grok-3, have gained significant attention for their
exceptional capabilities in natural language processing and multimodal data
understanding. Meanwhile, the rapid expansion of information services has
driven the growing need for intelligence, efficient, and adaptable wireless
networks. Wireless networks require the empowerment of RL-based LLMs while
these models also benefit from wireless networks to broaden their application
scenarios. Specifically, RL-based LLMs can enhance wireless communication
systems through intelligent resource allocation, adaptive network optimization,
and real-time decision-making. Conversely, wireless networks provide a vital
infrastructure for the efficient training, deployment, and distributed
inference of RL-based LLMs, especially in decentralized and edge computing
environments. This mutual empowerment highlights the need for a deeper
exploration of the interplay between these two domains. We first review recent
advancements in wireless communications, highlighting the associated challenges
and potential solutions. We then discuss the progress of RL-based LLMs,
focusing on key technologies for LLM training, challenges, and potential
solutions. Subsequently, we explore the mutual empowerment between these two
fields, highlighting key motivations, open challenges, and potential solutions.
Finally, we provide insights into future directions, applications, and their
societal impact to further explore this intersection, paving the way for
next-generation intelligent communication systems. Overall, this survey
provides a comprehensive overview of the relationship between RL-based LLMs and
wireless networks, offering a vision where these domains empower each other to
drive innovations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.09925v1' target='_blank'>PluralLLM: Pluralistic Alignment in LLMs via Federated Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mahmoud Srewa, Tianyu Zhao, Salma Elmalaki</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-13 00:45:27</h6>
<p class='card-text'>Ensuring Large Language Models (LLMs) align with diverse human preferences
while preserving privacy and fairness remains a challenge. Existing methods,
such as Reinforcement Learning from Human Feedback (RLHF), rely on centralized
data collection, making them computationally expensive and privacy-invasive. We
introduce PluralLLM a federated learning-based approach that enables multiple
user groups to collaboratively train a transformer-based preference predictor
without sharing sensitive data, which can also serve as a reward model for
aligning LLMs. Our method leverages Federated Averaging (FedAvg) to aggregate
preference updates efficiently, achieving 46% faster convergence, a 4%
improvement in alignment scores, and nearly the same group fairness measure as
in centralized training. Evaluated on a Q/A preference alignment task,
PluralLLM demonstrates that federated preference learning offers a scalable and
privacy-preserving alternative for aligning LLMs with diverse human values.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.09829v1' target='_blank'>SE(3)-Equivariant Robot Learning and Control: A Tutorial Survey</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Joohwan Seo, Soochul Yoo, Junwoo Chang, Hyunseok An, Hyunwoo Ryu, Soomi Lee, Arvind Kruthiventy, Jongeun CHoi, Roberto Horowitz</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-12 20:47:40</h6>
<p class='card-text'>Recent advances in deep learning and Transformers have driven major
breakthroughs in robotics by employing techniques such as imitation learning,
reinforcement learning, and LLM-based multimodal perception and
decision-making. However, conventional deep learning and Transformer models
often struggle to process data with inherent symmetries and invariances,
typically relying on large datasets or extensive data augmentation. Equivariant
neural networks overcome these limitations by explicitly integrating symmetry
and invariance into their architectures, leading to improved efficiency and
generalization. This tutorial survey reviews a wide range of equivariant deep
learning and control methods for robotics, from classic to state-of-the-art,
with a focus on SE(3)-equivariant models that leverage the natural 3D
rotational and translational symmetries in visual robotic manipulation and
control design. Using unified mathematical notation, we begin by reviewing key
concepts from group theory, along with matrix Lie groups and Lie algebras. We
then introduce foundational group-equivariant neural network design and show
how the group-equivariance can be obtained through their structure. Next, we
discuss the applications of SE(3)-equivariant neural networks in robotics in
terms of imitation learning and reinforcement learning. The SE(3)-equivariant
control design is also reviewed from the perspective of geometric control.
Finally, we highlight the challenges and future directions of equivariant
methods in developing more robust, sample-efficient, and multi-modal real-world
robotic systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.09758v1' target='_blank'>Multi-Agent LLM Actor-Critic Framework for Social Robot Navigation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weizheng Wang, Ike Obi, Byung-Cheol Min</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-12 18:59:53</h6>
<p class='card-text'>Recent advances in robotics and large language models (LLMs) have sparked
growing interest in human-robot collaboration and embodied intelligence. To
enable the broader deployment of robots in human-populated environments,
socially-aware robot navigation (SAN) has become a key research area. While
deep reinforcement learning approaches that integrate human-robot interaction
(HRI) with path planning have demonstrated strong benchmark performance, they
often struggle to adapt to new scenarios and environments. LLMs offer a
promising avenue for zero-shot navigation through commonsense inference.
However, most existing LLM-based frameworks rely on centralized
decision-making, lack robust verification mechanisms, and face inconsistencies
in translating macro-actions into precise low-level control signals. To address
these challenges, we propose SAMALM, a decentralized multi-agent LLM
actor-critic framework for multi-robot social navigation. In this framework, a
set of parallel LLM actors, each reflecting distinct robot personalities or
configurations, directly generate control signals. These actions undergo a
two-tier verification process via a global critic that evaluates group-level
behaviors and individual critics that assess each robot's context. An
entropy-based score fusion mechanism further enhances self-verification and
re-query, improving both robustness and coordination. Experimental results
confirm that SAMALM effectively balances local autonomy with global oversight,
yielding socially compliant behaviors and strong adaptability across diverse
multi-robot scenarios. More details and videos about this work are available
at: https://sites.google.com/view/SAMALM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.09516v1' target='_blank'>Search-R1: Training LLMs to Reason and Leverage Search Engines with
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, Jiawei Han</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-12 16:26:39</h6>
<p class='card-text'>Efficiently acquiring external knowledge and up-to-date information is
essential for effective reasoning and text generation in large language models
(LLMs). Retrieval augmentation and tool-use training approaches where a search
engine is treated as a tool lack complex multi-turn retrieval flexibility or
require large-scale supervised data. Prompting advanced LLMs with reasoning
capabilities during inference to use search engines is not optimal, since the
LLM does not learn how to optimally interact with the search engine. This paper
introduces Search-R1, an extension of the DeepSeek-R1 model where the LLM
learns -- solely through reinforcement learning (RL) -- to autonomously
generate (multiple) search queries during step-by-step reasoning with real-time
retrieval. Search-R1 optimizes LLM rollouts with multi-turn search
interactions, leveraging retrieved token masking for stable RL training and a
simple outcome-based reward function. Experiments on seven question-answering
datasets show that Search-R1 improves performance by 26% (Qwen2.5-7B), 21%
(Qwen2.5-3B), and 10% (LLaMA3.2-3B) over SOTA baselines. This paper further
provides empirical insights into RL optimization methods, LLM choices, and
response length dynamics in retrieval-augmented reasoning. The code and model
checkpoints are available at https://github.com/PeterGriffinJin/Search-R1.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.09501v1' target='_blank'>ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziyu Wan, Yunxiang Li, Yan Song, Hanjing Wang, Linyi Yang, Mark Schmidt, Jun Wang, Weinan Zhang, Shuyue Hu, Ying Wen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-12 16:05:31</h6>
<p class='card-text'>Recent research on Reasoning of Large Language Models (LLMs) has sought to
further enhance their performance by integrating meta-thinking -- enabling
models to monitor, evaluate, and control their reasoning processes for more
adaptive and effective problem-solving. However, current single-agent work
lacks a specialized design for acquiring meta-thinking, resulting in low
efficacy. To address this challenge, we introduce Reinforced Meta-thinking
Agents (ReMA), a novel framework that leverages Multi-Agent Reinforcement
Learning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think
about thinking. ReMA decouples the reasoning process into two hierarchical
agents: a high-level meta-thinking agent responsible for generating strategic
oversight and plans, and a low-level reasoning agent for detailed executions.
Through iterative reinforcement learning with aligned objectives, these agents
explore and learn collaboration, leading to improved generalization and
robustness. Experimental results demonstrate that ReMA outperforms single-agent
RL baselines on complex reasoning tasks, including competitive-level
mathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation
studies further illustrate the evolving dynamics of each distinct agent,
providing valuable insights into how the meta-thinking reasoning process
enhances the reasoning capabilities of LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.09025v1' target='_blank'>Aligning to What? Limits to RLHF Based Alignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Logan Barnhart, Reza Akbarian Bafghi, Stephen Becker, Maziar Raissi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-12 03:24:44</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) is increasingly used to
align large language models (LLMs) with human preferences. However, the
effectiveness of RLHF in addressing underlying biases remains unclear. This
study investigates the relationship between RLHF and both covert and overt
biases in LLMs, particularly focusing on biases against African Americans. We
applied various RLHF techniques (DPO, ORPO, and RLOO) to Llama 3 8B and
evaluated the covert and overt biases of the resulting models using
matched-guise probing and explicit bias testing. We performed additional tests
with DPO on different base models and datasets; among several implications, we
found that SFT before RLHF calcifies model biases. Additionally, we extend the
tools for measuring biases to multi-modal models. Through our experiments we
collect evidence that indicates that current alignment techniques are
inadequate for nebulous tasks such as mitigating covert biases, highlighting
the need for capable datasets, data curating techniques, or alignment tools.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08919v1' target='_blank'>Backtracking for Safety</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bilgehan Sel, Dingcheng Li, Phillip Wallis, Vaishakh Keshava, Ming Jin, Siddhartha Reddy Jonnalagadda</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 22:04:22</h6>
<p class='card-text'>Large language models (LLMs) have demonstrated remarkable capabilities across
various tasks, but ensuring their safety and alignment with human values
remains crucial. Current safety alignment methods, such as supervised
fine-tuning and reinforcement learning-based approaches, can exhibit
vulnerabilities to adversarial attacks and often result in shallow safety
alignment, primarily focusing on preventing harmful content in the initial
tokens of the generated output. While methods like resetting can help recover
from unsafe generations by discarding previous tokens and restarting the
generation process, they are not well-suited for addressing nuanced safety
violations like toxicity that may arise within otherwise benign and lengthy
generations. In this paper, we propose a novel backtracking method designed to
address these limitations. Our method allows the model to revert to a safer
generation state, not necessarily at the beginning, when safety violations
occur during generation. This approach enables targeted correction of
problematic segments without discarding the entire generated text, thereby
preserving efficiency. We demonstrate that our method dramatically reduces
toxicity appearing through the generation process with minimal impact to
efficiency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08525v1' target='_blank'>GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based
  VLM Agent Training</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tong Wei, Yijun Yang, Junliang Xing, Yuanchun Shi, Zongqing Lu, Deheng Ye</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 15:17:02</h6>
<p class='card-text'>Reinforcement learning with verifiable outcome rewards (RLVR) has effectively
scaled up chain-of-thought (CoT) reasoning in large language models (LLMs).
Yet, its efficacy in training vision-language model (VLM) agents for
goal-directed action reasoning in visual environments is less established. This
work investigates this problem through extensive experiments on complex card
games, such as 24 points, and embodied tasks from ALFWorld. We find that when
rewards are based solely on action outcomes, RL fails to incentivize CoT
reasoning in VLMs, instead leading to a phenomenon we termed thought collapse,
characterized by a rapid loss of diversity in the agent's thoughts,
state-irrelevant and incomplete reasoning, and subsequent invalid actions,
resulting in negative rewards. To counteract thought collapse, we highlight the
necessity of process guidance and propose an automated corrector that evaluates
and refines the agent's reasoning at each RL step. This simple and scalable GTR
(Guided Thought Reinforcement) framework trains reasoning and action
simultaneously without the need for dense, per-step human labeling. Our
experiments demonstrate that GTR significantly enhances the performance and
generalization of the LLaVA-7b model across various visual environments,
achieving 3-5 times higher task success rates compared to SoTA models with
notably smaller model sizes.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08338v1' target='_blank'>Trinity: A Modular Humanoid Robot AI System</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jingkai Sun, Qiang Zhang, Gang Han, Wen Zhao, Zhe Yong, Yan He, Jiaxu Wang, Jiahang Cao, Yijie Guo, Renjing Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 11:50:36</h6>
<p class='card-text'>In recent years, research on humanoid robots has garnered increasing
attention. With breakthroughs in various types of artificial intelligence
algorithms, embodied intelligence, exemplified by humanoid robots, has been
highly anticipated. The advancements in reinforcement learning (RL) algorithms
have significantly improved the motion control and generalization capabilities
of humanoid robots. Simultaneously, the groundbreaking progress in large
language models (LLM) and visual language models (VLM) has brought more
possibilities and imagination to humanoid robots. LLM enables humanoid robots
to understand complex tasks from language instructions and perform long-term
task planning, while VLM greatly enhances the robots' understanding and
interaction with their environment. This paper introduces
\textcolor{magenta}{Trinity}, a novel AI system for humanoid robots that
integrates RL, LLM, and VLM. By combining these technologies, Trinity enables
efficient control of humanoid robots in complex environments. This innovative
approach not only enhances the capabilities but also opens new avenues for
future research and applications of humanoid robotics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08271v1' target='_blank'>LangTime: A Language-Guided Unified Model for Time Series Forecasting
  with Proximal Policy Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenzhe Niu, Zongxia Xie, Yanru Sun, Wei He, Man Xu, Chao Hao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 10:40:39</h6>
<p class='card-text'>Recent research has shown an increasing interest in utilizing pre-trained
large language models (LLMs) for a variety of time series applications.
However, there are three main challenges when using LLMs as foundational models
for time series forecasting: (1) Cross-domain generalization. (2)
Cross-modality alignment. (3) Error accumulation in autoregressive frameworks.
To address these challenges, we proposed LangTime, a language-guided unified
model for time series forecasting that incorporates cross-domain pre-training
with reinforcement learning-based fine-tuning. Specifically, LangTime
constructs Temporal Comprehension Prompts (TCPs), which include dataset-wise
and channel-wise instructions, to facilitate domain adaptation and condense
time series into a single token, enabling LLMs to understand better and align
temporal data. To improve autoregressive forecasting, we introduce TimePPO, a
reinforcement learning-based fine-tuning algorithm. TimePPO mitigates error
accumulation by leveraging a multidimensional rewards function tailored for
time series and a repeat-based value estimation strategy. Extensive experiments
demonstrate that LangTime achieves state-of-the-art cross-domain forecasting
performance, while TimePPO fine-tuning effectively enhances the stability and
accuracy of autoregressive forecasting.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08199v1' target='_blank'>A Cascading Cooperative Multi-agent Framework for On-ramp Merging
  Control Integrating Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Miao Zhang, Zhenlong Fang, Tianyi Wang, Qian Zhang, Shuai Lu, Junfeng Jiao, Tianyu Shi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 09:08:04</h6>
<p class='card-text'>Traditional Reinforcement Learning (RL) suffers from replicating human-like
behaviors, generalizing effectively in multi-agent scenarios, and overcoming
inherent interpretability issues.These tasks are compounded when deep
environment understanding, agent coordination and dynamic optimization are
required. While Large Language Model (LLM) enhanced methods have shown promise
in generalization and interoperability, they often neglect necessary
multi-agent coordination. Therefore, we introduce the Cascading Cooperative
Multi-agent (CCMA) framework, integrating RL for individual interactions, a
fine-tuned LLM for regional cooperation, a reward function for global
optimization, and the Retrieval-augmented Generation mechanism to dynamically
optimize decision-making across complex driving scenarios. Our experiments
demonstrate that the CCMA outperforms existing RL methods, demonstrating
significant improvements in both micro and macro-level performance in complex
driving environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08123v1' target='_blank'>LLM4MAC: An LLM-Driven Reinforcement Learning Framework for MAC Protocol
  Emergence</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Renxuan Tan, Rongpeng Li, Zhifeng Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 07:38:14</h6>
<p class='card-text'>With the advent of 6G systems, emerging hyper-connected ecosystems
necessitate agile and adaptive medium access control (MAC) protocols to contend
with network dynamics and diverse service requirements. We propose LLM4MAC, a
novel framework that harnesses large language models (LLMs) within a
reinforcement learning paradigm to drive MAC protocol emergence. By
reformulating uplink data transmission scheduling as a semantics-generalized
partially observable Markov game (POMG), LLM4MAC encodes network operations in
natural language, while proximal policy optimization (PPO) ensures continuous
alignment with the evolving network dynamics. A structured identity embedding
(SIE) mechanism further enables robust coordination among heterogeneous agents.
Extensive simulations demonstrate that on top of a compact LLM, which is
purposefully selected to balance performance with resource efficiency, the
protocol emerging from LLM4MAC outperforms comparative baselines in throughput
and generalization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08026v1' target='_blank'>In Prospect and Retrospect: Reflective Memory Management for Long-term
  Personalized Dialogue Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhen Tan, Jun Yan, I-Hung Hsu, Rujun Han, Zifeng Wang, Long T. Le, Yiwen Song, Yanfei Chen, Hamid Palangi, George Lee, Anand Iyer, Tianlong Chen, Huan Liu, Chen-Yu Lee, Tomas Pfister</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 04:15:52</h6>
<p class='card-text'>Large Language Models (LLMs) have made significant progress in open-ended
dialogue, yet their inability to retain and retrieve relevant information from
long-term interactions limits their effectiveness in applications requiring
sustained personalization. External memory mechanisms have been proposed to
address this limitation, enabling LLMs to maintain conversational continuity.
However, existing approaches struggle with two key challenges. First, rigid
memory granularity fails to capture the natural semantic structure of
conversations, leading to fragmented and incomplete representations. Second,
fixed retrieval mechanisms cannot adapt to diverse dialogue contexts and user
interaction patterns. In this work, we propose Reflective Memory Management
(RMM), a novel mechanism for long-term dialogue agents, integrating forward-
and backward-looking reflections: (1) Prospective Reflection, which dynamically
summarizes interactions across granularities-utterances, turns, and
sessions-into a personalized memory bank for effective future retrieval, and
(2) Retrospective Reflection, which iteratively refines the retrieval in an
online reinforcement learning (RL) manner based on LLMs' cited evidence.
Experiments show that RMM demonstrates consistent improvement across various
metrics and benchmarks. For example, RMM shows more than 10% accuracy
improvement over the baseline without memory management on the LongMemEval
dataset.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07572v1' target='_blank'>Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuxiao Qu, Matthew Y. R. Yang, Amrith Setlur, Lewis Tunstall, Edward Emanuel Beeching, Ruslan Salakhutdinov, Aviral Kumar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 17:40:43</h6>
<p class='card-text'>Training models to effectively use test-time compute is crucial for improving
the reasoning performance of LLMs. Current methods mostly do so via fine-tuning
on search traces or running RL with 0/1 outcome reward, but do these approaches
efficiently utilize test-time compute? Would these approaches continue to scale
as the budget improves? In this paper, we try to answer these questions. We
formalize the problem of optimizing test-time compute as a meta-reinforcement
learning (RL) problem, which provides a principled perspective on spending
test-time compute. This perspective enables us to view the long output stream
from the LLM as consisting of several episodes run at test time and leads us to
use a notion of cumulative regret over output tokens as a way to measure the
efficacy of test-time compute. Akin to how RL algorithms can best tradeoff
exploration and exploitation over training, minimizing cumulative regret would
also provide the best balance between exploration and exploitation in the token
stream. While we show that state-of-the-art models do not minimize regret, one
can do so by maximizing a dense reward bonus in conjunction with the outcome
0/1 reward RL. This bonus is the ''progress'' made by each subsequent block in
the output stream, quantified by the change in the likelihood of eventual
success. Using these insights, we develop Meta Reinforcement Fine-Tuning, or
MRT, a new class of fine-tuning methods for optimizing test-time compute. MRT
leads to a 2-3x relative gain in performance and roughly a 1.5x gain in token
efficiency for math reasoning compared to outcome-reward RL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07365v1' target='_blank'>MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, Wenqi Shao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 14:23:12</h6>
<p class='card-text'>We present MM-Eureka, a multimodal reasoning model that successfully extends
large-scale rule-based reinforcement learning (RL) to multimodal reasoning.
While rule-based RL has shown remarkable success in improving LLMs' reasoning
abilities in text domains, its application to multimodal settings has remained
challenging. Our work reproduces key characteristics of text-based RL systems
like DeepSeek-R1 in the multimodal space, including steady increases in
accuracy reward and response length, and the emergence of reflection behaviors.
We demonstrate that both instruction-tuned and pre-trained models can develop
strong multimodal reasoning capabilities through rule-based RL without
supervised fine-tuning, showing superior data efficiency compared to
alternative approaches. We open-source our complete pipeline to foster further
research in this area. We release all our codes, models, data, etc. at
https://github.com/ModalMinds/MM-EUREKA</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07065v1' target='_blank'>Boosting the Generalization and Reasoning of Vision Language Models with
  Curriculum Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Huilin Deng, Ding Zou, Rui Ma, Hongchen Luo, Yang Cao, Yu Kang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 08:48:50</h6>
<p class='card-text'>While state-of-the-art vision-language models (VLMs) have demonstrated
remarkable capabilities in complex visual-text tasks, their success heavily
relies on massive model scaling, limiting their practical deployment.
Small-scale VLMs offer a more practical alternative but face significant
challenges when trained with traditional supervised fine-tuning (SFT),
particularly in two aspects: out-of-domain (OOD) generalization and reasoning
abilities, which significantly lags behind the contemporary Large language
models (LLMs). To address these challenges, we propose Curriculum Reinforcement
Finetuning (Curr-ReFT), a novel post-training paradigm specifically designed
for small-scale VLMs. Inspired by the success of reinforcement learning in
LLMs, Curr-ReFT comprises two sequential stages: (1) Curriculum Reinforcement
Learning, which ensures steady progression of model capabilities through
difficulty-aware reward design, transitioning from basic visual perception to
complex reasoning tasks; and (2) Rejected Sampling-based Self-improvement,
which maintains the fundamental capabilities of VLMs through selective learning
from high-quality multimodal and language examples. Extensive experiments
demonstrate that models trained with Curr-ReFT paradigm achieve
state-of-the-art performance across various visual tasks in both in-domain and
out-of-domain settings. Moreover, our Curr-ReFT enhanced 3B model matches the
performance of 32B-parameter models, demonstrating that efficient training
paradigms can effectively bridge the gap between small and large models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06949v2' target='_blank'>LexPro-1.0 Technical Report</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haotian Chen, Yanyu Xu, Boyan Wang, Chaoyue Zhao, Xiaoyu Han, Fang Wang, Lizhen Cui, Yonghui Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 05:54:23</h6>
<p class='card-text'>In this report, we introduce our first-generation reasoning model,
LexPro-1.0, a large language model designed for the highly specialized Chinese
legal domain, offering comprehensive capabilities to meet diverse realistic
needs. Existing legal LLMs face two primary challenges. Firstly, their design
and evaluation are predominantly driven by computer science perspectives,
leading to insufficient incorporation of legal expertise and logic, which is
crucial for high-precision legal applications, such as handling complex
prosecutorial tasks. Secondly, these models often underperform due to a lack of
comprehensive training data from the legal domain, limiting their ability to
effectively address real-world legal scenarios. To address this, we first
compile millions of legal documents covering over 20 types of crimes from 31
provinces in China for model training. From the extensive dataset, we further
select high-quality for supervised fine-tuning, ensuring enhanced relevance and
precision. The model further undergoes large-scale reinforcement learning
without additional supervision, emphasizing the enhancement of its reasoning
capabilities and explainability. To validate its effectiveness in complex legal
applications, we also conduct human evaluations with legal experts. We develop
fine-tuned models based on DeepSeek-R1-Distilled versions, available in three
dense configurations: 14B, 32B, and 70B.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06781v1' target='_blank'>Dr Genre: Reinforcement Learning from Decoupled LLM Feedback for Generic
  Text Rewriting</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yufei Li, John Nham, Ganesh Jawahar, Lei Shu, David Uthus, Yun-Hsuan Sung, Chengrun Yang, Itai Rolnick, Yi Qiao, Cong Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-09 21:23:52</h6>
<p class='card-text'>Generic text rewriting is a prevalent large language model (LLM) application
that covers diverse real-world tasks, such as style transfer, fact correction,
and email editing. These tasks vary in rewriting objectives (e.g., factual
consistency vs. semantic preservation), making it challenging to develop a
unified model that excels across all dimensions. Existing methods often
specialize in either a single task or a specific objective, limiting their
generalizability. In this work, we introduce a generic model proficient in
factuality, stylistic, and conversational rewriting tasks. To simulate
real-world user rewrite requests, we construct a conversational rewrite
dataset, ChatRewrite, that presents ``natural''-sounding instructions, from raw
emails using LLMs. Combined with other popular rewrite datasets, including
LongFact for the factuality rewrite task and RewriteLM for the stylistic
rewrite task, this forms a broad benchmark for training and evaluating generic
rewrite models. To align with task-specific objectives, we propose Dr Genre, a
Decoupled-reward learning framework for Generic rewriting, that utilizes
objective-oriented reward models with a task-specific weighting. Evaluation
shows that \approach delivers higher-quality rewrites across all targeted
tasks, improving objectives including instruction following (agreement),
internal consistency (coherence), and minimal unnecessary edits (conciseness).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06749v2' target='_blank'>Vision-R1: Incentivizing Reasoning Capability in Multimodal Large
  Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, Shaohui Lin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-09 20:06:45</h6>
<p class='card-text'>DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning
capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by
this breakthrough, we explore how RL can be utilized to enhance the reasoning
capability of MLLMs. However, direct training with RL struggles to activate
complex reasoning capabilities such as questioning and reflection in MLLMs, due
to the absence of substantial high-quality multimodal reasoning data. To
address this issue, we propose the reasoning MLLM, Vision-R1, to improve
multimodal reasoning capability. Specifically, we first construct a
high-quality multimodal CoT dataset without human annotations by leveraging an
existing MLLM and DeepSeek-R1 through modality bridging and data filtering to
obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as
cold-start initialization data for Vision-R1. To mitigate the optimization
challenges caused by overthinking after cold start, we propose Progressive
Thinking Suppression Training (PTST) strategy and employ Group Relative Policy
Optimization (GRPO) with the hard formatting result reward function to
gradually refine the model's ability to learn correct and complex reasoning
processes on a 10K multimodal math dataset. Comprehensive experiments show our
model achieves an average improvement of $\sim$6% across various multimodal
math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely
used MathVista benchmark, which is only 0.4% lower than the leading reasoning
model, OpenAI O1. The datasets and code will be released in:
https://github.com/Osilly/Vision-R1 .</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06639v1' target='_blank'>Reinforcement Learning with Verifiable Rewards: GRPO's Effective Loss,
  Dynamics, and Success Amplification</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Youssef Mroueh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-09 14:36:45</h6>
<p class='card-text'>Group Relative Policy Optimization (GRPO) was introduced and used
successfully to train DeepSeek R1 models for promoting reasoning capabilities
of LLMs using verifiable or binary rewards. We show in this paper that GRPO
with verifiable rewards can be written as a Kullback Leibler ($\mathsf{KL}$)
regularized contrastive loss, where the contrastive samples are synthetic data
sampled from the old policy. The optimal GRPO policy $\pi_{n}$ can be expressed
explicitly in terms of the binary reward, as well as the first and second order
statistics of the old policy ($\pi_{n-1}$) and the reference policy $\pi_0$.
Iterating this scheme, we obtain a sequence of policies $\pi_{n}$ for which we
can quantify the probability of success $p_n$. We show that the probability of
success of the policy satisfies a recurrence that converges to a fixed point of
a function that depends on the initial probability of success $p_0$ and the
regularization parameter $\beta$ of the $\mathsf{KL}$ regularizer. We show that
the fixed point $p^*$ is guaranteed to be larger than $p_0$, thereby
demonstrating that GRPO effectively amplifies the probability of success of the
policy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06358v1' target='_blank'>Language Model Personalization via Reward Factorization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Idan Shenfeld, Felix Faltings, Pulkit Agrawal, Aldo Pacchiano</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-08 23:41:20</h6>
<p class='card-text'>Modern large language models (LLMs) are optimized for human-aligned responses
using Reinforcement Learning from Human Feedback (RLHF). However, existing RLHF
approaches assume a universal preference model and fail to account for
individual user preferences, limiting their effectiveness in personalized
applications. We introduce a framework that extends RLHF to enable user
personalization by leveraging the assumption that user preferences lie in a
low-dimensional space. Instead of training a separate model per user, we
represent user-specific rewards as a linear combination of base reward
functions. Using only ~10 user responses, our method can infer user-specific
rewards and align LLM outputs accordingly. We validate our approach through
experiments with both synthetic and real users, demonstrating significant
personalization achieved by our method. In human evaluations, our method
achieves a 67% win rate over default GPT-4o responses.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06223v1' target='_blank'>Reinforced Diffuser for Red Teaming Large Vision-Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ruofan Wang, Xiang Zheng, Xiaosen Wang, Cong Wang, Xingjun Ma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-08 13:51:40</h6>
<p class='card-text'>The rapid advancement of large Vision-Language Models (VLMs) has raised
significant safety concerns, particularly regarding their vulnerability to
jailbreak attacks. While existing research primarily focuses on VLMs'
susceptibility to harmful instructions, this work identifies a critical yet
overlooked vulnerability: current alignment mechanisms often fail to address
the risks posed by toxic text continuation tasks. To investigate this issue, we
propose a novel Red Team Diffuser (RTD) framework, which leverages
reinforcement learning to generate red team images that effectively induce
highly toxic continuations from target black-box VLMs. The RTD pipeline begins
with a greedy search for high-quality image prompts that maximize the toxicity
of VLM-generated sentence continuations, guided by a Large Language Model
(LLM). These prompts are then used as input for the reinforcement fine-tuning
of a diffusion model, which employs toxicity and alignment rewards to further
amplify harmful outputs. Experimental results demonstrate the effectiveness of
RTD, increasing the toxicity rate of LLaVA outputs by 10.69% on the original
attack set and 8.91% on a hold-out set. Moreover, RTD exhibits strong
cross-model transferability, raising the toxicity rate by 5.1% on Gemini and
26.83% on LLaMA. These findings reveal significant deficiencies in existing
alignment strategies, particularly their inability to prevent harmful
continuations. Our work underscores the urgent need for more robust and
adaptive alignment mechanisms to ensure the safe deployment of VLMs in
real-world applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06034v1' target='_blank'>Rank-R1: Enhancing Reasoning in LLM-based Document Rerankers via
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shengyao Zhuang, Xueguang Ma, Bevan Koopman, Jimmy Lin, Guido Zuccon</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-08 03:14:26</h6>
<p class='card-text'>In this paper, we introduce Rank-R1, a novel LLM-based reranker that performs
reasoning over both the user query and candidate documents before performing
the ranking task. Existing document reranking methods based on large language
models (LLMs) typically rely on prompting or fine-tuning LLMs to order or label
candidate documents according to their relevance to a query. For Rank-R1, we
use a reinforcement learning algorithm along with only a small set of relevance
labels (without any reasoning supervision) to enhance the reasoning ability of
LLM-based rerankers. Our hypothesis is that adding reasoning capabilities to
the rerankers can improve their relevance assessement and ranking capabilities.
Our experiments on the TREC DL and BRIGHT datasets show that Rank-R1 is highly
effective, especially for complex queries. In particular, we find that Rank-R1
achieves effectiveness on in-domain datasets at par with that of supervised
fine-tuning methods, but utilizing only 18\% of the training data used by the
fine-tuning methods. We also find that the model largely outperforms zero-shot
and supervised fine-tuning when applied to out-of-domain datasets featuring
complex queries, especially when a 14B-size model is used. Finally, we
qualitatively observe that Rank-R1's reasoning process improves the
explainability of the ranking results, opening new opportunities for search
engine results presentation and fruition.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.05592v1' target='_blank'>R1-Searcher: Incentivizing the Search Capability in LLMs via
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, Ji-Rong Wen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-07 17:14:44</h6>
<p class='card-text'>Existing Large Reasoning Models (LRMs) have shown the potential of
reinforcement learning (RL) to enhance the complex reasoning capabilities of
Large Language Models~(LLMs). While they achieve remarkable performance on
challenging tasks such as mathematics and coding, they often rely on their
internal knowledge to solve problems, which can be inadequate for
time-sensitive or knowledge-intensive questions, leading to inaccuracies and
hallucinations. To address this, we propose \textbf{R1-Searcher}, a novel
two-stage outcome-based RL approach designed to enhance the search capabilities
of LLMs. This method allows LLMs to autonomously invoke external search systems
to access additional knowledge during the reasoning process. Our framework
relies exclusively on RL, without requiring process rewards or distillation for
a cold start. % effectively generalizing to out-of-domain datasets and
supporting both Base and Instruct models. Our experiments demonstrate that our
method significantly outperforms previous strong RAG methods, even when
compared to the closed-source GPT-4o-mini.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04874v1' target='_blank'>Memory Is All You Need: Testing How Model Memory Affects LLM Performance
  in Annotation Tasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Joan C. Timoneda, Sebastián Vallejo Vera</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 16:39:18</h6>
<p class='card-text'>Generative Large Language Models (LLMs) have shown promising results in text
annotation using zero-shot and few-shot learning. Yet these approaches do not
allow the model to retain information from previous annotations, making each
response independent from the preceding ones. This raises the question of
whether model memory -- the LLM having knowledge about its own previous
annotations in the same task -- affects performance. In this article, using
OpenAI's GPT-4o and Meta's Llama 3.1 on two political science datasets, we
demonstrate that allowing the model to retain information about its own
previous classifications yields significant performance improvements: between 5
and 25\% when compared to zero-shot and few-shot learning. Moreover, memory
reinforcement, a novel approach we propose that combines model memory and
reinforcement learning, yields additional performance gains in three out of our
four tests. These findings have important implications for applied researchers
looking to improve performance and efficiency in LLM annotation tasks.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>