<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>LLM-agent - 2025-03-02</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>LLM-agent - 2025-03-02</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20383v1' target='_blank'>Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security
  Analysis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jeffrey Yang Fan Chiang, Seungjae Lee, Jia-Bin Huang, Furong Huang, Yizheng Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 18:56:26</h6>
<p class='card-text'>Recent advancements in Web AI agents have demonstrated remarkable
capabilities in addressing complex web navigation tasks. However, emerging
research shows that these agents exhibit greater vulnerability compared to
standalone Large Language Models (LLMs), despite both being built upon the same
safety-aligned models. This discrepancy is particularly concerning given the
greater flexibility of Web AI Agent compared to standalone LLMs, which may
expose them to a wider range of adversarial user inputs. To build a scaffold
that addresses these concerns, this study investigates the underlying factors
that contribute to the increased vulnerability of Web AI agents. Notably, this
disparity stems from the multifaceted differences between Web AI agents and
standalone LLMs, as well as the complex signals - nuances that simple
evaluation metrics, such as success rate, often fail to capture. To tackle
these challenges, we propose a component-level analysis and a more granular,
systematic evaluation framework. Through this fine-grained investigation, we
identify three critical factors that amplify the vulnerability of Web AI
agents; (1) embedding user goals into the system prompt, (2) multi-step action
generation, and (3) observational capabilities. Our findings highlights the
pressing need to enhance security and robustness in AI agent design and provide
actionable insights for targeted defense strategies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20379v1' target='_blank'>Multi-Agent Verification: Scaling Test-Time Compute with Multiple
  Verifiers</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shalev Lifshitz, Sheila A. McIlraith, Yilun Du</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 18:53:30</h6>
<p class='card-text'>By utilizing more computational resources at test-time, large language models
(LLMs) can improve without additional training. One common strategy uses
verifiers to evaluate candidate outputs. In this work, we propose a novel
scaling dimension for test-time compute: scaling the number of verifiers. We
introduce Multi-Agent Verification (MAV) as a test-time compute paradigm that
combines multiple verifiers to improve performance. We propose using Aspect
Verifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of
outputs, as one possible choice for the verifiers in a MAV system. AVs are a
convenient building block for MAV since they can be easily combined without
additional training. Moreover, we introduce BoN-MAV, a simple multi-agent
verification algorithm that combines best-of-n sampling with multiple
verifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency
and reward model verification, and we demonstrate both weak-to-strong
generalization, where combining weak verifiers improves even stronger LLMs, and
self-improvement, where the same base model is used to both generate and verify
outputs. Our results establish scaling the number of verifiers as a promising
new dimension for improving language model performance at test-time.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20364v1' target='_blank'>Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with
  Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix
  Factorization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ryan C. Barron, Maksim E. Eren, Olga M. Serafimova, Cynthia Matuszek, Boian S. Alexandrov</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 18:35:39</h6>
<p class='card-text'>Agentic Generative AI, powered by Large Language Models (LLMs) with
Retrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores
(VSs), represents a transformative technology applicable to specialized domains
such as legal systems, research, recommender systems, cybersecurity, and global
security, including proliferation research. This technology excels at inferring
relationships within vast unstructured or semi-structured datasets. The legal
domain here comprises complex data characterized by extensive, interrelated,
and semi-structured knowledge systems with complex relations. It comprises
constitutions, statutes, regulations, and case law. Extracting insights and
navigating the intricate networks of legal documents and their relations is
crucial for effective legal research. Here, we introduce a generative AI system
that integrates RAG, VS, and KG, constructed via Non-Negative Matrix
Factorization (NMF), to enhance legal information retrieval and AI reasoning
and minimize hallucinations. In the legal system, these technologies empower AI
agents to identify and analyze complex connections among cases, statutes, and
legal precedents, uncovering hidden relationships and predicting legal
trends-challenging tasks that are essential for ensuring justice and improving
operational efficiency. Our system employs web scraping techniques to
systematically collect legal texts, such as statutes, constitutional
provisions, and case law, from publicly accessible platforms like Justia. It
bridges the gap between traditional keyword-based searches and contextual
understanding by leveraging advanced semantic representations, hierarchical
relationships, and latent topic discovery. This framework supports legal
document clustering, summarization, and cross-referencing, for scalable,
interpretable, and accurate retrieval for semi-structured data while advancing
computational law and AI.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20140v1' target='_blank'>Telephone Surveys Meet Conversational AI: Evaluating a LLM-Based
  Telephone Survey System at Scale</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Max M. Lang, Sol Eskenazi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 14:31:42</h6>
<p class='card-text'>Telephone surveys remain a valuable tool for gathering insights but typically
require substantial resources in training and coordinating human interviewers.
This work presents an AI-driven telephone survey system integrating
text-to-speech (TTS), a large language model (LLM), and speech-to-text (STT)
that mimics the versatility of human-led interviews on scale.
  We tested the system across two populations, a pilot study in the United
States (n = 75) and a large-scale deployment in Peru (n = 2,739), inviting
participants via web-based links and contacting them via direct phone calls.
The AI agent successfully administered open-ended and closed-ended questions,
handled basic clarifications, and dynamically navigated branching logic,
allowing fast large-scale survey deployment without interviewer recruitment or
training.
  Our findings demonstrate that while the AI system's probing for qualitative
depth was more limited than human interviewers, overall data quality approached
human-led standards for structured items. This study represents one of the
first successful large-scale deployments of an LLM-based telephone interviewer
in a real-world survey context. The AI-powered telephone survey system has the
potential for expanding scalable, consistent data collecting across market
research, social science, and public opinion studies, thus improving
operational efficiency while maintaining appropriate data quality for research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20073v1' target='_blank'>Collab-Overcooked: Benchmarking and Evaluating Large Language Models as
  Collaborative Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haochen Sun, Shuwen Zhang, Lei Ren, Hao Xu, Hao Fu, Caixia Yuan, Xiaojie Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 13:31:13</h6>
<p class='card-text'>Large language models (LLMs) based agent systems have made great strides in
real-world applications beyond traditional NLP tasks. This paper proposes a new
LLM-powered Multi-Agent System (LLM-MAS) benchmark, Collab-Overcooked, built on
the popular Overcooked-AI game with more applicable and challenging tasks in
interactive environments. Collab-Overcooked extends existing benchmarks from
two novel perspectives. First, it provides a multi-agent framework supporting
diverse tasks and objectives and encourages collaboration through natural
language communication. Second, it introduces a spectrum of process-oriented
evaluation metrics to assess the fine-grained collaboration capabilities of
different LLM agents, a dimension often overlooked in prior work. We conduct
extensive experiments over 10 popular LLMs and show that, while the LLMs
present a strong ability in goal interpretation, there is a significant
discrepancy in active collaboration and continuous adaption that are critical
for efficiently fulfilling complicated tasks. Notably, we highlight the
strengths and weaknesses in LLM-MAS and provide insights for improving and
evaluating LLM-MAS on a unified and open-sourced benchmark. Environments, 30
open-ended tasks, and an integrated evaluation package are now publicly
available at https://github.com/YusaeMeow/Collab-Overcooked.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19917v1' target='_blank'>Picking the Cream of the Crop: Visual-Centric Data Selection with
  Collaborative Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhenyu Liu, Yunxin Li, Baotian Hu, Wenhan Luo, Yaowei Wang, Min Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 09:37:30</h6>
<p class='card-text'>To improve Multimodal Large Language Models' (MLLMs) ability to process
images and complex instructions, researchers predominantly curate large-scale
visual instruction tuning datasets, which are either sourced from existing
vision tasks or synthetically generated using LLMs and image descriptions.
However, they often suffer from critical flaws, including misaligned
instruction-image pairs and low-quality images. Such issues hinder training
efficiency and limit performance improvements, as models waste resources on
noisy or irrelevant data with minimal benefit to overall capability. To address
this issue, we propose a \textbf{Vi}sual-Centric \textbf{S}election approach
via \textbf{A}gents Collaboration (ViSA), which centers on image quality
assessment and image-instruction relevance evaluation. Specifically, our
approach consists of 1) an image information quantification method via visual
agents collaboration to select images with rich visual information, and 2) a
visual-centric instruction quality assessment method to select high-quality
instruction data related to high-quality images. Finally, we reorganize 80K
instruction data from large open-source datasets. Extensive experiments
demonstrate that ViSA outperforms or is comparable to current state-of-the-art
models on seven benchmarks, using only 2.5\% of the original data, highlighting
the efficiency of our data selection approach. Moreover, we conduct ablation
studies to validate the effectiveness of each component of our method. The code
is available at https://github.com/HITsz-TMG/ViSA.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19860v1' target='_blank'>MIND: Towards Immersive Psychological Healing with Multi-agent Inner
  Dialogue</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yujia Chen, Changsong Li, Yiming Wang, Qingqing Xiao, Nan Zhang, Zifan Kong, Peng Wang, Binyu Yan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 08:04:27</h6>
<p class='card-text'>Mental health issues are worsening in today's competitive society, such as
depression and anxiety. Traditional healings like counseling and chatbots fail
to engage effectively, they often provide generic responses lacking emotional
depth. Although large language models (LLMs) have the potential to create more
human-like interactions, they still struggle to capture subtle emotions. This
requires LLMs to be equipped with human-like adaptability and warmth. To fill
this gap, we propose the MIND (Multi-agent INner Dialogue), a novel paradigm
that provides more immersive psychological healing environments. Considering
the strong generative and role-playing ability of LLM agents, we predefine an
interactive healing framework and assign LLM agents different roles within the
framework to engage in interactive inner dialogues with users, thereby
providing an immersive healing experience. We conduct extensive human
experiments in various real-world healing dimensions, and find that MIND
provides a more user-friendly experience than traditional paradigms. This
demonstrates that MIND effectively leverages the significant potential of LLMs
in psychological healing.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19629v1' target='_blank'>Agentic Mixture-of-Workflows for Multi-Modal Chemical Search</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tiffany J. Callahan, Nathaniel H. Park, Sara Capponi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 23:48:02</h6>
<p class='card-text'>The vast and complex materials design space demands innovative strategies to
integrate multidisciplinary scientific knowledge and optimize materials
discovery. While large language models (LLMs) have demonstrated promising
reasoning and automation capabilities across various domains, their application
in materials science remains limited due to a lack of benchmarking standards
and practical implementation frameworks. To address these challenges, we
introduce Mixture-of-Workflows for Self-Corrective Retrieval-Augmented
Generation (CRAG-MoW) - a novel paradigm that orchestrates multiple agentic
workflows employing distinct CRAG strategies using open-source LLMs. Unlike
prior approaches, CRAG-MoW synthesizes diverse outputs through an orchestration
agent, enabling direct evaluation of multiple LLMs across the same problem
domain. We benchmark CRAG-MoWs across small molecules, polymers, and chemical
reactions, as well as multi-modal nuclear magnetic resonance (NMR) spectral
retrieval. Our results demonstrate that CRAG-MoWs achieve performance
comparable to GPT-4o while being preferred more frequently in comparative
evaluations, highlighting the advantage of structured retrieval and multi-agent
synthesis. By revealing performance variations across data types, CRAG-MoW
provides a scalable, interpretable, and benchmark-driven approach to optimizing
AI architectures for materials discovery. These insights are pivotal in
addressing fundamental gaps in benchmarking LLMs and autonomous AI agents for
scientific applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19622v1' target='_blank'>Weaker LLMs' Opinions Also Matter: Mixture of Opinions Enhances LLM's
  Mathematical Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yanan Chen, Ali Pesaranghader, Tanmana Sadhu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 23:22:02</h6>
<p class='card-text'>Recent advances in Large Language Models (LLMs) have raised interest in their
formal reasoning capabilities, particularly in mathematics. While closed LLMs
like GPT-4 perform well on mathematical benchmarks, e.g., GSM8K, it remains
unclear whether small to medium-sized open LLMs can achieve similar
performance, questioning their reliability. To close this gap, we propose a
post-training approach leveraging a mixture of opinions (MoO) from weaker
ancillary LLMs to enhance a (relatively) stronger LLM's reasoning. For that,
each post-training sample is augmented with Chain-of-Thought (CoT) reasoning
steps and answers from ancillary LLMs, enabling the main LLM to learn from
diverse perspectives. We compare MoO with standard supervised fine-tuning
(SFT), few-shot prompting, and the Mixture of Agents (MoA) method on
mathematical reasoning benchmarks. Our results show that incorporating weaker
LLMs' opinions improves mathematical reasoning by an average of 5%,
highlighting the value of diverse perspectives in reasoning tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19559v1' target='_blank'>Stay Focused: Problem Drift in Multi-Agent Debate</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jonas Becker, Lars Benedikt Kaesberg, Andreas Stephan, Jan Philip Wahle, Terry Ruas, Bela Gipp</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 20:54:51</h6>
<p class='card-text'>Multi-agent debate - multiple instances of large language models discussing
problems in turn-based interaction - has shown promise for solving knowledge
and reasoning tasks. However, these methods show limitations, particularly when
scaling them to longer reasoning chains. In this study, we unveil a new issue
of multi-agent debate: discussions drift away from the initial problem over
multiple turns. We define this phenomenon as problem drift and quantify its
presence across ten tasks (i.e., three generative, three knowledge, three
reasoning, and one instruction-following task). To identify the reasons for
this issue, we perform a human study with eight experts on discussions
suffering from problem drift, who find the most common issues are a lack of
progress (35% of cases), low-quality feedback (26% of cases), and a lack of
clarity (25% of cases). To systematically address the issue of problem drift,
we propose DRIFTJudge, a method based on LLM-as-a-judge, to detect problem
drift at test-time. We further propose DRIFTPolicy, a method to mitigate 31% of
problem drift cases. Our study can be seen as a first step to understanding a
key limitation of multi-agent debate, highlighting pathways for improving their
effectiveness in the future.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19545v1' target='_blank'>Winning Big with Small Models: Knowledge Distillation vs. Self-Training
  for Reducing Hallucination in QA Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ashley Lewis, Michael White, Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 20:34:58</h6>
<p class='card-text'>The deployment of Large Language Models (LLMs) in customer support is
constrained by hallucination-generating false information-and the high cost of
proprietary models. To address these challenges, we propose a
retrieval-augmented question-answering (QA) pipeline and explore how to balance
human input and automation. Using a dataset of questions about a Samsung Smart
TV user manual, we demonstrate that synthetic data generated by LLMs
outperforms crowdsourced data in reducing hallucination in finetuned models. We
also compare self-training (fine-tuning models on their own outputs) and
knowledge distillation (fine-tuning on stronger models' outputs, e.g., GPT-4o),
and find that self-training achieves comparable hallucination reduction. We
conjecture that this surprising finding can be attributed to increased exposure
bias issues in the knowledge distillation case and support this conjecture with
post hoc analysis. We also improve robustness to unanswerable questions and
retrieval failures with contextualized "I don't know" responses. These findings
show that scalable, cost-efficient QA systems can be built using synthetic data
and self-training with open-source models, reducing reliance on proprietary
tools or costly human annotations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19500v1' target='_blank'>Conversational Planning for Personal Plans</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Konstantina Christakopoulou, Iris Qu, John Canny, Andrew Goodridge, Cj Adams, Minmin Chen, Maja MatariÄ‡</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 19:04:26</h6>
<p class='card-text'>The language generation and reasoning capabilities of large language models
(LLMs) have enabled conversational systems with impressive performance in a
variety of tasks, from code generation, to composing essays, to passing STEM
and legal exams, to a new paradigm for knowledge search. Besides those
short-term use applications, LLMs are increasingly used to help with real-life
goals or tasks that take a long time to complete, involving multiple sessions
across days, weeks, months, or even years. Thus to enable conversational
systems for long term interactions and tasks, we need language-based agents
that can plan for long horizons. Traditionally, such capabilities were
addressed by reinforcement learning agents with hierarchical planning
capabilities. In this work, we explore a novel architecture where the LLM acts
as the meta-controller deciding the agent's next macro-action, and tool use
augmented LLM-based option policies execute the selected macro-action. We
instantiate this framework for a specific set of macro-actions enabling
adaptive planning for users' personal plans through conversation and follow-up
questions collecting user feedback. We show how this paradigm can be applicable
in scenarios ranging from tutoring for academic and non-academic tasks to
conversational coaching for personal health plans.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19400v1' target='_blank'>TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem
  Understanding</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Max Ku, Thomas Chong, Jonathan Leung, Krish Shah, Alvin Yu, Wenhu Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 18:50:09</h6>
<p class='card-text'>Understanding domain-specific theorems often requires more than just
text-based reasoning; effective communication through structured visual
explanations is crucial for deeper comprehension. While large language models
(LLMs) demonstrate strong performance in text-based theorem reasoning, their
ability to generate coherent and pedagogically meaningful visual explanations
remains an open challenge. In this work, we introduce TheoremExplainAgent, an
agentic approach for generating long-form theorem explanation videos (over 5
minutes) using Manim animations. To systematically evaluate multimodal theorem
explanations, we propose TheoremExplainBench, a benchmark covering 240 theorems
across multiple STEM disciplines, along with 5 automated evaluation metrics.
Our results reveal that agentic planning is essential for generating detailed
long-form videos, and the o3-mini agent achieves a success rate of 93.8% and an
overall score of 0.77. However, our quantitative and qualitative studies show
that most of the videos produced exhibit minor issues with visual element
layout. Furthermore, multimodal explanations expose deeper reasoning flaws that
text-based explanations fail to reveal, highlighting the importance of
multimodal explanations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19328v1' target='_blank'>Agentic Reward Modeling: Integrating Human Preferences with Verifiable
  Correctness Signals for Reliable Reward Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hao Peng, Yunjia Qi, Xiaozhi Wang, Zijun Yao, Bin Xu, Lei Hou, Juanzi Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 17:19:12</h6>
<p class='card-text'>Reward models (RMs) are crucial for the training and inference-time scaling
up of large language models (LLMs). However, existing reward models primarily
focus on human preferences, neglecting verifiable correctness signals which
have shown strong potential in training LLMs. In this paper, we propose agentic
reward modeling, a reward system that combines reward models with verifiable
correctness signals from different aspects to provide reliable rewards. We
empirically implement a reward agent, named RewardAgent, that combines human
preference rewards with two verifiable signals: factuality and instruction
following, to provide more reliable rewards. We conduct comprehensive
experiments on existing reward model benchmarks and inference time best-of-n
searches on real-world downstream tasks. RewardAgent significantly outperforms
vanilla reward models, demonstrating its effectiveness. We further construct
training preference pairs using RewardAgent and train an LLM with the DPO
objective, achieving superior performance on various NLP benchmarks compared to
conventional reward models. Our codes are publicly released to facilitate
further research (https://github.com/THU-KEG/Agentic-Reward-Modeling).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19298v1' target='_blank'>Agent-centric Information Access</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Evangelos Kanoulas, Panagiotis Eustratiadis, Yongkang Li, Yougang Lyu, Vaishali Pal, Gabrielle Poerwawinata, Jingfen Qiao, Zihan Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 16:56:19</h6>
<p class='card-text'>As large language models (LLMs) become more specialized, we envision a future
where millions of expert LLMs exist, each trained on proprietary data and
excelling in specific domains. In such a system, answering a query requires
selecting a small subset of relevant models, querying them efficiently, and
synthesizing their responses. This paper introduces a framework for
agent-centric information access, where LLMs function as knowledge agents that
are dynamically ranked and queried based on their demonstrated expertise.
Unlike traditional document retrieval, this approach requires inferring
expertise on the fly, rather than relying on static metadata or predefined
model descriptions. This shift introduces several challenges, including
efficient expert selection, cost-effective querying, response aggregation
across multiple models, and robustness against adversarial manipulation. To
address these issues, we propose a scalable evaluation framework that leverages
retrieval-augmented generation and clustering techniques to construct and
assess thousands of specialized models, with the potential to scale toward
millions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19193v1' target='_blank'>Simulation of Language Evolution under Regulated Social Media Platforms:
  A Synergistic Approach of Large Language Models and Genetic Algorithms</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jinyu Cai, Yusei Ishimizu, Mingyue Zhang, Munan Li, Jialong Li, Kenji Tei</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 14:59:27</h6>
<p class='card-text'>Social media platforms frequently impose restrictive policies to moderate
user content, prompting the emergence of creative evasion language strategies.
This paper presents a multi-agent framework based on Large Language Models
(LLMs) to simulate the iterative evolution of language strategies under
regulatory constraints. In this framework, participant agents, as social media
users, continuously evolve their language expression, while supervisory agents
emulate platform-level regulation by assessing policy violations. To achieve a
more faithful simulation, we employ a dual design of language strategies
(constraint and expression) to differentiate conflicting goals and utilize an
LLM-driven GA (Genetic Algorithm) for the selection, mutation, and crossover of
language strategies. The framework is evaluated using two distinct scenarios:
an abstract password game and a realistic simulated illegal pet trade scenario.
Experimental results demonstrate that as the number of dialogue rounds
increases, both the number of uninterrupted dialogue turns and the accuracy of
information transmission improve significantly. Furthermore, a user study with
40 participants validates the real-world relevance of the generated dialogues
and strategies. Moreover, ablation studies validate the importance of the GA,
emphasizing its contribution to long-term adaptability and improved overall
results.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19175v1' target='_blank'>MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic
  Differential Diagnosis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Daniel Rose, Chia-Chien Hung, Marco Lepri, Israa Alqassem, Kiril Gashteovski, Carolin Lawrence</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 14:31:43</h6>
<p class='card-text'>Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical
decision-making, in which physicians iteratively refine a ranked list of
possible diseases based on symptoms, antecedents, and medical knowledge. While
recent advances in large language models have shown promise in supporting DDx,
existing approaches face key limitations, including single-dataset evaluations,
isolated optimization of components, unrealistic assumptions about complete
patient profiles, and single-attempt diagnosis. We introduce a Modular
Explainable DDx Agent (MEDDxAgent) framework designed for interactive DDx,
where diagnostic reasoning evolves through iterative learning, rather than
assuming a complete patient profile is accessible. MEDDxAgent integrates three
modular components: (1) an orchestrator (DDxDriver), (2) a history taking
simulator, and (3) two specialized agents for knowledge retrieval and diagnosis
strategy. To ensure robust evaluation, we introduce a comprehensive DDx
benchmark covering respiratory, skin, and rare diseases. We analyze single-turn
diagnostic approaches and demonstrate the importance of iterative refinement
when patient profiles are not available at the outset. Our broad evaluation
demonstrates that MEDDxAgent achieves over 10% accuracy improvements in
interactive DDx across both large and small LLMs, while offering critical
explainability into its diagnostic reasoning process.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19135v1' target='_blank'>A Temporal Planning Framework for Multi-Agent Systems via LLM-Aided
  Knowledge Base Management</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Enrico Saccon, Ahmet Tikna, Davide De Martini, Edoardo Lamon, Luigi Palopoli, Marco Roveri</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 13:51:28</h6>
<p class='card-text'>This paper presents a novel framework, called PLANTOR (PLanning with Natural
language for Task-Oriented Robots), that integrates Large Language Models
(LLMs) with Prolog-based knowledge management and planning for multi-robot
tasks. The system employs a two-phase generation of a robot-oriented knowledge
base, ensuring reusability and compositional reasoning, as well as a three-step
planning procedure that handles temporal dependencies, resource constraints,
and parallel task execution via mixed-integer linear programming. The final
plan is converted into a Behaviour Tree for direct use in ROS2. We tested the
framework in multi-robot assembly tasks within a block world and an
arch-building scenario. Results demonstrate that LLMs can produce accurate
knowledge bases with modest human feedback, while Prolog guarantees formal
correctness and explainability. This approach underscores the potential of LLM
integration for advanced robotics tasks requiring flexible, scalable, and
human-understandable planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19098v1' target='_blank'>Language-Driven Opinion Dynamics in Agent-Based Simulations with LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Erica Cau, Valentina Pansanella, Dino Pedreschi, Giulio Rossetti</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 12:43:22</h6>
<p class='card-text'>Understanding how opinions evolve is crucial for addressing issues such as
polarization, radicalization, and consensus in social systems. While much
research has focused on identifying factors influencing opinion change, the
role of language and argumentative fallacies remains underexplored. This paper
aims to fill this gap by investigating how language - along with social
dynamics - influences opinion evolution through LODAS, a Language-Driven
Opinion Dynamics Model for Agent-Based Simulations. The model simulates debates
around the "Ship of Theseus" paradox, in which agents with discrete opinions
interact with each other and evolve their opinions by accepting, rejecting, or
ignoring the arguments presented. We study three different scenarios: balanced,
polarized, and unbalanced opinion distributions. Agreeableness and sycophancy
emerge as two main characteristics of LLM agents, and consensus around the
presented statement emerges almost in any setting. Moreover, such AI agents are
often producers of fallacious arguments in the attempt of persuading their
peers and - for their complacency - they are also highly influenced by
arguments built on logical fallacies. These results highlight the potential of
this framework not only for simulating social dynamics but also for exploring
from another perspective biases and shortcomings of LLMs, which may impact
their interactions with humans.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19091v1' target='_blank'>Nexus: A Lightweight and Scalable Multi-Agent Framework for Complex
  Tasks Automation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Humza Sami, Mubashir ul Islam, Samy Charas, Asav Gandhi, Pierre-Emmanuel Gaillardon, Valerio Tenace</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 12:37:47</h6>
<p class='card-text'>Recent advancements in Large Language Models (LLMs) have substantially
evolved Multi-Agent Systems (MASs) capabilities, enabling systems that not only
automate tasks but also leverage near-human reasoning capabilities. To achieve
this, LLM-based MASs need to be built around two critical principles: (i) a
robust architecture that fully exploits LLM potential for specific tasks -- or
related task sets -- and ($ii$) an effective methodology for equipping LLMs
with the necessary capabilities to perform tasks and manage information
efficiently. It goes without saying that a priori architectural designs can
limit the scalability and domain adaptability of a given MAS.
  To address these challenges, in this paper we introduce Nexus: a lightweight
Python framework designed to easily build and manage LLM-based MASs. Nexus
introduces the following innovations: (i) a flexible multi-supervisor
hierarchy, (ii) a simplified workflow design, and (iii) easy installation and
open-source flexibility: Nexus can be installed via pip and is distributed
under a permissive open-source license, allowing users to freely modify and
extend its capabilities.
  Experimental results demonstrate that architectures built with Nexus exhibit
state-of-the-art performance across diverse domains. In coding tasks,
Nexus-driven MASs achieve a 99% pass rate on HumanEval and a flawless 100% on
VerilogEval-Human, outperforming cutting-edge reasoning language models such as
o3-mini and DeepSeek-R1. Moreover, these architectures display robust
proficiency in complex reasoning and mathematical problem solving, achieving
correct solutions for all randomly selected problems from the MATH dataset. In
the realm of multi-objective optimization, Nexus-based architectures
successfully address challenging timing closure tasks on designs from the VTR
benchmark suite, while guaranteeing, on average, a power saving of nearly 30%.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19067v1' target='_blank'>IndicEval-XL: Bridging Linguistic Diversity in Code Generation Across
  Indic Languages</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ujjwal Singh, Aditi Sharma, Nikhil Gupta, Deepakshi, Vivek Kumar Jha</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 11:48:42</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated remarkable capabilities in
code generation from natural language prompts, revolutionizing software
development workflows. As we advance towards agent-based development paradigms,
these models form the cornerstone of next-generation software development
lifecycles. However, current benchmarks for evaluating multilingual code
generation capabilities are predominantly English-centric, limiting their
applicability across the global developer community. To address this
limitation, we present IndicEval-XL, a comprehensive benchmark for code
generation that incorporates 6 major Indic languages, collectively spoken by
approximately 14\% of the world's population. Our benchmark bridges these
languages with 12 programming languages, creating a robust evaluation
framework. This work is particularly significant given India's representation
of one-eighth of the global population and the crucial role Indic languages
play in Indian society. IndicEval-XL represents a significant step toward
expanding the linguistic diversity in code generation systems and evaluation
frameworks. By developing resources that support multiple languages, we aim to
make AI-powered development tools more inclusive and accessible to developers
of various linguistic backgrounds. To facilitate further research and
development in this direction, we make our dataset and evaluation benchmark
publicly available at https://github.com/telekom/IndicEval-XL</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.18881v1' target='_blank'>Letters from Future Self: Augmenting the Letter-Exchange Exercise with
  LLM-based Agents to Enhance Young Adults' Career Exploration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hayeon Jeon, Suhwoo Yoon, Keyeun Lee, Seo Hyeong Kim, Esther Hehsun Kim, Seonghye Cho, Yena Ko, Soeun Yang, Laura Dabbish, John Zimmerman, Eun-mee Kim, Hajin Lim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 06:56:41</h6>
<p class='card-text'>Young adults often encounter challenges in career exploration. Self-guided
interventions, such as the letter-exchange exercise, where participants
envision and adopt the perspective of their future selves by exchanging letters
with their envisioned future selves, can support career development. However,
the broader adoption of such interventions may be limited without structured
guidance. To address this, we integrated Large Language Model (LLM)-based
agents that simulate participants' future selves into the letter-exchange
exercise and evaluated their effectiveness. A one-week experiment (N=36)
compared three conditions: (1) participants manually writing replies to
themselves from the perspective of their future selves (baseline), (2)
future-self agents generating letters to participants, and (3) future-self
agents engaging in chat conversations with participants. Results indicated that
exchanging letters with future-self agents enhanced participants' engagement
during the exercise, while overall benefits of the intervention on future
orientation, career self-concept, and psychological support remained comparable
across conditions. We discuss design implications for AI-augmented
interventions for supporting young adults' career exploration.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.18873v1' target='_blank'>Multi-LLM Collaborative Search for Complex Problem Solving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sen Yang, Yafu Li, Wai Lam, Yu Cheng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 06:31:04</h6>
<p class='card-text'>Large language models (LLMs) often struggle with complex reasoning tasks due
to their limitations in addressing the vast reasoning space and inherent
ambiguities of natural language. We propose the Mixture-of-Search-Agents (MoSA)
paradigm, a novel approach leveraging the collective expertise of multiple LLMs
to enhance search-based reasoning. MoSA integrates diverse reasoning pathways
by combining independent exploration with iterative refinement among LLMs,
mitigating the limitations of single-model approaches. Using Monte Carlo Tree
Search (MCTS) as a backbone, MoSA enables multiple agents to propose and
aggregate reasoning steps, resulting in improved accuracy. Our comprehensive
evaluation across four reasoning benchmarks demonstrates MoSA's consistent
performance improvements over single-agent and other multi-agent baselines,
particularly in complex mathematical and commonsense reasoning tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.18836v1' target='_blank'>REALM-Bench: A Real-World Planning Benchmark for LLMs and Multi-Agent
  Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Longling Geng, Edward Y. Chang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 05:24:22</h6>
<p class='card-text'>This benchmark suite provides a comprehensive evaluation framework for
assessing both individual LLMs and multi-agent systems in real-world planning
scenarios. The suite encompasses eleven designed problems that progress from
basic to highly complex, incorporating key aspects such as multi-agent
coordination, inter-agent dependencies, and dynamic environmental disruptions.
Each problem can be scaled along three dimensions: the number of parallel
planning threads, the complexity of inter-dependencies, and the frequency of
unexpected disruptions requiring real-time adaptation. The benchmark includes
detailed specifications, evaluation metrics, and baseline implementations using
contemporary frameworks like LangGraph, enabling rigorous testing of both
single-agent and multi-agent planning capabilities. Through standardized
evaluation criteria and scalable complexity, this benchmark aims to drive
progress in developing more robust and adaptable AI planning systems for
real-world applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.18822v1' target='_blank'>Data-Efficient Multi-Agent Spatial Planning with LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Huangyuan Su, Aaron Walsman, Daniel Garces, Sham Kakade, Stephanie Gil</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 04:53:07</h6>
<p class='card-text'>In this project, our goal is to determine how to leverage the world-knowledge
of pretrained large language models for efficient and robust learning in
multiagent decision making. We examine this in a taxi routing and assignment
problem where agents must decide how to best pick up passengers in order to
minimize overall waiting time. While this problem is situated on a graphical
road network, we show that with the proper prompting zero-shot performance is
quite strong on this task. Furthermore, with limited fine-tuning along with the
one-at-a-time rollout algorithm for look ahead, LLMs can out-compete existing
approaches with 50 times fewer environmental interactions. We also explore the
benefits of various linguistic prompting approaches and show that including
certain easy-to-compute information in the prompt significantly improves
performance. Finally, we highlight the LLM's built-in semantic understanding,
showing its ability to adapt to environmental factors through simple prompts.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.18770v2' target='_blank'>Reward Shaping to Mitigate Reward Hacking in RLHF</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiayi Fu, Xuandong Zhao, Chengyuan Yao, Heng Wang, Qi Han, Yanghua Xiao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 02:57:59</h6>
<p class='card-text'>Reinforcement Learning from Human Feedback (RLHF) is essential for aligning
large language models (LLMs) with human values. However, RLHF is susceptible to
reward hacking, where the agent exploits flaws in the reward function rather
than learning the intended behavior, thus degrading alignment. While reward
shaping helps stabilize RLHF and partially mitigate reward hacking, a
systematic investigation into shaping techniques and their underlying
principles remains lacking. To bridge this gap, we present a comprehensive
study of the prevalent reward shaping methods. Our analysis suggests three key
design principles: (1) RL reward is ideally bounded, (2) RL benefits from rapid
initial growth followed by gradual convergence, and (3) RL reward is best
formulated as a function of centered reward. Guided by these insights, we
propose Preference As Reward (PAR), a novel approach that leverages the latent
preferences embedded within the reward model itself as the signal for
reinforcement learning. We evaluated PAR on two base models, Gemma2-2B and
Llama3-8B, using two datasets, Ultrafeedback-Binarized and HH-RLHF.
Experimental results demonstrate PAR's superior performance over other reward
shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate at
least 5 percentage points higher than competing approaches. Furthermore, PAR
exhibits remarkable data efficiency, requiring only a single reference reward
for optimal performance, and maintains robustness against reward hacking even
after two full epochs of training. Code is available at
https://github.com/PorUna-byte/PAR.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.18754v1' target='_blank'>AgentSociety Challenge: Designing LLM Agents for User Modeling and
  Recommendation on Web Platforms</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuwei Yan, Yu Shang, Qingbin Zeng, Yu Li, Keyu Zhao, Zhiheng Zheng, Xuefei Ning, Tianji Wu, Shengen Yan, Yu Wang, Fengli Xu, Yong Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 02:10:25</h6>
<p class='card-text'>The AgentSociety Challenge is the first competition in the Web Conference
that aims to explore the potential of Large Language Model (LLM) agents in
modeling user behavior and enhancing recommender systems on web platforms. The
Challenge consists of two tracks: the User Modeling Track and the
Recommendation Track. Participants are tasked to utilize a combined dataset
from Yelp, Amazon, and Goodreads, along with an interactive environment
simulator, to develop innovative LLM agents. The Challenge has attracted 295
teams across the globe and received over 1,400 submissions in total over the
course of 37 official competition days. The participants have achieved 21.9%
and 20.3% performance improvement for Track 1 and Track 2 in the Development
Phase, and 9.1% and 15.9% in the Final Phase, representing a significant
accomplishment. This paper discusses the detailed designs of the Challenge,
analyzes the outcomes, and highlights the most successful LLM agent designs. To
support further research and development, we have open-sourced the benchmark
environment at https://tsinghua-fib-lab.github.io/AgentSocietyChallenge.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.18712v1' target='_blank'>TrajLLM: A Modular LLM-Enhanced Agent-Based Framework for Realistic
  Human Trajectory Simulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chenlu Ju, Jiaxin Liu, Shobhit Sinha, Hao Xue, Flora Salim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 00:13:26</h6>
<p class='card-text'>This work leverages Large Language Models (LLMs) to simulate human mobility,
addressing challenges like high costs and privacy concerns in traditional
models. Our hierarchical framework integrates persona generation, activity
selection, and destination prediction, using real-world demographic and
psychological data to create realistic movement patterns. Both physical models
and language models are employed to explore and demonstrate different
methodologies for human mobility simulation. By structuring data with
summarization and weighted density metrics, the system ensures scalable memory
management while retaining actionable insights. Preliminary results indicate
that LLM-driven simulations align with observed real-world patterns, offering
scalable, interpretable insights for social problems such as urban planning,
traffic management, and public health. The framework's ability to dynamically
generate personas and activities enables it to provide adaptable and realistic
daily routines. This study demonstrates the transformative potential of LLMs in
advancing mobility modeling for societal and urban applications. The source
code and interactive demo for our framework are available at
https://github.com/cju0/TrajLLM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.18702v1' target='_blank'>A Cooperative Multi-Agent Framework for Zero-Shot Named Entity
  Recognition</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zihan Wang, Ziqi Zhao, Yougang Lyu, Zhumin Chen, Maarten de Rijke, Zhaochun Ren</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-25 23:30:43</h6>
<p class='card-text'>Zero-shot named entity recognition (NER) aims to develop entity recognition
systems from unannotated text corpora. This task presents substantial
challenges due to minimal human intervention. Recent work has adapted large
language models (LLMs) for zero-shot NER by crafting specialized prompt
templates. It advances model self-learning abilities by incorporating
self-annotated demonstrations. However, two important challenges persist: (i)
Correlations between contexts surrounding entities are overlooked, leading to
wrong type predictions or entity omissions. (ii) The indiscriminate use of task
demonstrations, retrieved through shallow similarity-based strategies, severely
misleads LLMs during inference.
  In this paper, we introduce the cooperative multi-agent system (CMAS), a
novel framework for zero-shot NER that uses the collective intelligence of
multiple agents to address the challenges outlined above. CMAS has four main
agents: (i) a self-annotator, (ii) a type-related feature (TRF) extractor,
(iii) a demonstration discriminator, and (iv) an overall predictor. To
explicitly capture correlations between contexts surrounding entities, CMAS
reformulates NER into two subtasks: recognizing named entities and identifying
entity type-related features within the target sentence. To enable controllable
utilization of demonstrations, a demonstration discriminator is established to
incorporate the self-reflection mechanism, automatically evaluating helpfulness
scores for the target sentence. Experimental results show that CMAS
significantly improves zero-shot NER performance across six benchmarks,
including both domain-specific and general-domain scenarios. Furthermore, CMAS
demonstrates its effectiveness in few-shot settings and with various LLM
backbones.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.18690v1' target='_blank'>Hybrid Voting-Based Task Assignment in Role-Playing Games</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Daniel Weiner, Raj Korpan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-25 22:58:21</h6>
<p class='card-text'>In role-playing games (RPGs), the level of immersion is critical-especially
when an in-game agent conveys tasks, hints, or ideas to the player. For an
agent to accurately interpret the player's emotional state and contextual
nuances, a foundational level of understanding is required, which can be
achieved using a Large Language Model (LLM). Maintaining the LLM's focus across
multiple context changes, however, necessitates a more robust approach, such as
integrating the LLM with a dedicated task allocation model to guide its
performance throughout gameplay. In response to this need, we introduce
Voting-Based Task Assignment (VBTA), a framework inspired by human reasoning in
task allocation and completion. VBTA assigns capability profiles to agents and
task descriptions to tasks, then generates a suitability matrix that quantifies
the alignment between an agent's abilities and a task's requirements.
Leveraging six distinct voting methods, a pre-trained LLM, and integrating
conflict-based search (CBS) for path planning, VBTA efficiently identifies and
assigns the most suitable agent to each task. While existing approaches focus
on generating individual aspects of gameplay, such as single quests, or combat
encounters, our method shows promise when generating both unique combat
encounters and narratives because of its generalizable nature.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>