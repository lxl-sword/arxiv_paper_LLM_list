<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>LLM-agent - 2025-03-03</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>LLM-agent - 2025-03-03</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.21208v1' target='_blank'>ARIES: Autonomous Reasoning with LLMs on Interactive Thought Graph
  Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pedro Gimenes, Zeyu Cao, Jeffrey Wong, Yiren Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-28 16:28:13</h6>
<p class='card-text'>Recent research has shown that LLM performance on reasoning tasks can be
enhanced by scaling test-time compute. One promising approach, particularly
with decomposable problems, involves arranging intermediate solutions as a
graph on which transformations are performed to explore the solution space.
However, prior works rely on pre-determined, task-specific transformation
schedules which are subject to a set of searched hyperparameters. In this work,
we view thought graph transformations as actions in a Markov decision process,
and implement policy agents to drive effective action policies for the
underlying reasoning LLM agent. In particular, we investigate the ability for
another LLM to act as a policy agent on thought graph environments and
introduce ARIES, a multi-agent architecture for reasoning with LLMs. In ARIES,
reasoning LLM agents solve decomposed subproblems, while policy LLM agents
maintain visibility of the thought graph states, and dynamically adapt the
problem-solving strategy. Through extensive experiments, we observe that using
off-the-shelf LLMs as policy agents with no supervised fine-tuning (SFT) can
yield up to $29\%$ higher accuracy on HumanEval relative to static
transformation schedules, as well as reducing inference costs by $35\%$ and
avoid any search requirements. We also conduct a thorough analysis of observed
failure modes, highlighting that limitations on LLM sizes and the depth of
problem decomposition can be seen as challenges to scaling LLM-guided
reasoning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.21087v1' target='_blank'>PASemiQA: Plan-Assisted Agent for Question Answering on Semi-Structured
  Data with Text and Relational Information</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hansi Yang, Qi Zhang, Wei Jiang, Jianguo Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-28 14:26:47</h6>
<p class='card-text'>Large language models (LLMs) have shown impressive abilities in answering
questions across various domains, but they often encounter hallucination issues
on questions that require professional and up-to-date knowledge. To address
this limitation, retrieval-augmented generation (RAG) techniques have been
proposed, which retrieve relevant information from external sources to inform
their responses. However, existing RAG methods typically focus on a single type
of external data, such as vectorized text database or knowledge graphs, and
cannot well handle real-world questions on semi-structured data containing both
text and relational information. To bridge this gap, we introduce PASemiQA, a
novel approach that jointly leverages text and relational information in
semi-structured data to answer questions. PASemiQA first generates a plan to
identify relevant text and relational information to answer the question in
semi-structured data, and then uses an LLM agent to traverse the
semi-structured data and extract necessary information. Our empirical results
demonstrate the effectiveness of PASemiQA across different semi-structured
datasets from various domains, showcasing its potential to improve the accuracy
and reliability of question answering systems on semi-structured data.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.21037v1' target='_blank'>The amplifier effect of artificial agents in social contagion</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Eric Hitz, Mingmin Feng, Radu Tanase, Ren√© Algesheimer, Manuel S. Mariani</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-28 13:29:52</h6>
<p class='card-text'>Recent advances in artificial intelligence have led to the proliferation of
artificial agents in social contexts, ranging from education to online social
media and financial markets, among many others. The increasing rate at which
artificial and human agents interact makes it urgent to understand the
consequences of human-machine interactions for the propagation of new ideas,
products, and behaviors in society. Across two distinct empirical contexts, we
find here that artificial agents lead to significantly faster and wider social
contagion. To this end, we replicate a choice experiment previously conducted
with human subjects by using artificial agents powered by large language models
(LLMs). We use the experiment's results to measure the adoption thresholds of
artificial agents and their impact on the spread of social contagion. We find
that artificial agents tend to exhibit lower adoption thresholds than humans,
which leads to wider network-based social contagions. Our findings suggest that
the increased presence of artificial agents in real-world networks may
accelerate behavioral shifts, potentially in unforeseen ways.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20963v1' target='_blank'>Retrieval Augmented Generation for Topic Modeling in Organizational
  Research: An Introduction with Empirical Demonstration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Gerion Spielberger, Florian Artinger, Jochen Reb, Rudolf Kerschreiter</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-28 11:25:11</h6>
<p class='card-text'>Analyzing textual data is the cornerstone of qualitative research. While
traditional methods such as grounded theory and content analysis are widely
used, they are labor-intensive and time-consuming. Topic modeling offers an
automated complement. Yet, existing approaches, including LLM-based topic
modeling, still struggle with issues such as high data preprocessing
requirements, interpretability, and reliability. This paper introduces Agentic
Retrieval-Augmented Generation (Agentic RAG) as a method for topic modeling
with LLMs. It integrates three key components: (1) retrieval, enabling
automatized access to external data beyond an LLM's pre-trained knowledge; (2)
generation, leveraging LLM capabilities for text synthesis; and (3)
agent-driven learning, iteratively refining retrieval and query formulation
processes. To empirically validate Agentic RAG for topic modeling, we reanalyze
a Twitter/X dataset, previously examined by Mu et al. (2024a). Our findings
demonstrate that the approach is more efficient, interpretable and at the same
time achieves higher reliability and validity in comparison to the standard
machine learning approach but also in comparison to LLM prompting for topic
modeling. These results highlight Agentic RAG's ability to generate
semantically relevant and reproducible topics, positioning it as a robust,
scalable, and transparent alternative for AI-driven qualitative research in
leadership, managerial, and organizational research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20859v1' target='_blank'>The Power of Personality: A Human Simulation Perspective to Investigate
  Large Language Model Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yifan Duan, Yihong Tang, Xuefeng Bai, Kehai Chen, Juntao Li, Min Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-28 09:01:39</h6>
<p class='card-text'>Large language models (LLMs) excel in both closed tasks (including
problem-solving, and code generation) and open tasks (including creative
writing), yet existing explanations for their capabilities lack connections to
real-world human intelligence. To fill this gap, this paper systematically
investigates LLM intelligence through the lens of ``human simulation'',
addressing three core questions: (1) How do personality traits affect
problem-solving in closed tasks? (2) How do traits shape creativity in open
tasks? (3) How does single-agent performance influence multi-agent
collaboration? By assigning Big Five personality traits to LLM agents and
evaluating their performance in single- and multi-agent settings, we reveal
that specific traits significantly influence reasoning accuracy (closed tasks)
and creative output (open tasks). Furthermore, multi-agent systems exhibit
collective intelligence distinct from individual capabilities, driven by
distinguishing combinations of personalities. We demonstrate that LLMs
inherently simulate human behavior through next-token prediction, mirroring
human language, decision-making, and collaborative dynamics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20807v1' target='_blank'>Digital Player: Evaluating Large Language Models based Human-like Agent
  in Games</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiawei Wang, Kai Wang, Shaojie Lin, Runze Wu, Bihan Xu, Lingeng Jiang, Shiwei Zhao, Renyu Zhu, Haoyu Liu, Zhipeng Hu, Zhong Fan, Le Li, Tangjie Lyu, Changjie Fan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-28 07:46:55</h6>
<p class='card-text'>With the rapid advancement of Large Language Models (LLMs), LLM-based
autonomous agents have shown the potential to function as digital employees,
such as digital analysts, teachers, and programmers. In this paper, we develop
an application-level testbed based on the open-source strategy game "Unciv",
which has millions of active players, to enable researchers to build a "data
flywheel" for studying human-like agents in the "digital players" task. This
"Civilization"-like game features expansive decision-making spaces along with
rich linguistic interactions such as diplomatic negotiations and acts of
deception, posing significant challenges for LLM-based agents in terms of
numerical reasoning and long-term planning. Another challenge for "digital
players" is to generate human-like responses for social interaction,
collaboration, and negotiation with human players. The open-source project can
be found at https:/github.com/fuxiAIlab/CivAgent.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20791v1' target='_blank'>Cyber Defense Reinvented: Large Language Models as Threat Intelligence
  Copilots</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaoqun Liu, Jiacheng Liang, Qiben Yan, Muchao Ye, Jinyuan Jia, Zhaohan Xi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-28 07:16:09</h6>
<p class='card-text'>The exponential growth of cyber threat knowledge, exemplified by the
expansion of databases such as MITRE-CVE and NVD, poses significant challenges
for cyber threat analysis. Security professionals are increasingly burdened by
the sheer volume and complexity of information, creating an urgent need for
effective tools to navigate, synthesize, and act on large-scale data to counter
evolving threats proactively. However, conventional threat intelligence tools
often fail to scale with the dynamic nature of this data and lack the
adaptability to support diverse threat intelligence tasks.
  In this work, we introduce CYLENS, a cyber threat intelligence copilot
powered by large language models (LLMs). CYLENS is designed to assist security
professionals throughout the entire threat management lifecycle, supporting
threat attribution, contextualization, detection, correlation, prioritization,
and remediation. To ensure domain expertise, CYLENS integrates knowledge from
271,570 threat reports into its model parameters and incorporates six
specialized NLP modules to enhance reasoning capabilities. Furthermore, CYLENS
can be customized to meet the unique needs of different or ganizations,
underscoring its adaptability. Through extensive evaluations, we demonstrate
that CYLENS consistently outperforms industry-leading LLMs and state-of-the-art
cybersecurity agents. By detailing its design, development, and evaluation,
this work provides a blueprint for leveraging LLMs to address complex,
data-intensive cybersecurity challenges.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20757v1' target='_blank'>The Rise of Darkness: Safety-Utility Trade-Offs in Role-Playing Dialogue
  Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yihong Tang, Kehai Chen, Xuefeng Bai, Zhengyu Niu, Bo Wang, Jie Liu, Min Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-28 06:18:50</h6>
<p class='card-text'>Large Language Models (LLMs) have made remarkable advances in role-playing
dialogue agents, demonstrating their utility in character simulations. However,
it remains challenging for these agents to balance character portrayal utility
with content safety because this essential character simulation often comes
with the risk of generating unsafe content. To address this issue, we first
conduct a systematic exploration of the safety-utility trade-off across
multiple LLMs. Our analysis reveals that risk scenarios created by villain
characters and user queries (referred to as risk coupling) contribute to this
trade-off. Building on this, we propose a novel Adaptive Dynamic
Multi-Preference (ADMP) method, which dynamically adjusts safety-utility
preferences based on the degree of risk coupling and guides the model to
generate responses biased toward utility or safety. We further introduce
Coupling Margin Sampling (CMS) into coupling detection to enhance the model's
ability to handle high-risk scenarios. Experimental results demonstrate that
our approach improves safety metrics while maintaining utility.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20689v1' target='_blank'>ProAI: Proactive Multi-Agent Conversational AI with Structured Knowledge
  Base for Psychiatric Diagnosis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuqi Wu, Guangya Wan, Jingjing Li, Shengming Zhao, Lingfeng Ma, Tianyi Ye, Ion Pop, Yanbo Zhang, Jie Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-28 03:45:39</h6>
<p class='card-text'>Most LLM-driven conversational AI systems operate reactively, responding to
user prompts without guiding the interaction. Most LLM-driven conversational AI
systems operate reactively, responding to user prompts without guiding the
interaction. However, many real-world applications-such as psychiatric
diagnosis, consulting, and interviews-require AI to take a proactive role,
asking the right questions and steering conversations toward specific
objectives. Using mental health differential diagnosis as an application
context, we introduce ProAI, a goal-oriented, proactive conversational AI
framework. ProAI integrates structured knowledge-guided memory, multi-agent
proactive reasoning, and a multi-faceted evaluation strategy, enabling LLMs to
engage in clinician-style diagnostic reasoning rather than simple response
generation. Through simulated patient interactions, user experience assessment,
and professional clinical validation, we demonstrate that ProAI achieves up to
83.3% accuracy in mental disorder differential diagnosis while maintaining
professional and empathetic interaction standards. These results highlight the
potential for more reliable, adaptive, and goal-driven AI diagnostic
assistants, advancing LLMs beyond reactive dialogue systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20592v1' target='_blank'>Multi$^2$: Multi-Agent Test-Time Scalable Framework for Multi-Document
  Processing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Juntai Cao, Xiang Zhang, Raymond Li, Chuyuan Li, Shafiq Joty, Giuseppe Carenini</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 23:34:47</h6>
<p class='card-text'>Recent advances in test-time scaling have shown promising results in
improving Large Language Models (LLMs) performance through strategic
computation allocation during inference. While this approach has demonstrated
strong performance improvements in logical and mathematical reasoning tasks,
its application to natural language generation (NLG), especially summarization,
has yet to be explored. Multi-Document Summarization (MDS) is a challenging
task that focuses on extracting and synthesizing useful information from
multiple lengthy documents. Unlike reasoning tasks, MDS requires a more nuanced
approach to prompt design and ensemble, as there is no "best" prompt to satisfy
diverse summarization requirements. To address this, we propose a novel
framework that leverages inference-time scaling for this task. Precisely, we
take prompt ensemble approach by leveraging various prompt to first generate
candidate summaries and then ensemble them with an aggregator to produce a
refined summary. We also introduce two new evaluation metrics:
Consistency-Aware Preference (CAP) score and LLM Atom-Content-Unit (ACU) score,
to enhance LLM's contextual understanding while mitigating its positional bias.
Extensive experiments demonstrate the effectiveness of our approach in
improving summary quality while identifying and analyzing the scaling
boundaries in summarization tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20527v1' target='_blank'>Supervised Fine-Tuning LLMs to Behave as Pedagogical Agents in
  Programming Education</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Emily Ross, Yuval Kansal, Jake Renzella, Alexandra Vassar, Andrew Taylor</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 21:23:56</h6>
<p class='card-text'>Large language models (LLMs) are increasingly being explored in higher
education, yet their effectiveness as teaching agents remains underexamined. In
this paper, we present the development of GuideLM, a fine-tuned LLM designed
for programming education. GuideLM has been integrated into the Debugging C
Compiler (DCC), an educational C compiler that leverages LLMs to generate
pedagogically sound error explanations. Previously, DCC relied on off-the-shelf
OpenAI models, which, while accurate, often over-assisted students by directly
providing solutions despite contrary prompting.
  To address this, we employed supervised fine-tuning (SFT) on a dataset of 528
student-question/teacher-answer pairs, creating two models: GuideLM and
GuideLM-mini, fine-tuned on ChatGPT-4o and 4o-mini, respectively. We conducted
an expert analysis of 400 responses per model, comparing their pedagogical
effectiveness against base OpenAI models. Our evaluation, grounded in
constructivism and cognitive load theory, assessed factors such as conceptual
scaffolding, clarity, and Socratic guidance.
  Results indicate that GuideLM and GuideLM-mini improve pedagogical
performance, with an 8% increase in Socratic guidance and a 58% improvement in
economy of words compared to GPT-4o. However, this refinement comes at the cost
of a slight reduction in general accuracy. While further work is needed, our
findings suggest that fine-tuning LLMs with targeted datasets is a promising
approach for developing models better suited to educational contexts.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20513v1' target='_blank'>Personas Evolved: Designing Ethical LLM-Based Conversational Agent
  Personalities</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Smit Desai, Mateusz Dubiel, Nima Zargham, Thomas Mildner, Laura Spillner</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 20:46:54</h6>
<p class='card-text'>The emergence of Large Language Models (LLMs) has revolutionized
Conversational User Interfaces (CUIs), enabling more dynamic, context-aware,
and human-like interactions across diverse domains, from social sciences to
healthcare. However, the rapid adoption of LLM-based personas raises critical
ethical and practical concerns, including bias, manipulation, and unforeseen
social consequences. Unlike traditional CUIs, where personas are carefully
designed with clear intent, LLM-based personas generate responses dynamically
from vast datasets, making their behavior less predictable and harder to
govern. This workshop aims to bridge the gap between CUI and broader AI
communities by fostering a cross-disciplinary dialogue on the responsible
design and evaluation of LLM-based personas. Bringing together researchers,
designers, and practitioners, we will explore best practices, develop ethical
guidelines, and promote frameworks that ensure transparency, inclusivity, and
user-centered interactions. By addressing these challenges collaboratively, we
seek to shape the future of LLM-driven CUIs in ways that align with societal
values and expectations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20508v1' target='_blank'>TripCraft: A Benchmark for Spatio-Temporally Fine Grained Travel
  Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Soumyabrata Chaudhuri, Pranav Purkar, Ritwik Raghav, Shubhojit Mallick, Manish Gupta, Abhik Jana, Shreya Ghosh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 20:33:28</h6>
<p class='card-text'>Recent advancements in probing Large Language Models (LLMs) have explored
their latent potential as personalized travel planning agents, yet existing
benchmarks remain limited in real world applicability. Existing datasets, such
as TravelPlanner and TravelPlanner+, suffer from semi synthetic data reliance,
spatial inconsistencies, and a lack of key travel constraints, making them
inadequate for practical itinerary generation. To address these gaps, we
introduce TripCraft, a spatiotemporally coherent travel planning dataset that
integrates real world constraints, including public transit schedules, event
availability, diverse attraction categories, and user personas for enhanced
personalization. To evaluate LLM generated plans beyond existing binary
validation methods, we propose five continuous evaluation metrics, namely
Temporal Meal Score, Temporal Attraction Score, Spatial Score, Ordering Score,
and Persona Score which assess itinerary quality across multiple dimensions.
Our parameter informed setting significantly enhances meal scheduling,
improving the Temporal Meal Score from 61% to 80% in a 7 day scenario.
TripCraft establishes a new benchmark for LLM driven personalized travel
planning, offering a more realistic, constraint aware framework for itinerary
generation. Dataset and Codebase will be made publicly available upon
acceptance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20504v1' target='_blank'>A Thousand Words or An Image: Studying the Influence of Persona Modality
  in Multimodal LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Julius Broomfield, Kartik Sharma, Srijan Kumar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 20:25:00</h6>
<p class='card-text'>Large language models (LLMs) have recently demonstrated remarkable
advancements in embodying diverse personas, enhancing their effectiveness as
conversational agents and virtual assistants. Consequently, LLMs have made
significant strides in processing and integrating multimodal information.
However, even though human personas can be expressed in both text and image,
the extent to which the modality of a persona impacts the embodiment by the LLM
remains largely unexplored. In this paper, we investigate how do different
modalities influence the expressiveness of personas in multimodal LLMs. To this
end, we create a novel modality-parallel dataset of 40 diverse personas varying
in age, gender, occupation, and location. This consists of four modalities to
equivalently represent a persona: image-only, text-only, a combination of image
and small text, and typographical images, where text is visually stylized to
convey persona-related attributes. We then create a systematic evaluation
framework with 60 questions and corresponding metrics to assess how well LLMs
embody each persona across its attributes and scenarios. Comprehensive
experiments on $5$ multimodal LLMs show that personas represented by detailed
text show more linguistic habits, while typographical images often show more
consistency with the persona. Our results reveal that LLMs often overlook
persona-specific details conveyed through images, highlighting underlying
limitations and paving the way for future research to bridge this gap. We
release the data and code at https://github.com/claws-lab/persona-modality .</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20432v1' target='_blank'>Large Language Model Strategic Reasoning Evaluation through Behavioral
  Game Theory</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jingru Jia, Zehua Yuan, Junhao Pan, Paul E. McNamara, Deming Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 18:58:31</h6>
<p class='card-text'>Strategic decision-making involves interactive reasoning where agents adapt
their choices in response to others, yet existing evaluations of large language
models (LLMs) often emphasize Nash Equilibrium (NE) approximation, overlooking
the mechanisms driving their strategic choices. To bridge this gap, we
introduce an evaluation framework grounded in behavioral game theory,
disentangling reasoning capability from contextual effects. Testing 22
state-of-the-art LLMs, we find that GPT-o3-mini, GPT-o1, and DeepSeek-R1
dominate most games yet also demonstrate that the model scale alone does not
determine performance. In terms of prompting enhancement, Chain-of-Thought
(CoT) prompting is not universally effective, as it increases strategic
reasoning only for models at certain levels while providing limited gains
elsewhere. Additionally, we investigate the impact of encoded demographic
features on the models, observing that certain assignments impact the
decision-making pattern. For instance, GPT-4o shows stronger strategic
reasoning with female traits than males, while Gemma assigns higher reasoning
levels to heterosexual identities compared to other sexual orientations,
indicating inherent biases. These findings underscore the need for ethical
standards and contextual alignment to balance improved reasoning with fairness.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20383v1' target='_blank'>Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security
  Analysis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jeffrey Yang Fan Chiang, Seungjae Lee, Jia-Bin Huang, Furong Huang, Yizheng Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 18:56:26</h6>
<p class='card-text'>Recent advancements in Web AI agents have demonstrated remarkable
capabilities in addressing complex web navigation tasks. However, emerging
research shows that these agents exhibit greater vulnerability compared to
standalone Large Language Models (LLMs), despite both being built upon the same
safety-aligned models. This discrepancy is particularly concerning given the
greater flexibility of Web AI Agent compared to standalone LLMs, which may
expose them to a wider range of adversarial user inputs. To build a scaffold
that addresses these concerns, this study investigates the underlying factors
that contribute to the increased vulnerability of Web AI agents. Notably, this
disparity stems from the multifaceted differences between Web AI agents and
standalone LLMs, as well as the complex signals - nuances that simple
evaluation metrics, such as success rate, often fail to capture. To tackle
these challenges, we propose a component-level analysis and a more granular,
systematic evaluation framework. Through this fine-grained investigation, we
identify three critical factors that amplify the vulnerability of Web AI
agents; (1) embedding user goals into the system prompt, (2) multi-step action
generation, and (3) observational capabilities. Our findings highlights the
pressing need to enhance security and robustness in AI agent design and provide
actionable insights for targeted defense strategies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20379v1' target='_blank'>Multi-Agent Verification: Scaling Test-Time Compute with Multiple
  Verifiers</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shalev Lifshitz, Sheila A. McIlraith, Yilun Du</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 18:53:30</h6>
<p class='card-text'>By utilizing more computational resources at test-time, large language models
(LLMs) can improve without additional training. One common strategy uses
verifiers to evaluate candidate outputs. In this work, we propose a novel
scaling dimension for test-time compute: scaling the number of verifiers. We
introduce Multi-Agent Verification (MAV) as a test-time compute paradigm that
combines multiple verifiers to improve performance. We propose using Aspect
Verifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of
outputs, as one possible choice for the verifiers in a MAV system. AVs are a
convenient building block for MAV since they can be easily combined without
additional training. Moreover, we introduce BoN-MAV, a simple multi-agent
verification algorithm that combines best-of-n sampling with multiple
verifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency
and reward model verification, and we demonstrate both weak-to-strong
generalization, where combining weak verifiers improves even stronger LLMs, and
self-improvement, where the same base model is used to both generate and verify
outputs. Our results establish scaling the number of verifiers as a promising
new dimension for improving language model performance at test-time.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20364v1' target='_blank'>Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with
  Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix
  Factorization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ryan C. Barron, Maksim E. Eren, Olga M. Serafimova, Cynthia Matuszek, Boian S. Alexandrov</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 18:35:39</h6>
<p class='card-text'>Agentic Generative AI, powered by Large Language Models (LLMs) with
Retrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores
(VSs), represents a transformative technology applicable to specialized domains
such as legal systems, research, recommender systems, cybersecurity, and global
security, including proliferation research. This technology excels at inferring
relationships within vast unstructured or semi-structured datasets. The legal
domain here comprises complex data characterized by extensive, interrelated,
and semi-structured knowledge systems with complex relations. It comprises
constitutions, statutes, regulations, and case law. Extracting insights and
navigating the intricate networks of legal documents and their relations is
crucial for effective legal research. Here, we introduce a generative AI system
that integrates RAG, VS, and KG, constructed via Non-Negative Matrix
Factorization (NMF), to enhance legal information retrieval and AI reasoning
and minimize hallucinations. In the legal system, these technologies empower AI
agents to identify and analyze complex connections among cases, statutes, and
legal precedents, uncovering hidden relationships and predicting legal
trends-challenging tasks that are essential for ensuring justice and improving
operational efficiency. Our system employs web scraping techniques to
systematically collect legal texts, such as statutes, constitutional
provisions, and case law, from publicly accessible platforms like Justia. It
bridges the gap between traditional keyword-based searches and contextual
understanding by leveraging advanced semantic representations, hierarchical
relationships, and latent topic discovery. This framework supports legal
document clustering, summarization, and cross-referencing, for scalable,
interpretable, and accurate retrieval for semi-structured data while advancing
computational law and AI.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20140v1' target='_blank'>Telephone Surveys Meet Conversational AI: Evaluating a LLM-Based
  Telephone Survey System at Scale</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Max M. Lang, Sol Eskenazi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 14:31:42</h6>
<p class='card-text'>Telephone surveys remain a valuable tool for gathering insights but typically
require substantial resources in training and coordinating human interviewers.
This work presents an AI-driven telephone survey system integrating
text-to-speech (TTS), a large language model (LLM), and speech-to-text (STT)
that mimics the versatility of human-led interviews on scale.
  We tested the system across two populations, a pilot study in the United
States (n = 75) and a large-scale deployment in Peru (n = 2,739), inviting
participants via web-based links and contacting them via direct phone calls.
The AI agent successfully administered open-ended and closed-ended questions,
handled basic clarifications, and dynamically navigated branching logic,
allowing fast large-scale survey deployment without interviewer recruitment or
training.
  Our findings demonstrate that while the AI system's probing for qualitative
depth was more limited than human interviewers, overall data quality approached
human-led standards for structured items. This study represents one of the
first successful large-scale deployments of an LLM-based telephone interviewer
in a real-world survey context. The AI-powered telephone survey system has the
potential for expanding scalable, consistent data collecting across market
research, social science, and public opinion studies, thus improving
operational efficiency while maintaining appropriate data quality for research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20073v1' target='_blank'>Collab-Overcooked: Benchmarking and Evaluating Large Language Models as
  Collaborative Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haochen Sun, Shuwen Zhang, Lei Ren, Hao Xu, Hao Fu, Caixia Yuan, Xiaojie Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 13:31:13</h6>
<p class='card-text'>Large language models (LLMs) based agent systems have made great strides in
real-world applications beyond traditional NLP tasks. This paper proposes a new
LLM-powered Multi-Agent System (LLM-MAS) benchmark, Collab-Overcooked, built on
the popular Overcooked-AI game with more applicable and challenging tasks in
interactive environments. Collab-Overcooked extends existing benchmarks from
two novel perspectives. First, it provides a multi-agent framework supporting
diverse tasks and objectives and encourages collaboration through natural
language communication. Second, it introduces a spectrum of process-oriented
evaluation metrics to assess the fine-grained collaboration capabilities of
different LLM agents, a dimension often overlooked in prior work. We conduct
extensive experiments over 10 popular LLMs and show that, while the LLMs
present a strong ability in goal interpretation, there is a significant
discrepancy in active collaboration and continuous adaption that are critical
for efficiently fulfilling complicated tasks. Notably, we highlight the
strengths and weaknesses in LLM-MAS and provide insights for improving and
evaluating LLM-MAS on a unified and open-sourced benchmark. Environments, 30
open-ended tasks, and an integrated evaluation package are now publicly
available at https://github.com/YusaeMeow/Collab-Overcooked.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20426v1' target='_blank'>Among Them: A game-based framework for assessing persuasion capabilities
  of LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mateusz Idziejczak, Vasyl Korzavatykh, Mateusz Stawicki, Andrii Chmutov, Marcin Korcz, Iwo B≈ÇƒÖdek, Dariusz Brzezinski</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 12:26:21</h6>
<p class='card-text'>The proliferation of large language models (LLMs) and autonomous AI agents
has raised concerns about their potential for automated persuasion and social
influence. While existing research has explored isolated instances of LLM-based
manipulation, systematic evaluations of persuasion capabilities across
different models remain limited. In this paper, we present an Among Us-inspired
game framework for assessing LLM deception skills in a controlled environment.
The proposed framework makes it possible to compare LLM models by game
statistics, as well as quantify in-game manipulation according to 25 persuasion
strategies from social psychology and rhetoric. Experiments between 8 popular
language models of different types and sizes demonstrate that all tested models
exhibit persuasive capabilities, successfully employing 22 of the 25
anticipated techniques. We also find that larger models do not provide any
persuasion advantage over smaller models and that longer model outputs are
negatively correlated with the number of games won. Our study provides insights
into the deception capabilities of LLMs, as well as tools and data for
fostering future research on the topic.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19917v1' target='_blank'>Picking the Cream of the Crop: Visual-Centric Data Selection with
  Collaborative Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhenyu Liu, Yunxin Li, Baotian Hu, Wenhan Luo, Yaowei Wang, Min Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 09:37:30</h6>
<p class='card-text'>To improve Multimodal Large Language Models' (MLLMs) ability to process
images and complex instructions, researchers predominantly curate large-scale
visual instruction tuning datasets, which are either sourced from existing
vision tasks or synthetically generated using LLMs and image descriptions.
However, they often suffer from critical flaws, including misaligned
instruction-image pairs and low-quality images. Such issues hinder training
efficiency and limit performance improvements, as models waste resources on
noisy or irrelevant data with minimal benefit to overall capability. To address
this issue, we propose a \textbf{Vi}sual-Centric \textbf{S}election approach
via \textbf{A}gents Collaboration (ViSA), which centers on image quality
assessment and image-instruction relevance evaluation. Specifically, our
approach consists of 1) an image information quantification method via visual
agents collaboration to select images with rich visual information, and 2) a
visual-centric instruction quality assessment method to select high-quality
instruction data related to high-quality images. Finally, we reorganize 80K
instruction data from large open-source datasets. Extensive experiments
demonstrate that ViSA outperforms or is comparable to current state-of-the-art
models on seven benchmarks, using only 2.5\% of the original data, highlighting
the efficiency of our data selection approach. Moreover, we conduct ablation
studies to validate the effectiveness of each component of our method. The code
is available at https://github.com/HITsz-TMG/ViSA.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19860v1' target='_blank'>MIND: Towards Immersive Psychological Healing with Multi-agent Inner
  Dialogue</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yujia Chen, Changsong Li, Yiming Wang, Qingqing Xiao, Nan Zhang, Zifan Kong, Peng Wang, Binyu Yan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 08:04:27</h6>
<p class='card-text'>Mental health issues are worsening in today's competitive society, such as
depression and anxiety. Traditional healings like counseling and chatbots fail
to engage effectively, they often provide generic responses lacking emotional
depth. Although large language models (LLMs) have the potential to create more
human-like interactions, they still struggle to capture subtle emotions. This
requires LLMs to be equipped with human-like adaptability and warmth. To fill
this gap, we propose the MIND (Multi-agent INner Dialogue), a novel paradigm
that provides more immersive psychological healing environments. Considering
the strong generative and role-playing ability of LLM agents, we predefine an
interactive healing framework and assign LLM agents different roles within the
framework to engage in interactive inner dialogues with users, thereby
providing an immersive healing experience. We conduct extensive human
experiments in various real-world healing dimensions, and find that MIND
provides a more user-friendly experience than traditional paradigms. This
demonstrates that MIND effectively leverages the significant potential of LLMs
in psychological healing.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19629v1' target='_blank'>Agentic Mixture-of-Workflows for Multi-Modal Chemical Search</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tiffany J. Callahan, Nathaniel H. Park, Sara Capponi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 23:48:02</h6>
<p class='card-text'>The vast and complex materials design space demands innovative strategies to
integrate multidisciplinary scientific knowledge and optimize materials
discovery. While large language models (LLMs) have demonstrated promising
reasoning and automation capabilities across various domains, their application
in materials science remains limited due to a lack of benchmarking standards
and practical implementation frameworks. To address these challenges, we
introduce Mixture-of-Workflows for Self-Corrective Retrieval-Augmented
Generation (CRAG-MoW) - a novel paradigm that orchestrates multiple agentic
workflows employing distinct CRAG strategies using open-source LLMs. Unlike
prior approaches, CRAG-MoW synthesizes diverse outputs through an orchestration
agent, enabling direct evaluation of multiple LLMs across the same problem
domain. We benchmark CRAG-MoWs across small molecules, polymers, and chemical
reactions, as well as multi-modal nuclear magnetic resonance (NMR) spectral
retrieval. Our results demonstrate that CRAG-MoWs achieve performance
comparable to GPT-4o while being preferred more frequently in comparative
evaluations, highlighting the advantage of structured retrieval and multi-agent
synthesis. By revealing performance variations across data types, CRAG-MoW
provides a scalable, interpretable, and benchmark-driven approach to optimizing
AI architectures for materials discovery. These insights are pivotal in
addressing fundamental gaps in benchmarking LLMs and autonomous AI agents for
scientific applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19622v1' target='_blank'>Weaker LLMs' Opinions Also Matter: Mixture of Opinions Enhances LLM's
  Mathematical Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yanan Chen, Ali Pesaranghader, Tanmana Sadhu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 23:22:02</h6>
<p class='card-text'>Recent advances in Large Language Models (LLMs) have raised interest in their
formal reasoning capabilities, particularly in mathematics. While closed LLMs
like GPT-4 perform well on mathematical benchmarks, e.g., GSM8K, it remains
unclear whether small to medium-sized open LLMs can achieve similar
performance, questioning their reliability. To close this gap, we propose a
post-training approach leveraging a mixture of opinions (MoO) from weaker
ancillary LLMs to enhance a (relatively) stronger LLM's reasoning. For that,
each post-training sample is augmented with Chain-of-Thought (CoT) reasoning
steps and answers from ancillary LLMs, enabling the main LLM to learn from
diverse perspectives. We compare MoO with standard supervised fine-tuning
(SFT), few-shot prompting, and the Mixture of Agents (MoA) method on
mathematical reasoning benchmarks. Our results show that incorporating weaker
LLMs' opinions improves mathematical reasoning by an average of 5%,
highlighting the value of diverse perspectives in reasoning tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19559v1' target='_blank'>Stay Focused: Problem Drift in Multi-Agent Debate</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jonas Becker, Lars Benedikt Kaesberg, Andreas Stephan, Jan Philip Wahle, Terry Ruas, Bela Gipp</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 20:54:51</h6>
<p class='card-text'>Multi-agent debate - multiple instances of large language models discussing
problems in turn-based interaction - has shown promise for solving knowledge
and reasoning tasks. However, these methods show limitations, particularly when
scaling them to longer reasoning chains. In this study, we unveil a new issue
of multi-agent debate: discussions drift away from the initial problem over
multiple turns. We define this phenomenon as problem drift and quantify its
presence across ten tasks (i.e., three generative, three knowledge, three
reasoning, and one instruction-following task). To identify the reasons for
this issue, we perform a human study with eight experts on discussions
suffering from problem drift, who find the most common issues are a lack of
progress (35% of cases), low-quality feedback (26% of cases), and a lack of
clarity (25% of cases). To systematically address the issue of problem drift,
we propose DRIFTJudge, a method based on LLM-as-a-judge, to detect problem
drift at test-time. We further propose DRIFTPolicy, a method to mitigate 31% of
problem drift cases. Our study can be seen as a first step to understanding a
key limitation of multi-agent debate, highlighting pathways for improving their
effectiveness in the future.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19545v1' target='_blank'>Winning Big with Small Models: Knowledge Distillation vs. Self-Training
  for Reducing Hallucination in QA Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ashley Lewis, Michael White, Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 20:34:58</h6>
<p class='card-text'>The deployment of Large Language Models (LLMs) in customer support is
constrained by hallucination-generating false information-and the high cost of
proprietary models. To address these challenges, we propose a
retrieval-augmented question-answering (QA) pipeline and explore how to balance
human input and automation. Using a dataset of questions about a Samsung Smart
TV user manual, we demonstrate that synthetic data generated by LLMs
outperforms crowdsourced data in reducing hallucination in finetuned models. We
also compare self-training (fine-tuning models on their own outputs) and
knowledge distillation (fine-tuning on stronger models' outputs, e.g., GPT-4o),
and find that self-training achieves comparable hallucination reduction. We
conjecture that this surprising finding can be attributed to increased exposure
bias issues in the knowledge distillation case and support this conjecture with
post hoc analysis. We also improve robustness to unanswerable questions and
retrieval failures with contextualized "I don't know" responses. These findings
show that scalable, cost-efficient QA systems can be built using synthetic data
and self-training with open-source models, reducing reliance on proprietary
tools or costly human annotations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19500v1' target='_blank'>Conversational Planning for Personal Plans</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Konstantina Christakopoulou, Iris Qu, John Canny, Andrew Goodridge, Cj Adams, Minmin Chen, Maja Matariƒá</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 19:04:26</h6>
<p class='card-text'>The language generation and reasoning capabilities of large language models
(LLMs) have enabled conversational systems with impressive performance in a
variety of tasks, from code generation, to composing essays, to passing STEM
and legal exams, to a new paradigm for knowledge search. Besides those
short-term use applications, LLMs are increasingly used to help with real-life
goals or tasks that take a long time to complete, involving multiple sessions
across days, weeks, months, or even years. Thus to enable conversational
systems for long term interactions and tasks, we need language-based agents
that can plan for long horizons. Traditionally, such capabilities were
addressed by reinforcement learning agents with hierarchical planning
capabilities. In this work, we explore a novel architecture where the LLM acts
as the meta-controller deciding the agent's next macro-action, and tool use
augmented LLM-based option policies execute the selected macro-action. We
instantiate this framework for a specific set of macro-actions enabling
adaptive planning for users' personal plans through conversation and follow-up
questions collecting user feedback. We show how this paradigm can be applicable
in scenarios ranging from tutoring for academic and non-academic tasks to
conversational coaching for personal health plans.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19400v1' target='_blank'>TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem
  Understanding</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Max Ku, Thomas Chong, Jonathan Leung, Krish Shah, Alvin Yu, Wenhu Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 18:50:09</h6>
<p class='card-text'>Understanding domain-specific theorems often requires more than just
text-based reasoning; effective communication through structured visual
explanations is crucial for deeper comprehension. While large language models
(LLMs) demonstrate strong performance in text-based theorem reasoning, their
ability to generate coherent and pedagogically meaningful visual explanations
remains an open challenge. In this work, we introduce TheoremExplainAgent, an
agentic approach for generating long-form theorem explanation videos (over 5
minutes) using Manim animations. To systematically evaluate multimodal theorem
explanations, we propose TheoremExplainBench, a benchmark covering 240 theorems
across multiple STEM disciplines, along with 5 automated evaluation metrics.
Our results reveal that agentic planning is essential for generating detailed
long-form videos, and the o3-mini agent achieves a success rate of 93.8% and an
overall score of 0.77. However, our quantitative and qualitative studies show
that most of the videos produced exhibit minor issues with visual element
layout. Furthermore, multimodal explanations expose deeper reasoning flaws that
text-based explanations fail to reveal, highlighting the importance of
multimodal explanations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19328v1' target='_blank'>Agentic Reward Modeling: Integrating Human Preferences with Verifiable
  Correctness Signals for Reliable Reward Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hao Peng, Yunjia Qi, Xiaozhi Wang, Zijun Yao, Bin Xu, Lei Hou, Juanzi Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 17:19:12</h6>
<p class='card-text'>Reward models (RMs) are crucial for the training and inference-time scaling
up of large language models (LLMs). However, existing reward models primarily
focus on human preferences, neglecting verifiable correctness signals which
have shown strong potential in training LLMs. In this paper, we propose agentic
reward modeling, a reward system that combines reward models with verifiable
correctness signals from different aspects to provide reliable rewards. We
empirically implement a reward agent, named RewardAgent, that combines human
preference rewards with two verifiable signals: factuality and instruction
following, to provide more reliable rewards. We conduct comprehensive
experiments on existing reward model benchmarks and inference time best-of-n
searches on real-world downstream tasks. RewardAgent significantly outperforms
vanilla reward models, demonstrating its effectiveness. We further construct
training preference pairs using RewardAgent and train an LLM with the DPO
objective, achieving superior performance on various NLP benchmarks compared to
conventional reward models. Our codes are publicly released to facilitate
further research (https://github.com/THU-KEG/Agentic-Reward-Modeling).</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>