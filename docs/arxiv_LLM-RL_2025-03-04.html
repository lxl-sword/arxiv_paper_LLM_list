<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>LLM-RL - 2025-03-04</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>LLM-RL - 2025-03-04</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.21321v1' target='_blank'>LLM Post-Training: A Deep Dive into Reasoning Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Komal Kumar, Tajamul Ashraf, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, Phillip H. S. Torr, Salman Khan, Fahad Shahbaz Khan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-28 18:59:54</h6>
<p class='card-text'>Large Language Models (LLMs) have transformed the natural language processing
landscape and brought to life diverse applications. Pretraining on vast
web-scale data has laid the foundation for these models, yet the research
community is now increasingly shifting focus toward post-training techniques to
achieve further breakthroughs. While pretraining provides a broad linguistic
foundation, post-training methods enable LLMs to refine their knowledge,
improve reasoning, enhance factual accuracy, and align more effectively with
user intents and ethical considerations. Fine-tuning, reinforcement learning,
and test-time scaling have emerged as critical strategies for optimizing LLMs
performance, ensuring robustness, and improving adaptability across various
real-world tasks. This survey provides a systematic exploration of
post-training methodologies, analyzing their role in refining LLMs beyond
pretraining, addressing key challenges such as catastrophic forgetting, reward
hacking, and inference-time trade-offs. We highlight emerging directions in
model alignment, scalable adaptation, and inference-time reasoning, and outline
future research directions. We also provide a public repository to continually
track developments in this fast-evolving field:
https://github.com/mbzuai-oryx/Awesome-LLM-Post-training.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.21231v1' target='_blank'>ByteScale: Efficient Scaling of LLM Training with a 2048K Context Length
  on More Than 12,000 GPUs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hao Ge, Junda Feng, Qi Huang, Fangcheng Fu, Xiaonan Nie, Lei Zuo, Haibin Lin, Bin Cui, Xin Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-28 17:01:03</h6>
<p class='card-text'>Scaling long-context ability is essential for Large Language Models (LLMs).
To amortize the memory consumption across multiple devices in long-context
training, inter-data partitioning (a.k.a. Data Parallelism) and intra-data
partitioning (a.k.a. Context Parallelism) are commonly used. Current training
frameworks predominantly treat the two techniques as orthogonal, and establish
static communication groups to organize the devices as a static mesh (e.g., a
2D mesh). However, the sequences for LLM training typically vary in lengths, no
matter for texts, multi-modalities or reinforcement learning. The mismatch
between data heterogeneity and static mesh causes redundant communication and
imbalanced computation, degrading the training efficiency.
  In this work, we introduce ByteScale, an efficient, flexible, and scalable
LLM training framework for large-scale mixed training of long and short
sequences. The core of ByteScale is a novel parallelism strategy, namely Hybrid
Data Parallelism (HDP), which unifies the inter- and intra-data partitioning
with a dynamic mesh design. In particular, we build a communication optimizer,
which eliminates the redundant communication for short sequences by data-aware
sharding and dynamic communication, and further compresses the communication
cost for long sequences by selective offloading. Besides, we also develop a
balance scheduler to mitigate the imbalanced computation by parallelism-aware
data assignment. We evaluate ByteScale with the model sizes ranging from 7B to
141B, context lengths from 256K to 2048K, on a production cluster with more
than 12,000 GPUs. Experiment results show that ByteScale outperforms the
state-of-the-art training system by up to 7.89x.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20548v1' target='_blank'>$Q\sharp$: Provably Optimal Distributional RL for LLM Post-Training</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jin Peng Zhou, Kaiwen Wang, Jonathan Chang, Zhaolin Gao, Nathan Kallus, Kilian Q. Weinberger, Kiant√© Brantley, Wen Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 21:43:00</h6>
<p class='card-text'>Reinforcement learning (RL) post-training is crucial for LLM alignment and
reasoning, but existing policy-based methods, such as PPO and DPO, can fall
short of fixing shortcuts inherited from pre-training. In this work, we
introduce $Q\sharp$, a value-based algorithm for KL-regularized RL that guides
the reference policy using the optimal regularized $Q$ function. We propose to
learn the optimal $Q$ function using distributional RL on an aggregated online
dataset. Unlike prior value-based baselines that guide the model using
unregularized $Q$-values, our method is theoretically principled and provably
learns the optimal policy for the KL-regularized RL problem. Empirically,
$Q\sharp$ outperforms prior baselines in math reasoning benchmarks while
maintaining a smaller KL divergence to the reference policy. Theoretically, we
establish a reduction from KL-regularized RL to no-regret online learning,
providing the first bounds for deterministic MDPs under only realizability.
Thanks to distributional RL, our bounds are also variance-dependent and
converge faster when the reference policy has small variance. In sum, our
results highlight $Q\sharp$ as an effective approach for post-training LLMs,
offering both improved performance and theoretical guarantees. The code can be
found at https://github.com/jinpz/q_sharp.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>