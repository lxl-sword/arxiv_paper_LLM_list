<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>LLM-planning - 2025-03-12</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>LLM-planning - 2025-03-12</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08683v1' target='_blank'>CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous
  Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Changxing Liu, Genjia Liu, Zijun Wang, Jinchang Yang, Siheng Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 17:58:42</h6>
<p class='card-text'>Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise
for improving safety by addressing the perception and prediction uncertainties
inherent in single-agent systems. However, traditional cooperative methods are
constrained by rigid collaboration protocols and limited generalization to
unseen interactive scenarios. While LLM-based approaches offer generalized
reasoning capabilities, their challenges in spatial planning and unstable
inference latency hinder their direct application in cooperative driving. To
address these limitations, we propose CoLMDriver, the first full-pipeline
LLM-based cooperative driving system, enabling effective language-based
negotiation and real-time driving control. CoLMDriver features a parallel
driving pipeline with two key components: (i) an LLM-based negotiation module
under an actor-critic paradigm, which continuously refines cooperation policies
through feedback from previous decisions of all vehicles; and (ii) an
intention-guided waypoint generator, which translates negotiation outcomes into
executable waypoints. Additionally, we introduce InterDrive, a CARLA-based
simulation benchmark comprising 10 challenging interactive driving scenarios
for evaluating V2V cooperation. Experimental results demonstrate that
CoLMDriver significantly outperforms existing approaches, achieving an 11%
higher success rate across diverse highly interactive V2V driving scenarios.
Code will be released on https://github.com/cxliu0314/CoLMDriver.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08604v1' target='_blank'>EMMOE: A Comprehensive Benchmark for Embodied Mobile Manipulation in
  Open Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dongping Li, Tielong Cai, Tianci Tang, Wenhao Chai, Katherine Rose Driggs-Campbell, Gaoang Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 16:42:36</h6>
<p class='card-text'>Developing autonomous home robots controlled by natural language has long
been a pursuit of human. While advancements in large language models (LLMs) and
embodied intelligence make this goal closer, several challenges persist: the
lack of a unified benchmark for more complex robot tasks, limited evaluation
methods and metrics, data incompatibility between LLMs and mobile manipulation
trajectories. To address these issues, we introduce Embodied Mobile
Manipulation in Open Environments (EMMOE), which requires agents to interpret
user instructions and execute long-horizon everyday tasks in continuous space.
EMMOE seamlessly integrates high-level and low-level embodied tasks into a
unified framework, along with three new metrics for more diverse assessment.
Additionally, we collect EMMOE-100, which features in various task attributes,
detailed process annotations, re-plans after failures, and two sub-datasets for
LLM training. Furthermore, we design HomieBot, a sophisticated agent system
consists of LLM with Direct Preference Optimization (DPO), light weighted
navigation and manipulation models, and multiple error detection mechanisms.
Finally, we demonstrate HomieBot's performance and the evaluation of different
models and policies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08537v1' target='_blank'>Chemical reasoning in LLMs unlocks steerable synthesis planning and
  reaction mechanism elucidation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Andres M Bran, Theo A Neukomm, Daniel P Armstrong, Zlatko Jonƒçev, Philippe Schwaller</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 15:27:17</h6>
<p class='card-text'>While machine learning algorithms have been shown to excel at specific
chemical tasks, they have struggled to capture the strategic thinking that
characterizes expert chemical reasoning, limiting their widespread adoption.
Here we demonstrate that large language models (LLMs) can serve as powerful
chemical reasoning engines when integrated with traditional search algorithms,
enabling a new approach to computer-aided chemistry that mirrors human expert
thinking. Rather than using LLMs to directly manipulate chemical structures, we
leverage their ability to evaluate chemical strategies and guide search
algorithms toward chemically meaningful solutions. We demonstrate this paradigm
through two fundamental challenges: strategy-aware retrosynthetic planning and
mechanism elucidation. In retrosynthetic planning, our method allows chemists
to specify desired synthetic strategies in natural language to find routes that
satisfy these constraints in vast searches. In mechanism elucidation, LLMs
guide the search for plausible reaction mechanisms by combining chemical
principles with systematic exploration. Our approach shows strong performance
across diverse chemical tasks, with larger models demonstrating increasingly
sophisticated chemical reasoning. Our approach establishes a new paradigm for
computer-aided chemistry that combines the strategic understanding of LLMs with
the precision of traditional chemical tools, opening possibilities for more
intuitive and powerful chemical reasoning systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08508v1' target='_blank'>LightPlanner: Unleashing the Reasoning Capabilities of Lightweight Large
  Language Models in Task Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weijie Zhou, Yi Peng, Manli Tao, Chaoyang Zhao, Honghui Dong, Ming Tang, Jinqiao Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 14:57:53</h6>
<p class='card-text'>In recent years, lightweight large language models (LLMs) have garnered
significant attention in the robotics field due to their low computational
resource requirements and suitability for edge deployment. However, in task
planning -- particularly for complex tasks that involve dynamic semantic logic
reasoning -- lightweight LLMs have underperformed. To address this limitation,
we propose a novel task planner, LightPlanner, which enhances the performance
of lightweight LLMs in complex task planning by fully leveraging their
reasoning capabilities. Unlike conventional planners that use fixed skill
templates, LightPlanner controls robot actions via parameterized function
calls, dynamically generating parameter values. This approach allows for
fine-grained skill control and improves task planning success rates in complex
scenarios. Furthermore, we introduce hierarchical deep reasoning. Before
generating each action decision step, LightPlanner thoroughly considers three
levels: action execution (feedback verification), semantic parsing (goal
consistency verification), and parameter generation (parameter validity
verification). This ensures the correctness of subsequent action controls.
Additionally, we incorporate a memory module to store historical actions,
thereby reducing context length and enhancing planning efficiency for long-term
tasks. We train the LightPlanner-1.5B model on our LightPlan-40k dataset, which
comprises 40,000 action controls across tasks with 2 to 13 action steps.
Experiments demonstrate that our model achieves the highest task success rate
despite having the smallest number of parameters. In tasks involving spatial
semantic reasoning, the success rate exceeds that of ReAct by 14.9 percent.
Moreover, we demonstrate LightPlanner's potential to operate on edge devices.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08338v1' target='_blank'>Trinity: A Modular Humanoid Robot AI System</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jingkai Sun, Qiang Zhang, Gang Han, Wen Zhao, Zhe Yong, Yan He, Jiaxu Wang, Jiahang Cao, Yijie Guo, Renjing Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 11:50:36</h6>
<p class='card-text'>In recent years, research on humanoid robots has garnered increasing
attention. With breakthroughs in various types of artificial intelligence
algorithms, embodied intelligence, exemplified by humanoid robots, has been
highly anticipated. The advancements in reinforcement learning (RL) algorithms
have significantly improved the motion control and generalization capabilities
of humanoid robots. Simultaneously, the groundbreaking progress in large
language models (LLM) and visual language models (VLM) has brought more
possibilities and imagination to humanoid robots. LLM enables humanoid robots
to understand complex tasks from language instructions and perform long-term
task planning, while VLM greatly enhances the robots' understanding and
interaction with their environment. This paper introduces
\textcolor{magenta}{Trinity}, a novel AI system for humanoid robots that
integrates RL, LLM, and VLM. By combining these technologies, Trinity enables
efficient control of humanoid robots in complex environments. This innovative
approach not only enhances the capabilities but also opens new avenues for
future research and applications of humanoid robotics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08330v1' target='_blank'>KiteRunner: Language-Driven Cooperative Local-Global Navigation Policy
  with UAV Mapping in Outdoor Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shibo Huang, Chenfan Shi, Jian Yang, Hanlin Dong, Jinpeng Mi, Ke Li, Jianfeng Zhang, Miao Ding, Peidong Liang, Xiong You, Xian Wei</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 11:44:29</h6>
<p class='card-text'>Autonomous navigation in open-world outdoor environments faces challenges in
integrating dynamic conditions, long-distance spatial reasoning, and semantic
understanding. Traditional methods struggle to balance local planning, global
planning, and semantic task execution, while existing large language models
(LLMs) enhance semantic comprehension but lack spatial reasoning capabilities.
Although diffusion models excel in local optimization, they fall short in
large-scale long-distance navigation. To address these gaps, this paper
proposes KiteRunner, a language-driven cooperative local-global navigation
strategy that combines UAV orthophoto-based global planning with diffusion
model-driven local path generation for long-distance navigation in open-world
scenarios. Our method innovatively leverages real-time UAV orthophotography to
construct a global probability map, providing traversability guidance for the
local planner, while integrating large models like CLIP and GPT to interpret
natural language instructions. Experiments demonstrate that KiteRunner achieves
5.6% and 12.8% improvements in path efficiency over state-of-the-art methods in
structured and unstructured environments, respectively, with significant
reductions in human interventions and execution time.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08302v1' target='_blank'>General-Purpose Aerial Intelligent Agents Empowered by Large Language
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ji Zhao, Xiao Lin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 11:13:58</h6>
<p class='card-text'>The emergence of large language models (LLMs) opens new frontiers for
unmanned aerial vehicle (UAVs), yet existing systems remain confined to
predefined tasks due to hardware-software co-design challenges. This paper
presents the first aerial intelligent agent capable of open-world task
execution through tight integration of LLM-based reasoning and robotic
autonomy. Our hardware-software co-designed system addresses two fundamental
limitations: (1) Onboard LLM operation via an edge-optimized computing
platform, achieving 5-6 tokens/sec inference for 14B-parameter models at 220W
peak power; (2) A bidirectional cognitive architecture that synergizes slow
deliberative planning (LLM task planning) with fast reactive control (state
estimation, mapping, obstacle avoidance, and motion planning). Validated
through preliminary results using our prototype, the system demonstrates
reliable task planning and scene understanding in communication-constrained
environments, such as sugarcane monitoring, power grid inspection, mine tunnel
exploration, and biological observation applications. This work establishes a
novel framework for embodied aerial artificial intelligence, bridging the gap
between task planning and robotic autonomy in open environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08174v1' target='_blank'>Investigating the Effectiveness of a Socratic Chain-of-Thoughts
  Reasoning Method for Task Planning in Robotics, A Case Study</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Veronica Bot, Zheyuan Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 08:36:37</h6>
<p class='card-text'>Large language models (LLMs) have demonstrated unprecedented capability in
reasoning with natural language. Coupled with this development is the emergence
of embodied AI in robotics. Despite showing promise for verbal and written
reasoning tasks, it remains unknown whether LLMs are capable of navigating
complex spatial tasks with physical actions in the real world. To this end, it
is of interest to investigate applying LLMs to robotics in zero-shot learning
scenarios, and in the absence of fine-tuning - a feat which could significantly
improve human-robot interaction, alleviate compute cost, and eliminate
low-level programming tasks associated with robot tasks.
  To explore this question, we apply GPT-4(Omni) with a simulated Tiago robot
in Webots engine for an object search task. We evaluate the effectiveness of
three reasoning strategies based on Chain-of-Thought (CoT) sub-task list
generation with the Socratic method (SocraCoT) (in order of increasing rigor):
(1) Non-CoT/Non-SocraCoT, (2) CoT only, and (3) SocraCoT. Performance was
measured in terms of the proportion of tasks successfully completed and
execution time (N = 20). Our preliminary results show that when combined with
chain-of-thought reasoning, the Socratic method can be used for code generation
for robotic tasks that require spatial awareness. In extension of this finding,
we propose EVINCE-LoC; a modified EVINCE method that could further enhance
performance in highly complex and or dynamic testing scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08162v1' target='_blank'>FASIONAD++ : Integrating High-Level Instruction and Information
  Bottleneck in FAt-Slow fusION Systems for Enhanced Safety in Autonomous
  Driving with Adaptive Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kangan Qian, Ziang Luo, Sicong Jiang, Zilin Huang, Jinyu Miao, Zhikun Ma, Tianze Zhu, Jiayin Li, Yangfan He, Zheng Fu, Yining Shi, Boyue Wang, Hezhe Lin, Ziyu Chen, Jiangbo Yu, Xinyu Jiao, Mengmeng Yang, Kun Jiang, Diange Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 08:27:01</h6>
<p class='card-text'>Ensuring safe, comfortable, and efficient planning is crucial for autonomous
driving systems. While end-to-end models trained on large datasets perform well
in standard driving scenarios, they struggle with complex low-frequency events.
Recent Large Language Models (LLMs) and Vision Language Models (VLMs)
advancements offer enhanced reasoning but suffer from computational
inefficiency. Inspired by the dual-process cognitive model "Thinking, Fast and
Slow", we propose $\textbf{FASIONAD}$ -- a novel dual-system framework that
synergizes a fast end-to-end planner with a VLM-based reasoning module. The
fast system leverages end-to-end learning to achieve real-time trajectory
generation in common scenarios, while the slow system activates through
uncertainty estimation to perform contextual analysis and complex scenario
resolution. Our architecture introduces three key innovations: (1) A dynamic
switching mechanism enabling slow system intervention based on real-time
uncertainty assessment; (2) An information bottleneck with high-level plan
feedback that optimizes the slow system's guidance capability; (3) A
bidirectional knowledge exchange where visual prompts enhance the slow system's
reasoning while its feedback refines the fast planner's decision-making. To
strengthen VLM reasoning, we develop a question-answering mechanism coupled
with reward-instruct training strategy. In open-loop experiments, FASIONAD
achieves a $6.7\%$ reduction in average $L2$ trajectory error and $28.1\%$
lower collision rate.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08084v1' target='_blank'>Instruction-Augmented Long-Horizon Planning: Embedding Grounding
  Mechanisms in Embodied Mobile Manipulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fangyuan Wang, Shipeng Lyu, Peng Zhou, Anqing Duan, Guodong Guo, David Navarro-Alarcon</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 06:37:33</h6>
<p class='card-text'>Enabling humanoid robots to perform long-horizon mobile manipulation planning
in real-world environments based on embodied perception and comprehension
abilities has been a longstanding challenge. With the recent rise of large
language models (LLMs), there has been a notable increase in the development of
LLM-based planners. These approaches either utilize human-provided textual
representations of the real world or heavily depend on prompt engineering to
extract such representations, lacking the capability to quantitatively
understand the environment, such as determining the feasibility of manipulating
objects. To address these limitations, we present the Instruction-Augmented
Long-Horizon Planning (IALP) system, a novel framework that employs LLMs to
generate feasible and optimal actions based on real-time sensor feedback,
including grounded knowledge of the environment, in a closed-loop interaction.
Distinct from prior works, our approach augments user instructions into PDDL
problems by leveraging both the abstract reasoning capabilities of LLMs and
grounding mechanisms. By conducting various real-world long-horizon tasks, each
consisting of seven distinct manipulatory skills, our results demonstrate that
the IALP system can efficiently solve these tasks with an average success rate
exceeding 80%. Our proposed method can operate as a high-level planner,
equipping robots with substantial autonomy in unstructured environments through
the utilization of multi-modal sensor inputs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07902v1' target='_blank'>LTLCodeGen: Code Generation of Syntactically Correct Temporal Logic for
  Robot Task Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Behrad Rabiei, Mahesh Kumar A. R., Zhirui Dai, Surya L. S. R. Pilla, Qiyue Dong, Nikolay Atanasov</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 22:43:13</h6>
<p class='card-text'>This paper focuses on planning robot navigation tasks from natural language
specifications. We develop a modular approach, where a large language model
(LLM) translates the natural language instructions into a linear temporal logic
(LTL) formula with propositions defined by object classes in a semantic
occupancy map. The LTL formula and the semantic occupancy map are provided to a
motion planning algorithm to generate a collision-free robot path that
satisfies the natural language instructions. Our main contribution is
LTLCodeGen, a method to translate natural language to syntactically correct LTL
using code generation. We demonstrate the complete task planning method in
real-world experiments involving human speech to provide navigation
instructions to a mobile robot. We also thoroughly evaluate our approach in
simulated and real-world experiments in comparison to end-to-end LLM task
planning and state-of-the-art LLM-to-LTL translation methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07885v1' target='_blank'>Safety Guardrails for LLM-Enabled Robots</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zachary Ravichandran, Alexander Robey, Vijay Kumar, George J. Pappas, Hamed Hassani</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 22:01:56</h6>
<p class='card-text'>Although the integration of large language models (LLMs) into robotics has
unlocked transformative capabilities, it has also introduced significant safety
concerns, ranging from average-case LLM errors (e.g., hallucinations) to
adversarial jailbreaking attacks, which can produce harmful robot behavior in
real-world settings. Traditional robot safety approaches do not address the
novel vulnerabilities of LLMs, and current LLM safety guardrails overlook the
physical risks posed by robots operating in dynamic real-world environments. In
this paper, we propose RoboGuard, a two-stage guardrail architecture to ensure
the safety of LLM-enabled robots. RoboGuard first contextualizes pre-defined
safety rules by grounding them in the robot's environment using a root-of-trust
LLM, which employs chain-of-thought (CoT) reasoning to generate rigorous safety
specifications, such as temporal logic constraints. RoboGuard then resolves
potential conflicts between these contextual safety specifications and a
possibly unsafe plan using temporal logic control synthesis, which ensures
safety compliance while minimally violating user preferences. Through extensive
simulation and real-world experiments that consider worst-case jailbreaking
attacks, we demonstrate that RoboGuard reduces the execution of unsafe plans
from 92% to below 2.5% without compromising performance on safe plans. We also
demonstrate that RoboGuard is resource-efficient, robust against adaptive
attacks, and significantly enhanced by enabling its root-of-trust LLM to
perform CoT reasoning. These results underscore the potential of RoboGuard to
mitigate the safety risks and enhance the reliability of LLM-enabled robots.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07459v1' target='_blank'>MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for
  Complex Medical Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiangru Tang, Daniel Shao, Jiwoong Sohn, Jiapeng Chen, Jiayi Zhang, Jinyu Xiang, Fang Wu, Yilun Zhao, Chenglin Wu, Wenqi Shi, Arman Cohan, Mark Gerstein</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 15:38:44</h6>
<p class='card-text'>Large Language Models (LLMs) have shown impressive performance on existing
medical question-answering benchmarks. This high performance makes it
increasingly difficult to meaningfully evaluate and differentiate advanced
methods. We present MedAgentsBench, a benchmark that focuses on challenging
medical questions requiring multi-step clinical reasoning, diagnosis
formulation, and treatment planning-scenarios where current models still
struggle despite their strong performance on standard tests. Drawing from seven
established medical datasets, our benchmark addresses three key limitations in
existing evaluations: (1) the prevalence of straightforward questions where
even base models achieve high performance, (2) inconsistent sampling and
evaluation protocols across studies, and (3) lack of systematic analysis of the
interplay between performance, cost, and inference time. Through experiments
with various base models and reasoning methods, we demonstrate that the latest
thinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in
complex medical reasoning tasks. Additionally, advanced search-based agent
methods offer promising performance-to-cost ratios compared to traditional
approaches. Our analysis reveals substantial performance gaps between model
families on complex questions and identifies optimal model selections for
different computational constraints. Our benchmark and evaluation framework are
publicly available at https://github.com/gersteinlab/medagents-benchmark.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07323v1' target='_blank'>Dynamic Path Navigation for Motion Agents with LLM Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yubo Zhao, Qi Wu, Yifan Wang, Yu-Wing Tai, Chi-Keung Tang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 13:39:09</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated strong generalizable reasoning
and planning capabilities. However, their efficacies in spatial path planning
and obstacle-free trajectory generation remain underexplored. Leveraging LLMs
for navigation holds significant potential, given LLMs' ability to handle
unseen scenarios, support user-agent interactions, and provide global control
across complex systems, making them well-suited for agentic planning and
humanoid motion generation. As one of the first studies in this domain, we
explore the zero-shot navigation and path generation capabilities of LLMs by
constructing a dataset and proposing an evaluation protocol. Specifically, we
represent paths using anchor points connected by straight lines, enabling
movement in various directions. This approach offers greater flexibility and
practicality compared to previous methods while remaining simple and intuitive
for LLMs. We demonstrate that, when tasks are well-structured in this manner,
modern LLMs exhibit substantial planning proficiency in avoiding obstacles
while autonomously refining navigation with the generated motion to reach the
target. Further, this spatial reasoning ability of a single LLM motion agent
interacting in a static environment can be seamlessly generalized in
multi-motion agents coordination in dynamic environments. Unlike traditional
approaches that rely on single-step planning or local policies, our
training-free LLM-based method enables global, dynamic, closed-loop planning,
and autonomously resolving collision issues.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07317v1' target='_blank'>Self-Corrective Task Planning by Inverse Prompting with Large Language
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiho Lee, Hayun Lee, Jonghyeon Kim, Kyungjae Lee, Eunwoo Kim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 13:35:51</h6>
<p class='card-text'>In robot task planning, large language models (LLMs) have shown significant
promise in generating complex and long-horizon action sequences. However, it is
observed that LLMs often produce responses that sound plausible but are not
accurate. To address these problems, existing methods typically employ
predefined error sets or external knowledge sources, requiring human efforts
and computation resources. Recently, self-correction approaches have emerged,
where LLM generates and refines plans, identifying errors by itself. Despite
their effectiveness, they are more prone to failures in correction due to
insufficient reasoning. In this paper, we introduce InversePrompt, a novel
self-corrective task planning approach that leverages inverse prompting to
enhance interpretability. Our method incorporates reasoning steps to provide
clear, interpretable feedback. It generates inverse actions corresponding to
the initially generated actions and verifies whether these inverse actions can
restore the system to its original state, explicitly validating the logical
coherence of the generated plans. The results on benchmark datasets show an
average 16.3% higher success rate over existing LLM-based task planning
methods. Our approach offers clearer justifications for feedback in real-world
environments, resulting in more successful task completion than existing
self-correction approaches across various scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07314v1' target='_blank'>Automated Movie Generation via Multi-Agent CoT Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weijia Wu, Zeyu Zhu, Mike Zheng Shou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 13:33:27</h6>
<p class='card-text'>Existing long-form video generation frameworks lack automated planning,
requiring manual input for storylines, scenes, cinematography, and character
interactions, resulting in high costs and inefficiencies. To address these
challenges, we present MovieAgent, an automated movie generation via
multi-agent Chain of Thought (CoT) planning. MovieAgent offers two key
advantages: 1) We firstly explore and define the paradigm of automated
movie/long-video generation. Given a script and character bank, our MovieAgent
can generates multi-scene, multi-shot long-form videos with a coherent
narrative, while ensuring character consistency, synchronized subtitles, and
stable audio throughout the film. 2) MovieAgent introduces a hierarchical
CoT-based reasoning process to automatically structure scenes, camera settings,
and cinematography, significantly reducing human effort. By employing multiple
LLM agents to simulate the roles of a director, screenwriter, storyboard
artist, and location manager, MovieAgent streamlines the production pipeline.
Experiments demonstrate that MovieAgent achieves new state-of-the-art results
in script faithfulness, character consistency, and narrative coherence. Our
hierarchical framework takes a step forward and provides new insights into
fully automated movie generation. The code and project website are available
at: https://github.com/showlab/MovieAgent and
https://weijiawu.github.io/MovieAgent.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07282v1' target='_blank'>A Graph-based Verification Framework for Fact-Checking</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yani Huang, Richong Zhang, Zhijie Nie, Junfan Chen, Xuefeng Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 13:02:29</h6>
<p class='card-text'>Fact-checking plays a crucial role in combating misinformation. Existing
methods using large language models (LLMs) for claim decomposition face two key
limitations: (1) insufficient decomposition, introducing unnecessary complexity
to the verification process, and (2) ambiguity of mentions, leading to
incorrect verification results. To address these challenges, we suggest
introducing a claim graph consisting of triplets to address the insufficient
decomposition problem and reduce mention ambiguity through graph structure.
Based on this core idea, we propose a graph-based framework, GraphFC, for
fact-checking. The framework features three key components: graph construction,
which builds both claim and evidence graphs; graph-guided planning, which
prioritizes the triplet verification order; and graph-guided checking, which
verifies the triples one by one between claim and evidence graphs. Extensive
experiments show that GraphFC enables fine-grained decomposition while
resolving referential ambiguities through relational constraints, achieving
state-of-the-art performance across three datasets.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07044v1' target='_blank'>DatawiseAgent: A Notebook-Centric LLM Agent Framework for Automated Data
  Science</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziming You, Yumiao Zhang, Dexuan Xu, Yiwei Lou, Yandong Yan, Wei Wang, Huaming Zhang, Yu Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 08:32:33</h6>
<p class='card-text'>Data Science tasks are multifaceted, dynamic, and often domain-specific.
Existing LLM-based approaches largely concentrate on isolated phases,
neglecting the interdependent nature of many data science tasks and limiting
their capacity for comprehensive end-to-end support. We propose DatawiseAgent,
a notebook-centric LLM agent framework that unifies interactions among user,
agent and the computational environment through markdown and executable code
cells, supporting flexible and adaptive automated data science. Built on a
Finite State Transducer(FST), DatawiseAgent orchestrates four stages, including
DSF-like planning, incremental execution, self-debugging, and post-filtering.
Specifically, the DFS-like planning stage systematically explores the solution
space, while incremental execution harnesses real-time feedback and
accommodates LLM's limited capabilities to progressively complete tasks. The
self-debugging and post-filtering modules further enhance reliability by
diagnosing and correcting errors and pruning extraneous information. Extensive
experiments on diverse tasks, including data analysis, visualization, and data
modeling, show that DatawiseAgent consistently outperforms or matches
state-of-the-art methods across multiple model settings. These results
highlight its potential to generalize across data science scenarios and lay the
groundwork for more efficient, fully automated workflows.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07020v1' target='_blank'>Combating Partial Perception Deficit in Autonomous Driving with
  Multimodal LLM Commonsense</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuting Hu, Chenhui Xu, Ruiyang Qin, Dancheng Liu, Amir Nassereldine, Yiyu Shi, Jinjun Xiong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 08:01:41</h6>
<p class='card-text'>Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07006v1' target='_blank'>HELM: Human-Preferred Exploration with Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shuhao Liao, Xuxin Lv, Yuhong Cao, Jeric Lew, Wenjun Wu, Guillaume Sartoretti</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 07:40:01</h6>
<p class='card-text'>In autonomous exploration tasks, robots are required to explore and map
unknown environments while efficiently planning in dynamic and uncertain
conditions. Given the significant variability of environments, human operators
often have specific preference requirements for exploration, such as
prioritizing certain areas or optimizing for different aspects of efficiency.
However, existing methods struggle to accommodate these human preferences
adaptively, often requiring extensive parameter tuning or network retraining.
With the recent advancements in Large Language Models (LLMs), which have been
widely applied to text-based planning and complex reasoning, their potential
for enhancing autonomous exploration is becoming increasingly promising.
Motivated by this, we propose an LLM-based human-preferred exploration
framework that seamlessly integrates a mobile robot system with LLMs. By
leveraging the reasoning and adaptability of LLMs, our approach enables
intuitive and flexible preference control through natural language while
maintaining a task success rate comparable to state-of-the-art traditional
methods. Experimental results demonstrate that our framework effectively
bridges the gap between human intent and policy preference in autonomous
exploration, offering a more user-friendly and adaptable solution for
real-world robotic applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06902v1' target='_blank'>A Query Optimization Method Utilizing Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiming Yao, Haoyang Li, Jing Zhang, Cuiping Li, Hong Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 04:07:56</h6>
<p class='card-text'>Query optimization is a critical task in database systems, focused on
determining the most efficient way to execute a query from an enormous set of
possible strategies. Traditional approaches rely on heuristic search methods
and cost predictions, but these often struggle with the complexity of the
search space and inaccuracies in performance estimation, leading to suboptimal
plan choices. This paper presents LLMOpt, a novel framework that leverages
Large Language Models (LLMs) to address these challenges through two innovative
components: (1) LLM for Plan Candidate Generation (LLMOpt(G)), which eliminates
heuristic search by utilizing the reasoning abilities of LLMs to directly
generate high-quality query plans, and (2) LLM for Plan Candidate Selection
(LLMOpt(S)), a list-wise cost model that compares candidates globally to
enhance selection accuracy. To adapt LLMs for query optimization, we propose
fine-tuning pre-trained models using optimization data collected offline.
Experimental results on the JOB, JOB-EXT, and Stack benchmarks show that
LLMOpt(G) and LLMOpt(S) outperform state-of-the-art methods, including
PostgreSQL, BAO, and HybridQO. Notably, LLMOpt(S) achieves the best practical
performance, striking a balance between plan quality and inference efficiency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06892v1' target='_blank'>SafePlan: Leveraging Formal Logic and Chain-of-Thought Reasoning for
  Enhanced Safety in LLM-based Robotic Task Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ike Obi, Vishnunandan L. N. Venkatesh, Weizheng Wang, Ruiqi Wang, Dayoon Suh, Temitope I. Amosa, Wonse Jo, Byung-Cheol Min</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 03:37:36</h6>
<p class='card-text'>Robotics researchers increasingly leverage large language models (LLM) in
robotics systems, using them as interfaces to receive task commands, generate
task plans, form team coalitions, and allocate tasks among multi-robot and
human agents. However, despite their benefits, the growing adoption of LLM in
robotics has raised several safety concerns, particularly regarding executing
malicious or unsafe natural language prompts. In addition, ensuring that task
plans, team formation, and task allocation outputs from LLMs are adequately
examined, refined, or rejected is crucial for maintaining system integrity. In
this paper, we introduce SafePlan, a multi-component framework that combines
formal logic and chain-of-thought reasoners for enhancing the safety of
LLM-based robotics systems. Using the components of SafePlan, including Prompt
Sanity COT Reasoner and Invariant, Precondition, and Postcondition COT
reasoners, we examined the safety of natural language task prompts, task plans,
and task allocation outputs generated by LLM-based robotic systems as means of
investigating and enhancing system safety profile. Our results show that
SafePlan outperforms baseline models by leading to 90.5% reduction in harmful
task prompt acceptance while still maintaining reasonable acceptance of safe
tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06866v1' target='_blank'>Graphormer-Guided Task Planning: Beyond Static Rules with LLM Safety
  Perception</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wanjing Huang, Tongjie Pan, Yalan Ye</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 02:43:54</h6>
<p class='card-text'>Recent advancements in large language models (LLMs) have expanded their role
in robotic task planning. However, while LLMs have been explored for generating
feasible task sequences, their ability to ensure safe task execution remains
underdeveloped. Existing methods struggle with structured risk perception,
making them inadequate for safety-critical applications where low-latency
hazard adaptation is required. To address this limitation, we propose a
Graphormer-enhanced risk-aware task planning framework that combines LLM-based
decision-making with structured safety modeling. Our approach constructs a
dynamic spatio-semantic safety graph, capturing spatial and contextual risk
factors to enable online hazard detection and adaptive task refinement. Unlike
existing methods that rely on predefined safety constraints, our framework
introduces a context-aware risk perception module that continuously refines
safety predictions based on real-time task execution. This enables a more
flexible and scalable approach to robotic planning, allowing for adaptive
safety compliance beyond static rules. To validate our framework, we conduct
experiments in the AI2-THOR environment. The experiments results validates
improvements in risk detection accuracy, rising safety notice, and task
adaptability of our framework in continuous environments compared to static
rule-based and LLM-only baselines. Our project is available at
https://github.com/hwj20/GGTP</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06552v1' target='_blank'>Multimodal Programming in Computer Science with Interactive Assistance
  Powered by Large Language Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rajan Das Gupta, Md. Tanzib Hosain, M. F. Mridha, Salah Uddin Ahmed</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-09 10:48:47</h6>
<p class='card-text'>LLM chatbot interfaces allow students to get instant, interactive assistance
with homework, but doing so carelessly may not advance educational objectives.
In this study, an interactive homework help system based on DeepSeek R1 is
developed and first implemented for students enrolled in a large computer
science beginning programming course. In addition to an assist button in a
well-known code editor, our assistant also has a feedback option in our
command-line automatic evaluator. It wraps student work in a personalized
prompt that advances our educational objectives without offering answers
straight away. We have discovered that our assistant can recognize students'
conceptual difficulties and provide ideas, plans, and template code in
pedagogically appropriate ways. However, among other mistakes, it occasionally
incorrectly labels the correct student code as incorrect or encourages students
to use correct-but-lesson-inappropriate approaches, which can lead to long and
frustrating journeys for the students. After discussing many development and
deployment issues, we provide our conclusions and future actions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06216v1' target='_blank'>A Novel Distributed PV Power Forecasting Approach Based on Time-LLM</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Huapeng Lin, Miao Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-08 13:37:31</h6>
<p class='card-text'>Distributed photovoltaic (DPV) systems are essential for advancing renewable
energy applications and achieving energy independence. Accurate DPV power
forecasting can optimize power system planning and scheduling while
significantly reducing energy loss, thus enhancing overall system efficiency
and reliability. However, solar energy's intermittent nature and DPV systems'
spatial distribution create significant forecasting challenges. Traditional
methods often rely on costly external data, such as numerical weather
prediction (NWP) and satellite images, which are difficult to scale for smaller
DPV systems. To tackle this issue, this study has introduced an advanced large
language model (LLM)-based time series forecasting framework Time-LLM to
improve the DPV power forecasting accuracy and generalization ability. By
reprogramming, the framework aligns historical power data with natural language
modalities, facilitating efficient modeling of time-series data. Then
Qwen2.5-3B model is integrated as the backbone LLM to process input data by
leveraging its pattern recognition and inference abilities, achieving a balance
between efficiency and performance. Finally, by using a flatten and linear
projection layer, the LLM's high-dimensional output is transformed into the
final forecasts. Experimental results indicate that Time-LLM outperforms
leading recent advanced time series forecasting models, such as
Transformer-based methods and MLP-based models, achieving superior accuracy in
both short-term and long-term forecasting. Time-LLM also demonstrates
exceptional adaptability in few-shot and zero-shot learning scenarios. To the
best of the authors' knowledge, this study is the first attempt to explore the
application of LLMs to DPV power forecasting, which can offer a scalable
solution that eliminates reliance on costly external data sources and improve
real-world forecasting accuracy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06074v1' target='_blank'>Towards Conversational AI for Disease Management</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Anil Palepu, Valentin Li√©vin, Wei-Hung Weng, Khaled Saab, David Stutz, Yong Cheng, Kavita Kulkarni, S. Sara Mahdavi, Jo√´lle Barral, Dale R. Webster, Katherine Chou, Avinatan Hassidim, Yossi Matias, James Manyika, Ryutaro Tanno, Vivek Natarajan, Adam Rodman, Tao Tu, Alan Karthikesalingam, Mike Schaekermann</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-08 05:48:58</h6>
<p class='card-text'>While large language models (LLMs) have shown promise in diagnostic dialogue,
their capabilities for effective management reasoning - including disease
progression, therapeutic response, and safe medication prescription - remain
under-explored. We advance the previously demonstrated diagnostic capabilities
of the Articulate Medical Intelligence Explorer (AMIE) through a new LLM-based
agentic system optimised for clinical management and dialogue, incorporating
reasoning over the evolution of disease and multiple patient visit encounters,
response to therapy, and professional competence in medication prescription. To
ground its reasoning in authoritative clinical knowledge, AMIE leverages
Gemini's long-context capabilities, combining in-context retrieval with
structured reasoning to align its output with relevant and up-to-date clinical
practice guidelines and drug formularies. In a randomized, blinded virtual
Objective Structured Clinical Examination (OSCE) study, AMIE was compared to 21
primary care physicians (PCPs) across 100 multi-visit case scenarios designed
to reflect UK NICE Guidance and BMJ Best Practice guidelines. AMIE was
non-inferior to PCPs in management reasoning as assessed by specialist
physicians and scored better in both preciseness of treatments and
investigations, and in its alignment with and grounding of management plans in
clinical guidelines. To benchmark medication reasoning, we developed RxQA, a
multiple-choice question benchmark derived from two national drug formularies
(US, UK) and validated by board-certified pharmacists. While AMIE and PCPs both
benefited from the ability to access external drug information, AMIE
outperformed PCPs on higher difficulty questions. While further research would
be needed before real-world translation, AMIE's strong performance across
evaluations marks a significant step towards conversational AI as a tool in
disease management.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.05891v2' target='_blank'>MastermindEval: A Simple But Scalable Reasoning Benchmark</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jonas Golde, Patrick Haller, Fabio Barth, Alan Akbik</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-07 19:24:59</h6>
<p class='card-text'>Recent advancements in large language models (LLMs) have led to remarkable
performance across a wide range of language understanding and mathematical
tasks. As a result, increasing attention has been given to assessing the true
reasoning capabilities of LLMs, driving research into commonsense, numerical,
logical, and qualitative reasoning. However, with the rapid progress of
reasoning-focused models such as OpenAI's o1 and DeepSeek's R1, there has been
a growing demand for reasoning benchmarks that can keep pace with ongoing model
developments. In this paper, we introduce MastermindEval, a simple, scalable,
and interpretable deductive reasoning benchmark inspired by the board game
Mastermind. Our benchmark supports two evaluation paradigms: (1) agentic
evaluation, in which the model autonomously plays the game, and (2) deductive
reasoning evaluation, in which the model is given a pre-played game state with
only one possible valid code to infer. In our experimental results we (1) find
that even easy Mastermind instances are difficult for current models and (2)
demonstrate that the benchmark is scalable to possibly more advanced models in
the future Furthermore, we investigate possible reasons why models cannot
deduce the final solution and find that current models are limited in deducing
the concealed code as the number of statement to combine information from is
increasing.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.05193v1' target='_blank'>Memory-augmented Query Reconstruction for LLM-based Knowledge Graph
  Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mufan Xu, Gewen Liang, Kehai Chen, Wei Wang, Xun Zhou, Muyun Yang, Tiejun Zhao, Min Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-07 07:28:32</h6>
<p class='card-text'>Large language models (LLMs) have achieved remarkable performance on
knowledge graph question answering (KGQA) tasks by planning and interacting
with knowledge graphs. However, existing methods often confuse tool utilization
with knowledge reasoning, harming readability of model outputs and giving rise
to hallucinatory tool invocations, which hinder the advancement of KGQA. To
address this issue, we propose Memory-augmented Query Reconstruction for
LLM-based Knowledge Graph Reasoning (MemQ) to decouple LLM from tool invocation
tasks using LLM-built query memory. By establishing a memory module with
explicit descriptions of query statements, the proposed MemQ facilitates the
KGQA process with natural language reasoning and memory-augmented query
reconstruction. Meanwhile, we design an effective and readable reasoning to
enhance the LLM's reasoning capability in KGQA. Experimental results that MemQ
achieves state-of-the-art performance on widely used benchmarks WebQSP and CWQ.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04723v2' target='_blank'>Shifting Long-Context LLMs Research from Input to Output</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuhao Wu, Yushi Bai, Zhiqing Hu, Shangqing Tu, Ming Shan Hee, Juanzi Li, Roy Ka-Wei Lee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 18:59:37</h6>
<p class='card-text'>Recent advancements in long-context Large Language Models (LLMs) have
primarily concentrated on processing extended input contexts, resulting in
significant strides in long-context comprehension. However, the equally
critical aspect of generating long-form outputs has received comparatively less
attention. This paper advocates for a paradigm shift in NLP research toward
addressing the challenges of long-output generation. Tasks such as novel
writing, long-term planning, and complex reasoning require models to understand
extensive contexts and produce coherent, contextually rich, and logically
consistent extended text. These demands highlight a critical gap in current LLM
capabilities. We underscore the importance of this under-explored domain and
call for focused efforts to develop foundational LLMs tailored for generating
high-quality, long-form outputs, which hold immense potential for real-world
applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04691v2' target='_blank'>Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pengcheng Qiu, Chaoyi Wu, Shuyu Liu, Weike Zhao, Zhuoxia Chen, Hongfei Gu, Chuanjin Peng, Ya Zhang, Yanfeng Wang, Weidi Xie</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 18:35:39</h6>
<p class='card-text'>Recent advancements in reasoning-enhanced large language models (LLMs), such
as DeepSeek-R1 and OpenAI-o3, have demonstrated significant progress. However,
their application in professional medical contexts remains underexplored,
particularly in evaluating the quality of their reasoning processes alongside
final outputs. Here, we introduce MedR-Bench, a benchmarking dataset of 1,453
structured patient cases, annotated with reasoning references derived from
clinical case reports. Spanning 13 body systems and 10 specialties, it includes
both common and rare diseases. To comprehensively evaluate LLM performance, we
propose a framework encompassing three critical examination recommendation,
diagnostic decision-making, and treatment planning, simulating the entire
patient care journey. To assess reasoning quality, we present the Reasoning
Evaluator, a novel automated system that objectively scores free-text reasoning
responses based on efficiency, actuality, and completeness using dynamic
cross-referencing and evidence checks. Using this benchmark, we evaluate five
state-of-the-art reasoning LLMs, including DeepSeek-R1, OpenAI-o3-mini, and
Gemini-2.0-Flash Thinking, etc. Our results show that current LLMs achieve over
85% accuracy in relatively simple diagnostic tasks when provided with
sufficient examination results. However, performance declines in more complex
tasks, such as examination recommendation and treatment planning. While
reasoning outputs are generally reliable, with factuality scores exceeding 90%,
critical reasoning steps are frequently missed. These findings underscore both
the progress and limitations of clinical LLMs. Notably, open-source models like
DeepSeek-R1 are narrowing the gap with proprietary systems, highlighting their
potential to drive accessible and equitable advancements in healthcare.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>