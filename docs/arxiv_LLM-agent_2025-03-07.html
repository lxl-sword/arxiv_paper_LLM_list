<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>LLM-agent - 2025-03-07</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>LLM-agent - 2025-03-07</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04691v1' target='_blank'>Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pengcheng Qiu, Chaoyi Wu, Shuyu Liu, Weike Zhao, Ya Zhang, Yanfeng Wang, Weidi Xie</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 18:35:39</h6>
<p class='card-text'>The latest reasoning-enhanced large language models (reasoning LLMs), such as
DeepSeek-R1 and OpenAI-o3, have demonstrated remarkable success. However, the
application of such reasoning enhancements to the highly professional medical
domain has not been clearly evaluated, particularly regarding with not only
assessing the final generation but also examining the quality of their
reasoning processes. In this study, we present MedR-Bench, a reasoning-focused
medical evaluation benchmark comprising 1,453 structured patient cases with
reasoning references mined from case reports. Our benchmark spans 13 body
systems and 10 specialty disorders, encompassing both common and rare diseases.
In our evaluation, we introduce a versatile framework consisting of three
critical clinical stages: assessment recommendation, diagnostic
decision-making, and treatment planning, comprehensively capturing the LLMs'
performance across the entire patient journey in healthcare. For metrics, we
propose a novel agentic system, Reasoning Evaluator, designed to automate and
objectively quantify free-text reasoning responses in a scalable manner from
the perspectives of efficiency, factuality, and completeness by dynamically
searching and performing cross-referencing checks. As a result, we assess five
state-of-the-art reasoning LLMs, including DeepSeek-R1, OpenAI-o3-mini, and
others. Our results reveal that current LLMs can handle relatively simple
diagnostic tasks with sufficient critical assessment results, achieving
accuracy generally over 85%. However, they still struggle with more complex
tasks, such as assessment recommendation and treatment planning. In reasoning,
their reasoning processes are generally reliable, with factuality scores
exceeding 90%, though they often omit critical reasoning steps. Our study
clearly reveals further development directions for current clinical LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04629v1' target='_blank'>SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and
  Multi-dimensional Evaluation for Automated Survey Writing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiangchao Yan, Shiyang Feng, Jiakang Yuan, Renqiu Xia, Bin Wang, Bo Zhang, Lei Bai</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 17:15:48</h6>
<p class='card-text'>Survey paper plays a crucial role in scientific research, especially given
the rapid growth of research publications. Recently, researchers have begun
using LLMs to automate survey generation for better efficiency. However, the
quality gap between LLM-generated surveys and those written by human remains
significant, particularly in terms of outline quality and citation accuracy. To
close these gaps, we introduce SurveyForge, which first generates the outline
by analyzing the logical structure of human-written outlines and referring to
the retrieved domain-related articles. Subsequently, leveraging high-quality
papers retrieved from memory by our scholar navigation agent, SurveyForge can
automatically generate and refine the content of the generated article.
Moreover, to achieve a comprehensive evaluation, we construct SurveyBench,
which includes 100 human-written survey papers for win-rate comparison and
assesses AI-generated survey papers across three dimensions: reference,
outline, and content quality. Experiments demonstrate that SurveyForge can
outperform previous works such as AutoSurvey.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04596v1' target='_blank'>The Next Frontier of LLM Applications: Open Ecosystems and Hardware
  Synergy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xinyi Hou, Yanjie Zhao, Haoyu Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 16:38:23</h6>
<p class='card-text'>Large Language Model (LLM) applications, including LLM app stores and
autonomous agents, are shaping the future of AI ecosystems. However, platform
silos, fragmented hardware integration, and the absence of standardized
interfaces limit scalability, interoperability, and resource efficiency. While
LLM app stores democratize AI, their closed ecosystems restrict modular AI
reuse and cross-platform portability. Meanwhile, agent-based frameworks offer
flexibility but often lack seamless integration across diverse environments.
This paper envisions the future of LLM applications and proposes a three-layer
decoupled architecture grounded in software engineering principles such as
layered system design, service-oriented architectures, and hardware-software
co-design. This architecture separates application logic, communication
protocols, and hardware execution, enhancing modularity, efficiency, and
cross-platform compatibility. Beyond architecture, we highlight key security
and privacy challenges for safe, scalable AI deployment and outline research
directions in software and security engineering. This vision aims to foster
open, secure, and interoperable LLM ecosystems, guiding future advancements in
AI applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04479v1' target='_blank'>ToolFuzz -- Automated Agent Tool Testing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ivan Milev, Mislav BalunoviÄ‡, Maximilian Baader, Martin Vechev</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 14:29:52</h6>
<p class='card-text'>Large Language Model (LLM) Agents leverage the advanced reasoning
capabilities of LLMs in real-world applications. To interface with an
environment, these agents often rely on tools, such as web search or database
APIs. As the agent provides the LLM with tool documentation along the user
query, the completeness and correctness of this documentation is critical.
However, tool documentation is often over-, under-, or ill-specified, impeding
the agent's accuracy. Standard software testing approaches struggle to identify
these errors as they are expressed in natural language. Thus, despite its
importance, there currently exists no automated method to test the tool
documentation for agents. To address this issue, we present ToolFuzz, the first
method for automated testing of tool documentations. ToolFuzz is designed to
discover two types of errors: (1) user queries leading to tool runtime errors
and (2) user queries that lead to incorrect agent responses. ToolFuzz can
generate a large and diverse set of natural inputs, effectively finding tool
description errors at a low false positive rate. Further, we present two
straightforward prompt-engineering approaches. We evaluate all three tool
testing approaches on 32 common LangChain tools and 35 newly created custom
tools and 2 novel benchmarks to further strengthen the assessment. We find that
many publicly available tools suffer from underspecification. Specifically, we
show that ToolFuzz identifies 20x more erroneous inputs compared to the
prompt-engineering approaches, making it a key component for building reliable
AI agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04392v1' target='_blank'>AgentSafe: Safeguarding Large Language Model-based Multi-agent Systems
  via Hierarchical Data Management</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Junyuan Mao, Fanci Meng, Yifan Duan, Miao Yu, Xiaojun Jia, Junfeng Fang, Yuxuan Liang, Kun Wang, Qingsong Wen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 12:41:54</h6>
<p class='card-text'>Large Language Model based multi-agent systems are revolutionizing autonomous
communication and collaboration, yet they remain vulnerable to security threats
like unauthorized access and data breaches. To address this, we introduce
AgentSafe, a novel framework that enhances MAS security through hierarchical
information management and memory protection. AgentSafe classifies information
by security levels, restricting sensitive data access to authorized agents.
AgentSafe incorporates two components: ThreatSieve, which secures communication
by verifying information authority and preventing impersonation, and
HierarCache, an adaptive memory management system that defends against
unauthorized access and malicious poisoning, representing the first systematic
defense for agent memory. Experiments across various LLMs show that AgentSafe
significantly boosts system resilience, achieving defense success rates above
80% under adversarial conditions. Additionally, AgentSafe demonstrates
scalability, maintaining robust performance as agent numbers and information
complexity grow. Results underscore effectiveness of AgentSafe in securing MAS
and its potential for real-world application.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04280v1' target='_blank'>Towards Autonomous Reinforcement Learning for Real-World Robotic
  Manipulation with Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:NiccolÃ² Turcato, Matteo Iovino, Aris Synodinos, Alberto Dalla Libera, Ruggero Carli, Pietro Falco</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 10:08:44</h6>
<p class='card-text'>Recent advancements in Large Language Models (LLMs) and Visual Language
Models (VLMs) have significantly impacted robotics, enabling high-level
semantic motion planning applications. Reinforcement Learning (RL), a
complementary paradigm, enables agents to autonomously optimize complex
behaviors through interaction and reward signals. However, designing effective
reward functions for RL remains challenging, especially in real-world tasks
where sparse rewards are insufficient and dense rewards require elaborate
design. In this work, we propose Autonomous Reinforcement learning for Complex
HumanInformed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4,
a pre-trained LLM, to generate reward functions directly from natural language
task descriptions. The rewards are used to train RL agents in simulated
environments, where we formalize the reward generation process to enhance
feasibility. Additionally, GPT-4 automates the coding of task success criteria,
creating a fully automated, one-shot procedure for translating human-readable
text into deployable robot skills. Our approach is validated through extensive
simulated experiments on single-arm and bi-manual manipulation tasks using an
ABB YuMi collaborative robot, highlighting its practicality and effectiveness.
Tasks are demonstrated on the real robot setup.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04188v1' target='_blank'>Measuring temporal effects of agent knowledge by date-controlled tool
  use</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:R. Patrick Xian, Qiming Cui, Stefan Bauer, Reza Abbasi-Asl</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 08:03:51</h6>
<p class='card-text'>Temporal progression is an integral part of knowledge accumulation and
update. Web search is frequently adopted as grounding for agent knowledge, yet
its inappropriate configuration affects the quality of agent responses. Here,
we construct a tool-based out-of-sample testing framework to measure the
knowledge variability of large language model (LLM) agents from distinct
date-controlled tools (DCTs). We demonstrate the temporal effects of an LLM
agent as a writing assistant, which can use web search to help complete
scientific publication abstracts. We show that temporal effects of the search
engine translates into tool-dependent agent performance but can be alleviated
with base model choice and explicit reasoning instructions such as
chain-of-thought prompting. Our results indicate that agent evaluation should
take a dynamical view and account for the temporal influence of tools and the
updates of external resources.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04153v1' target='_blank'>KidneyTalk-open: No-code Deployment of a Private Large Language Model
  with Medical Documentation-Enhanced Knowledge Database for Kidney Disease</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yongchao Long, Chao Yang, Gongzheng Tang, Jinwei Wang, Zhun Sui, Yuxi Zhou, Shenda Hong, Luxia Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 07:01:36</h6>
<p class='card-text'>Privacy-preserving medical decision support for kidney disease requires
localized deployment of large language models (LLMs) while maintaining clinical
reasoning capabilities. Current solutions face three challenges: 1) Cloud-based
LLMs pose data security risks; 2) Local model deployment demands technical
expertise; 3) General LLMs lack mechanisms to integrate medical knowledge.
Retrieval-augmented systems also struggle with medical document processing and
clinical usability. We developed KidneyTalk-open, a desktop system integrating
three technical components: 1) No-code deployment of state-of-the-art (SOTA)
open-source LLMs (such as DeepSeek-r1, Qwen2.5) via local inference engine; 2)
Medical document processing pipeline combining context-aware chunking and
intelligent filtering; 3) Adaptive Retrieval and Augmentation Pipeline (AddRep)
employing agents collaboration for improving the recall rate of medical
documents. A graphical interface was designed to enable clinicians to manage
medical documents and conduct AI-powered consultations without technical
expertise. Experimental validation on 1,455 challenging nephrology exam
questions demonstrates AddRep's effectiveness: achieving 29.1% accuracy (+8.1%
over baseline) with intelligent knowledge integration, while maintaining
robustness through 4.9% rejection rate to suppress hallucinations. Comparative
case studies with the mainstream products (AnythingLLM, Chatbox, GPT4ALL)
demonstrate KidneyTalk-open's superior performance in real clinical query.
KidneyTalk-open represents the first no-code medical LLM system enabling secure
documentation-enhanced medical Q&A on desktop. Its designs establishes a new
framework for privacy-sensitive clinical AI applications. The system
significantly lowers technical barriers while improving evidence traceability,
enabling more medical staff or patients to use SOTA open-source LLMs
conveniently.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04149v1' target='_blank'>Dynamic Benchmarking of Reasoning Capabilities in Code Large Language
  Models Under Data Contamination</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Simin Chen, Pranav Pusarla, Baishakhi Ray</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 06:56:59</h6>
<p class='card-text'>The rapid evolution of code largelanguage models underscores the need for
effective and transparent benchmarking of their reasoning capabilities.
However, the current benchmarking approach heavily depends on publicly
available, human-created datasets. The widespread use of these fixed benchmark
datasets makes the benchmarking process to be static and thus particularly
susceptible to data contamination, an unavoidable consequence of the extensive
data collection processes used to train Code LLMs. Existing approaches that
address data contamination often suffer from human effort limitations and
imbalanced problem complexity. To tackle these challenges, we propose \tool, a
novel benchmarking suite for evaluating Code LLMs under potential data
contamination. Given a seed programming problem, \tool employs multiple agents
to extract and modify the context without altering the core logic, generating
semantically equivalent variations. We introduce a dynamic data generation
methods and conduct empirical studies on two seed datasets across 21 Code LLMs.
Results show that \tool effectively benchmarks reasoning capabilities under
contamination risks while generating diverse problem sets to ensure consistent
and reliable evaluations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04110v1' target='_blank'>InterChat: Enhancing Generative Visual Analytics using Multimodal
  Interactions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Juntong Chen, Jiang Wu, Jiajing Guo, Vikram Mohanty, Xueming Li, Jorge Piazentin Ono, Wenbin He, Liu Ren, Dongyu Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 05:35:19</h6>
<p class='card-text'>The rise of Large Language Models (LLMs) and generative visual analytics
systems has transformed data-driven insights, yet significant challenges
persist in accurately interpreting users' analytical and interaction intents.
While language inputs offer flexibility, they often lack precision, making the
expression of complex intents inefficient, error-prone, and time-intensive. To
address these limitations, we investigate the design space of multimodal
interactions for generative visual analytics through a literature review and
pilot brainstorming sessions. Building on these insights, we introduce a highly
extensible workflow that integrates multiple LLM agents for intent inference
and visualization generation. We develop InterChat, a generative visual
analytics system that combines direct manipulation of visual elements with
natural language inputs. This integration enables precise intent communication
and supports progressive, visually driven exploratory data analyses. By
employing effective prompt engineering, and contextual interaction linking,
alongside intuitive visualization and interaction designs, InterChat bridges
the gap between user interactions and LLM-driven visualizations, enhancing both
interpretability and usability. Extensive evaluations, including two usage
scenarios, a user study, and expert feedback, demonstrate the effectiveness of
InterChat. Results show significant improvements in the accuracy and efficiency
of handling complex visual analytics tasks, highlighting the potential of
multimodal interactions to redefine user engagement and analytical depth in
generative visual analytics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04094v1' target='_blank'>PokÃ©Champ: an Expert-level Minimax Language Agent</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Seth Karten, Andy Luu Nguyen, Chi Jin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 05:06:27</h6>
<p class='card-text'>We introduce Pok\'eChamp, a minimax agent powered by Large Language Models
(LLMs) for Pok\'emon battles. Built on a general framework for two-player
competitive games, Pok\'eChamp leverages the generalist capabilities of LLMs to
enhance minimax tree search. Specifically, LLMs replace three key modules: (1)
player action sampling, (2) opponent modeling, and (3) value function
estimation, enabling the agent to effectively utilize gameplay history and
human knowledge to reduce the search space and address partial observability.
Notably, our framework requires no additional LLM training. We evaluate
Pok\'eChamp in the popular Gen 9 OU format. When powered by GPT-4o, it achieves
a win rate of 76% against the best existing LLM-based bot and 84% against the
strongest rule-based bot, demonstrating its superior performance. Even with an
open-source 8-billion-parameter Llama 3.1 model, Pok\'eChamp consistently
outperforms the previous best LLM-based bot, Pok\'ellmon powered by GPT-4o,
with a 64% win rate. Pok\'eChamp attains a projected Elo of 1300-1500 on the
Pok\'emon Showdown online ladder, placing it among the top 30%-10% of human
players. In addition, this work compiles the largest real-player Pok\'emon
battle dataset, featuring over 3 million games, including more than 500k
high-Elo matches. Based on this dataset, we establish a series of battle
benchmarks and puzzles to evaluate specific battling skills. We further provide
key updates to the local game engine. We hope this work fosters further
research that leverage Pok\'emon battle as benchmark to integrate LLM
technologies with game-theoretic algorithms addressing general multiagent
problems. Videos, code, and dataset available at
https://sites.google.com/view/pokechamp-llm.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03889v1' target='_blank'>Pretrained LLMs as Real-Time Controllers for Robot Operated Serial
  Production Line</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Muhammad Waseem, Kshitij Bhatta, Chen Li, Qing Chang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 20:43:49</h6>
<p class='card-text'>The manufacturing industry is undergoing a transformative shift, driven by
cutting-edge technologies like 5G, AI, and cloud computing. Despite these
advancements, effective system control, which is crucial for optimizing
production efficiency, remains a complex challenge due to the intricate,
knowledge-dependent nature of manufacturing processes and the reliance on
domain-specific expertise. Conventional control methods often demand heavy
customization, considerable computational resources, and lack transparency in
decision-making. In this work, we investigate the feasibility of using Large
Language Models (LLMs), particularly GPT-4, as a straightforward, adaptable
solution for controlling manufacturing systems, specifically, mobile robot
scheduling. We introduce an LLM-based control framework to assign mobile robots
to different machines in robot assisted serial production lines, evaluating its
performance in terms of system throughput. Our proposed framework outperforms
traditional scheduling approaches such as First-Come-First-Served (FCFS),
Shortest Processing Time (SPT), and Longest Processing Time (LPT). While it
achieves performance that is on par with state-of-the-art methods like
Multi-Agent Reinforcement Learning (MARL), it offers a distinct advantage by
delivering comparable throughput without the need for extensive retraining.
These results suggest that the proposed LLM-based solution is well-suited for
scenarios where technical expertise, computational resources, and financial
investment are limited, while decision transparency and system scalability are
critical concerns.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03750v1' target='_blank'>The MASK Benchmark: Disentangling Honesty From Accuracy in AI Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Richard Ren, Arunim Agarwal, Mantas Mazeika, Cristina Menghini, Robert Vacareanu, Brad Kenstler, Mick Yang, Isabelle Barrass, Alice Gatti, Xuwang Yin, Eduardo Trevino, Matias Geralnik, Adam Khoja, Dean Lee, Summer Yue, Dan Hendrycks</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 18:59:23</h6>
<p class='card-text'>As large language models (LLMs) become more capable and agentic, the
requirement for trust in their outputs grows significantly, yet at the same
time concerns have been mounting that models may learn to lie in pursuit of
their goals. To address these concerns, a body of work has emerged around the
notion of "honesty" in LLMs, along with interventions aimed at mitigating
deceptive behaviors. However, evaluations of honesty are currently highly
limited, with no benchmark combining large scale and applicability to all
models. Moreover, many benchmarks claiming to measure honesty in fact simply
measure accuracy--the correctness of a model's beliefs--in disguise. In this
work, we introduce a large-scale human-collected dataset for measuring honesty
directly, allowing us to disentangle accuracy from honesty for the first time.
Across a diverse set of LLMs, we find that while larger models obtain higher
accuracy on our benchmark, they do not become more honest. Surprisingly, while
most frontier LLMs obtain high scores on truthfulness benchmarks, we find a
substantial propensity in frontier LLMs to lie when pressured to do so,
resulting in low honesty scores on our benchmark. We find that simple methods,
such as representation engineering interventions, can improve honesty. These
results underscore the growing need for robust evaluations and effective
interventions to ensure LLMs remain trustworthy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03704v1' target='_blank'>A Practical Memory Injection Attack against LLM Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shen Dong, Shaocheng Xu, Pengfei He, Yige Li, Jiliang Tang, Tianming Liu, Hui Liu, Zhen Xiang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 17:53:24</h6>
<p class='card-text'>Agents based on large language models (LLMs) have demonstrated strong
capabilities in a wide range of complex, real-world applications. However, LLM
agents with a compromised memory bank may easily produce harmful outputs when
the past records retrieved for demonstration are malicious. In this paper, we
propose a novel Memory INJection Attack, MINJA, that enables the injection of
malicious records into the memory bank by only interacting with the agent via
queries and output observations. These malicious records are designed to elicit
a sequence of malicious reasoning steps leading to undesirable agent actions
when executing the victim user's query. Specifically, we introduce a sequence
of bridging steps to link the victim query to the malicious reasoning steps.
During the injection of the malicious record, we propose an indication prompt
to guide the agent to autonomously generate our designed bridging steps. We
also propose a progressive shortening strategy that gradually removes the
indication prompt, such that the malicious record will be easily retrieved when
processing the victim query comes after. Our extensive experiments across
diverse agents demonstrate the effectiveness of MINJA in compromising agent
memory. With minimal requirements for execution, MINJA enables any user to
influence agent memory, highlighting practical risks of LLM agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03686v1' target='_blank'>MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rui Ye, Shuo Tang, Rui Ge, Yaxin Du, Zhenfei Yin, Siheng Chen, Jing Shao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 17:27:59</h6>
<p class='card-text'>LLM-based multi-agent systems (MAS) have shown significant potential in
tackling diverse tasks. However, to design effective MAS, existing approaches
heavily rely on manual configurations or multiple calls of advanced LLMs,
resulting in inadaptability and high inference costs. In this paper, we
simplify the process of building an MAS by reframing it as a generative
language task, where the input is a user query and the output is a
corresponding MAS. To address this novel task, we unify the representation of
MAS as executable code and propose a consistency-oriented data construction
pipeline to create a high-quality dataset comprising coherent and consistent
query-MAS pairs. Using this dataset, we train MAS-GPT, an open-source
medium-sized LLM that is capable of generating query-adaptive MAS within a
single LLM inference. The generated MAS can be seamlessly applied to process
user queries and deliver high-quality responses. Extensive experiments on 9
benchmarks and 5 LLMs show that the proposed MAS-GPT consistently outperforms
10+ baseline MAS methods on diverse settings, indicating MAS-GPT's high
effectiveness, efficiency and strong generalization ability. Code will be
available at https://github.com/rui-ye/MAS-GPT.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03800v1' target='_blank'>Multi-Agent Systems Powered by Large Language Models: Applications in
  Swarm Intelligence</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Cristian Jimenez-Romero, Alper Yegenoglu, Christian Blum</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 17:13:27</h6>
<p class='card-text'>This work examines the integration of large language models (LLMs) into
multi-agent simulations by replacing the hard-coded programs of agents with
LLM-driven prompts. The proposed approach is showcased in the context of two
examples of complex systems from the field of swarm intelligence: ant colony
foraging and bird flocking. Central to this study is a toolchain that
integrates LLMs with the NetLogo simulation platform, leveraging its Python
extension to enable communication with GPT-4o via the OpenAI API. This
toolchain facilitates prompt-driven behavior generation, allowing agents to
respond adaptively to environmental data. For both example applications
mentioned above, we employ both structured, rule-based prompts and autonomous,
knowledge-driven prompts. Our work demonstrates how this toolchain enables LLMs
to study self-organizing processes and induce emergent behaviors within
multi-agent environments, paving the way for new approaches to exploring
intelligent systems and modeling swarm intelligence inspired by natural
phenomena. We provide the code, including simulation files and data at
https://github.com/crjimene/swarm_gpt.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03669v1' target='_blank'>Attentive Reasoning Queries: A Systematic Method for Optimizing
  Instruction-Following in Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bar Karov, Dor Zohar, Yam Marcovitz</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 17:03:48</h6>
<p class='card-text'>We present Attentive Reasoning Queries (ARQs), a novel structured reasoning
approach that significantly improves instruction-following in Large Language
Models through domain-specialized reasoning blueprints. While LLMs demonstrate
remarkable capabilities across diverse tasks, they often fail to maintain
adherence to complex, use-case-specific instructions during multi-turn
conversations, presenting challenges for business-critical applications. ARQs
address this limitation by guiding LLMs through systematic reasoning steps with
targeted queries that reinstate critical instructions and facilitate
intermediate reasoning throughout the completion process. In extensive testing
within Parlant, our framework for reliable customer-facing agents in which ARQs
were born out of necessity, they achieved a 90.2% success rate across 87 test
scenarios, outperforming both Chain-of-Thought reasoning (86.1%) and direct
response generation (81.5%). ARQs showed particular strength in addressing
persistent failure modes like guideline re-application and hallucination
prevention. Our analysis also revealed that ARQs can potentially be more
computationally efficient than free-form reasoning when carefully designed.
These findings demonstrate that structured reasoning approaches provide
effective mechanisms for controlling how LLMs process information and make
decisions in complex scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03586v1' target='_blank'>Benchmarking LLMs and LLM-based Agents in Practical Vulnerability
  Detection for Code Repositories</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alperen Yildiz, Sin G. Teo, Yiling Lou, Yebo Feng, Chong Wang, Dinil M. Divakaran</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 15:22:24</h6>
<p class='card-text'>Large Language Models (LLMs) have shown promise in software vulnerability
detection, particularly on function-level benchmarks like Devign and BigVul.
However, real-world detection requires interprocedural analysis, as
vulnerabilities often emerge through multi-hop function calls rather than
isolated functions. While repository-level benchmarks like ReposVul and VulEval
introduce interprocedural context, they remain computationally expensive, lack
pairwise evaluation of vulnerability fixes, and explore limited context
retrieval, limiting their practicality.
  We introduce JitVul, a JIT vulnerability detection benchmark linking each
function to its vulnerability-introducing and fixing commits. Built from 879
CVEs spanning 91 vulnerability types, JitVul enables comprehensive evaluation
of detection capabilities. Our results show that ReAct Agents, leveraging
thought-action-observation and interprocedural context, perform better than
LLMs in distinguishing vulnerable from benign code. While prompting strategies
like Chain-of-Thought help LLMs, ReAct Agents require further refinement. Both
methods show inconsistencies, either misidentifying vulnerabilities or
over-analyzing security guards, indicating significant room for improvement.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03796v1' target='_blank'>Human Implicit Preference-Based Policy Fine-tuning for Multi-Agent
  Reinforcement Learning in USV Swarm</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hyeonjun Kim, Kanghoon Lee, Junho Park, Jiachen Li, Jinkyoo Park</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 14:33:18</h6>
<p class='card-text'>Multi-Agent Reinforcement Learning (MARL) has shown promise in solving
complex problems involving cooperation and competition among agents, such as an
Unmanned Surface Vehicle (USV) swarm used in search and rescue, surveillance,
and vessel protection. However, aligning system behavior with user preferences
is challenging due to the difficulty of encoding expert intuition into reward
functions. To address the issue, we propose a Reinforcement Learning with Human
Feedback (RLHF) approach for MARL that resolves credit-assignment challenges
through an Agent-Level Feedback system categorizing feedback into intra-agent,
inter-agent, and intra-team types. To overcome the challenges of direct human
feedback, we employ a Large Language Model (LLM) evaluator to validate our
approach using feedback scenarios such as region constraints, collision
avoidance, and task allocation. Our method effectively refines USV swarm
policies, addressing key challenges in multi-agent systems while maintaining
fairness and performance consistency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03505v1' target='_blank'>Parallelized Planning-Acting for Efficient LLM-based Multi-Agent Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yaoru Li, Shunyu Liu, Tongya Zheng, Mingli Song</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 13:53:10</h6>
<p class='card-text'>Recent advancements in Large Language Model(LLM)-based Multi-Agent
Systems(MAS) have demonstrated remarkable potential for tackling complex
decision-making tasks. However, existing frameworks inevitably rely on
serialized execution paradigms, where agents must complete sequential LLM
planning before taking action. This fundamental constraint severely limits
real-time responsiveness and adaptation, which is crucial in dynamic
environments with ever-changing scenarios. In this paper, we propose a novel
parallelized planning-acting framework for LLM-based MAS, featuring a
dual-thread architecture with interruptible execution to enable concurrent
planning and acting. Specifically, our framework comprises two core threads:(1)
a planning thread driven by a centralized memory system, maintaining
synchronization of environmental states and agent communication to support
dynamic decision-making; and (2) an acting thread equipped with a comprehensive
skill library, enabling automated task execution through recursive
decomposition. Extensive experiments on challenging Minecraft demonstrate the
effectiveness of the proposed framework.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03503v1' target='_blank'>Collaborative Expert LLMs Guided Multi-Objective Molecular Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiajun Yu, Yizhen Zheng, Huan Yee Koh, Shirui Pan, Tianyue Wang, Haishuai Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 13:47:55</h6>
<p class='card-text'>Molecular optimization is a crucial yet complex and time-intensive process
that often acts as a bottleneck for drug development. Traditional methods rely
heavily on trial and error, making multi-objective optimization both
time-consuming and resource-intensive. Current AI-based methods have shown
limited success in handling multi-objective optimization tasks, hampering their
practical utilization. To address this challenge, we present MultiMol, a
collaborative large language model (LLM) system designed to guide
multi-objective molecular optimization. MultiMol comprises two agents,
including a data-driven worker agent and a literature-guided research agent.
The data-driven worker agent is a large language model being fine-tuned to
learn how to generate optimized molecules considering multiple objectives,
while the literature-guided research agent is responsible for searching
task-related literature to find useful prior knowledge that facilitates
identifying the most promising optimized candidates. In evaluations across six
multi-objective optimization tasks, MultiMol significantly outperforms existing
methods, achieving a 82.30% success rate, in sharp contrast to the 27.50%
success rate of current strongest methods. To further validate its practical
impact, we tested MultiMol on two real-world challenges. First, we enhanced the
selectivity of Xanthine Amine Congener (XAC), a promiscuous ligand that binds
both A1R and A2AR, successfully biasing it towards A1R. Second, we improved the
bioavailability of Saquinavir, an HIV-1 protease inhibitor with known
bioavailability limitations. Overall, these results indicate that MultiMol
represents a highly promising approach for multi-objective molecular
optimization, holding great potential to accelerate the drug development
process and contribute to the advancement of pharmaceutical research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03462v1' target='_blank'>Open-Source Large Language Models as Multilingual Crowdworkers:
  Synthesizing Open-Domain Dialogues in Several Languages With No Examples in
  Targets and No Machine Translation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ahmed Njifenjou, Virgile Sucal, Bassam Jabaian, Fabrice LefÃ¨vre</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 12:52:14</h6>
<p class='card-text'>The prevailing paradigm in the domain of Open-Domain Dialogue agents
predominantly focuses on the English language, encompassing both models and
datasets. Furthermore, the financial and temporal investments required for
crowdsourcing such datasets for finetuning are substantial, particularly when
multiple languages are involved. Fortunately, advancements in Large Language
Models (LLMs) have unveiled a plethora of possibilities across diverse tasks.
Specifically, instruction-tuning has enabled LLMs to execute tasks based on
natural language instructions, occasionally surpassing the performance of human
crowdworkers. Additionally, these models possess the capability to function in
various languages within a single thread. Consequently, to generate new samples
in different languages, we propose leveraging these capabilities to replicate
the data collection process. We introduce a pipeline for generating Open-Domain
Dialogue data in multiple Target Languages using LLMs, with demonstrations
provided in a unique Source Language. By eschewing explicit Machine Translation
in this approach, we enhance the adherence to language-specific nuances. We
apply this methodology to the PersonaChat dataset. To enhance the openness of
generated dialogues and mimic real life scenarii, we added the notion of speech
events corresponding to the type of conversation the speakers are involved in
and also that of common ground which represents the premises of a conversation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03459v2' target='_blank'>Unified Mind Model: Reimagining Autonomous Agents in the LLM Era</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pengbo Hu, Xiang Ying</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 12:49:44</h6>
<p class='card-text'>Large language models (LLMs) have recently demonstrated remarkable
capabilities across domains, tasks, and languages (e.g., ChatGPT and GPT-4),
reviving the research of general autonomous agents with human-like cognitive
abilities. Such human-level agents require semantic comprehension and
instruction-following capabilities, which exactly fall into the strengths of
LLMs. Although there have been several initial attempts to build human-level
agents based on LLMs, the theoretical foundation remains a challenging open
problem. In this paper, we propose a novel theoretical cognitive architecture,
the Unified Mind Model (UMM), which offers guidance to facilitate the rapid
creation of autonomous agents with human-level cognitive abilities.
Specifically, our UMM starts with the global workspace theory and further
leverage LLMs to enable the agent with various cognitive abilities, such as
multi-modal perception, planning, reasoning, tool use, learning, memory,
reflection and motivation. Building upon UMM, we then develop an agent-building
engine, MindOS, which allows users to quickly create domain-/task-specific
autonomous agents without any programming effort.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03444v1' target='_blank'>Taxation Perspectives from Large Language Models: A Case Study on
  Additional Tax Penalties</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Eunkyung Choi, Young Jin Suh, Hun Park, Wonseok Hwang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 12:24:20</h6>
<p class='card-text'>How capable are large language models (LLMs) in the domain of taxation?
Although numerous studies have explored the legal domain in general, research
dedicated to taxation remain scarce. Moreover, the datasets used in these
studies are either simplified, failing to reflect the real-world complexities,
or unavailable as open source. To address this gap, we introduce PLAT, a new
benchmark designed to assess the ability of LLMs to predict the legitimacy of
additional tax penalties. PLAT is constructed to evaluate LLMs' understanding
of tax law, particularly in cases where resolving the issue requires more than
just applying related statutes. Our experiments with six LLMs reveal that their
baseline capabilities are limited, especially when dealing with conflicting
issues that demand a comprehensive understanding. However, we found that
enabling retrieval, self-reasoning, and discussion among multiple agents with
specific role assignments, this limitation can be mitigated.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03303v1' target='_blank'>SEOE: A Scalable and Reliable Semantic Evaluation Framework for Open
  Domain Event Detection</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yi-Fan Lu, Xian-Ling Mao, Tian Lan, Tong Zhang, Yu-Shi Zhu, Heyan Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 09:37:05</h6>
<p class='card-text'>Automatic evaluation for Open Domain Event Detection (ODED) is a highly
challenging task, because ODED is characterized by a vast diversity of
un-constrained output labels from various domains. Nearly all existing
evaluation methods for ODED usually first construct evaluation benchmarks with
limited labels and domain coverage, and then evaluate ODED methods using
metrics based on token-level label matching rules. However, this kind of
evaluation framework faces two issues: (1) The limited evaluation benchmarks
lack representatives of the real world, making it difficult to accurately
reflect the performance of various ODED methods in real-world scenarios; (2)
Evaluation metrics based on token-level matching rules fail to capture semantic
similarity between predictions and golden labels. To address these two problems
above, we propose a scalable and reliable Semantic-level Evaluation framework
for Open domain Event detection (SEOE) by constructing a more representative
evaluation benchmark and introducing a semantic evaluation metric.
Specifically, our proposed framework first constructs a scalable evaluation
benchmark that currently includes 564 event types covering 7 major domains,
with a cost-effective supplementary annotation strategy to ensure the
benchmark's representativeness. The strategy also allows for the supplement of
new event types and domains in the future. Then, the proposed SEOE leverages
large language models (LLMs) as automatic evaluation agents to compute a
semantic F1-score, incorporating fine-grained definitions of semantically
similar labels to enhance the reliability of the evaluation. Extensive
experiments validate the representatives of the benchmark and the reliability
of the semantic evaluation metric. Existing ODED methods are thoroughly
evaluated, and the error patterns of predictions are analyzed, revealing
several insightful findings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03258v1' target='_blank'>Exploring the Potential of Large Language Models as Predictors in
  Dynamic Text-Attributed Graphs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Runlin Lei, Jiarui Ji, Haipeng Ding, Lu Yi, Zhewei Wei, Yongchao Liu, Chuntao Hong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 08:28:11</h6>
<p class='card-text'>With the rise of large language models (LLMs), there has been growing
interest in Graph Foundation Models (GFMs) for graph-based tasks. By leveraging
LLMs as predictors, GFMs have demonstrated impressive generalizability across
various tasks and datasets. However, existing research on LLMs as predictors
has predominantly focused on static graphs, leaving their potential in dynamic
graph prediction unexplored. In this work, we pioneer using LLMs for predictive
tasks on dynamic graphs. We identify two key challenges: the constraints
imposed by context length when processing large-scale historical data and the
significant variability in domain characteristics, both of which complicate the
development of a unified predictor. To address these challenges, we propose the
GraphAgent-Dynamic (GAD) Framework, a multi-agent system that leverages
collaborative LLMs. In contrast to using a single LLM as the predictor, GAD
incorporates global and local summary agents to generate domain-specific
knowledge, enhancing its transferability across domains. Additionally,
knowledge reflection agents enable adaptive updates to GAD's knowledge,
maintaining a unified and self-consistent architecture. In experiments, GAD
demonstrates performance comparable to or even exceeds that of full-supervised
graph neural networks without dataset-specific training. Finally, to enhance
the task-specific performance of LLM-based predictors, we discuss potential
improvements, such as dataset-specific fine-tuning to LLMs. By developing
tailored strategies for different tasks, we provide new insights for the future
design of LLM-based predictors.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03205v1' target='_blank'>MA-LoT: Multi-Agent Lean-based Long Chain-of-Thought Reasoning enhances
  Formal Theorem Proving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ruida Wang, Rui Pan, Yuxin Li, Jipeng Zhang, Yizhen Jia, Shizhe Diao, Renjie Pi, Junjie Hu, Tong Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 05:50:31</h6>
<p class='card-text'>Solving mathematical problems using computer-verifiable languages like Lean
has significantly impacted mathematical and computer science communities.
State-of-the-art methods utilize single Large Language Models (LLMs) as agents
or provers to either generate complete proof or perform tree searches. However,
single-agent methods inherently lack a structured way to combine high-level
reasoning in Natural Language (NL) with Formal Language (FL) verification
feedback. To solve these issues, we propose MA-LoT: Multi-Agent Lean-based Long
Chain-of-Thought framework, (to the best of our knowledge), the first
multi-agent framework for Lean4 theorem proving that balance high-level NL
reasoning and FL verification in Long CoT. Using this structured interaction,
our approach enables deeper insights and long-term coherence in proof
generation, with which past methods struggle. We do this by leveraging emergent
formal reasoning ability in Long CoT using our novel LoT-Transfer Learning
training-inference pipeline. Extensive experiments show that our framework
achieves 54.51% accuracy rate on the Lean4 version of MiniF2F-Test dataset,
largely outperforming GPT-4 (22.95%), single-agent tree search
(InternLM-Step-Prover, 50.70%), and whole-proof generation
(DeepSeek-Prover-v1.5, 48.36%) baselines. Furthermore, our findings highlight
the potential of combining Long CoT with formal verification for a more
insightful generation in a broader perspective.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03154v2' target='_blank'>Dango: A Mixed-Initiative Data Wrangling System using Large Language
  Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wei-Hao Chen, Weixi Tong, Amanda Case, Tianyi Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 03:54:51</h6>
<p class='card-text'>Data wrangling is a time-consuming and challenging task in a data science
pipeline. While many tools have been proposed to automate or facilitate data
wrangling, they often misinterpret user intent, especially in complex tasks. We
propose Dango, a mixed-initiative multi-agent system for data wrangling.
Compared to existing tools, Dango enhances user communication of intent by
allowing users to demonstrate on multiple tables and use natural language
prompts in a conversation interface, enabling users to clarify their intent by
answering LLM-posed multiple-choice clarification questions, and providing
multiple forms of feedback such as step-by-step natural language explanations
and data provenance to help users evaluate the data wrangling scripts. We
conducted a within-subjects user study with 38 participants and demonstrated
that Dango's features can significantly improve intent clarification, accuracy,
and efficiency in data wrangling. Furthermore, we demonstrated the
generalizability of Dango by applying it to a broader set of data wrangling
tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02976v1' target='_blank'>Teaching AI to Handle Exceptions: Supervised Fine-Tuning with
  Human-Aligned Judgment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Matthew DosSantos DiSorbo, Harang Ju, Sinan Aral</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 20:00:37</h6>
<p class='card-text'>Large language models (LLMs), initially developed for generative AI, are now
evolving into agentic AI systems, which make decisions in complex, real-world
contexts. Unfortunately, while their generative capabilities are
well-documented, their decision-making processes remain poorly understood. This
is particularly evident when models are handling exceptions, a critical and
challenging aspect of decision-making made relevant by the inherent
incompleteness of contracts. Here we demonstrate that LLMs, even ones that
excel at reasoning, deviate significantly from human judgments because they
adhere strictly to policies, even when such adherence is impractical,
suboptimal, or even counterproductive. We then evaluate three approaches to
tuning AI agents to handle exceptions: ethical framework prompting,
chain-of-thought reasoning, and supervised fine-tuning. We find that while
ethical framework prompting fails and chain-of-thought prompting provides only
slight improvements, supervised fine-tuning, specifically with human
explanations, yields markedly better results. Surprisingly, in our experiments,
supervised fine-tuning even enabled models to generalize human-like
decision-making to novel scenarios, demonstrating transfer learning of
human-aligned decision-making across contexts. Furthermore, fine-tuning with
explanations, not just labels, was critical for alignment, suggesting that
aligning LLMs with human judgment requires explicit training on how decisions
are made, not just which decisions are made. These findings highlight the need
to address LLMs' shortcomings in handling exceptions in order to guide the
development of agentic AI toward models that can effectively align with human
judgment and simultaneously adapt to novel contexts.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02760v1' target='_blank'>From Metaphor to Mechanism: How LLMs Decode Traditional Chinese Medicine
  Symbolic Language for Modern Clinical Relevance</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiacheng Tang, Nankai Wu, Fan Gao, Chengxiao Dai, Mengyao Zhao, Xinjie Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 16:22:49</h6>
<p class='card-text'>Metaphorical expressions are abundant in Traditional Chinese Medicine (TCM),
conveying complex disease mechanisms and holistic health concepts through
culturally rich and often abstract terminology. Bridging these metaphors to
anatomically driven Western medical (WM) concepts poses significant challenges
for both automated language processing and real-world clinical practice. To
address this gap, we propose a novel multi-agent and chain-of-thought (CoT)
framework designed to interpret TCM metaphors accurately and map them to WM
pathophysiology. Specifically, our approach combines domain-specialized agents
(TCM Expert, WM Expert) with a Coordinator Agent, leveraging stepwise
chain-of-thought prompts to ensure transparent reasoning and conflict
resolution. We detail a methodology for building a metaphor-rich TCM dataset,
discuss strategies for effectively integrating multi-agent collaboration and
CoT reasoning, and articulate the theoretical underpinnings that guide metaphor
interpretation across distinct medical paradigms. We present a comprehensive
system design and highlight both the potential benefits and limitations of our
approach, while leaving placeholders for future experimental validation. Our
work aims to support clinical decision-making, cross-system educational
initiatives, and integrated healthcare research, ultimately offering a robust
scaffold for reconciling TCM's symbolic language with the mechanistic focus of
Western medicine.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>